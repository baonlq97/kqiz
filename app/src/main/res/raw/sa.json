{
    "data": [
        {
            "id": 2127,
            "attributes": {
                "createdAt": "2023-09-09T20:33:29.899Z",
                "updatedAt": "2023-09-09T20:33:29.899Z",
                "content": "<p>A company uses DynamoDB as a data store for various kinds of customer data, such as user profiles, user events, clicks, and visited links. Some of these use-cases require a high request rate (millions of requests per second), low predictable latency, and reliability. The company now wants to add a caching layer to support high read volumes.</p>\n\n<p>As a solutions architect, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. Therefore, this is a correct option.</p>\n\n<p>DAX Overview:\n<img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html</a></p>\n\n<p><strong>ElastiCache</strong> - Amazon ElastiCache for Memcached is an ideal front-end for data stores like Amazon RDS or Amazon DynamoDB, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements. Therefore, this is also a correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>RDS</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. RDS cannot be used as a caching layer for DynamoDB.</p>\n\n<p><strong>Elasticsearch</strong> - Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It cannot be used as a caching layer for DynamoDB.</p>\n\n<p><strong>Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. It cannot be used as a caching layer for DynamoDB.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/faqs/\">https://aws.amazon.com/elasticache/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 8880,
                        "content": "<p>Elasticsearch</p>",
                        "isValid": false
                    },
                    {
                        "id": 8881,
                        "content": "<p>Redshift</p>",
                        "isValid": false
                    },
                    {
                        "id": 8882,
                        "content": "<p>DynamoDB Accelerator (DAX)</p>",
                        "isValid": true
                    },
                    {
                        "id": 8883,
                        "content": "<p>RDS</p>",
                        "isValid": false
                    },
                    {
                        "id": 8884,
                        "content": "<p>ElastiCache</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2128,
            "attributes": {
                "createdAt": "2023-09-09T20:33:30.008Z",
                "updatedAt": "2023-09-09T20:33:30.008Z",
                "content": "<p>A video analytics organization has been acquired by a leading media company. The analytics organization has 10 independent applications with an on-premises data footprint of about 70TB for each application. The CTO of the media company has set a timeline of two weeks to carry out the data migration from on-premises data center to AWS Cloud and establish connectivity.</p>\n\n<p>Which of the following are the MOST cost-effective options for completing the data transfer and establishing connectivity? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Order 10 Snowball Edge Storage Optimized devices to complete the one-time data transfer</strong></p>\n\n<p>Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases.</p>\n\n<p>As each Snowball Edge Storage Optimized device can handle 80TB of data, you can order 10 such devices to take care of the data transfer for all applications.</p>\n\n<p>Exam Alert:</p>\n\n<p>The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space.</p>\n\n<p><strong>Setup Site-to-Site VPN to establish on-going connectivity between the on-premises data center and AWS Cloud</strong></p>\n\n<p>AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.</p>\n\n<p>Therefore this option is the right fit for the given use-case as the connectivity can be easily established within the given timeframe.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Order 1 Snowmobile to complete the one-time data transfer</strong> -  Each Snowmobile has a total capacity of up to 100 petabytes. To migrate large datasets of 10PB or more in a single location, you should use Snowmobile. For datasets less than 10PB or distributed in multiple locations, you should use Snowball. So Snowmobile is not the right fit for this use-case.</p>\n\n<p><strong>Setup AWS direct connect to establish connectivity between the on-premises data center and AWS Cloud</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. Direct Connect involves significant monetary investment and takes at least a month to set up, therefore it's not the correct fit for this use-case.</p>\n\n<p><strong>Order 70 Snowball Edge Storage Optimized devices to complete the one-time data transfer</strong> - As the data-transfer can be completed with just 10 Snowball Edge Storage Optimized devices, there is no need to order 70 devices.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/snowball/faqs/\">https://aws.amazon.com/snowball/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/vpn/\">https://aws.amazon.com/vpn/</a></p>\n\n<p><a href=\"https://aws.amazon.com/snowmobile/faqs/\">https://aws.amazon.com/snowmobile/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/directconnect/\">https://aws.amazon.com/directconnect/</a></p>\n",
                "options": [
                    {
                        "id": 8885,
                        "content": "<p>Order 70 Snowball Edge Storage Optimized devices to complete the one-time data transfer</p>",
                        "isValid": false
                    },
                    {
                        "id": 8886,
                        "content": "<p>Order 10 Snowball Edge Storage Optimized devices to complete the one-time data transfer</p>",
                        "isValid": true
                    },
                    {
                        "id": 8887,
                        "content": "<p>Setup Site-to-Site VPN to establish on-going connectivity between the on-premises data center and AWS Cloud</p>",
                        "isValid": true
                    },
                    {
                        "id": 8888,
                        "content": "<p>Setup AWS direct connect to establish connectivity between the on-premises data center and AWS Cloud</p>",
                        "isValid": false
                    },
                    {
                        "id": 8889,
                        "content": "<p>Order 1 Snowmobile to complete the one-time data transfer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2129,
            "attributes": {
                "createdAt": "2023-09-09T20:33:30.124Z",
                "updatedAt": "2023-09-09T20:33:30.124Z",
                "content": "<p>An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account.</p>\n\n<p>As a solutions architect, which of the following steps would you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment</strong></p>\n\n<p>IAM roles allow you to delegate access to users or services that normally don't have access to your organization's AWS resources. IAM users or AWS services can assume a role to obtain temporary security credentials that can be used to make AWS API calls. Consequently, you don't have to share long-term credentials for access to a resource. Using IAM roles, it is possible to access cross-account resources.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment</strong> - There is no need to create new IAM user credentials for the production environment, as you can use IAM roles to access cross-account resources.</p>\n\n<p><strong>It is not possible to access cross-account resources</strong> - You can use IAM roles to access cross-account resources.</p>\n\n<p><strong>Both IAM roles and IAM users can be used interchangeably for cross-account access</strong> - IAM roles and IAM users are separate IAM entities and should not be mixed. Only IAM roles can be used to access cross-account resources.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/iam/features/manage-roles/\">https://aws.amazon.com/iam/features/manage-roles/</a></p>\n",
                "options": [
                    {
                        "id": 8890,
                        "content": "<p>Both IAM roles and IAM users can be used interchangeably for cross-account access</p>",
                        "isValid": false
                    },
                    {
                        "id": 8891,
                        "content": "<p>Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment</p>",
                        "isValid": false
                    },
                    {
                        "id": 8892,
                        "content": "<p>Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment</p>",
                        "isValid": true
                    },
                    {
                        "id": 8893,
                        "content": "<p>It is not possible to access cross-account resources</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2130,
            "attributes": {
                "createdAt": "2023-09-09T20:33:30.219Z",
                "updatedAt": "2023-09-09T20:33:30.219Z",
                "content": "<p>A retail company has set up a Network Load Balancer (NLB) having a target group that is configured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances (across 3 Availability Zones) that run the web service. The company is getting poor feedback from its customers regarding the application's availability as the NLB is unable to detect HTTP errors for the application. These HTTP errors require a manual restart of the EC2 instances that run the web service.</p>\n\n<p>The company has hired you as an AWS Certified Solutions Architect Associate to build the best-fit solution that does not require custom development/scripting effort. Which of the following will you suggest?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Replace the Network Load Balancer (NLB) with an Application Load Balancer (ALB) and configure HTTP health checks on the ALB by pointing to the URL of the application. Leverage the Auto Scaling group to replace unhealthy instances</strong></p>\n\n<p>A Network Load Balancer (NLB) functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration.</p>\n\n<p>A load balancer serves as the single point of contact for clients. The load balancer distributes incoming traffic across multiple targets, such as Amazon EC2 instances. This increases the availability of your application. You add one or more listeners to your load balancer.</p>\n\n<p>A listener checks for connection requests from clients, using the protocol and port that you configure, and forwards requests to a target group. Each target group routes requests to one or more registered targets, such as EC2 instances, using the TCP protocol and the port number that you specify.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q61-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html</a></p>\n\n<p>For the given use case, you need to swap out the NLB with an ALB. This would allow you to use HTTP-based health checks to detect when the web application faces errors. You can then leverage the Auto Scaling group to use the ALB's health checks to identify and replace unhealthy instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q61-i4.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a CloudWatch alarm to monitor the UnhealthyHostCount metric for the NLB. Leverage the Auto Scaling group to replace unhealthy instances when the alarm is in the ALARM state</strong> - The Elastic Load Balancing (ELB) service provides you with Amazon CloudWatch metrics (HealthyHostCount and UnhealthyHostCount) to monitor the targets behind your load balancers. Although the unhealthy host count metric gives the aggregate number of failed hosts, there is a common pain point when you create an alarm for unhealthy hosts based on these metrics. This is because there is no easy way for you to tell which target was or is unhealthy. Building a solution using the Cloudwatch alarm requires significant development/scripting effort to identify the unhealthy target, so this option is incorrect.</p>\n\n<p><strong>Configure HTTP health checks on the Network Load Balancer (NLB) by pointing to the URL of the application. Leverage the Auto Scaling group to replace unhealthy instances</strong> - The NLB uses HTTP, HTTPS, and TCP as possible protocols when performing health checks on targets. The default is the TCP protocol. If the target type is ALB, the supported health check protocols are HTTP and HTTPS. Although it is now possible to configure an ALB as a target of an NLB, it would end up being a costlier and inefficient solution than just swapping out the NLB with the ALB, so this solution is not the best fit.</p>\n\n<p><strong>Set up a cron job on the EC2 instances to inspect the web application's logs at a regular frequency. When HTTP errors are detected, force an application restart</strong> - This option requires significant development/scripting effort to identify the unhealthy target. It's not as elegant a solution as directly leveraging the HTTP health check capabilities of the ALB. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-cloudwatch-metrics.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-cloudwatch-metrics.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/identifying-unhealthy-targets-of-elastic-load-balancer/\">https://aws.amazon.com/blogs/networking-and-content-delivery/identifying-unhealthy-targets-of-elastic-load-balancer/</a></p>\n",
                "options": [
                    {
                        "id": 8894,
                        "content": "<p>Configure HTTP health checks on the Network Load Balancer (NLB) by pointing to the URL of the application. Leverage the Auto Scaling group to replace unhealthy instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 8895,
                        "content": "<p>Replace the Network Load Balancer (NLB) with an Application Load Balancer (ALB) and configure HTTP health checks on the ALB by pointing to the URL of the application. Leverage the Auto Scaling group to replace unhealthy instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 8896,
                        "content": "<p>Set up a CloudWatch alarm to monitor the UnhealthyHostCount metric for the NLB. Leverage the Auto Scaling group to replace unhealthy instances when the alarm is in the ALARM state</p>",
                        "isValid": false
                    },
                    {
                        "id": 8897,
                        "content": "<p>Set up a cron job on the EC2 instances to inspect the web application's logs at a regular frequency. When HTTP errors are detected, force an application restart</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2131,
            "attributes": {
                "createdAt": "2023-09-09T20:33:30.330Z",
                "updatedAt": "2023-09-09T20:33:30.330Z",
                "content": "<p>A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects.</p>\n\n<p>As a solutions architect, what are your recommendations to address these guidelines? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Enable versioning on the bucket</strong> - Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket.\nVersioning-enabled buckets enable you to recover objects from accidental deletion or overwrite.</p>\n\n<p>For example:</p>\n\n<p>If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version.\nIf you delete an object, instead of removing it permanently, Amazon S3 inserts a delete marker, which becomes the current object version. You can always restore the previous version. Hence, this is the correct option.</p>\n\n<p>Versioning Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q20-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n\n<p><strong>Enable MFA delete on the bucket</strong> - To provide additional protection, multi-factor authentication (MFA) delete can be enabled. MFA delete requires secondary authentication to take place before objects can be permanently deleted from an Amazon S3 bucket. Hence, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an event trigger on deleting any S3 object. The event invokes an SNS notification via email to the IT manager</strong> - Sending an event trigger after object deletion does not meet the objective of preventing object deletion by mistake because the object has already been deleted. So, this option is incorrect.</p>\n\n<p><strong>Establish a process to get managerial approval for deleting S3 objects</strong> - This option for getting managerial approval is just a distractor.</p>\n\n<p><strong>Change the configuration on AWS S3 console so that the user needs to provide additional confirmation while deleting any S3 object</strong> - There is no provision to set up S3 configuration to ask for additional confirmation before deleting an object. This option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html</a></p>\n",
                "options": [
                    {
                        "id": 8898,
                        "content": "<p>Enable MFA delete on the bucket</p>",
                        "isValid": true
                    },
                    {
                        "id": 8899,
                        "content": "<p>Enable versioning on the bucket</p>",
                        "isValid": true
                    },
                    {
                        "id": 8900,
                        "content": "<p>Change the configuration on AWS S3 console so that the user needs to provide additional confirmation while deleting any S3 object</p>",
                        "isValid": false
                    },
                    {
                        "id": 8901,
                        "content": "<p>Create an event trigger on deleting any S3 object. The event invokes an SNS notification via email to the IT manager</p>",
                        "isValid": false
                    },
                    {
                        "id": 8902,
                        "content": "<p>Establish a process to get managerial approval for deleting S3 objects</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2132,
            "attributes": {
                "createdAt": "2023-09-09T20:33:30.420Z",
                "updatedAt": "2023-09-09T20:33:30.420Z",
                "content": "<p>A company runs a data processing workflow that takes about 60 minutes to complete. The workflow can withstand disruptions and it can be started and stopped multiple times.</p>\n\n<p>Which is the most cost-effective solution to build a solution for the workflow?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use EC2 spot instances to run the workflow processes</strong></p>\n\n<p>EC2 instance types:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q21-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>Amazon EC2 Spot instances allow you to request spare Amazon EC2 computing capacity for up to 90% off the On-Demand price.</p>\n\n<p>Spot instances are recommended for:</p>\n\n<p>Applications that have flexible start and end times\nApplications that are feasible only at very low compute prices\nUsers with urgent computing needs for large amounts of additional capacity</p>\n\n<p>For the given use case, spot instances offer the most cost-effective solution as the workflow can withstand disruptions and can be started and stopped multiple times.</p>\n\n<p>For example, considering a process that runs for an hour and needs about 1024 MB of memory, spot instance pricing for a t2.micro instance (having 1024 MB of RAM) is $0.0035 per hour.</p>\n\n<p>Spot instance pricing:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q21-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/spot/pricing/\">https://aws.amazon.com/ec2/spot/pricing/</a></p>\n\n<p>Contrast this with the pricing of a Lambda function (having 1024 MB of allocated memory), which comes out to $0.0000000167 per 1ms or $0.06 per hour ($0.0000000167 * 1000 * 60  * 60 per hour).</p>\n\n<p>Lambda function pricing:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q21-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/lambda/pricing/\">https://aws.amazon.com/lambda/pricing/</a></p>\n\n<p>Thus, a spot instance turns out to be about 20 times cost effective than a Lambda function to meet the requirements of the given use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Lambda function to run the workflow processes</strong> - As mentioned in the explanation above, a Lambda function turns out to be 20 times more expensive than a spot instance to meet the workflow requirements of the given use case, so this option is incorrect. You should also note that the maximum execution time of a Lambda function is 15 minutes, so the workflow process would be disrupted for sure. On the other hand, it is certainly possible that the workflow process can be completed in a single run on the spot instance (the average frequency of stop instance interruption across all Regions and instance types is &lt;10%).</p>\n\n<p><strong>Use EC2 on-demand instances to run the workflow processes</strong></p>\n\n<p><strong>Use EC2 reserved instances to run the workflow processes</strong></p>\n\n<p>You should note that both on-demand and reserved instances are more expensive than spot instances. In addition, reserved instances have a term of 1 year or 3 years, so they are not suited for the given workflow. Therefore, both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p><a href=\"https://aws.amazon.com/ec2/spot/pricing/\">https://aws.amazon.com/ec2/spot/pricing/</a></p>\n\n<p><a href=\"https://aws.amazon.com/lambda/pricing/\">https://aws.amazon.com/lambda/pricing/</a></p>\n\n<p><a href=\"https://aws.amazon.com/ec2/spot/instance-advisor/\">https://aws.amazon.com/ec2/spot/instance-advisor/</a></p>\n",
                "options": [
                    {
                        "id": 8903,
                        "content": "<p>Use EC2 on-demand instances to run the workflow processes</p>",
                        "isValid": false
                    },
                    {
                        "id": 8904,
                        "content": "<p>Use AWS Lambda function to run the workflow processes</p>",
                        "isValid": false
                    },
                    {
                        "id": 8905,
                        "content": "<p>Use EC2 reserved instances to run the workflow processes</p>",
                        "isValid": false
                    },
                    {
                        "id": 8906,
                        "content": "<p>Use EC2 spot instances to run the workflow processes</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2133,
            "attributes": {
                "createdAt": "2023-09-09T20:33:30.556Z",
                "updatedAt": "2023-09-09T20:33:30.556Z",
                "content": "<p>An IT security consultancy is working on a solution to protect data stored in S3 from any malicious activity as well as check for any vulnerabilities on EC2 instances.</p>\n\n<p>As a solutions architect, which of the following solutions would you suggest to help address the given requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon GuardDuty to monitor any malicious activity on data stored in S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on EC2 instances</strong></p>\n\n<p>Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately.</p>\n\n<p>How GuardDuty works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png\">\nvia - <a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n\n<p>Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. Amazon Inspector assessments are offered to you as pre-defined rules packages mapped to common security best practices and vulnerability definitions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon GuardDuty to monitor any malicious activity on data stored in S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on EC2 instances</strong></p>\n\n<p><strong>Use Amazon Inspector to monitor any malicious activity on data stored in S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on EC2 instances</strong></p>\n\n<p><strong>Use Amazon Inspector to monitor any malicious activity on data stored in S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on EC2 instances</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n\n<p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p>\n",
                "options": [
                    {
                        "id": 8907,
                        "content": "<p>Use Amazon Inspector to monitor any malicious activity on data stored in S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 8908,
                        "content": "<p>Use Amazon GuardDuty to monitor any malicious activity on data stored in S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 8909,
                        "content": "<p>Use Amazon GuardDuty to monitor any malicious activity on data stored in S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on EC2 instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 8910,
                        "content": "<p>Use Amazon Inspector to monitor any malicious activity on data stored in S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on EC2 instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2134,
            "attributes": {
                "createdAt": "2023-09-09T20:33:30.673Z",
                "updatedAt": "2023-09-09T20:33:30.673Z",
                "content": "<p>A media agency stores its re-creatable assets on Amazon S3 buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible.</p>\n\n<p>As a Solutions Architect, can you suggest a way to lower the storage costs while fulfilling the business requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days</strong> - S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed and re-creatable data but do not require the availability and resilience of S3 Standard or S3 Standard-IA. The minimum storage duration is 30 days before you can transition objects from S3 Standard to S3 One Zone-IA.</p>\n\n<p>S3 One Zone-IA offers the same high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. S3 Storage Classes can be configured at the object level, and a single bucket can contain objects stored across S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes.</p>\n\n<p>Constraints for Lifecycle storage class transitions:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p>\n\n<p>Supported S3 lifecycle transitions:\n<img src=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/images/lifecycle-transitions-v2.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days</strong></p>\n\n<p><strong>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days</strong></p>\n\n<p>As mentioned earlier, the minimum storage duration is 30 days before you can transition objects from S3 Standard to S3 One Zone-IA or S3 Standard-IA, so both these options are added as distractors.</p>\n\n<p><strong>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days</strong> - S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. But, it costs more than S3 One Zone-IA because of the redundant storage across availability zones. As the data is re-creatable, so you don't need to incur this additional cost.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p>\n",
                "options": [
                    {
                        "id": 8911,
                        "content": "<p>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days</p>",
                        "isValid": false
                    },
                    {
                        "id": 8912,
                        "content": "<p>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days</p>",
                        "isValid": false
                    },
                    {
                        "id": 8913,
                        "content": "<p>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days</p>",
                        "isValid": false
                    },
                    {
                        "id": 8914,
                        "content": "<p>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2135,
            "attributes": {
                "createdAt": "2023-09-09T20:33:30.765Z",
                "updatedAt": "2023-09-09T20:33:30.765Z",
                "content": "<p>The product team at a startup has figured out a market need to support both stateful and stateless client-server communications via the APIs developed using its platform. You have been hired by the startup as a solutions architect to build a solution to fulfill this market need using AWS API Gateway.</p>\n\n<p>Which of the following would you identify as correct?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>API Gateway creates RESTful APIs that enable stateless client-server communication and API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server</strong></p>\n\n<p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the front door for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>API Gateway creates RESTful APIs that:</p>\n\n<p>Are HTTP-based.</p>\n\n<p>Enable stateless client-server communication.</p>\n\n<p>Implement standard HTTP methods such as GET, POST, PUT, PATCH, and DELETE.</p>\n\n<p>API Gateway creates WebSocket APIs that:</p>\n\n<p>Adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server.\nRoute incoming messages based on message content.</p>\n\n<p>So API Gateway supports stateless RESTful APIs as well as stateful WebSocket APIs. Therefore this option is correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>API Gateway creates RESTful APIs that enable stateful client-server communication and API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server</strong></p>\n\n<p><strong>API Gateway creates RESTful APIs that enable stateless client-server communication and API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server</strong></p>\n\n<p><strong>API Gateway creates RESTful APIs that enable stateful client-server communication and API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server</strong></p>\n\n<p>These three options contradict the earlier details provided in the explanation. To summarize, API Gateway supports stateless RESTful APIs and stateful WebSocket APIs. Hence these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html</a></p>\n",
                "options": [
                    {
                        "id": 8915,
                        "content": "<p>API Gateway creates RESTful APIs that enable stateful client-server communication and API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server</p>",
                        "isValid": false
                    },
                    {
                        "id": 8916,
                        "content": "<p>API Gateway creates RESTful APIs that enable stateless client-server communication and API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server</p>",
                        "isValid": true
                    },
                    {
                        "id": 8917,
                        "content": "<p>API Gateway creates RESTful APIs that enable stateful client-server communication and API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server</p>",
                        "isValid": false
                    },
                    {
                        "id": 8918,
                        "content": "<p>API Gateway creates RESTful APIs that enable stateless client-server communication and API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2136,
            "attributes": {
                "createdAt": "2023-09-09T20:33:30.894Z",
                "updatedAt": "2023-09-09T20:33:30.894Z",
                "content": "<p>A software engineering intern at an e-commerce company is documenting the process flow to provision EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes HR payroll data. He wants to highlight those volume types that cannot be used as a boot volume.</p>\n\n<p>Can you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Throughput Optimized HDD (st1)</strong></p>\n\n<p><strong>Cold HDD (sc1)</strong></p>\n\n<p>The EBS volume types fall into two categories:</p>\n\n<p>SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS.</p>\n\n<p>HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS.</p>\n\n<p>Throughput Optimized HDD (st1) and Cold HDD (sc1) volume types CANNOT be used as a boot volume, so these two options are correct.</p>\n\n<p>Please see this detailed overview of the volume types for EBS volumes.\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q43-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>General Purpose SSD (gp2)</strong></p>\n\n<p><strong>Provisioned IOPS SSD (io1)</strong></p>\n\n<p><strong>Instance Store</strong></p>\n\n<p>General Purpose SSD (gp2), Provisioned IOPS SSD (io1), and Instance Store can be used as a boot volume.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html</a></p>\n",
                "options": [
                    {
                        "id": 8919,
                        "content": "<p>General Purpose SSD (gp2)</p>",
                        "isValid": false
                    },
                    {
                        "id": 8920,
                        "content": "<p>Throughput Optimized HDD (st1)</p>",
                        "isValid": true
                    },
                    {
                        "id": 8921,
                        "content": "<p>Cold HDD (sc1)</p>",
                        "isValid": true
                    },
                    {
                        "id": 8922,
                        "content": "<p>Provisioned IOPS SSD (io1)</p>",
                        "isValid": false
                    },
                    {
                        "id": 8923,
                        "content": "<p>Instance Store</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2137,
            "attributes": {
                "createdAt": "2023-09-09T20:33:31.002Z",
                "updatedAt": "2023-09-09T20:33:31.002Z",
                "content": "<p>A new DevOps engineer has just joined a development team and wants to understand the replication capabilities for RDS Multi-AZ as well as RDS Read-replicas.</p>\n\n<p>Which of the following correctly summarizes these capabilities for the given database?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Multi-AZ follows synchronous replication and spans at least two Availability Zones within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</strong></p>\n\n<p>Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones within a single region.</p>\n\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance.</p>\n\n<p>Amazon RDS replicates all databases in the source DB instance. Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review this comparison vis-a-vis Multi-AZ vs Read Replica for RDS:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q52-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p>Incorrect Options:</p>\n\n<p><strong>Multi-AZ follows asynchronous replication and spans one Availability Zone within a single region. Read replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</strong></p>\n\n<p><strong>Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</strong></p>\n\n<p><strong>Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</strong></p>\n\n<p>These three options contradict the earlier details provided in the explanation. To summarize, Multi-AZ follows synchronous replication for RDS. Hence these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n",
                "options": [
                    {
                        "id": 8924,
                        "content": "<p>Multi-AZ follows synchronous replication and spans at least two Availability Zones within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</p>",
                        "isValid": true
                    },
                    {
                        "id": 8925,
                        "content": "<p>Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</p>",
                        "isValid": false
                    },
                    {
                        "id": 8926,
                        "content": "<p>Multi-AZ follows asynchronous replication and spans one Availability Zone within a single region. Read replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</p>",
                        "isValid": false
                    },
                    {
                        "id": 8927,
                        "content": "<p>Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2138,
            "attributes": {
                "createdAt": "2023-09-09T20:33:31.099Z",
                "updatedAt": "2023-09-09T20:33:31.099Z",
                "content": "<p>A company manages a multi-tier social media application that runs on EC2 instances behind an Application Load Balancer. The instances run in an EC2 Auto Scaling group across multiple Availability Zones and use an Amazon Aurora database. As a solutions architect, you have been tasked to make the application more resilient to periodic spikes in request rates.</p>\n\n<p>Which of the following solutions would you recommend for the given use-case? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p>You can use Aurora replicas and CloudFront distribution to make the application more resilient to spikes in request rates.</p>\n\n<p><strong>Use Aurora Replica</strong></p>\n\n<p>Aurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region.</p>\n\n<p><strong>Use CloudFront distribution in front of the Application Load Balancer</strong></p>\n\n<p>Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content.</p>\n\n<p>CloudFront offers an origin failover feature to help support your data resiliency needs. CloudFront is a global service that delivers your content through a worldwide network of data centers called edge locations or points of presence (POPs). If your content is not already cached in an edge location, CloudFront retrieves it from an origin that you've identified as the source for the definitive version of the content.</p>\n\n<p>Incorrect options:</p>\n\n<p><em>* Use AWS Shield</em>* - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency. There are two tiers of AWS Shield - Standard and Advanced. Shield cannot be used to improve application resiliency to handle spikes in traffic.</p>\n\n<p><strong>Use AWS Global Accelerator</strong> - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Since CloudFront is better for improving application resiliency to handle spikes in traffic, so this option is ruled out.</p>\n\n<p><strong>Use AWS Direct Connect</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. Direct Connect cannot be used to improve application resiliency to handle spikes in traffic.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/disaster-recovery-resiliency.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/disaster-recovery-resiliency.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/global-accelerator/faqs/\">https://aws.amazon.com/global-accelerator/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/global-accelerator/latest/dg/disaster-recovery-resiliency.html\">https://docs.aws.amazon.com/global-accelerator/latest/dg/disaster-recovery-resiliency.html</a></p>\n",
                "options": [
                    {
                        "id": 8928,
                        "content": "<p>Use AWS Global Accelerator</p>",
                        "isValid": false
                    },
                    {
                        "id": 8929,
                        "content": "<p>Use CloudFront distribution in front of the Application Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 8930,
                        "content": "<p>Use Aurora Replica</p>",
                        "isValid": true
                    },
                    {
                        "id": 8931,
                        "content": "<p>Use AWS Direct Connect</p>",
                        "isValid": false
                    },
                    {
                        "id": 8932,
                        "content": "<p>Use AWS Shield</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2139,
            "attributes": {
                "createdAt": "2023-09-09T20:33:31.205Z",
                "updatedAt": "2023-09-09T20:33:31.205Z",
                "content": "<p>The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an EC2 instance 1A which is running in region A. Later, he takes a snapshot of the instance 1A and then creates a new AMI in region A from this snapshot. This AMI is then copied into another region B. The founder provisions an instance 1B in region B using this new AMI in region B.</p>\n\n<p>At this point in time, what entities exist in region B?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>1 EC2 instance, 1 AMI and 1 snapshot exist in region B</strong></p>\n\n<p>An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance.\nWhen the new AMI is copied from region A into region B, it automatically creates a snapshot in region B because AMIs are based on the underlying snapshots. Further, an instance is created from this AMI in region B. Hence, we have\n1 EC2 instance, 1 AMI and 1 snapshot in region B.</p>\n\n<p>AMI Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>1 EC2 instance and 1 AMI exist in region B</strong></p>\n\n<p><strong>1 EC2 instance and 2 AMIs exist in region B</strong></p>\n\n<p><strong>1 EC2 instance and 1 snapshot exist in region B</strong></p>\n\n<p>As mentioned earlier in the explanation, when the new AMI is copied from region A into region B, it also creates a snapshot in region B because AMIs are based on the underlying snapshots. In addition, an instance is created from this AMI in region B. So, we have 1 EC2 instance, 1 AMI and 1 snapshot in region B. Hence all three options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html</a></p>\n",
                "options": [
                    {
                        "id": 8933,
                        "content": "<p>1 EC2 instance and 1 snapshot exist in region B</p>",
                        "isValid": false
                    },
                    {
                        "id": 8934,
                        "content": "<p>1 EC2 instance and 1 AMI exist in region B</p>",
                        "isValid": false
                    },
                    {
                        "id": 8935,
                        "content": "<p>1 EC2 instance, 1 AMI and 1 snapshot exist in region B</p>",
                        "isValid": true
                    },
                    {
                        "id": 8936,
                        "content": "<p>1 EC2 instance and 2 AMIs exist in region B</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2140,
            "attributes": {
                "createdAt": "2023-09-09T20:33:31.299Z",
                "updatedAt": "2023-09-09T20:33:31.299Z",
                "content": "<p>An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost.</p>\n\n<p>Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon FSx for Lustre</strong></p>\n\n<p>Amazon FSx for Lustre makes it easy and cost-effective to launch and run the world’s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. The open-source Lustre file system is designed for applications that require fast storage – where you want your storage to keep up with your compute. FSx for Lustre integrates with Amazon S3, making it easy to process data sets with the Lustre file system. When linked to an S3 bucket, an FSx for Lustre file system transparently presents S3 objects as files and allows you to write changed data back to S3.</p>\n\n<p>FSx for Lustre provides the ability to both process the 'hot data' in a parallel and distributed fashion as well as easily store the 'cold data' on Amazon S3. Therefore this option is the BEST fit for the given problem statement.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon FSx for Windows File Server</strong> - Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol.  It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration.\nFSx for Windows does not allow you to present S3 objects as files and does not allow you to write changed data back to S3. Therefore you cannot reference the \"cold data\" with quick access for reads and updates at low cost. Hence this option is not correct.</p>\n\n<p><strong>Amazon EMR</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nEMR does not offer the same storage and processing speed as FSx for Lustre. So it is not the right fit for the given high-performance workflow scenario.</p>\n\n<p><strong>AWS Glue</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing.\nAWS Glue does not offer the same storage and processing speed as FSx for Lustre. So it is not the right fit for the given high-performance workflow scenario.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/fsx/lustre/\">https://aws.amazon.com/fsx/lustre/</a></p>\n\n<p><a href=\"https://aws.amazon.com/fsx/windows/faqs/\">https://aws.amazon.com/fsx/windows/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 8937,
                        "content": "<p>Amazon EMR</p>",
                        "isValid": false
                    },
                    {
                        "id": 8938,
                        "content": "<p>AWS Glue</p>",
                        "isValid": false
                    },
                    {
                        "id": 8939,
                        "content": "<p>Amazon FSx for Windows File Server</p>",
                        "isValid": false
                    },
                    {
                        "id": 8940,
                        "content": "<p>Amazon FSx for Lustre</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2141,
            "attributes": {
                "createdAt": "2023-09-09T20:33:31.390Z",
                "updatedAt": "2023-09-09T20:33:31.390Z",
                "content": "<p>A company uses Amazon S3 buckets for storing sensitive customer data. The company has defined different retention periods for different objects present in the Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected.</p>\n\n<p>Which of the following options represent a valid configuration for setting up retention periods for objects in Amazon S3 buckets? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>When you apply a retention period to an object version explicitly, you specify a <code>Retain Until Date</code> for the object version</strong> - You can place a retention period on an object version either explicitly or through a bucket default setting. When you apply a retention period to an object version explicitly, you specify a <code>Retain Until Date</code> for the object version. Amazon S3 stores the Retain Until Date setting in the object version's metadata and protects the object version until the retention period expires.</p>\n\n<p><strong>Different versions of a single object can have different retention modes and periods</strong> - Like all other Object Lock settings, retention periods apply to individual object versions. Different versions of a single object can have different retention modes and periods.</p>\n\n<p>For example, suppose that you have an object that is 15 days into a 30-day retention period, and you PUT an object into Amazon S3 with the same name and a 60-day retention period. In this case, your PUT succeeds, and Amazon S3 creates a new version of the object with a 60-day retention period. The older version maintains its original retention period and becomes deletable in 15 days.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You cannot place a retention period on an object version through a bucket default setting</strong> - You can place a retention period on an object version either explicitly or through a bucket default setting.</p>\n\n<p><strong>When you use bucket default settings, you specify a <code>Retain Until Date</code> for the object version</strong> - When you use bucket default settings, you don't specify a Retain Until Date. Instead, you specify a duration, in either days or years, for which every object version placed in the bucket should be protected.</p>\n\n<p><strong>The bucket default settings will override any explicit retention mode or period you request on an object version</strong> - If your request to place an object version in a bucket contains an explicit retention mode and period, those settings override any bucket default settings for that object version.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html</a></p>\n",
                "options": [
                    {
                        "id": 8941,
                        "content": "<p>When you use bucket default settings, you specify a <code>Retain Until Date</code> for the object version</p>",
                        "isValid": false
                    },
                    {
                        "id": 8942,
                        "content": "<p>Different versions of a single object can have different retention modes and periods</p>",
                        "isValid": true
                    },
                    {
                        "id": 8943,
                        "content": "<p>You cannot place a retention period on an object version through a bucket default setting</p>",
                        "isValid": false
                    },
                    {
                        "id": 8944,
                        "content": "<p>The bucket default settings will override any explicit retention mode or period you request on an object version</p>",
                        "isValid": false
                    },
                    {
                        "id": 8945,
                        "content": "<p>When you apply a retention period to an object version explicitly, you specify a <code>Retain Until Date</code> for the object version</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2142,
            "attributes": {
                "createdAt": "2023-09-09T20:33:31.490Z",
                "updatedAt": "2023-09-09T20:33:31.490Z",
                "content": "<p>The IT department at a consulting firm is conducting a training workshop for new developers. As part of an evaluation exercise on Amazon S3, the new developers were asked to identify the invalid storage class lifecycle transitions for objects stored on S3.</p>\n\n<p>Can you spot the INVALID lifecycle transitions from the options below? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p>As the question wants to know about the INVALID lifecycle transitions, the following options are the correct answers -</p>\n\n<p><strong>S3 Intelligent-Tiering =&gt; S3 Standard</strong></p>\n\n<p><strong>S3 One Zone-IA =&gt; S3 Standard-IA</strong></p>\n\n<p>Following are the unsupported life cycle transitions for S3 storage classes -\nAny storage class to the S3 Standard storage class.\nAny storage class to the Reduced Redundancy storage class.\nThe S3 Intelligent-Tiering storage class to the S3 Standard-IA storage class.\nThe S3 One Zone-IA storage class to the S3 Standard-IA or S3 Intelligent-Tiering storage classes.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>S3 Standard =&gt; S3 Intelligent-Tiering</strong></p>\n\n<p><strong>S3 Standard-IA =&gt; S3 Intelligent-Tiering</strong></p>\n\n<p><strong>S3 Standard-IA =&gt; S3 One Zone-IA</strong></p>\n\n<p>Here are the supported life cycle transitions for S3 storage classes -\nThe S3 Standard storage class to any other storage class.\nAny storage class to the S3 Glacier or S3 Glacier Deep Archive storage classes.\nThe S3 Standard-IA storage class to the S3 Intelligent-Tiering or S3 One Zone-IA storage classes.\nThe S3 Intelligent-Tiering storage class to the S3 One Zone-IA storage class.\nThe S3 Glacier storage class to the S3 Glacier Deep Archive storage class.</p>\n\n<p>Amazon S3 supports a waterfall model for transitioning between storage classes, as shown in the diagram below.\n<img src=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/images/SupportedTransitionsWaterfallModel.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p>\n",
                "options": [
                    {
                        "id": 8946,
                        "content": "<p>S3 Standard-IA =&gt; S3 Intelligent-Tiering</p>",
                        "isValid": false
                    },
                    {
                        "id": 8947,
                        "content": "<p>S3 Intelligent-Tiering =&gt; S3 Standard</p>",
                        "isValid": true
                    },
                    {
                        "id": 8948,
                        "content": "<p>S3 One Zone-IA =&gt; S3 Standard-IA</p>",
                        "isValid": true
                    },
                    {
                        "id": 8949,
                        "content": "<p>S3 Standard-IA =&gt; S3 One Zone-IA</p>",
                        "isValid": false
                    },
                    {
                        "id": 8950,
                        "content": "<p>S3 Standard =&gt; S3 Intelligent-Tiering</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2143,
            "attributes": {
                "createdAt": "2023-09-09T20:33:31.584Z",
                "updatedAt": "2023-09-09T20:33:31.584Z",
                "content": "<p>A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Solutions Architect Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution.</p>\n\n<p>Which of the following will you recommend to meet these requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Push score updates to Kinesis Data Streams which uses a Lambda function to process these updates and then store these processed updates in DynamoDB</strong></p>\n\n<p>To help ingest real-time data or streaming data at large scales, you can use Amazon Kinesis Data Streams (KDS). KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources. The data collected is available in milliseconds, enabling real-time analytics. KDS provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications.</p>\n\n<p>Lambda integrates natively with Kinesis Data Streams. The polling, checkpointing, and error handling complexities are abstracted when you use this native integration. The processed data can then be configured to be saved in DynamoDB.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Push score updates to an SQS queue which uses a fleet of EC2 instances (with Auto Scaling) to process these updates in the SQS queue and then store these processed updates in an RDS MySQL database</strong></p>\n\n<p><strong>Push score updates to Kinesis Data Streams which uses a fleet of EC2 instances (with Auto Scaling) to process the updates in Kinesis Data Streams and then store these processed updates in DynamoDB</strong></p>\n\n<p><strong>Push score updates to an SNS topic, subscribe a Lambda function to this SNS topic to process the updates, and then store these processed updates in a SQL database running on Amazon EC2</strong></p>\n\n<p>These three options use EC2 instances as part of the solution architecture. The use-case seeks to minimize the management overhead required to maintain the solution. However, EC2 instances involve several maintenance activities such as managing the guest operating system and software deployed to the guest operating system, including updates and security patches, etc. Hence these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/\">https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/</a></p>\n",
                "options": [
                    {
                        "id": 8951,
                        "content": "<p>Push score updates to an SQS queue which uses a fleet of EC2 instances (with Auto Scaling) to process these updates in the SQS queue and then store these processed updates in an RDS MySQL database</p>",
                        "isValid": false
                    },
                    {
                        "id": 8952,
                        "content": "<p>Push score updates to Kinesis Data Streams which uses a Lambda function to process these updates and then store these processed updates in DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 8953,
                        "content": "<p>Push score updates to Kinesis Data Streams which uses a fleet of EC2 instances (with Auto Scaling) to process the updates in Kinesis Data Streams and then store these processed updates in DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 8954,
                        "content": "<p>Push score updates to an SNS topic, subscribe a Lambda function to this SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2144,
            "attributes": {
                "createdAt": "2023-09-09T20:33:31.689Z",
                "updatedAt": "2023-09-09T20:33:31.689Z",
                "content": "<p>A geological research agency maintains the seismological data for the last 100 years. The data has a velocity of 1GB per minute. You would like to store the data with only the most relevant attributes to build a predictive model for earthquakes.</p>\n\n<p>What AWS services would you use to build the most cost-effective solution with the LEAST amount of infrastructure maintenance?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Ingest the data in Kinesis Data Firehose and use an intermediary Lambda function to filter and transform the incoming stream before the output is dumped on S3</strong></p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</p>\n\n<p>Kinesis Data Firehose Overview\n<img src=\"https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p>The correct option is to ingest the data in Kinesis Data Firehose and use a Lambda function to filter and transform the incoming data before the output is dumped on S3. This way you only need to store a sliced version of the data with only the relevant data attributes required for your model. Also it should be noted that this solution is entirely serverless and requires no infrastructure maintenance.</p>\n\n<p>Kinesis Data Firehose to S3:\n<img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-s3.png\">\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Ingest the data in Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to S3</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications, and set up your destination for processed data. Kinesis Data Analytics cannot directly ingest data from the source as it ingests data either from Kinesis Data Streams or Kinesis Data Firehose, so this option is ruled out.</p>\n\n<p><strong>Ingest the data in Kinesis Data Streams and use an intermediary Lambda function to filter and transform the incoming stream before the output is dumped on S3</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. Amazon Kinesis Data Streams is integrated with a number of AWS services, including Amazon Kinesis Data Firehose for near real-time transformation.</p>\n\n<p>Kinesis Data Streams cannot directly write the output to S3. Unlike Firehose, KDS does not offer a ready-made integration via an intermediary Lambda function to reliably dump data into S3. You will need to do a lot of custom coding to get the Lambda function to process the incoming stream and then store the transformed output to S3 with the constraint that the buffer is maintained reliably and no transformed data is lost. So this option is incorrect.</p>\n\n<p><strong>Ingest the data in a Spark Streaming Cluster on EMR use Spark Streaming transformations before writing to S3</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nUsing an EMR cluster would imply managing the underlying infrastructure so it’s ruled out because the correct solution for the given use-case should require the least amount of infrastructure maintenance.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n",
                "options": [
                    {
                        "id": 8955,
                        "content": "<p>Ingest the data in Kinesis Data Firehose and use an intermediary Lambda function to filter and transform the incoming stream before the output is dumped on S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 8956,
                        "content": "<p>Ingest the data in a Spark Streaming Cluster on EMR use Spark Streaming transformations before writing to S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 8957,
                        "content": "<p>Ingest the data in Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 8958,
                        "content": "<p>Ingest the data in Kinesis Data Streams and use an intermediary Lambda function to filter and transform the incoming stream before the output is dumped on S3</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2145,
            "attributes": {
                "createdAt": "2023-09-09T20:33:31.791Z",
                "updatedAt": "2023-09-09T20:33:31.791Z",
                "content": "<p>A company is in the process of migrating its on-premises SMB file shares to AWS so the company can get out of the business of managing multiple file servers across dozens of offices. The company has 200TB of data in its file servers. The existing on-premises applications and native Windows workloads should continue to have low latency access to this data without any disruptions after the migration. The company also wants any new applications deployed on AWS to have access to this migrated data.</p>\n\n<p>Which of the following is the best solution to meet this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS</strong></p>\n\n<p>For user or team file shares, and file-based application migrations, Amazon FSx File Gateway provides low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. For applications deployed on AWS, you may access your file shares directly from Amazon FSx in AWS.</p>\n\n<p>For your native Windows workloads and users, or your SMB clients, Amazon FSx for Windows File Server provides all of the benefits of a native Windows SMB environment that is fully managed and secured and scaled like any other AWS service. You get detailed reporting, replication, backup, failover, and support for native Windows tools like DFS and Active Directory.</p>\n\n<p>Amazon FSx File Gateway:\n<img src=\"https://d1.awsstatic.com/cloud-storage/Amazon%20FSx%20File%20Gateway%20How%20It%20Works%20Diagram.edbf58e4917d47d04e5a5c22132d44bd92733bf5.png\">\nvia - <a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS</strong> - When you need to access S3 using a file system protocol, you should use File Gateway. You get a local cache in the gateway that provides high throughput and low latency over SMB.</p>\n\n<p>Amazon Storage Gateway’s File Gateway does not support file shares for native Windows workloads, so this option is incorrect.</p>\n\n<p>Amazon Storage Gateway’s File Gateway:\n<img src=\"https://d1.awsstatic.com/cloud-storage/Amazon%20S3%20File%20Gateway%20How%20It%20Works%20Diagram.96e9f7180c6ec8b6212b4d6fadc4a9ac4507b421.png\"></p>\n\n<p><strong>Use Amazon Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon S3. The applications deployed on AWS can access this data directly from Amazon S3</strong> - - When you need to access S3 using a file system protocol, you should use File Gateway. You get a local cache in the gateway that provides high throughput and low latency over SMB.</p>\n\n<p>The given use case requires native Windows support for the applications. File Gateway can only be used to access S3 objects using a file system protocol, so this option is incorrect.</p>\n\n<p><strong>Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon EFS. The applications deployed on AWS can access this data directly from Amazon EFS</strong> - Amazon FSx File Gateway provides access to fully managed file shares in Amazon FSx for Windows File Server and it does not support EFS. You should also note that EFS uses the Network File System version 4 (NFS v4) protocol and it does not support SMB protocol. Therefore this option is incorrect for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/file/fsx/\">https://aws.amazon.com/storagegateway/file/fsx/</a></p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/faqs/\">https://aws.amazon.com/storagegateway/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/storage/aws-reinvent-recap-choosing-storage-for-on-premises-file-based-workloads/\">https://aws.amazon.com/blogs/storage/aws-reinvent-recap-choosing-storage-for-on-premises-file-based-workloads/</a></p>\n",
                "options": [
                    {
                        "id": 8959,
                        "content": "<p>Use Amazon Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon S3. The applications deployed on AWS can access this data directly from Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 8960,
                        "content": "<p>Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS</p>",
                        "isValid": true
                    },
                    {
                        "id": 8961,
                        "content": "<p>Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon EFS. The applications deployed on AWS can access this data directly from Amazon EFS</p>",
                        "isValid": false
                    },
                    {
                        "id": 8962,
                        "content": "<p>Use Amazon Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2146,
            "attributes": {
                "createdAt": "2023-09-09T20:33:31.894Z",
                "updatedAt": "2023-09-09T20:33:31.894Z",
                "content": "<p>An e-commerce company is looking for a solution with high availability, as it plans to migrate its flagship application to a fleet of Amazon EC2 instances. The solution should allow for content-based routing as part of the architecture.</p>\n\n<p>As a Solutions Architect, which of the following will you suggest for the company?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use an Application Load Balancer for distributing traffic to the EC2 instances spread across different Availability Zones. Configure Auto Scaling group to mask any failure of an instance</strong></p>\n\n<p>The Application Load Balancer (ALB) is best suited for load balancing HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers. Operating at the individual request level (Layer 7), the Application Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request.</p>\n\n<p>This is the correct option since the question has a specific requirement for content-based routing which can be configured via the Application Load Balancer. Different AZs provide high availability to the overall architecture and Auto Scaling group will help mask any instance failures.</p>\n\n<p>More info on Application Load Balancer:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q2-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/\">https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a Network Load Balancer for distributing traffic to the EC2 instances spread across different Availability Zones. Configure a Private IP address to mask any failure of an instance</strong> - Network Load Balancer cannot facilitate content-based routing so this option is incorrect.</p>\n\n<p><strong>Use an Auto Scaling group for distributing traffic to the EC2 instances spread across different Availability Zones. Configure an Elastic IP address to mask any failure of an instance</strong></p>\n\n<p><strong>Use an Auto Scaling group for distributing traffic to the EC2 instances spread across different Availability Zones. Configure a Public IP address to mask any failure of an instance</strong></p>\n\n<p>Both these options are incorrect as you cannot use the Auto Scaling group to distribute traffic to the EC2 instances.</p>\n\n<p>An Elastic IP address is a static, public, IPv4 address allocated to your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. Elastic IPs do not change and remain allocated to your account until you delete them.</p>\n\n<p>More info on Elastic Load Balancer:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q2-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf\">https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf</a></p>\n\n<p>You can span your Auto Scaling group across multiple Availability Zones within a Region and then attaching a load balancer to distribute incoming traffic across those zones.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q2-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/\">https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf\">https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p>\n",
                "options": [
                    {
                        "id": 8963,
                        "content": "<p>Use an Application Load Balancer for distributing traffic to the EC2 instances spread across different Availability Zones. Configure Auto Scaling group to mask any failure of an instance</p>",
                        "isValid": true
                    },
                    {
                        "id": 8964,
                        "content": "<p>Use a Network Load Balancer for distributing traffic to the EC2 instances spread across different Availability Zones. Configure a Private IP address to mask any failure of an instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 8965,
                        "content": "<p>Use an Auto Scaling group for distributing traffic to the EC2 instances spread across different Availability Zones. Configure an Elastic IP address to mask any failure of an instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 8966,
                        "content": "<p>Use an Auto Scaling group for distributing traffic to the EC2 instances spread across different Availability Zones. Configure a Public IP address to mask any failure of an instance</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2147,
            "attributes": {
                "createdAt": "2023-09-09T20:33:32.006Z",
                "updatedAt": "2023-09-09T20:33:32.006Z",
                "content": "<p>The engineering team at a data analytics company has observed that its flagship application functions at its peak performance when the underlying EC2 instances have a CPU utilization of about 50%. The application is built on a fleet of EC2 instances managed under an Auto Scaling group. The workflow requests are handled by an internal Application Load Balancer that routes the requests to the instances.</p>\n\n<p>As a solutions architect, what would you recommend so that the application runs near its peak performance state?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%</strong></p>\n\n<p>An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies.</p>\n\n<p>With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value.</p>\n\n<p>For example, you can use target tracking scaling to:</p>\n\n<p>Configure a target tracking scaling policy to keep the average aggregate CPU utilization of your Auto Scaling group at 50 percent. This meets the requirements specified in the given use-case and therefore, this is the correct option.</p>\n\n<p>Target Tracking Policy Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q30-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%</strong></p>\n\n<p><strong>Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%</strong></p>\n\n<p>With step scaling and simple scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process.\nNeither step scaling nor simple scaling can be configured to use a target metric for CPU utilization, hence both these options are incorrect.</p>\n\n<p><strong>Configure the Auto Scaling group to use a Cloudwatch alarm triggered on a CPU utilization threshold of 50%</strong> - An Auto Scaling group cannot directly use a Cloudwatch alarm as the source for a scale-in or scale-out event, hence this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html</a></p>\n",
                "options": [
                    {
                        "id": 8967,
                        "content": "<p>Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%</p>",
                        "isValid": false
                    },
                    {
                        "id": 8968,
                        "content": "<p>Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%</p>",
                        "isValid": true
                    },
                    {
                        "id": 8969,
                        "content": "<p>Configure the Auto Scaling group to use a Cloudwatch alarm triggered on a CPU utilization threshold of 50%</p>",
                        "isValid": false
                    },
                    {
                        "id": 8970,
                        "content": "<p>Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2148,
            "attributes": {
                "createdAt": "2023-09-09T20:33:32.140Z",
                "updatedAt": "2023-09-09T20:33:32.140Z",
                "content": "<p>A data analytics company measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 GB.</p>\n\n<p>Which of the following is the fastest way to upload the daily compressed file into S3?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Upload the compressed file using multipart upload with S3 transfer acceleration</strong></p>\n\n<p>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</p>\n\n<p>Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance. If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upload the compressed file in a single operation</strong> - In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput - you can upload parts in parallel to improve throughput. Therefore, this option is not correct.</p>\n\n<p><strong>Upload the compressed file using multipart upload</strong> - Although using multipart upload would certainly speed up the process, combining with S3 transfer acceleration would further improve the transfer speed. Therefore just using multipart upload is not the correct option.</p>\n\n<p><strong>FTP the compressed file into an EC2 instance that runs in the same region as the S3 bucket. Then transfer the file from the EC2 instance into the S3 bucket</strong> -  This is a roundabout process of getting the file into S3 and added as a distractor. Although it is technically feasible to follow this process, it would involve a lot of scripting and certainly would not be the fastest way to get the file into S3.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p>\n",
                "options": [
                    {
                        "id": 8971,
                        "content": "<p>Upload the compressed file using multipart upload</p>",
                        "isValid": false
                    },
                    {
                        "id": 8972,
                        "content": "<p>FTP the compressed file into an EC2 instance that runs in the same region as the S3 bucket. Then transfer the file from the EC2 instance into the S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 8973,
                        "content": "<p>Upload the compressed file using multipart upload with S3 transfer acceleration</p>",
                        "isValid": true
                    },
                    {
                        "id": 8974,
                        "content": "<p>Upload the compressed file in a single operation</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2149,
            "attributes": {
                "createdAt": "2023-09-09T20:33:32.234Z",
                "updatedAt": "2023-09-09T20:33:32.234Z",
                "content": "<p>A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort.</p>\n\n<p>Which of the following options represents the best solution for this use case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Leverage multi-AZ configuration of RDS Custom for Oracle that allows the database administrators to access and customize the database environment and the underlying operating system</strong></p>\n\n<p>Amazon RDS is a managed service that makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while managing time-consuming database administration tasks. Amazon RDS can automatically back up your database and keep your database software up to date with the latest version. However, RDS does not allow you to access the host OS of the database.</p>\n\n<p>For the given use-case, you need to use RDS Custom for Oracle as it allows you to access and customize your database server host and operating system, for example by applying special patches and changing the database software settings to support third-party applications that require privileged access. RDS Custom for Oracle facilitates these functionalities with minimum infrastructure maintenance effort. You need to set up the RDS Custom for Oracle in multi-AZ configuration for high availability.</p>\n\n<p>RDS Custom for Oracle:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q3-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/\">https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage multi-AZ configuration of RDS for Oracle that allows the database administrators to access and customize the database environment and the underlying operating system</strong></p>\n\n<p><strong>Leverage cross AZ read-replica configuration of RDS for Oracle that allows the database administrators to access and customize the database environment and the underlying operating system</strong></p>\n\n<p>RDS for Oracle does not allow you to access and customize your database server host and operating system. Therefore, both these options are incorrect.</p>\n\n<p><strong>Deploy the Oracle database layer on multiple EC2 instances spread across two Availability Zones (AZ). This deployment configuration guarantees high availability and also allows the database administrators to access and customize the database environment and the underlying operating system</strong> - The use case requires that the best solution should involve minimum infrastructure maintenance effort. When you use EC2 instances to host the databases, you need to manage the server health, server maintenance, server patching, and database maintenance tasks yourself. In addition, you will also need to manage the multi-AZ configuration by deploying EC2 instances across two Availability Zones, perhaps by using an Auto-scaling group. These steps entail significant maintenance effort. Hence this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/\">https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 8975,
                        "content": "<p>Deploy the Oracle database layer on multiple EC2 instances spread across two Availability Zones (AZ). This deployment configuration guarantees high availability and also allows the database administrators to access and customize the database environment and the underlying operating system</p>",
                        "isValid": false
                    },
                    {
                        "id": 8976,
                        "content": "<p>Leverage cross AZ read-replica configuration of RDS for Oracle that allows the database administrators to access and customize the database environment and the underlying operating system</p>",
                        "isValid": false
                    },
                    {
                        "id": 8977,
                        "content": "<p>Leverage multi-AZ configuration of RDS for Oracle that allows the database administrators to access and customize the database environment and the underlying operating system</p>",
                        "isValid": false
                    },
                    {
                        "id": 8978,
                        "content": "<p>Leverage multi-AZ configuration of RDS Custom for Oracle that allows the database administrators to access and customize the database environment and the underlying operating system</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2150,
            "attributes": {
                "createdAt": "2023-09-09T20:33:32.329Z",
                "updatedAt": "2023-09-09T20:33:32.329Z",
                "content": "<p>A file-hosting service uses Amazon S3 under the hood to power its storage offerings. Currently all the customer files are uploaded directly under a single S3 bucket. The engineering team has started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than 5000 requests per second.</p>\n\n<p>Which of the following is the MOST resource efficient and cost-optimal way of addressing this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Change the application architecture to create customer-specific custom prefixes within the single bucket and then upload the daily files into those prefixed locations</strong></p>\n\n<p>Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3. Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket.</p>\n\n<p>There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second. Please see this example for more clarity on prefixes:\nif you have a file f1 stored in an S3 object path like so <code>s3://your_bucket_name/folder1/sub_folder_1/f1</code>, then <code>/folder1/sub_folder_1/</code> becomes the prefix for file f1.</p>\n\n<p>Some data lake applications on Amazon S3 scan millions or billions of objects for queries that run over petabytes of data. These data lake applications achieve single-instance transfer rates that maximize the network interface used for their Amazon EC2 instance, which can be up to 100 Gb/s on a single instance. These applications then aggregate throughput across multiple instances to get multiple terabits per second. Therefore creating customer-specific custom prefixes within the single bucket and then uploading the daily files into those prefixed locations is the BEST solution for the given constraints.</p>\n\n<p>Optimizing Amazon S3 Performance:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q51-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the application architecture to create a new S3 bucket for each customer and then upload each customer's files directly under the respective buckets</strong> - Creating a new S3 bucket for each new customer is an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some customers may use the service sparingly but the bucket name is locked for them forever. Moreover, this is really not required as we can use S3 prefixes to improve the performance.</p>\n\n<p><strong>Change the application architecture to create a new S3 bucket for each day's data and then upload the daily files directly under that day's bucket</strong> - Creating a new S3 bucket for each new day's data is also an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some of the bucket names may not be available for daily data processing. Moreover, this is really not required as we can use S3 prefixes to improve the performance.</p>\n\n<p><strong>Change the application architecture to use EFS instead of Amazon S3 for storing the customers' uploaded files</strong> - EFS is a costlier storage option compared to S3, so it is ruled out.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html</a></p>\n",
                "options": [
                    {
                        "id": 8979,
                        "content": "<p>Change the application architecture to create a new S3 bucket for each day's data and then upload the daily files directly under that day's bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 8980,
                        "content": "<p>Change the application architecture to create customer-specific custom prefixes within the single bucket and then upload the daily files into those prefixed locations</p>",
                        "isValid": true
                    },
                    {
                        "id": 8981,
                        "content": "<p>Change the application architecture to use EFS instead of Amazon S3 for storing the customers' uploaded files</p>",
                        "isValid": false
                    },
                    {
                        "id": 8982,
                        "content": "<p>Change the application architecture to create a new S3 bucket for each customer and then upload each customer's files directly under the respective buckets</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2151,
            "attributes": {
                "createdAt": "2023-09-09T20:33:32.490Z",
                "updatedAt": "2023-09-09T20:33:32.490Z",
                "content": "<p>A leading video streaming service delivers billions of hours of content from Amazon S3 to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline.</p>\n\n<p>Which of the following is the MOST cost-effective strategy for storing this intermediary query data?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Store the intermediary query results in S3 Standard storage class</strong></p>\n\n<p>S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.\nAs there is no minimum storage duration charge and no retrieval fee (remember that intermediary query results are heavily referenced by other parts of the analytics pipeline), this is the MOST cost-effective storage class amongst the given options.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the intermediary query results in S3 Glacier Instant Retrieval storage class</strong> - S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is ideal for archive data that needs immediate access, such as medical images, news media assets, or user-generated content archives.</p>\n\n<p>The minimum storage duration charge is 90 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.</p>\n\n<p><strong>Store the intermediary query results in S3 Standard-Infrequent Access storage class</strong> - S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.</p>\n\n<p><strong>Store the intermediary query results in S3 One Zone-Infrequent Access storage class</strong> - S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.</p>\n\n<p>To summarize again, S3 Standard-IA and S3 One Zone-IA have a minimum storage duration charge of 30 days (so instead of 24 hours, you end up paying for 30 days). S3 Standard-IA and S3 One Zone-IA also have retrieval charges (as the results are heavily referenced by other parts of the analytics pipeline, so the retrieval costs would be pretty high). Therefore, these storage classes are not cost optimal for the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n",
                "options": [
                    {
                        "id": 8983,
                        "content": "<p>Store the intermediary query results in S3 Glacier Instant Retrieval storage class</p>",
                        "isValid": false
                    },
                    {
                        "id": 8984,
                        "content": "<p>Store the intermediary query results in S3 Standard-Infrequent Access storage class</p>",
                        "isValid": false
                    },
                    {
                        "id": 8985,
                        "content": "<p>Store the intermediary query results in S3 Standard storage class</p>",
                        "isValid": true
                    },
                    {
                        "id": 8986,
                        "content": "<p>Store the intermediary query results in S3 One Zone-Infrequent Access storage class</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2152,
            "attributes": {
                "createdAt": "2023-09-09T20:33:32.589Z",
                "updatedAt": "2023-09-09T20:33:32.589Z",
                "content": "<p>The development team at an e-commerce startup has set up multiple microservices running on EC2 instances under an Application Load Balancer. The team wants to route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants requests for https://www.example.com/orders to go to a specific microservice and requests for https://www.example.com/products to go to another microservice.</p>\n\n<p>Which of the following features of Application Load Balancers can be used for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Path-based Routing</strong></p>\n\n<p>Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.</p>\n\n<p>If your application is composed of several individual services, an Application Load Balancer can route a request to a service based on the content of the request. Here are the different types -</p>\n\n<p>Host-based Routing:</p>\n\n<p>You can route a client request based on the Host field of the HTTP header allowing you to route to multiple domains from the same load balancer.</p>\n\n<p>Path-based Routing:</p>\n\n<p>You can route a client request based on the URL path of the HTTP header.</p>\n\n<p>HTTP header-based routing:</p>\n\n<p>You can route a client request based on the value of any standard or custom HTTP header.</p>\n\n<p>HTTP method-based routing:</p>\n\n<p>You can route a client request based on any standard or custom HTTP method.</p>\n\n<p>Query string parameter-based routing:</p>\n\n<p>You can route a client request based on the query string or query parameters.</p>\n\n<p>Source IP address CIDR-based routing:</p>\n\n<p>You can route a client request based on source IP address CIDR from where the request originates.</p>\n\n<p>Path-based Routing Overview:</p>\n\n<p>You can use path conditions to define rules that route requests based on the URL in the request (also known as path-based routing).</p>\n\n<p>The path pattern is applied only to the path of the URL, not to its query parameters.\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q48-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#path-conditions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#path-conditions</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Query string parameter-based routing</strong></p>\n\n<p><strong>HTTP header-based routing</strong></p>\n\n<p><strong>Host-based Routing</strong></p>\n\n<p>As mentioned earlier in the explanation, none of these three types of routing support requests based on the URL path of the HTTP header. Hence these three are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a></p>\n",
                "options": [
                    {
                        "id": 8987,
                        "content": "<p>HTTP header-based routing</p>",
                        "isValid": false
                    },
                    {
                        "id": 8988,
                        "content": "<p>Path-based Routing</p>",
                        "isValid": true
                    },
                    {
                        "id": 8989,
                        "content": "<p>Host-based Routing</p>",
                        "isValid": false
                    },
                    {
                        "id": 8990,
                        "content": "<p>Query string parameter-based routing</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2153,
            "attributes": {
                "createdAt": "2023-09-09T20:33:32.683Z",
                "updatedAt": "2023-09-09T20:33:32.683Z",
                "content": "<p>The flagship application for a gaming company connects to an Amazon Aurora database and the entire technology stack is currently deployed in the United States. Now, the company has plans to expand to Europe and Asia for its operations. It needs the <code>games</code> table to be accessible globally but needs the <code>users</code> and <code>games_played</code> tables to be regional only.</p>\n\n<p>How would you implement this with minimal application refactoring?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use an Amazon Aurora Global Database for the <code>games</code> table and use Amazon Aurora for the <code>users</code> and <code>games_played</code> tables</strong></p>\n\n<p>Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. Aurora is not an in-memory database.</p>\n\n<p>Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case.</p>\n\n<p>For the given use-case, we, therefore, need to have two Aurora clusters, one for the global table (games table) and the other one for the local tables (users and games_played tables).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an Amazon Aurora Global Database for the <code>games</code> table and use DynamoDB tables for the <code>users</code> and <code>games_played</code> tables</strong></p>\n\n<p><strong>Use a DynamoDB global table for the <code>games</code> table and use Amazon Aurora for the <code>users</code> and <code>games_played</code> tables</strong></p>\n\n<p><strong>Use a DynamoDB global table for the <code>games</code> table and use DynamoDB tables for the <code>users</code> and <code>games_played</code> tables</strong></p>\n\n<p>Here, we want minimal application refactoring. DynamoDB and Aurora have a completely different API, due to Aurora being SQL and DynamoDB being NoSQL. So all three options are incorrect, as they have DynamoDB as one of the components.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/aurora/faqs/\">https://aws.amazon.com/rds/aurora/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 8991,
                        "content": "<p>Use a DynamoDB global table for the <code>games</code> table and use Amazon Aurora for the <code>users</code> and <code>games_played</code> tables</p>",
                        "isValid": false
                    },
                    {
                        "id": 8992,
                        "content": "<p>Use a DynamoDB global table for the <code>games</code> table and use DynamoDB tables for the <code>users</code> and <code>games_played</code> tables</p>",
                        "isValid": false
                    },
                    {
                        "id": 8993,
                        "content": "<p>Use an Amazon Aurora Global Database for the <code>games</code> table and use Amazon Aurora for the <code>users</code> and <code>games_played</code> tables</p>",
                        "isValid": true
                    },
                    {
                        "id": 8994,
                        "content": "<p>Use an Amazon Aurora Global Database for the <code>games</code> table and use DynamoDB tables for the <code>users</code> and <code>games_played</code> tables</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2154,
            "attributes": {
                "createdAt": "2023-09-09T20:33:32.809Z",
                "updatedAt": "2023-09-09T20:33:32.809Z",
                "content": "<p>A gaming company uses Amazon Aurora as its primary database service. The company has now deployed 5 multi-AZ read replicas to increase the read throughput and for use as failover target. The replicas have been assigned the following failover priority tiers and corresponding instance sizes are given in parentheses: tier-1 (16TB), tier-1 (32TB), tier-10 (16TB), tier-15 (16TB), tier-15 (32TB).</p>\n\n<p>In the event of a failover, Amazon Aurora will promote which of the following read replicas?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Tier-1 (32TB)</strong></p>\n\n<p>Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs).</p>\n\n<p>For Amazon Aurora, each Read Replica is associated with a priority tier (0-15).  In the event of a failover, Amazon Aurora will promote the Read Replica that has the highest priority (the lowest numbered tier). If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size. If two or more Aurora Replicas share the same priority and size, then Amazon Aurora promotes an arbitrary replica in the same promotion tier.</p>\n\n<p>Therefore, for this problem statement, the Tier-1 (32TB) replica will be promoted.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Tier-15 (32TB)</strong></p>\n\n<p><strong>Tier-1 (16TB)</strong></p>\n\n<p><strong>Tier-10 (16TB)</strong></p>\n\n<p>Given the failover rules discussed earlier in the explanation, these three options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html</a></p>\n",
                "options": [
                    {
                        "id": 8995,
                        "content": "<p>Tier-1 (32TB)</p>",
                        "isValid": true
                    },
                    {
                        "id": 8996,
                        "content": "<p>Tier-15 (32TB)</p>",
                        "isValid": false
                    },
                    {
                        "id": 8997,
                        "content": "<p>Tier-1 (16TB)</p>",
                        "isValid": false
                    },
                    {
                        "id": 8998,
                        "content": "<p>Tier-10 (16TB)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2155,
            "attributes": {
                "createdAt": "2023-09-09T20:33:32.912Z",
                "updatedAt": "2023-09-09T20:33:32.912Z",
                "content": "<p>A telecom company operates thousands of hardware devices like switches, routers, cables, etc. The real-time status data for these devices must be fed into a communications application for notifications. Simultaneously, another analytics application needs to read the same real-time status data and analyze all the connecting lines that may go down because of any device failures.</p>\n\n<p>As a Solutions Architect, which of the following solutions would you suggest, so that both the applications can consume the real-time status data concurrently?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong> - Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</p>\n\n<p>AWS recommends Amazon Kinesis Data Streams for use cases with requirements that are similar to the following:</p>\n\n<ol>\n<li>Routing related records to the same record processor (as in streaming MapReduce). For example, counting and aggregation are simpler when all records for a given key are routed to the same record processor.</li>\n<li>Ordering of records. For example, you want to transfer log data from the application host to the processing/archival host while maintaining the order of log statements.</li>\n<li>Ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.</li>\n<li>Ability to consume records in the same order a few hours later. For example, you have a billing application and an audit application that runs a few hours behind the billing application. Because Amazon Kinesis Data Streams stores data for up to 365 days, you can run the audit application up to 365 days behind the billing application.</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS is a notification service and cannot be used for real-time processing of data.</p>\n\n<p><strong>Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. Since multiple applications need to consume the same data stream concurrently, Kinesis is a better choice when compared to the combination of SQS with SNS.</p>\n\n<p><strong>Amazon Simple Queue Service (SQS) with Amazon Simple Email Service (Amazon SES)</strong> - As discussed above, Kinesis is a better option for this use case in comparison to SQS. Also, SES does not fit this use-case. Hence, this option is an incorrect answer.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 8999,
                        "content": "<p>Amazon Simple Notification Service (SNS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9000,
                        "content": "<p>Amazon Kinesis Data Streams</p>",
                        "isValid": true
                    },
                    {
                        "id": 9001,
                        "content": "<p>Amazon Simple Queue Service (SQS) with Amazon Simple Email Service (Amazon SES)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9002,
                        "content": "<p>Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2156,
            "attributes": {
                "createdAt": "2023-09-09T20:33:33.000Z",
                "updatedAt": "2023-09-09T20:33:33.000Z",
                "content": "<p>A research group needs a fleet of EC2 instances for a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application architecture would ensure the replacement instance has access to the required dataset.</p>\n\n<p>Which of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of EC2 instances?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Instance Store based EC2 instances</strong></p>\n\n<p>An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host instance. Instance store is ideal for the temporary storage of information that changes frequently such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Instance store volumes are included as part of the instance's usage cost.</p>\n\n<p>As Instance Store based volumes provide high random I/O performance at low cost (as the storage is part of the instance's usage cost) and the resilient architecture can adjust for the loss of any instance, therefore you should use Instance Store based EC2 instances for this use-case.</p>\n\n<p>EC2 Instance Store Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q43-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use EBS based EC2 instances</strong> - EBS based volumes would need to use Provisioned IOPS (io1) as the storage type and that would incur additional costs. As we are looking for the most cost-optimal solution, this option is ruled out.</p>\n\n<p><strong>Use EC2 instances with EFS mount points</strong> - Using EFS implies that extra resources would have to be provisioned (compared to using instance store where the storage is located on disks that are physically attached to the host instance itself). As we are looking for the most resource-efficient solution, this option is also ruled out.</p>\n\n<p><strong>Use EC2 instances with access to S3 based storage</strong> - Using EC2 instances with access to S3 based storage does not deliver high random I/O performance, this option is just added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p>\n",
                "options": [
                    {
                        "id": 9003,
                        "content": "<p>Use EC2 instances with EFS mount points</p>",
                        "isValid": false
                    },
                    {
                        "id": 9004,
                        "content": "<p>Use EC2 instances with access to S3 based storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 9005,
                        "content": "<p>Use EBS based EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9006,
                        "content": "<p>Use Instance Store based EC2 instances</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2157,
            "attributes": {
                "createdAt": "2023-09-09T20:33:33.114Z",
                "updatedAt": "2023-09-09T20:33:33.114Z",
                "content": "<p>As part of a pilot program, a biotechnology company wants to integrate data files from its on-premises analytical application with AWS Cloud via an NFS interface.</p>\n\n<p>Which of the following AWS service is the MOST efficient solution for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Storage Gateway - File Gateway</strong></p>\n\n<p>AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access.</p>\n\n<p>AWS Storage Gateway's file interface, or file gateway, offers you a seamless way to connect to the cloud in order to store application data files and backup images as durable objects on Amazon S3 cloud storage. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. As the company wants to integrate data files from its analytical instruments into AWS via an NFS interface, therefore AWS Storage Gateway - File Gateway is the correct answer.</p>\n\n<p>File Gateway Overview:\n<img src=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/images/file-gateway-concepts-diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Storage Gateway - Volume Gateway</strong> - You can configure the AWS Storage Gateway service as a Volume Gateway to present cloud-based iSCSI block storage volumes to your on-premises applications. Volume Gateway does not support NFS interface, so this option is not correct.</p>\n\n<p><strong>AWS Storage Gateway - Tape Gateway</strong> - AWS Storage Gateway - Tape Gateway allows moving tape backups to the cloud. Tape Gateway does not support NFS interface, so this option is not correct.</p>\n\n<p><strong>AWS Site-to-Site VPN</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN (Site-to-Site VPN) connection. It uses internet protocol security (IPSec) communications to create encrypted VPN tunnels between two locations. You cannot use AWS Site-to-Site VPN to integrate data files via the NFS interface, so this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/\">https://aws.amazon.com/storagegateway/</a></p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/volume/\">https://aws.amazon.com/storagegateway/volume/</a></p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/vtl/\">https://aws.amazon.com/storagegateway/vtl/</a></p>\n",
                "options": [
                    {
                        "id": 9007,
                        "content": "<p>AWS Storage Gateway - Tape Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 9008,
                        "content": "<p>AWS Site-to-Site VPN</p>",
                        "isValid": false
                    },
                    {
                        "id": 9009,
                        "content": "<p>AWS Storage Gateway - File Gateway</p>",
                        "isValid": true
                    },
                    {
                        "id": 9010,
                        "content": "<p>AWS Storage Gateway - Volume Gateway</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2158,
            "attributes": {
                "createdAt": "2023-09-09T20:33:33.211Z",
                "updatedAt": "2023-09-09T20:33:33.211Z",
                "content": "<p>A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Solutions Architect Associate to build a multi-tier solution to store and retrieve this location data for analysis.</p>\n\n<p>Which of the following options addresses the given use case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Leverage Amazon API Gateway with Kinesis Data Analytics</strong></p>\n\n<p>You can use Kinesis Data Analytics to transform and analyze streaming data in real-time with Apache Flink. Kinesis Data Analytics enables you to quickly build end-to-end stream processing applications for log analytics, clickstream analytics, Internet of Things (IoT), ad tech, gaming, etc. The four most common use cases are streaming extract-transform-load (ETL), continuous metric generation, responsive real-time analytics, and interactive querying of data streams. Kinesis Data Analytics for Apache Flink applications provides your application 50 GB of running application storage per Kinesis Processing Unit (KPU).</p>\n\n<p>Amazon API Gateway is a fully managed service that allows you to publish, maintain, monitor, and secure APIs at any scale. Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs.</p>\n\n<p>Amazon API Gateway:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q57-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/\">https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/</a></p>\n\n<p>For the given use case, you can use Amazon API Gateway to create a REST API that handles incoming requests having location data from the trucks and sends it to the Kinesis Data Analytics application on the back end.</p>\n\n<p>Kinesis Data Analytics:\n<img src=\"https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Amazon-Kinesis-Data-Analytics_HIW.82e3aa53a5c87db03c766218b3d51f1a110c60eb.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-analytics/\">https://aws.amazon.com/kinesis/data-analytics/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon Athena with S3</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena cannot be used to build a REST API to consume data from the source. So this option is incorrect.</p>\n\n<p><strong>Leverage QuickSight with Redshift</strong> - QuickSight is a cloud-native, serverless business intelligence service. Quicksight cannot be used to build a REST API to consume data from the source. Redshift is a fully managed AWS cloud data warehouse. So this option is incorrect.</p>\n\n<p><strong>Leverage Amazon API Gateway with AWS Lambda</strong> - You cannot use Lambda to store and retrieve the location data for analysis, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-kinesis.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-kinesis.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/\">https://aws.amazon.com/kinesis/data-analytics/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9011,
                        "content": "<p>Leverage Amazon Athena with S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9012,
                        "content": "<p>Leverage Amazon API Gateway with Kinesis Data Analytics</p>",
                        "isValid": true
                    },
                    {
                        "id": 9013,
                        "content": "<p>Leverage QuickSight with Redshift</p>",
                        "isValid": false
                    },
                    {
                        "id": 9014,
                        "content": "<p>Leverage Amazon API Gateway with AWS Lambda</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2159,
            "attributes": {
                "createdAt": "2023-09-09T20:33:33.342Z",
                "updatedAt": "2023-09-09T20:33:33.342Z",
                "content": "<p>A junior scientist working with the Deep Space Research Laboratory at NASA is trying to upload a high-resolution image of a nebula into Amazon S3. The image size is approximately 3GB. The junior scientist is using S3 Transfer Acceleration (S3TA) for faster image upload. It turns out that S3TA did not result in an accelerated transfer.</p>\n\n<p>Given this scenario, which of the following is correct regarding the charges for this image transfer?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The junior scientist does not need to pay any transfer charges for the image upload</strong></p>\n\n<p>There are no S3 data transfer charges when data is transferred in from the internet. Also with S3TA, you pay only for transfers that are accelerated. Therefore the junior scientist does not need to pay any transfer charges for the image upload because S3TA did not result in an accelerated transfer.</p>\n\n<p>S3 Transfer Acceleration (S3TA) Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q40-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/s3/transfer-acceleration/\">https://aws.amazon.com/s3/transfer-acceleration/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The junior scientist only needs to pay S3TA transfer charges for the image upload</strong> - Since S3TA did not result in an accelerated transfer, there are no S3TA transfer charges to be paid.</p>\n\n<p><strong>The junior scientist only needs to pay S3 transfer charges for the image upload</strong> - There are no S3 data transfer charges when data is transferred in from the internet. So this option is incorrect.</p>\n\n<p><strong>The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload</strong> - There are no S3 data transfer charges when data is transferred in from the internet. Since S3TA did not result in an accelerated transfer, there are no S3TA transfer charges to be paid.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/transfer-acceleration/\">https://aws.amazon.com/s3/transfer-acceleration/</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/pricing/\">https://aws.amazon.com/s3/pricing/</a></p>\n",
                "options": [
                    {
                        "id": 9015,
                        "content": "<p>The junior scientist does not need to pay any transfer charges for the image upload</p>",
                        "isValid": true
                    },
                    {
                        "id": 9016,
                        "content": "<p>The junior scientist only needs to pay S3 transfer charges for the image upload</p>",
                        "isValid": false
                    },
                    {
                        "id": 9017,
                        "content": "<p>The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload</p>",
                        "isValid": false
                    },
                    {
                        "id": 9018,
                        "content": "<p>The junior scientist only needs to pay S3TA transfer charges for the image upload</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2160,
            "attributes": {
                "createdAt": "2023-09-09T20:33:33.472Z",
                "updatedAt": "2023-09-09T20:33:33.472Z",
                "content": "<p>A financial services company uses Amazon GuardDuty for analyzing its AWS account metadata to meet the compliance guidelines. However, the company has now decided to stop using GuardDuty service. All the existing findings have to be deleted and cannot persist anywhere on AWS Cloud.</p>\n\n<p>Which of the following techniques will help the company meet this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately.</p>\n\n<p><strong>Disable the service in the general settings</strong> - Disabling the service will delete all remaining data, including your findings and configurations before relinquishing the service permissions and resetting the service. So, this is the correct option for our use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Suspend the service in the general settings</strong> - You can stop Amazon GuardDuty from analyzing your data sources at any time by choosing to suspend the service in the general settings. This will immediately stop the service from analyzing data, but does not delete your existing findings or configurations.</p>\n\n<p><strong>De-register the service under services tab</strong> - This is a made-up option, used only as a distractor.</p>\n\n<p><strong>Raise a service request with Amazon to completely delete the data from all their backups</strong> - There is no need to create a service request as you can delete the existing findings by disabling the service.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/guardduty/faqs/\">https://aws.amazon.com/guardduty/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9019,
                        "content": "<p>Raise a service request with Amazon to completely delete the data from all their backups</p>",
                        "isValid": false
                    },
                    {
                        "id": 9020,
                        "content": "<p>Disable the service in the general settings</p>",
                        "isValid": true
                    },
                    {
                        "id": 9021,
                        "content": "<p>Suspend the service in the general settings</p>",
                        "isValid": false
                    },
                    {
                        "id": 9022,
                        "content": "<p>De-register the service under services tab</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2161,
            "attributes": {
                "createdAt": "2023-09-09T20:33:33.568Z",
                "updatedAt": "2023-09-09T20:33:33.568Z",
                "content": "<p>A news network uses Amazon S3 to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination S3 bucket.</p>\n\n<p>Which of the following are the MOST cost-effective options to improve the file upload speed into S3? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use Amazon S3 Transfer Acceleration to enable faster file uploads into the destination S3 bucket</strong> - Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</p>\n\n<p><strong>Use multipart uploads for faster file uploads into the destination S3 bucket</strong> - Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput, therefore it facilitates faster file uploads.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create multiple AWS direct connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into S3</strong> - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations.\nDirect connect takes significant time (several months) to be provisioned and is an overkill for the given use-case.</p>\n\n<p><strong>Create multiple site-to-site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into S3</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet.\nVPN Connections are a good solution if you have low to modest bandwidth requirements and can tolerate the inherent variability in Internet-based connectivity. Site-to-site VPN will not help in accelerating the file transfer speeds into S3 for the given use-case.</p>\n\n<p><strong>Use AWS Global Accelerator for faster file uploads into the destination S3 bucket</strong> - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. AWS Global Accelerator will not help in accelerating the file transfer speeds into S3 for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p>\n",
                "options": [
                    {
                        "id": 9023,
                        "content": "<p>Use AWS Global Accelerator for faster file uploads into the destination S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 9024,
                        "content": "<p>Create multiple AWS direct connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9025,
                        "content": "<p>Use multipart uploads for faster file uploads into the destination S3 bucket</p>",
                        "isValid": true
                    },
                    {
                        "id": 9026,
                        "content": "<p>Use Amazon S3 Transfer Acceleration to enable faster file uploads into the destination S3 bucket</p>",
                        "isValid": true
                    },
                    {
                        "id": 9027,
                        "content": "<p>Create multiple site-to-site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into S3</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2162,
            "attributes": {
                "createdAt": "2023-09-09T20:33:33.661Z",
                "updatedAt": "2023-09-09T20:33:33.661Z",
                "content": "<p>A technology blogger wants to write a review on the comparative pricing for various storage types available on AWS Cloud. The blogger has created a test file of size 1GB with some random data. Next he copies this test file into AWS S3 Standard storage class, provisions an EBS volume (General Purpose SSD (gp2)) with 100GB of provisioned storage and copies the test file into the EBS volume, and lastly copies the test file into an EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file.</p>\n\n<p>What is the correct order of the storage charges incurred for the test file on these three storage types?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Cost of test file storage on S3 Standard &lt; Cost of test file storage on EFS &lt; Cost of test file storage on EBS</strong></p>\n\n<p>With Amazon EFS, you pay only for the resources that you use. The EFS Standard Storage pricing is $0.30 per GB per month. Therefore the cost for storing the test file on EFS is $0.30 for the month.</p>\n\n<p>For EBS General Purpose SSD (gp2) volumes, the charges are $0.10 per GB-month of provisioned storage. Therefore, for a provisioned storage of 100GB for this use-case, the monthly cost on EBS is $0.10*100 = $10. This cost is irrespective of how much storage is actually consumed by the test file.</p>\n\n<p>For S3 Standard storage, the pricing is $0.023 per GB per month. Therefore, the monthly storage cost on S3 for the test file is $0.023.</p>\n\n<p>Therefore this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Cost of test file storage on S3 Standard &lt; Cost of test file storage on EBS &lt; Cost of test file storage on EFS</strong></p>\n\n<p><strong>Cost of test file storage on EFS &lt; Cost of test file storage on S3 Standard &lt; Cost of test file storage on EBS</strong></p>\n\n<p><strong>Cost of test file storage on EBS &lt; Cost of test file storage on S3 Standard &lt; Cost of test file storage on EFS</strong></p>\n\n<p>Following the computations shown earlier in the explanation, these three options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ebs/pricing/\">https://aws.amazon.com/ebs/pricing/</a></p>\n\n<p>https://aws.amazon.com/s3/pricing/(https://aws.amazon.com/s3/pricing/)</p>\n\n<p><a href=\"https://aws.amazon.com/efs/pricing/\">https://aws.amazon.com/efs/pricing/</a></p>\n",
                "options": [
                    {
                        "id": 9028,
                        "content": "<p>Cost of test file storage on EFS &lt; Cost of test file storage on S3 Standard &lt; Cost of test file storage on EBS</p>",
                        "isValid": false
                    },
                    {
                        "id": 9029,
                        "content": "<p>Cost of test file storage on S3 Standard &lt; Cost of test file storage on EBS &lt; Cost of test file storage on EFS</p>",
                        "isValid": false
                    },
                    {
                        "id": 9030,
                        "content": "<p>Cost of test file storage on S3 Standard &lt; Cost of test file storage on EFS &lt; Cost of test file storage on EBS</p>",
                        "isValid": true
                    },
                    {
                        "id": 9031,
                        "content": "<p>Cost of test file storage on EBS &lt; Cost of test file storage on S3 Standard &lt; Cost of test file storage on EFS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2163,
            "attributes": {
                "createdAt": "2023-09-09T20:33:33.763Z",
                "updatedAt": "2023-09-09T20:33:33.763Z",
                "content": "<p>A retail company uses Amazon EC2 instances, API Gateway, Amazon RDS, Elastic Load Balancer and CloudFront services. To improve the security of these services, the Risk Advisory group has suggested a feasibility check for using the Amazon GuardDuty service.</p>\n\n<p>Which of the following would you identify as data sources supported by GuardDuty?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>VPC Flow Logs, DNS logs, CloudTrail events</strong> - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. With the cloud, the collection and aggregation of account and network activities is simplified, but it can be time-consuming for security teams to continuously analyze event log data for potential threats. With GuardDuty, you now have an intelligent and cost-effective option for continuous threat detection in AWS. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats.</p>\n\n<p>GuardDuty analyzes tens of billions of events across multiple AWS data sources, such as AWS CloudTrail events, Amazon VPC Flow Logs, and DNS logs.</p>\n\n<p>With a few clicks in the AWS Management Console, GuardDuty can be enabled with no software or hardware to deploy or maintain. By integrating with Amazon EventBridge Events, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems.</p>\n\n<p>How GuardDuty works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png\">\nvia - <a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>VPC Flow Logs, API Gateway logs, S3 access logs</strong></p>\n\n<p><strong>ELB logs, DNS logs, CloudTrail events</strong></p>\n\n<p><strong>CloudFront logs, API Gateway logs, CloudTrail events</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n",
                "options": [
                    {
                        "id": 9032,
                        "content": "<p>ELB logs, DNS logs, CloudTrail events</p>",
                        "isValid": false
                    },
                    {
                        "id": 9033,
                        "content": "<p>CloudFront logs, API Gateway logs, CloudTrail events</p>",
                        "isValid": false
                    },
                    {
                        "id": 9034,
                        "content": "<p>VPC Flow Logs, DNS logs, CloudTrail events</p>",
                        "isValid": true
                    },
                    {
                        "id": 9035,
                        "content": "<p>VPC Flow Logs, API Gateway logs, S3 access logs</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2164,
            "attributes": {
                "createdAt": "2023-09-09T20:33:33.860Z",
                "updatedAt": "2023-09-09T20:33:33.860Z",
                "content": "<p>A retail company's dynamic website is hosted using on-premises servers in its data center in the United States. The company is launching its website in Asia, and it wants to optimize the website loading times for new users in Asia. The website's backend must remain in the United States. The website is being launched in a few days, and an immediate solution is needed.</p>\n\n<p>What would you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon CloudFront with a custom origin pointing to the on-premises servers</strong></p>\n\n<p>Amazon CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. Amazon CloudFront uses standard cache control headers you set on your files to identify static and dynamic content. You can use different origins for different types of content on a single site – e.g. Amazon S3 for static objects, Amazon EC2 for dynamic content, and custom origins for third-party content.</p>\n\n<p>Amazon CloudFront:\n<img src=\"https://d1.awsstatic.com/products/cloudfront/product-page-diagram_CloudFront_HIW.475cd71e52ebbb9acbe55fd1b242c75ebb619a2e.png\">\nvia - <a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p>\n\n<p>An origin server stores the original, definitive version of your objects. If you're serving content over HTTP, your origin server is either an Amazon S3 bucket or an HTTP server, such as a web server. Your HTTP server can run on an Amazon Elastic Compute Cloud (Amazon EC2) instance or on a server that you manage; these servers are also known as custom origins.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q62-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a></p>\n\n<p>Amazon CloudFront employs a global network of edge locations and regional edge caches that cache copies of your content close to your viewers. Amazon CloudFront ensures that end-user requests are served by the closest edge location. As a result, viewer requests travel a short distance, improving performance for your viewers. Therefore for the given use case, the users in Asia will enjoy a low latency experience while using the website even though the on-premises servers continue to be in the US.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Route 53</strong> - This option has been added as a distractor. CloudFront cannot have a custom origin pointing to the DNS record of the website on Route 53.</p>\n\n<p><strong>Migrate the website to Amazon S3. Use cross-Region replication between AWS Regions in the US and Asia</strong> - The use case states that the company operates a dynamic website. You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. By contrast, a dynamic website relies on server-side processing, including server-side scripts, such as PHP, JSP, or ASP.NET. Amazon S3 does not support server-side scripting, but AWS has other resources for hosting dynamic websites.  So this option is incorrect.</p>\n\n<p><strong>Leverage a Route 53 geo-proximity routing policy pointing to on-premises servers</strong> - Since the on-premises servers continue to be in the US, so even using a Route 53 geo-proximity routing policy that directs the users in Asia to the on-premises servers in the US would not reduce the latency for the users in Asia. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html</a></p>\n",
                "options": [
                    {
                        "id": 9036,
                        "content": "<p>Leverage a Route 53 geo-proximity routing policy pointing to on-premises servers</p>",
                        "isValid": false
                    },
                    {
                        "id": 9037,
                        "content": "<p>Migrate the website to Amazon S3. Use cross-Region replication between AWS Regions in the US and Asia</p>",
                        "isValid": false
                    },
                    {
                        "id": 9038,
                        "content": "<p>Use Amazon CloudFront with a custom origin pointing to the on-premises servers</p>",
                        "isValid": true
                    },
                    {
                        "id": 9039,
                        "content": "<p>Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Route 53</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2165,
            "attributes": {
                "createdAt": "2023-09-09T20:33:33.950Z",
                "updatedAt": "2023-09-09T20:33:33.950Z",
                "content": "<p>A financial services company recently launched an initiative to improve the security of its AWS resources and it had enabled AWS Shield Advanced across multiple AWS accounts owned by the company. Upon analysis, the company has found that the costs incurred are much higher than expected.</p>\n\n<p>Which of the following would you attribute as the underlying reason for the unexpectedly high costs for AWS Shield Advanced service?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once</strong> - If your organization has multiple AWS accounts, then you can subscribe multiple AWS Accounts to AWS Shield Advanced by individually enabling it on each account using the AWS Management Console or API. You will pay the monthly fee once as long as the AWS accounts are all under a single consolidated billing, and you own all the AWS accounts and resources in those accounts.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs</strong> - AWS Shield Advanced does offer protection to resources outside of AWS. This should not cause unexpected spike in billing costs.</p>\n\n<p><strong>AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs</strong> - AWS Shield Standard is automatically enabled for all AWS customers at no additional cost. AWS Shield Advanced is an optional paid service.</p>\n\n<p><strong>Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts</strong> - This option has been added as a distractor. Savings Plans is a flexible pricing model that offers low prices on EC2, Lambda, and Fargate usage, in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a 1 or 3 year term. Savings Plans is not applicable for the AWS Shield Advanced service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/shield/faqs/\">https://aws.amazon.com/shield/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/savingsplans/faq/\">https://aws.amazon.com/savingsplans/faq/</a></p>\n",
                "options": [
                    {
                        "id": 9040,
                        "content": "<p>Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once</p>",
                        "isValid": true
                    },
                    {
                        "id": 9041,
                        "content": "<p>AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs</p>",
                        "isValid": false
                    },
                    {
                        "id": 9042,
                        "content": "<p>Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts</p>",
                        "isValid": false
                    },
                    {
                        "id": 9043,
                        "content": "<p>AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2166,
            "attributes": {
                "createdAt": "2023-09-09T20:33:34.064Z",
                "updatedAt": "2023-09-09T20:33:34.064Z",
                "content": "<p>The DevOps team at an e-commerce company wants to perform some maintenance work on a specific EC2 instance that is part of an Auto Scaling group using a step scaling policy. The team is facing a maintenance challenge - every time the team deploys a maintenance patch, the instance health check status shows as out of service for a few minutes. This causes the Auto Scaling group to provision another replacement instance immediately.</p>\n\n<p>As a solutions architect, which are the MOST time/resource efficient steps that you would recommend so that the maintenance work can be completed at the earliest? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service</strong> - You can put an instance that is in the InService state into the Standby state, update some software or troubleshoot the instance, and then return the instance to service. Instances that are on standby are still part of the Auto Scaling group, but they do not actively handle application traffic.</p>\n\n<p>How Standby State Works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q32-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html</a></p>\n\n<p><strong>Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again</strong> - The ReplaceUnhealthy process terminates instances that are marked as unhealthy and then creates new instances to replace them. Amazon EC2 Auto Scaling stops replacing instances that are marked as unhealthy. Instances that fail EC2 or Elastic Load Balancing health checks are still marked as unhealthy. As soon as you resume the ReplaceUnhealthly process, Amazon EC2 Auto Scaling replaces instances that were marked unhealthy while this process was suspended.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Take a snapshot of the instance, create a new AMI and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue</strong> - Taking the snapshot of the existing instance to create a new AMI and then creating a new instance in order to apply the maintenance patch is not time/resource optimal, hence this option is ruled out.</p>\n\n<p><strong>Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy</strong> - It's not recommended to delete the Auto Scaling group just to apply a maintenance patch on a specific instance.</p>\n\n<p><strong>Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again</strong> - Amazon EC2 Auto Scaling does not execute scaling actions that are scheduled to run during the suspension period. This option is not relevant to the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html</a></p>\n",
                "options": [
                    {
                        "id": 9044,
                        "content": "<p>Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again</p>",
                        "isValid": true
                    },
                    {
                        "id": 9045,
                        "content": "<p>Take a snapshot of the instance, create a new AMI and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue</p>",
                        "isValid": false
                    },
                    {
                        "id": 9046,
                        "content": "<p>Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 9047,
                        "content": "<p>Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service</p>",
                        "isValid": true
                    },
                    {
                        "id": 9048,
                        "content": "<p>Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2167,
            "attributes": {
                "createdAt": "2023-09-09T20:33:34.177Z",
                "updatedAt": "2023-09-09T20:33:34.177Z",
                "content": "<p>One of the biggest football leagues in Europe has granted the distribution rights for live streaming its matches in the US to a silicon valley based streaming services company. As per the terms of distribution, the company must make sure that only users from the US are able to live stream the matches on their platform. Users from other countries in the world must be denied access to these live-streamed matches.</p>\n\n<p>Which of the following options would allow the company to enforce these streaming restrictions? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights</strong></p>\n\n<p>Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights.</p>\n\n<p><strong>Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront web distribution</strong></p>\n\n<p>You can use georestriction, also known as geo-blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront web distribution. When a user requests your content, CloudFront typically serves the requested content regardless of where the user is located. If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following:\nAllow your users to access your content only if they're in one of the countries on a whitelist of approved countries.\nPrevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries. So this option is also correct.</p>\n\n<p>Route 53 Routing Policy Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q38-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Route 53 based latency routing policy to restrict distribution of content to only the locations in which you have distribution rights</strong> - Use latency based routing when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server.</p>\n\n<p><strong>Use Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights</strong> - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software.</p>\n\n<p><strong>Use Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights</strong> - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records</p>\n\n<p>Weighted routing or failover routing or latency routing cannot be used to restrict the distribution of content to only the locations in which you have distribution rights. So all three options above are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo</a></p>\n",
                "options": [
                    {
                        "id": 9049,
                        "content": "<p>Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront web distribution</p>",
                        "isValid": true
                    },
                    {
                        "id": 9050,
                        "content": "<p>Use Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights</p>",
                        "isValid": false
                    },
                    {
                        "id": 9051,
                        "content": "<p>Use Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights</p>",
                        "isValid": true
                    },
                    {
                        "id": 9052,
                        "content": "<p>Use Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights</p>",
                        "isValid": false
                    },
                    {
                        "id": 9053,
                        "content": "<p>Use Route 53 based latency routing policy to restrict distribution of content to only the locations in which you have distribution rights</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2168,
            "attributes": {
                "createdAt": "2023-09-09T20:33:34.267Z",
                "updatedAt": "2023-09-09T20:33:34.267Z",
                "content": "<p>The payroll department at a company initiates several computationally intensive workloads on EC2 instances at a designated hour on the last day of every month. The payroll department has noticed a trend of severe performance lag during this hour. The engineering team has figured out a solution by using Auto Scaling Group for these EC2 instances and making sure that 10 EC2 instances are available during this peak usage hour. For normal operations only 2 EC2 instances are enough to cater to the workload.</p>\n\n<p>As a solutions architect, which of the following steps would you recommend to implement the solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour</strong></p>\n\n<p>Scheduled scaling allows you to set your own scaling schedule. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the predictable traffic patterns of your web application. Scaling actions are performed automatically as a function of time and date.</p>\n\n<p>A scheduled action sets the minimum, maximum, and desired sizes to what is specified by the scheduled action at the time specified by the scheduled action. For the given use case, the correct solution is to set the desired capacity to 10. When we want to specify a range of instances, then we must use min and max values.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour</strong> - As mentioned earlier in the explanation, only when we want to specify a range of instances, then we must use min and max values. As the given use-case requires exactly 10 instances to be available during the peak hour, so we must set the desired capacity to 10. Hence this option is incorrect.</p>\n\n<p><strong>Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour</strong></p>\n\n<p><strong>Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour</strong></p>\n\n<p>Target tracking policy or simple tracking policy cannot be used to effect a scaling action at a certain designated hour. Both these options have been added as distractors.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p>\n",
                "options": [
                    {
                        "id": 9054,
                        "content": "<p>Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour</p>",
                        "isValid": true
                    },
                    {
                        "id": 9055,
                        "content": "<p>Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour</p>",
                        "isValid": false
                    },
                    {
                        "id": 9056,
                        "content": "<p>Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour</p>",
                        "isValid": false
                    },
                    {
                        "id": 9057,
                        "content": "<p>Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2169,
            "attributes": {
                "createdAt": "2023-09-09T20:33:34.373Z",
                "updatedAt": "2023-09-09T20:33:34.373Z",
                "content": "<p>A major bank is using SQS to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order.</p>\n\n<p>Which of the following options can be used to implement this system?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon SQS FIFO queue in batch mode of 4 messages per operation to process the messages at the peak rate</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues - Standard queues vs FIFO queues.</p>\n\n<p>For FIFO queues, the order in which messages are sent and received is strictly preserved (i.e. First-In-First-Out). On the other hand, the standard SQS queues offer best-effort ordering. This means that occasionally, messages might be delivered in an order different from which they were sent.</p>\n\n<p>By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). When you batch 10 messages per operation (maximum), FIFO queues can support up to 3,000 messages per second. Therefore you need to process 4 messages per operation so that the FIFO queue can support up to 1200 messages per second, which is well within the peak rate.</p>\n\n<p>FIFO Queues Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon SQS standard queue to process the messages</strong> - As messages need to be processed in order, therefore standard queues are ruled out.</p>\n\n<p><strong>Use Amazon SQS FIFO queue to process the messages</strong> - By default, FIFO queues support up to 300 messages per second and this is not sufficient to meet the message processing throughput per the given use-case. Hence this option is incorrect.</p>\n\n<p><strong>Use Amazon SQS FIFO queue in batch mode of 2 messages per operation to process the messages at the peak rate</strong> - As mentioned earlier in the explanation, you need to use FIFO queues in batch mode and process 4 messages per operation, so that the FIFO queue can support up to 1200 messages per second. With 2 messages per operation, you can only support up to 600 messages per second.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p>\n",
                "options": [
                    {
                        "id": 9058,
                        "content": "<p>Use Amazon SQS standard queue to process the messages</p>",
                        "isValid": false
                    },
                    {
                        "id": 9059,
                        "content": "<p>Use Amazon SQS FIFO queue in batch mode of 4 messages per operation to process the messages at the peak rate</p>",
                        "isValid": true
                    },
                    {
                        "id": 9060,
                        "content": "<p>Use Amazon SQS FIFO queue in batch mode of 2 messages per operation to process the messages at the peak rate</p>",
                        "isValid": false
                    },
                    {
                        "id": 9061,
                        "content": "<p>Use Amazon SQS FIFO queue to process the messages</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2170,
            "attributes": {
                "createdAt": "2023-09-09T20:33:34.471Z",
                "updatedAt": "2023-09-09T20:33:34.471Z",
                "content": "<p>An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on S3, runs into hundreds of Terabytes and should be available with millisecond latency.</p>\n\n<p>As a solutions architect, which is the MOST cost-effective storage class that you would recommend to be used for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon S3 Standard-Infrequent Access (S3 Standard-IA)</strong></p>\n\n<p>Since the data is accessed only twice in a financial year but needs rapid access when required, the most cost-effective storage class for this use-case is S3 Standard-IA. S3 Standard-IA storage class is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. Standard-IA is designed for 99.9% availability compared to 99.99% availability of S3 Standard. However, the report creation process has failover and retry scenarios built into the workflow, so in case the data is not available owing to the 99.9% availability of S3 Standard-IA, the job will be auto re-invoked till data is successfully retrieved. Therefore this is the correct option.</p>\n\n<p>S3 Storage Classes Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q15-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 Standard</strong> - S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. As described above, S3 Standard-IA storage is a better fit than S3 Standard, hence using S3 standard is ruled out for the given use-case.</p>\n\n<p><strong>Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)</strong> - For a small monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access patterns and automatically moves objects that have not been accessed to lower-cost access tiers. The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Intelligent-Tiering, with a low per GB storage price and per GB retrieval fee. Moreover, Standard-IA has the same availability as that of S3 Intelligent-Tiering. So, it's cost-efficient to use S3 Standard-IA instead of S3 Intelligent-Tiering.</p>\n\n<p><strong>Amazon S3 Glacier Deep Archive</strong> - S3 Glacier Deep Archive is a secure, durable, and low-cost storage class for data archiving. S3 Glacier Deep Archive does not support millisecond latency, so this option is ruled out.</p>\n\n<p>For more details on the durability, availability, cost and access latency - please review this reference link:\n<a href=\"https://aws.amazon.com/s3/storage-classes\">https://aws.amazon.com/s3/storage-classes</a></p>\n",
                "options": [
                    {
                        "id": 9062,
                        "content": "<p>Amazon S3 Standard</p>",
                        "isValid": false
                    },
                    {
                        "id": 9063,
                        "content": "<p>Amazon S3 Glacier Deep Archive</p>",
                        "isValid": false
                    },
                    {
                        "id": 9064,
                        "content": "<p>Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9065,
                        "content": "<p>Amazon S3 Standard-Infrequent Access (S3 Standard-IA)</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2171,
            "attributes": {
                "createdAt": "2023-09-09T20:33:34.634Z",
                "updatedAt": "2023-09-09T20:33:34.634Z",
                "content": "<p>A leading carmaker would like to build a new car-as-a-sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the carmaker does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data.</p>\n\n<p>Given these constraints, which of the following solutions is the BEST fit to develop this car-as-a-sensor service?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Ingest the sensor data in an Amazon SQS standard queue, which is polled by a Lambda function in batches and the data is written into an auto-scaled DynamoDB table for downstream processing</strong></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>AWS manages all ongoing operations and underlying infrastructure needed to provide a highly available and scalable message queuing service. With SQS, there is no upfront cost, no need to acquire, install, and configure messaging software, and no time-consuming build-out and maintenance of supporting infrastructure. SQS queues are dynamically created and scale automatically so you can build and grow applications quickly and efficiently.</p>\n\n<p>As there is no need to manually provision the capacity, so this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Ingest the sensor data in Kinesis Data Firehose, which directly writes the data into an auto-scaled DynamoDB table for downstream processing</strong></p>\n\n<p>Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic.</p>\n\n<p>Firehose cannot directly write into a DynamoDB table, so this option is incorrect.</p>\n\n<p><strong>Ingest the sensor data in an Amazon SQS standard queue, which is polled by an application running on an EC2 instance and the data is written into an auto-scaled DynamoDB table for downstream processing</strong></p>\n\n<p><strong>Ingest the sensor data in a Kinesis Data Streams, which is polled by an application running on an EC2 instance and the data is written into an auto-scaled DynamoDB table for downstream processing</strong></p>\n\n<p>Using an application on an EC2 instance is ruled out as the carmaker wants to use fully serverless components. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9066,
                        "content": "<p>Ingest the sensor data in Kinesis Data Streams, which is polled by an application running on an EC2 instance and the data is written into an auto-scaled DynamoDB table for downstream processing</p>",
                        "isValid": false
                    },
                    {
                        "id": 9067,
                        "content": "<p>Ingest the sensor data in Kinesis Data Firehose, which directly writes the data into an auto-scaled DynamoDB table for downstream processing</p>",
                        "isValid": false
                    },
                    {
                        "id": 9068,
                        "content": "<p>Ingest the sensor data in an Amazon SQS standard queue, which is polled by a Lambda function in batches and the data is written into an auto-scaled DynamoDB table for downstream processing</p>",
                        "isValid": true
                    },
                    {
                        "id": 9069,
                        "content": "<p>Ingest the sensor data in an Amazon SQS standard queue, which is polled by an application running on an EC2 instance and the data is written into an auto-scaled DynamoDB table for downstream processing</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2172,
            "attributes": {
                "createdAt": "2023-09-09T20:33:34.749Z",
                "updatedAt": "2023-09-09T20:33:34.749Z",
                "content": "<p>A large financial institution operates an on-premises data center with hundreds of PB of data managed on Microsoft’s Distributed File System (DFS). The CTO wants the organization to transition into a hybrid cloud environment and run data-intensive analytics workloads that support DFS.</p>\n\n<p>Which of the following AWS services can facilitate the migration of these workloads?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon FSx for Windows File Server</strong></p>\n\n<p>Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol.  It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration.\nAmazon FSx supports the use of Microsoft’s Distributed File System (DFS) to organize shares into a single folder structure up to hundreds of PB in size. So this option is correct.</p>\n\n<p>How FSx for Windows File Server Works:\n<img src=\"https://d1.awsstatic.com/r2018/b/FSx-Windows/FSx_Windows_File_Server_How-it-Works.9396055e727c3903de991e7f3052ec295c86f274.png\">\nvia - <a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon FSx for Lustre</strong></p>\n\n<p>Amazon FSx for Lustre makes it easy and cost-effective to launch and run the world’s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. Amazon FSx enables you to use Lustre file systems for any workload where storage speed matters.\nFSx for Lustre does not support Microsoft’s Distributed File System (DFS), so this option is incorrect.</p>\n\n<p><strong>AWS Managed Microsoft AD</strong></p>\n\n<p>AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use managed Active Directory in the AWS Cloud. AWS Managed Microsoft AD is built on the actual Microsoft Active Directory and does not require you to synchronize or replicate data from your existing Active Directory to the cloud. AWS Managed Microsoft AD does not support Microsoft’s Distributed File System (DFS), so this option is incorrect.</p>\n\n<p><strong>Microsoft SQL Server on Amazon</strong></p>\n\n<p>Microsoft SQL Server on AWS offers you the flexibility to run Microsoft SQL Server database on AWS Cloud. Microsoft SQL Server on AWS does not support Microsoft’s Distributed File System (DFS), so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p>\n",
                "options": [
                    {
                        "id": 9070,
                        "content": "<p>AWS Managed Microsoft AD</p>",
                        "isValid": false
                    },
                    {
                        "id": 9071,
                        "content": "<p>Amazon FSx for Lustre</p>",
                        "isValid": false
                    },
                    {
                        "id": 9072,
                        "content": "<p>Amazon FSx for Windows File Server</p>",
                        "isValid": true
                    },
                    {
                        "id": 9073,
                        "content": "<p>Microsoft SQL Server on Amazon</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2173,
            "attributes": {
                "createdAt": "2023-09-09T20:33:34.885Z",
                "updatedAt": "2023-09-09T20:33:34.885Z",
                "content": "<p>A company has a web application that runs 24*7 in the production environment. The development team at the company runs a clone of the same application in the dev environment for up to 8 hours every day. The company wants to build the MOST cost-optimal solution by deploying these applications using the best-fit pricing options for EC2 instances.</p>\n\n<p>What would you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use reserved EC2 instances for the production application and on-demand instances for the dev application</strong></p>\n\n<p>There are multiple pricing options for EC2 instances, such as On-Demand, Savings Plans, Reserved Instances, and Spot Instances.</p>\n\n<p>EC2 Instances Pricing Options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q47-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>Amazon EC2 Reserved Instances (RI) provide a significant discount (up to 72%) compared to On-Demand pricing and provide a capacity reservation when used in a specific Availability Zone. RIs provide you with a significant discount (up to 72%) compared to On-Demand instance pricing. You have the flexibility to change families, OS types, and tenancies while benefitting from RI pricing when you use Convertible RIs.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q47-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>For the given use case, you can use reserved EC2 instances for the production application as it is run 24*7. This way you can get a 72% discount if you avail a 3-year term. You can use on-demand instances for the dev application since it is only used for up to 8 hours per day. On-demand offers the flexibility to only pay for the EC2 instance when it is being used (0 to 8 hours for the given use case).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use reserved EC2 instances for the production application and spot block instances for the dev application</strong> - Spot blocks can only be used for a span of up to 6 hours, so this option does not meet the requirements of the given use case where the dev application can be up and running up to 8 hours. You should also note that AWS has stopped offering Spot blocks to new customers.</p>\n\n<p><strong>Use reserved EC2 instances for the production application and spot instances for the dev application</strong></p>\n\n<p><strong>Use on-demand EC2 instances for the production application and spot instances for the dev application</strong></p>\n\n<p>Amazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS cloud. Spot Instances are available at up to a 90% discount compared to On-Demand prices. You can use Spot Instances for various stateless, fault-tolerant, or flexible applications.</p>\n\n<p><img src=\"https://d1.awsstatic.com/products/EC2/Spot/product-page-diagram_EC2-Spot-Instances.6c3c51f4c6a28cd71d8fef8231510b5619e84eea.png\">\nvia - <a href=\"https://aws.amazon.com/ec2/spot/\">https://aws.amazon.com/ec2/spot/</a></p>\n\n<p>Spot instances can be taken back by AWS with two minutes of notice, so spot instances cannot be reliably used for running the dev application (which can be up and running for up to 8 hours). So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-ec2-spot-blocks-for-defined-duration-workloads/\">https://aws.amazon.com/blogs/aws/new-ec2-spot-blocks-for-defined-duration-workloads/</a></p>\n\n<p><a href=\"https://aws.amazon.com/ec2/spot/\">https://aws.amazon.com/ec2/spot/</a></p>\n",
                "options": [
                    {
                        "id": 9074,
                        "content": "<p>Use reserved EC2 instances for the production application and spot block instances for the dev application</p>",
                        "isValid": false
                    },
                    {
                        "id": 9075,
                        "content": "<p>Use reserved EC2 instances for the production application and on-demand instances for the dev application</p>",
                        "isValid": true
                    },
                    {
                        "id": 9076,
                        "content": "<p>Use on-demand EC2 instances for the production application and spot instances for the dev application</p>",
                        "isValid": false
                    },
                    {
                        "id": 9077,
                        "content": "<p>Use reserved EC2 instances for the production application and spot instances for the dev application</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2174,
            "attributes": {
                "createdAt": "2023-09-09T20:33:35.004Z",
                "updatedAt": "2023-09-09T20:33:35.004Z",
                "content": "<p>An ivy-league university is assisting NASA to find potential landing sites for exploration vehicles of unmanned missions to our neighboring planets. The university uses High Performance Computing (HPC) driven application architecture to identify these landing sites.</p>\n\n<p>Which of the following EC2 instance topologies should this application be deployed on?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput</strong></p>\n\n<p>The key thing to understand in this question is that HPC workloads need to achieve low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. Cluster placement groups pack instances close together inside an Availability Zone. These are recommended for applications that benefit from low network latency, high network throughput, or both. Therefore this option is the correct answer.</p>\n\n<p>Cluster Placement Group:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively</strong> - A partition placement group spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. A partition placement group can have a maximum of seven partitions per Availability Zone.\nSince a partition placement group can have partitions in multiple Availability Zones in the same region, therefore instances will not have low-latency network performance. Hence the partition placement group is not the right fit for HPC applications.</p>\n\n<p>Partition Placement Group:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q23-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n\n<p><strong>The EC2 instances should be deployed in a spread placement group so that there are no correlated failures</strong> - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The instances are placed across distinct underlying hardware to reduce correlated failures. You can have a maximum of seven running instances per Availability Zone per group. Since a spread placement group can span multiple Availability Zones in the same Region, therefore instances will not have low-latency network performance. Hence spread placement group is not the right fit for HPC applications.</p>\n\n<p>Spread Placement Group:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q23-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n\n<p><strong>The EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements</strong> - An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling. You do not use Auto Scaling groups per se to meet HPC requirements.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 9078,
                        "content": "<p>The EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput</p>",
                        "isValid": true
                    },
                    {
                        "id": 9079,
                        "content": "<p>The EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively</p>",
                        "isValid": false
                    },
                    {
                        "id": 9080,
                        "content": "<p>The EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements</p>",
                        "isValid": false
                    },
                    {
                        "id": 9081,
                        "content": "<p>The EC2 instances should be deployed in a spread placement group so that there are no correlated failures</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2175,
            "attributes": {
                "createdAt": "2023-09-09T20:33:35.112Z",
                "updatedAt": "2023-09-09T20:33:35.112Z",
                "content": "<p>The engineering team at a Spanish professional football club has built a notification system for its website using Amazon SNS notifications which are then handled by a Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak football season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the website.</p>\n\n<p>As a solutions architect, which of the following would you suggest as the BEST possible solution to this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for Lambda, so the team needs to contact AWS support to raise the account limit</strong></p>\n\n<p>Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.</p>\n\n<p>How SNS Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/SNS/product-page-diagram_SNS_how-it-works_1.53a464980bf0d5a868b141e9a8b2acf12abc503f.png\">\nvia - <a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n\n<p>With AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consume—there’s no charge when your code isn’t running.</p>\n\n<p>AWS Lambda currently supports 1000 concurrent executions per AWS account per region. If your Amazon SNS message deliveries to AWS Lambda contribute to crossing these concurrency quotas, your Amazon SNS message deliveries will be throttled. You need to contact AWS support to raise the account limit. Therefore this option is correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit</strong> - Amazon SNS leverages the proven AWS cloud to dynamically scale with your application. You don't need to contact AWS support, as SNS is a fully managed service, taking care of the heavy lifting related to capacity planning, provisioning, monitoring, and patching. Therefore, this option is incorrect.</p>\n\n<p><strong>The engineering team needs to provision more servers running the SNS service</strong></p>\n\n<p><strong>The engineering team needs to provision more servers running the Lambda service</strong></p>\n\n<p>As both Lambda and SNS are serverless and fully managed services, the engineering team cannot provision more servers. Both of these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sns/faqs/\">https://aws.amazon.com/sns/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9082,
                        "content": "<p>The engineering team needs to provision more servers running the SNS service</p>",
                        "isValid": false
                    },
                    {
                        "id": 9083,
                        "content": "<p>The engineering team needs to provision more servers running the Lambda service</p>",
                        "isValid": false
                    },
                    {
                        "id": 9084,
                        "content": "<p>Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for Lambda, so the team needs to contact AWS support to raise the account limit</p>",
                        "isValid": true
                    },
                    {
                        "id": 9085,
                        "content": "<p>Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2176,
            "attributes": {
                "createdAt": "2023-09-09T20:33:35.203Z",
                "updatedAt": "2023-09-09T20:33:35.203Z",
                "content": "<p>The engineering team at an in-home fitness company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Power the on-demand, live leaderboard using ElastiCache Redis as it meets the in-memory, high availability, low latency requirements</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.\nElastiCache for Redis can be used to power the live leaderboard, so this option is correct.</p>\n\n<p>ElastiCache for Redis Overview:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\"></p>\n\n<p><strong>Power the on-demand, live leaderboard using DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.\nDAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. So DynamoDB with DAX can be used to power the live leaderboard.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Power the on-demand, live leaderboard using AWS Neptune as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Neptune is not an in-memory database, so this option is not correct.</p>\n\n<p><strong>Power the on-demand, live leaderboard using DynamoDB as it meets the in-memory, high availability, low latency requirements</strong> - DynamoDB is not an in-memory database, so this option is not correct.</p>\n\n<p><strong>Power the on-demand, live leaderboard using RDS Aurora as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. Aurora is not an in-memory database, so this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n",
                "options": [
                    {
                        "id": 9086,
                        "content": "<p>Power the on-demand, live leaderboard using DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements</p>",
                        "isValid": true
                    },
                    {
                        "id": 9087,
                        "content": "<p>Power the on-demand, live leaderboard using RDS Aurora as it meets the in-memory, high availability, low latency requirements</p>",
                        "isValid": false
                    },
                    {
                        "id": 9088,
                        "content": "<p>Power the on-demand, live leaderboard using ElastiCache Redis as it meets the in-memory, high availability, low latency requirements</p>",
                        "isValid": true
                    },
                    {
                        "id": 9089,
                        "content": "<p>Power the on-demand, live leaderboard using DynamoDB as it meets the in-memory, high availability, low latency requirements</p>",
                        "isValid": false
                    },
                    {
                        "id": 9090,
                        "content": "<p>Power the on-demand, live leaderboard using AWS Neptune as it meets the in-memory, high availability, low latency requirements</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2177,
            "attributes": {
                "createdAt": "2023-09-09T20:33:35.309Z",
                "updatedAt": "2023-09-09T20:33:35.309Z",
                "content": "<p>An IT consultant is helping the owner of a medium-sized business set up an AWS account. What are the security recommendations he must follow while creating the AWS account root user? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Create a strong password for the AWS account root user</strong></p>\n\n<p><strong>Enable Multi Factor Authentication (MFA) for the AWS account root user account</strong></p>\n\n<p>Here are some of the best practices while creating an AWS account root user:</p>\n\n<p>1) Use a strong password to help protect account-level access to the AWS Management Console.\n2) Never share your AWS account root user password or access keys with anyone.\n3) If you do have an access key for your AWS account root user, delete it. If you must keep it, rotate (change) the access key regularly. You should not encrypt the access keys and save them on Amazon S3.\n4) If you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to.\n5) Enable AWS multi-factor authentication (MFA) on your AWS account root user account.</p>\n\n<p>AWS Root Account Security Best Practices:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Encrypt the access keys and save them on Amazon S3</strong> - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Even an encrypted access key for the root user poses a significant security risk. Therefore, this option is incorrect.</p>\n\n<p><strong>Create AWS account root user access keys and share those keys only with the business owner</strong> - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Hence, this option is incorrect.</p>\n\n<p><strong>Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future</strong> - AWS recommends that you should never share your AWS account root user password or access keys with anyone. Sending an email with AWS account root user credentials creates a security risk as it can be misused by anyone reading the email. Hence, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#create-iam-users\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#create-iam-users</a></p>\n",
                "options": [
                    {
                        "id": 9091,
                        "content": "<p>Create a strong password for the AWS account root user</p>",
                        "isValid": true
                    },
                    {
                        "id": 9092,
                        "content": "<p>Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future</p>",
                        "isValid": false
                    },
                    {
                        "id": 9093,
                        "content": "<p>Create AWS account root user access keys and share those keys only with the business owner</p>",
                        "isValid": false
                    },
                    {
                        "id": 9094,
                        "content": "<p>Encrypt the access keys and save them on Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9095,
                        "content": "<p>Enable Multi Factor Authentication (MFA) for the AWS account root user account</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2178,
            "attributes": {
                "createdAt": "2023-09-09T20:33:35.402Z",
                "updatedAt": "2023-09-09T20:33:35.402Z",
                "content": "<p>The engineering team at an e-commerce company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend to the company?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Direct Connect plus VPN to establish a connection between the data center and AWS Cloud</strong></p>\n\n<p>AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations.</p>\n\n<p>With AWS Direct Connect plus VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections.</p>\n\n<p>This solution combines the AWS managed benefits of the VPN solution with low latency, increased bandwidth, more consistent benefits of the AWS Direct Connect solution, and an end-to-end, secure IPsec connection. Therefore, AWS Direct Connect plus VPN is the correct solution for this use-case.</p>\n\n<p>AWS Direct Connect Plus VPN:\n<img src=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/images/image10.png\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use site-to-site VPN to establish a connection between the data center and AWS Cloud</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.\nHowever, Site-to-site VPN cannot provide low latency and high throughput connection, therefore this option is ruled out.</p>\n\n<p><strong>Use VPC transit gateway to establish a connection between the data center and AWS Cloud</strong> - A transit gateway is a network transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. A transit gateway by itself cannot establish a low latency and high throughput connection between a data center and AWS Cloud. Hence this option is incorrect.</p>\n\n<p><strong>Use AWS Direct Connect to establish a connection between the data center and AWS Cloud</strong> - AWS Direct Connect by itself cannot provide an encrypted connection between a data center and AWS Cloud, so this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/directconnect/\">https://aws.amazon.com/directconnect/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html</a></p>\n",
                "options": [
                    {
                        "id": 9096,
                        "content": "<p>Use site-to-site VPN to establish a connection between the data center and AWS Cloud</p>",
                        "isValid": false
                    },
                    {
                        "id": 9097,
                        "content": "<p>Use AWS Direct Connect plus VPN to establish a connection between the data center and AWS Cloud</p>",
                        "isValid": true
                    },
                    {
                        "id": 9098,
                        "content": "<p>Use AWS Direct Connect to establish a connection between the data center and AWS Cloud</p>",
                        "isValid": false
                    },
                    {
                        "id": 9099,
                        "content": "<p>Use VPC transit gateway to establish a connection between the data center and AWS Cloud</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2179,
            "attributes": {
                "createdAt": "2023-09-09T20:33:35.501Z",
                "updatedAt": "2023-09-09T20:33:35.501Z",
                "content": "<p>A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The API stores the user data in DynamoDB and any static content, such as images, are served via S3. On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users.</p>\n\n<p>As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</strong></p>\n\n<p>DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second.</p>\n\n<p>DAX is tightly integrated with DynamoDB—you simply provision a DAX cluster, use the DAX client SDK to point your existing DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache DynamoDB reads.</p>\n\n<p>CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content.</p>\n\n<p>So, you can use CloudFront to improve application performance to serve static content from S3.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.</p>\n\n<p>ElastiCache for Redis Overview:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p>Although you can integrate Redis with DynamoDB, it's much more involved than using DAX which is a much better fit.</p>\n\n<p><strong>Enable DAX for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database.</p>\n\n<p>ElastiCache Memcached cannot be used as a cache to serve static content from S3, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n",
                "options": [
                    {
                        "id": 9100,
                        "content": "<p>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 9101,
                        "content": "<p>Enable DAX for DynamoDB and ElastiCache Memcached for S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9102,
                        "content": "<p>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9103,
                        "content": "<p>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2180,
            "attributes": {
                "createdAt": "2023-09-09T20:33:35.614Z",
                "updatedAt": "2023-09-09T20:33:35.614Z",
                "content": "<p>A development team requires permissions to list an S3 bucket and delete objects from that bucket. A systems administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows the principle of least privilege.</p>\n\n<pre><code>    \"Version\": \"2021-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::example-bucket\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n</code></pre>\n\n<p>Which statement should a solutions architect add to the policy to address this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>**</p>\n\n<pre><code>{\n    \"Action\": [\n        \"s3:DeleteObject\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n</code></pre>\n\n<p>**</p>\n\n<p>The main elements of a policy statement are:</p>\n\n<ol>\n<li><p>Effect: Specifies whether the statement will Allow or Deny an action (<code>Allow</code> is the effect defined here).</p></li>\n<li><p>Action: Describes a specific action or actions that will either be allowed or denied to run based on the Effect entered. API actions are unique to each service (<code>DeleteObject</code> is the action defined here).</p></li>\n<li><p>Resource: Specifies the resources—for example, an S3 bucket or objects—that the policy applies to in Amazon Resource Name (ARN) format ( <code>example-bucket/*</code> is the resource defined here).</p></li>\n</ol>\n\n<p>This policy provides the necessary delete permissions on the resources of the S3 bucket to the group.</p>\n\n<p>Incorrect options:</p>\n\n<p>**</p>\n\n<pre><code>{\n    \"Action\": [\n        \"s3:*Object\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n</code></pre>\n\n<p>** - This policy is incorrect as the action value is invalid</p>\n\n<p>**</p>\n\n<pre><code>{\n    \"Action\": [\n        \"s3:*\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n</code></pre>\n\n<p>** - This policy is incorrect since it allows all actions on the resource, which violates the principle of least privilege, as required by the given use case.</p>\n\n<p>**</p>\n\n<pre><code>{\n    \"Action\": [\n        \"s3:DeleteObject\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n</code></pre>\n\n<p>** - This is incorrect, as the resource name is incorrect. It should have a <code>/*</code> after the bucket name.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/techniques-for-writing-least-privilege-iam-policies/\">https://aws.amazon.com/blogs/security/techniques-for-writing-least-privilege-iam-policies/</a></p>\n",
                "options": [
                    {
                        "id": 9104,
                        "content": "<pre><code>{\n    \"Action\": [\n        \"s3:DeleteObject\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n</code></pre>",
                        "isValid": true
                    },
                    {
                        "id": 9105,
                        "content": "<pre><code>{\n    \"Action\": [\n        \"s3:DeleteObject\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 9106,
                        "content": "<pre><code>{\n    \"Action\": [\n        \"s3:*\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 9107,
                        "content": "<pre><code>{\n    \"Action\": [\n        \"s3:*Object\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n</code></pre>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2181,
            "attributes": {
                "createdAt": "2023-09-09T20:33:35.724Z",
                "updatedAt": "2023-09-09T20:33:35.724Z",
                "content": "<p>Which of the following features of an Amazon S3 bucket can only be suspended once they have been enabled?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Versioning</strong></p>\n\n<p>Once you version-enable a bucket, it can never return to an unversioned state. Versioning can only be suspended once it has been enabled.</p>\n\n<p>Versioning Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Server Access Logging</strong></p>\n\n<p><strong>Static Website Hosting</strong></p>\n\n<p><strong>Requester Pays</strong></p>\n\n<p>Server Access Logging, Static Website Hosting and Requester Pays features can be disabled even after they have been enabled.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n",
                "options": [
                    {
                        "id": 9108,
                        "content": "<p>Static Website Hosting</p>",
                        "isValid": false
                    },
                    {
                        "id": 9109,
                        "content": "<p>Server Access Logging</p>",
                        "isValid": false
                    },
                    {
                        "id": 9110,
                        "content": "<p>Requester Pays</p>",
                        "isValid": false
                    },
                    {
                        "id": 9111,
                        "content": "<p>Versioning</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2182,
            "attributes": {
                "createdAt": "2023-09-09T20:33:35.808Z",
                "updatedAt": "2023-09-09T20:33:35.808Z",
                "content": "<p>A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Elastic Container Service (ECS) with the EC2 launch type compared to the Elastic Container Service (ECS) with the Fargate launch type.</p>\n\n<p>Which of the following is correct regarding the pricing for these two services?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests</strong></p>\n\n<p>Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. ECS allows you to easily run, scale, and secure Docker container applications on AWS.</p>\n\n<p>ECS Overview:\n<img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png\">\nvia - <a href=\"https://aws.amazon.com/ecs/\">https://aws.amazon.com/ecs/</a></p>\n\n<p>With the Fargate launch type, you pay for the amount of vCPU and memory resources that your containerized application requests. vCPU and memory resources are calculated from the time your container images are pulled until the Amazon ECS Task* terminates, rounded up to the nearest second.\nWith the EC2 launch type, there is no additional charge for the EC2 launch type. You pay for AWS resources (e.g. EC2 instances or EBS volumes) you create to store and run your application.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Both ECS with EC2 launch type and ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests</strong></p>\n\n<p><strong>Both ECS with EC2 launch type and ECS with Fargate launch type are charged based on EC2 instances and EBS volumes used</strong></p>\n\n<p>As mentioned above - with the Fargate launch type, you pay for the amount of vCPU and memory resources. With EC2 launch type, you pay for AWS resources (e.g. EC2 instances or EBS volumes). Hence both these options are incorrect.</p>\n\n<p><strong>Both ECS with EC2 launch type and ECS with Fargate launch type are just charged based on Elastic Container Service used per hour</strong></p>\n\n<p>This is a made-up option and has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ecs/pricing/\">https://aws.amazon.com/ecs/pricing/</a></p>\n",
                "options": [
                    {
                        "id": 9112,
                        "content": "<p>Both ECS with EC2 launch type and ECS with Fargate launch type are charged based on EC2 instances and EBS volumes used</p>",
                        "isValid": false
                    },
                    {
                        "id": 9113,
                        "content": "<p>Both ECS with EC2 launch type and ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests</p>",
                        "isValid": false
                    },
                    {
                        "id": 9114,
                        "content": "<p>ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests</p>",
                        "isValid": true
                    },
                    {
                        "id": 9115,
                        "content": "<p>Both ECS with EC2 launch type and ECS with Fargate launch type are just charged based on Elastic Container Service used per hour</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2183,
            "attributes": {
                "createdAt": "2023-09-09T20:33:35.893Z",
                "updatedAt": "2023-09-09T20:33:35.893Z",
                "content": "<p>A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. The company is looking for AWS services that can be used for buffering or throttling to handle such traffic variations.</p>\n\n<p>Which of the following services can be used to support this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Throttling is the process of limiting the number of requests an authorized program can submit to a given operation in a given amount of time.</p>\n\n<p><strong>Amazon API Gateway, Amazon SQS and Amazon Kinesis</strong> - To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API using the token bucket algorithm, where a token counts for a request. Specifically, API Gateway sets a limit on a steady-state rate and a burst of request submissions against all APIs in your account. In the token bucket algorithm, the burst is the maximum bucket size.</p>\n\n<p>Amazon SQS - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers buffer capabilities to smooth out temporary volume spikes without losing messages or increasing latency.</p>\n\n<p>Amazon Kinesis - Amazon Kinesis is a fully managed, scalable service that can ingest, buffer, and process streaming data in real-time.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon SQS, Amazon SNS and AWS Lambda</strong> - Amazon SQS has the ability to buffer its messages. Amazon Simple Notification Service (SNS) cannot buffer messages and is generally used with SQS to provide the buffering facility. When requests come in faster than your Lambda function can scale, or when your function is at maximum concurrency, additional requests fail as the Lambda throttles those requests with error code 429 status code. So, this combination of services is incorrect.</p>\n\n<p><strong>Amazon Gateway Endpoints, Amazon SQS and Amazon Kinesis</strong> - A Gateway Endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. This cannot help in throttling or buffering of requests. Amazon SQS and Kinesis can buffer incoming data. Since Gateway Endpoint is an incorrect service for throttling or buffering, this option is incorrect.</p>\n\n<p><strong>Elastic Load Balancer, Amazon SQS, AWS Lambda</strong> - Elastic Load Balancer cannot throttle requests. Amazon SQS can be used to buffer messages. When requests come in faster than your Lambda function can scale, or when your function is at maximum concurrency, additional requests fail as the Lambda throttles those requests with error code 429 status code. So, this combination of services is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p>\n",
                "options": [
                    {
                        "id": 9116,
                        "content": "<p>Elastic Load Balancer, Amazon SQS, AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 9117,
                        "content": "<p>Amazon API Gateway, Amazon SQS and Amazon Kinesis</p>",
                        "isValid": true
                    },
                    {
                        "id": 9118,
                        "content": "<p>Amazon SQS, Amazon SNS and AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 9119,
                        "content": "<p>Amazon Gateway Endpoints, Amazon SQS and Amazon Kinesis</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2184,
            "attributes": {
                "createdAt": "2023-09-09T20:33:35.984Z",
                "updatedAt": "2023-09-09T20:33:35.984Z",
                "content": "<p>A social photo-sharing company uses Amazon S3 to store the images uploaded by the users. These images are kept encrypted in S3 by using AWS-KMS and the company manages its own Customer Master Key (CMK) for encryption. A member of the DevOps team accidentally deleted the CMK a day ago, thereby rendering the user's photo data unrecoverable. You have been contacted by the company to consult them on possible solutions to this crisis.</p>\n\n<p>As a solutions architect, which of the following steps would you recommend to solve this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>As the CMK was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the CMK deletion and recover the key</strong></p>\n\n<p>AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. AWS KMS is a secure and resilient service that uses hardware security modules that have been validated under FIPS 140-2.</p>\n\n<p>Deleting a customer master key (CMK) in AWS Key Management Service (AWS KMS) is destructive and potentially dangerous. Therefore, AWS KMS enforces a waiting period. To delete a CMK in AWS KMS you schedule key deletion. You can set the waiting period from a minimum of 7 days up to a maximum of 30 days. The default waiting period is 30 days. During the waiting period, the CMK status and key state is Pending deletion. To recover the CMK, you can cancel key deletion before the waiting period ends. After the waiting period ends you cannot cancel key deletion, and AWS KMS deletes the CMK.</p>\n\n<p>How Deleting Customer Master Keys Works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q35-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Contact AWS support to retrieve the CMK from their backup</strong></p>\n\n<p><strong>The CMK can be recovered by the AWS root account user</strong></p>\n\n<p>The AWS root account user cannot recover CMK and the AWS support does not have access to CMK via any backups. Both these options just serve as distractors.</p>\n\n<p><strong>The company should issue a notification on its web application informing the users about the loss of their data</strong> - This option is not required as the data can be recovered via the cancel key deletion feature.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html</a></p>\n",
                "options": [
                    {
                        "id": 9120,
                        "content": "<p>The company should issue a notification on its web application informing the users about the loss of their data</p>",
                        "isValid": false
                    },
                    {
                        "id": 9121,
                        "content": "<p>Contact AWS support to retrieve the CMK from their backup</p>",
                        "isValid": false
                    },
                    {
                        "id": 9122,
                        "content": "<p>As the CMK was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the CMK deletion and recover the key</p>",
                        "isValid": true
                    },
                    {
                        "id": 9123,
                        "content": "<p>The CMK can be recovered by the AWS root account user</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2185,
            "attributes": {
                "createdAt": "2023-09-09T20:33:36.093Z",
                "updatedAt": "2023-09-09T20:33:36.093Z",
                "content": "<p>The sourcing team at the US headquarters of a global e-commerce company is preparing a spreadsheet of the new product catalog. The spreadsheet is saved on an EFS file system created in us-east-1 region. The sourcing team counterparts from other AWS regions such as Asia Pacific and Europe also want to collaborate on this spreadsheet.</p>\n\n<p>As a solutions architect, what is your recommendation to enable this collaboration with the LEAST amount of operational overhead?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The spreadsheet on the EFS file system can be accessed in other AWS regions by using an inter-region VPC peering connection</strong></p>\n\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources.</p>\n\n<p>Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability. Amazon EC2 instances can access your file system across AZs, regions, and VPCs, while on-premises servers can access using AWS Direct Connect or AWS VPN.</p>\n\n<p>You can connect to Amazon EFS file systems from EC2 instances in other AWS regions using an inter-region VPC peering connection, and from on-premises servers using an AWS VPN connection. So this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region</strong></p>\n\n<p><strong>The spreadsheet data will have to be moved into an RDS MySQL database which can then be accessed from any AWS region</strong></p>\n\n<p>Copying the spreadsheet into S3 or RDS database is not the correct solution as it involves a lot of operational overhead. For RDS, one would need to write custom code to replicate the spreadsheet functionality running off of the database. S3 does not allow in-place edit of an object. Additionally, it's also not POSIX compliant. So one would need to develop a custom application to \"simulate in-place edits\" to support collabaration as per the use-case. So both these options are ruled out.</p>\n\n<p><strong>The spreadsheet will have to be copied into EFS file systems of other AWS regions as EFS is a regional service and it does not allow access from other AWS regions</strong> - Creating copies of the spreadsheet into EFS file systems of other AWS regions would mean no collaboration would be possible between the teams. In this case, each team would work on \"its own file\" instead of a single file accessed and updated by all teams. Hence this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a></p>\n",
                "options": [
                    {
                        "id": 9124,
                        "content": "<p>The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region</p>",
                        "isValid": false
                    },
                    {
                        "id": 9125,
                        "content": "<p>The spreadsheet on the EFS file system can be accessed in other AWS regions by using an inter-region VPC peering connection</p>",
                        "isValid": true
                    },
                    {
                        "id": 9126,
                        "content": "<p>The spreadsheet will have to be copied into EFS file systems of other AWS regions as EFS is a regional service and it does not allow access from other AWS regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 9127,
                        "content": "<p>The spreadsheet data will have to be moved into an RDS MySQL database which can then be accessed from any AWS region</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2186,
            "attributes": {
                "createdAt": "2023-09-09T20:33:36.202Z",
                "updatedAt": "2023-09-09T20:33:36.202Z",
                "content": "<p>A US-based healthcare startup is building an interactive diagnostic tool for COVID-19 related assessments. The users would be required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in S3. The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom.</p>\n\n<p>Which of the following is the BEST solution for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use SSE-KMS to encrypt the user data on S3</strong></p>\n\n<p>AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created.\nSSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Therefore SSE-KMS is the correct solution for this use-case.</p>\n\n<p>Server Side Encryption in S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q22-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use SSE-S3 to encrypt the user data on S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. However this option does not provide the ability to audit trail the usage of the encryption keys.</p>\n\n<p><strong>Use SSE-C to encrypt the user data on S3</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects. However this option does not provide the ability to audit trail the usage of the encryption keys.</p>\n\n<p><strong>Use client-side encryption with client provided keys and then upload the encrypted user data to S3</strong> - Using client-side encryption is ruled out as the startup does not want to provide the encryption keys.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n",
                "options": [
                    {
                        "id": 9128,
                        "content": "<p>Use SSE-S3 to encrypt the user data on S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9129,
                        "content": "<p>Use client-side encryption with client provided keys and then upload the encrypted user data to S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9130,
                        "content": "<p>Use SSE-C to encrypt the user data on S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9131,
                        "content": "<p>Use SSE-KMS to encrypt the user data on S3</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2187,
            "attributes": {
                "createdAt": "2023-09-09T20:33:36.415Z",
                "updatedAt": "2023-09-09T20:33:36.415Z",
                "content": "<p>CloudFront offers a multi-tier cache in the form of regional edge caches that improve latency. However, there are certain content types that bypass the regional edge cache, and go directly to the origin.</p>\n\n<p>Which of the following content types skip the regional edge cache? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Dynamic content, as determined at request time (cache-behavior configured to forward all headers)</strong></p>\n\n<p>Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.</p>\n\n<p>CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content.</p>\n\n<p>Dynamic content, as determined at request time (cache-behavior configured to forward all headers), does not flow through regional edge caches, but goes directly to the origin. So this option is correct.</p>\n\n<p><strong>Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin</strong></p>\n\n<p>Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin from the POPs and do not proxy through the regional edge caches. So this option is also correct.</p>\n\n<p>How CloudFront Delivers Content:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q55-i1.jpg\">\nvia - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html</p>\n\n<p>Incorrect Options:</p>\n\n<p><strong>E-commerce assets such as product photos</strong></p>\n\n<p><strong>User-generated videos</strong></p>\n\n<p><strong>Static content such as style sheets, JavaScript files</strong></p>\n\n<p>The following type of content flows through the regional edge caches - user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos and static content such as style sheets, JavaScript files. Hence these three options are not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html</a></p>\n",
                "options": [
                    {
                        "id": 9132,
                        "content": "<p>Dynamic content, as determined at request time (cache-behavior configured to forward all headers)</p>",
                        "isValid": true
                    },
                    {
                        "id": 9133,
                        "content": "<p>Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin</p>",
                        "isValid": true
                    },
                    {
                        "id": 9134,
                        "content": "<p>User-generated videos</p>",
                        "isValid": false
                    },
                    {
                        "id": 9135,
                        "content": "<p>Static content such as style sheets, JavaScript files</p>",
                        "isValid": false
                    },
                    {
                        "id": 9136,
                        "content": "<p>E-commerce assets such as product photos</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2188,
            "attributes": {
                "createdAt": "2023-09-09T20:33:36.512Z",
                "updatedAt": "2023-09-09T20:33:36.512Z",
                "content": "<p>A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon EC2 instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company.</p>\n\n<p>Which configuration should be used to meet this changed requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS WAF is a web application firewall service that lets you monitor web requests and protect your web applications from malicious requests. Use AWS WAF to block or allow requests based on conditions that you specify, such as the IP addresses. You can also use AWS WAF preconfigured protections to block common attacks like SQL injection or cross-site scripting.</p>\n\n<p><strong>Configure AWS WAF on the Application Load Balancer in a VPC</strong></p>\n\n<p>You can use AWS WAF with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL). Geographic (Geo) Match Conditions in AWS WAF allows you to use AWS WAF to restrict application access based on the geographic location of your viewers. With geo match conditions you can choose the countries from which AWS WAF should allow access.</p>\n\n<p>Geo match conditions are important for many customers. For example, legal and licensing requirements restrict some customers from delivering their applications outside certain countries. These customers can configure a whitelist that allows only viewers in those countries. Other customers need to prevent the downloading of their encrypted software by users in certain countries. These customers can configure a blacklist so that end-users from those countries are blocked from downloading their software.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Geo Restriction feature of Amazon CloudFront in a VPC</strong> - Geo Restriction feature of CloudFront helps in restricting traffic based on the user's geographic location. But, CloudFront works from edge locations and doesn't belong to a VPC. Hence, this option itself is incorrect and given only as a distractor.</p>\n\n<p><strong>Configure the security group on the Application Load Balancer</strong></p>\n\n<p><strong>Configure the security group for the EC2 instances</strong></p>\n\n<p>Security Groups cannot restrict access based on the user's geographic location.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/\">https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/\">https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2016/12/AWS-WAF-now-available-on-Application-Load-Balancer/\">https://aws.amazon.com/about-aws/whats-new/2016/12/AWS-WAF-now-available-on-Application-Load-Balancer/</a></p>\n",
                "options": [
                    {
                        "id": 9137,
                        "content": "<p>Use Geo Restriction feature of Amazon CloudFront in a VPC</p>",
                        "isValid": false
                    },
                    {
                        "id": 9138,
                        "content": "<p>Configure AWS WAF on the Application Load Balancer in a VPC</p>",
                        "isValid": true
                    },
                    {
                        "id": 9139,
                        "content": "<p>Configure the security group on the Application Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 9140,
                        "content": "<p>Configure the security group for the EC2 instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2189,
            "attributes": {
                "createdAt": "2023-09-09T20:33:36.608Z",
                "updatedAt": "2023-09-09T20:33:36.608Z",
                "content": "<p>An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature.</p>\n\n<p>Which is the MOST effective way to address this issue so that such incidents do not recur?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use permissions boundary to control the maximum permissions employees can grant to the IAM principals</strong></p>\n\n<p>A permissions boundary can be used to control the maximum permissions employees can grant to the IAM principals (that is, users and roles) that they create and manage. As the IAM administrator, you can define one or more permissions boundaries using managed policies and allow your employee to create a principal with this boundary. The employee can then attach a permissions policy to this principal. However, the effective permissions of the principal are the intersection of the permissions boundary and permissions policy. As a result, the new principal cannot exceed the boundary that you defined. Therefore, using the permissions boundary offers the right solution for this use-case.</p>\n\n<p>Permission Boundary Example:\n<img src=\"https://d2908q01vomqb2.cloudfront.net/22d200f8670dbdb3e253a90eee5098477c95c23d/2018/07/03/delegated-admin-02.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/\">https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Remove full database access for all IAM users in the organization</strong> - It is not practical to remove full access for all IAM users in the organization because a select set of users need this access for database administration. So this option is not correct.</p>\n\n<p><strong>The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur</strong> -  Likewise the CTO is not expected to review the permissions for each new developer's IAM user, as this is best done via an automated procedure. This option has been added as a distractor.</p>\n\n<p><strong>Only root user should have full database access in the organization</strong> - As a best practice, the root user should not access the AWS account to carry out any administrative procedures. So this option is not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/\">https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/</a></p>\n",
                "options": [
                    {
                        "id": 9141,
                        "content": "<p>Only root user should have full database access in the organization</p>",
                        "isValid": false
                    },
                    {
                        "id": 9142,
                        "content": "<p>Use permissions boundary to control the maximum permissions employees can grant to the IAM principals</p>",
                        "isValid": true
                    },
                    {
                        "id": 9143,
                        "content": "<p>Remove full database access for all IAM users in the organization</p>",
                        "isValid": false
                    },
                    {
                        "id": 9144,
                        "content": "<p>The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2190,
            "attributes": {
                "createdAt": "2023-09-09T20:33:36.702Z",
                "updatedAt": "2023-09-09T20:33:36.702Z",
                "content": "<p>A gaming company is looking at improving the availability and performance of its global flagship application which utilizes UDP protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom DNS service.</p>\n\n<p>Which of the following AWS services represents the best solution for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Global Accelerator</strong> -  AWS Global Accelerator utilizes the Amazon global network, allowing you to improve the performance of your applications by lowering first-byte latency (the round trip time for a packet to go from a client to your endpoint and back again) and jitter (the variation of latency), and increasing throughput (the amount of time it takes to transfer data) as compared to the public internet.</p>\n\n<p>Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon CloudFront</strong> - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.</p>\n\n<p>AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery), while Global Accelerator improves performance for a wide range of applications over TCP or UDP.</p>\n\n<p><strong>AWS Elastic Load Balancing (ELB)</strong> - Both of the services, ELB and Global Accelerator solve the challenge of routing user requests to healthy application endpoints. AWS Global Accelerator relies on ELB to provide the traditional load balancing features such as support for internal and non-AWS endpoints, pre-warming, and Layer 7 routing. However, while ELB provides load balancing within one Region, AWS Global Accelerator provides traffic management across multiple Regions.</p>\n\n<p>A regional ELB load balancer is an ideal target for AWS Global Accelerator. By using a regional ELB load balancer, you can precisely distribute incoming application traffic across backends, such as Amazon EC2 instances or Amazon ECS tasks, within an AWS Region.</p>\n\n<p>If you have workloads that cater to a global client base, AWS recommends that you use AWS Global Accelerator. If you have workloads hosted in a single AWS Region and used by clients in and around the same Region, you can use an Application Load Balancer or Network Load Balancer to manage your resources.</p>\n\n<p><strong>Amazon Route 53</strong> - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. Route 53 is ruled out as the company wants to continue using its own custom DNS service.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/global-accelerator/faqs/\">https://aws.amazon.com/global-accelerator/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9145,
                        "content": "<p>Amazon CloudFront</p>",
                        "isValid": false
                    },
                    {
                        "id": 9146,
                        "content": "<p>Amazon Route 53</p>",
                        "isValid": false
                    },
                    {
                        "id": 9147,
                        "content": "<p>AWS Global Accelerator</p>",
                        "isValid": true
                    },
                    {
                        "id": 9148,
                        "content": "<p>AWS Elastic Load Balancing (ELB)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2191,
            "attributes": {
                "createdAt": "2023-09-09T20:33:36.803Z",
                "updatedAt": "2023-09-09T20:33:36.803Z",
                "content": "<p>A new DevOps engineer has joined a large financial services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management.</p>\n\n<p>As a solutions architect, which best practices would you recommend (Select two)?</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Enable MFA for privileged users</strong> - As per the AWS best practices, it is better to enable Multi Factor Authentication (MFA) for privileged users via an MFA-enabled mobile device or hardware MFA token.</p>\n\n<p><strong>Configure AWS CloudTrail to record all account activity</strong> - AWS recommends to turn on CloudTrail to log all IAM actions for monitoring and audit purposes.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a minimum number of accounts and share these account credentials among employees</strong> - AWS recommends that user account credentials should not be shared between users. So, this option is incorrect.</p>\n\n<p><strong>Grant maximum privileges to avoid assigning privileges again</strong> - AWS recommends granting the least privileges required to complete a certain job and avoid giving excessive privileges which can be misused. So, this option is incorrect.</p>\n\n<p><strong>Use user credentials to provide access specific permissions for Amazon EC2 instances</strong> - It is highly recommended to use roles to grant access permissions for EC2 instances working on different AWS services. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/iam/\">https://aws.amazon.com/iam/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/faqs/\">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9149,
                        "content": "<p>Enable MFA for privileged users</p>",
                        "isValid": true
                    },
                    {
                        "id": 9150,
                        "content": "<p>Grant maximum privileges to avoid assigning privileges again</p>",
                        "isValid": false
                    },
                    {
                        "id": 9151,
                        "content": "<p>Use user credentials to provide access specific permissions for Amazon EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9152,
                        "content": "<p>Configure AWS CloudTrail to log all IAM actions</p>",
                        "isValid": true
                    },
                    {
                        "id": 9153,
                        "content": "<p>Create a minimum number of accounts and share these account credentials among employees</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2192,
            "attributes": {
                "createdAt": "2023-09-09T20:33:36.894Z",
                "updatedAt": "2023-09-09T20:33:36.894Z",
                "content": "<p>You have multiple AWS accounts within a single AWS Region managed by AWS Organizations and you would like to ensure all EC2 instances in all these accounts can communicate privately. Which of the following solutions provides the capability at the CHEAPEST cost?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a VPC in an account and share one or more of its subnets with the other accounts using Resource Access Manager</strong></p>\n\n<p>AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM. RAM eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account you own. You can create resources centrally in a multi-account environment, and use RAM to share those resources across accounts in three simple steps: create a Resource Share, specify resources, and specify accounts. RAM is available to you at no additional charge.</p>\n\n<p>The correct solution is to share the subnet(s) within a VPC using RAM. This will allow all EC2 instances to be deployed in the same VPC (although from different accounts) and easily communicate with one another.</p>\n\n<p>How Resource Access Manager Works:\n<img src=\"https://d1.awsstatic.com/products/RAM/product-page-diagram_AWS-Resource-Access-Manager(1).379df75d48a8e2cc6160859b7ca3626a9b9be0c1.png\">\nvia - <a href=\"https://aws.amazon.com/ram/\">https://aws.amazon.com/ram/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Private Link between all the EC2 instances</strong> - AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network.\nPrivate Link is a distractor in this question. Private Link is leveraged to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering and allowing the connections between the two to remain within the AWS network.</p>\n\n<p><strong>Create a VPC peering connection between all VPCs</strong> - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection).\nVPC peering connections will work, but won't efficiently scale if you add more accounts (you'll have to create many connections).</p>\n\n<p><strong>Create a Transit Gateway and link all the VPC in all the accounts together</strong> - AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway.\nA Transit Gateway will work but will be an expensive solution. Here we want to minimize cost.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ram/\">https://aws.amazon.com/ram/</a></p>\n\n<p><a href=\"https://aws.amazon.com/privatelink/\">https://aws.amazon.com/privatelink/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\">https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p>\n",
                "options": [
                    {
                        "id": 9154,
                        "content": "<p>Create a Transit Gateway and link all the VPC in all the accounts together</p>",
                        "isValid": false
                    },
                    {
                        "id": 9155,
                        "content": "<p>Create a Private Link between all the EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9156,
                        "content": "<p>Create a VPC peering connection between all VPCs</p>",
                        "isValid": false
                    },
                    {
                        "id": 9157,
                        "content": "<p>Create a VPC in an account and share one or more of its subnets with the other accounts using Resource Access Manager</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2193,
            "attributes": {
                "createdAt": "2023-09-09T20:33:36.978Z",
                "updatedAt": "2023-09-09T20:33:36.978Z",
                "content": "<p>An IT company wants to optimize the costs incurred on its fleet of 100 EC2 instances for the next year. Based on historical analyses, the engineering team observed that 70 of these instances handle the compute services of its flagship application and need to be always available. The other 30 instances are used to handle batch jobs that can afford a delay in processing.</p>\n\n<p>As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Purchase 70 reserved instances and 30 spot instances</strong></p>\n\n<p>As 70 instances need to be always available, these can be purchased as reserved instances for a one-year duration.\nThe other 30 instances responsible for the batch job can be purchased as spot instances. Even if some of the spot instances are interrupted, other spot instances can continue with the job.</p>\n\n<p>Please see this detailed overview of various types of EC2 instances from a pricing perspective:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q35-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Purchase 70 on-demand instances and 30 spot instances</strong></p>\n\n<p><strong>Purchase 70 on-demand instances and 30 reserved instances</strong></p>\n\n<p>Purchasing 70 on-demand instances would be costlier than 70 reserved instances, so these two options are ruled out.</p>\n\n<p><strong>Purchase 70 reserved instances and 30 on-demand instances</strong> - Purchasing 30 instances as on-demand instances to handle the batch jobs would not be cost-optimal as these instances don't need to be always available. Spot instances are better at handling such batch jobs. So this option is not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n",
                "options": [
                    {
                        "id": 9158,
                        "content": "<p>Purchase 70 reserved instances and 30 on-demand instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9159,
                        "content": "<p>Purchase 70 reserved instances and 30 spot instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 9160,
                        "content": "<p>Purchase 70 on-demand instances and 30 reserved instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9161,
                        "content": "<p>Purchase 70 on-demand instances and 30 spot instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2194,
            "attributes": {
                "createdAt": "2023-09-09T20:33:37.074Z",
                "updatedAt": "2023-09-09T20:33:37.074Z",
                "content": "<p>An IT company has built a solution wherein a Redshift cluster writes data to an Amazon S3 bucket belonging to a different AWS account. However, it is found that the files created in the S3 bucket using the UNLOAD command from the Redshift cluster are not even accessible to the S3 bucket owner.</p>\n\n<p>What could be the reason for this denial of permission for the bucket owner?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>By default, an S3 object is owned by the AWS account that uploaded it. So the S3 bucket owner will not implicitly have access to the objects written by Redshift cluster</strong> - By default, an S3 object is owned by the AWS account that uploaded it. This is true even when the bucket is owned by another account. Because the Amazon Redshift data files from the UNLOAD command were put into your bucket by another account, you (the bucket owner) don't have default permission to access those files.</p>\n\n<p>To get access to the data files, an AWS Identity and Access Management (IAM) role with cross-account permissions must run the UNLOAD command again. Follow these steps to set up the Amazon Redshift cluster with cross-account permissions to the bucket:</p>\n\n<ol>\n<li><p>From the account of the S3 bucket, create an IAM role (Bucket Role) with permissions to the bucket.</p></li>\n<li><p>From the account of the Amazon Redshift cluster, create another IAM role (Cluster Role) with permissions to assume the Bucket Role.</p></li>\n<li><p>Update the Bucket Role to grant bucket access and create a trust relationship with the Cluster Role.</p></li>\n<li><p>From the Amazon Redshift cluster, run the UNLOAD command using the Cluster Role and Bucket Role.</p></li>\n</ol>\n\n<p>This solution doesn't apply to Amazon Redshift clusters or S3 buckets that use server-side encryption with AWS Key Management Service (AWS KMS).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When objects are uploaded to S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console</strong> - By default, an S3 object is owned by the AWS account that uploaded it. So, the bucket owner will not have any default permissions on the objects. Therefore, this option is incorrect.</p>\n\n<p><strong>The owner of an S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress</strong> - This is an incorrect statement, given only as a distractor.</p>\n\n<p><strong>When two different AWS accounts are accessing an S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures</strong> - This is an incorrect statement, given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-redshift-unload/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-redshift-unload/</a></p>\n",
                "options": [
                    {
                        "id": 9162,
                        "content": "<p>The owner of an S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress</p>",
                        "isValid": false
                    },
                    {
                        "id": 9163,
                        "content": "<p>By default, an S3 object is owned by the AWS account that uploaded it. So the S3 bucket owner will not implicitly have access to the objects written by the Redshift cluster</p>",
                        "isValid": true
                    },
                    {
                        "id": 9164,
                        "content": "<p>When objects are uploaded to S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console</p>",
                        "isValid": false
                    },
                    {
                        "id": 9165,
                        "content": "<p>When two different AWS accounts are accessing an S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2195,
            "attributes": {
                "createdAt": "2023-09-09T20:33:37.184Z",
                "updatedAt": "2023-09-09T20:33:37.184Z",
                "content": "<p>You have been hired as a Solutions Architect to advise a company on the various authentication/authorization mechanisms that AWS offers to authorize an API call within the API Gateway. The company would prefer a solution that offers built-in user management.</p>\n\n<p>Which of the following solutions would you suggest as the best fit for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Cognito User Pools</strong> - A user pool is a user directory in Amazon Cognito. You can leverage Amazon Cognito User Pools to either provide built-in user management or integrate with external identity providers, such as Facebook, Twitter, Google+, and Amazon. Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK).</p>\n\n<p>User pools provide:\n1. Sign-up and sign-in services.\n2. A built-in, customizable web UI to sign in users.\n3. Social sign-in with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as sign-in with SAML identity providers from your user pool.\n4. User directory management and user profiles.\n5. Security features such as multi-factor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.\n6. Customized workflows and user migration through AWS Lambda triggers.</p>\n\n<p>After creating an Amazon Cognito user pool, in API Gateway, you must then create a COGNITO_USER_POOLS authorizer that uses the user pool.</p>\n\n<p>Amazon Cognito User Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/identity-and-access-management.html\">https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/identity-and-access-management.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS_IAM authorization</strong> - For consumers who currently are located within your AWS environment or have the means to retrieve AWS Identity and Access Management (IAM) temporary credentials to access your environment, you can use AWS_IAM authorization and add least-privileged permissions to the respective IAM role to securely invoke your API. API Gateway API Keys is not a security mechanism and should not be used for authorization unless it’s a public API. It should be used primarily to track a consumer’s usage across your API.</p>\n\n<p><strong>Use API Gateway Lambda authorizer</strong> - If you have an existing Identity Provider (IdP), you can use an API Gateway Lambda authorizer to invoke a Lambda function to authenticate/validate a given user against your IdP. You can use a Lambda authorizer for custom validation logic based on identity metadata.</p>\n\n<p>A Lambda authorizer can send additional information derived from a bearer token or request context values to your backend service. For example, the authorizer can return a map containing user IDs, user names, and scope. By using Lambda authorizers, your backend does not need to map authorization tokens to user-centric data, allowing you to limit the exposure of such information to just the authorization function.</p>\n\n<p>When using Lambda authorizers, AWS strictly advises against passing credentials or any sort of sensitive data via query string parameters or headers, so this is not as secure as using Cognito User Pools.</p>\n\n<p>In addition, both these options do not offer built-in user management.</p>\n\n<p><strong>Use Amazon Cognito Identity Pools</strong> - The two main components of Amazon Cognito are user pools and identity pools. Identity pools provide AWS credentials to grant your users access to other AWS services. To enable users in your user pool to access AWS resources, you can configure an identity pool to exchange user pool tokens for AWS credentials. So, identity pools aren't an authentication mechanism in themselves and hence aren't a choice for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/identity-and-access-management.html\">https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/identity-and-access-management.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-enable-cognito-user-pool.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-enable-cognito-user-pool.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p>\n",
                "options": [
                    {
                        "id": 9166,
                        "content": "<p>Use AWS_IAM authorization</p>",
                        "isValid": false
                    },
                    {
                        "id": 9167,
                        "content": "<p>Use Amazon Cognito User Pools</p>",
                        "isValid": true
                    },
                    {
                        "id": 9168,
                        "content": "<p>Use API Gateway Lambda authorizer</p>",
                        "isValid": false
                    },
                    {
                        "id": 9169,
                        "content": "<p>Use Amazon Cognito Identity Pools</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2196,
            "attributes": {
                "createdAt": "2023-09-09T20:33:37.299Z",
                "updatedAt": "2023-09-09T20:33:37.299Z",
                "content": "<p>A social photo-sharing web application is hosted on EC2 instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in S3 and the leaderboard data is maintained in DynamoDB. The EC2 instances need to access both S3 and DynamoDB for these features.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend as the MOST secure option?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Attach the appropriate IAM role to the EC2 instance profile so that the instance can access S3 and DynamoDB</strong></p>\n\n<p>Applications that run on an EC2 instance must include AWS credentials in their AWS API requests. You could have your developers store AWS credentials directly within the EC2 instance and allow applications in that instance to use those credentials. But developers would then have to manage the credentials and ensure that they securely pass the credentials to each instance and update each EC2 instance when it's time to rotate the credentials.</p>\n\n<p>Instead, you should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute long-term credentials (such as a username and password or access keys) to an EC2 instance. The role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the role-supplied temporary credentials to sign API requests. Therefore, this option is correct.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the EC2 instances. EC2 instances can use these credentials to access S3 and DynamoDB</strong></p>\n\n<p><strong>Configure AWS CLI on the EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access S3 and DynamoDB via AWS CLI</strong></p>\n\n<p><strong>Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to S3 and DynamoDB</strong></p>\n\n<p>Keeping the AWS credentials (encrypted or plain text) on the EC2 instance is a bad security practice, therefore these three options using the AWS credentials are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html</a></p>\n",
                "options": [
                    {
                        "id": 9170,
                        "content": "<p>Attach the appropriate IAM role to the EC2 instance profile so that the instance can access S3 and DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 9171,
                        "content": "<p>Configure AWS CLI on the EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access S3 and DynamoDB via AWS CLI</p>",
                        "isValid": false
                    },
                    {
                        "id": 9172,
                        "content": "<p>Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the EC2 instances. EC2 instances can use these credentials to access S3 and DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 9173,
                        "content": "<p>Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to S3 and DynamoDB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2197,
            "attributes": {
                "createdAt": "2023-09-09T20:33:37.433Z",
                "updatedAt": "2023-09-09T20:33:37.433Z",
                "content": "<p>Upon a security review of your AWS account, an AWS consultant has found that a few RDS databases are un-encrypted. As a Solutions Architect, what steps must be taken to encrypt the RDS databases?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Take a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database</strong></p>\n\n<p>Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups.</p>\n\n<p>You can encrypt your Amazon RDS DB instances and snapshots at rest by enabling the encryption option for your Amazon RDS DB instances. Data that is encrypted at rest includes the underlying storage for DB instances, its automated backups, read replicas, and snapshots.</p>\n\n<p>You can only enable encryption for an Amazon RDS DB instance when you create it, not after the DB instance is created. However, because you can encrypt a copy of an unencrypted DB snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. So this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database</strong> - If the master is not encrypted, the read replicas cannot be encrypted. So this option is incorrect.</p>\n\n<p><strong>Enable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ</strong> - Multi-AZ is to help with High Availability, not encryption. So this option is incorrect.</p>\n\n<p><strong>Enable encryption on the RDS database using the AWS Console</strong> - There is no direct option to encrypt an RDS database using the AWS Console.</p>\n\n<p>Steps to encrypt an un-encrypted RDS database:\nCreate a snapshot of the un-encrypted database\nCopy the snapshot and enable encryption for the snapshot\nRestore the database from the encrypted snapshot\nMigrate applications to the new database, and delete the old database</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p>\n",
                "options": [
                    {
                        "id": 9174,
                        "content": "<p>Take a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database</p>",
                        "isValid": true
                    },
                    {
                        "id": 9175,
                        "content": "<p>Enable encryption on the RDS database using the AWS Console</p>",
                        "isValid": false
                    },
                    {
                        "id": 9176,
                        "content": "<p>Create a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database</p>",
                        "isValid": false
                    },
                    {
                        "id": 9177,
                        "content": "<p>Enable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2198,
            "attributes": {
                "createdAt": "2023-09-09T20:33:37.529Z",
                "updatedAt": "2023-09-09T20:33:37.529Z",
                "content": "<p>An application runs big data workloads on EC2 instances. The application runs 24x7 all round the year and needs at least 20 instances to maintain a minimum acceptable performance threshold and the application needs 300 instances to handle spikes in the workload. Based on historical workloads processed by the application, it needs 80 instances 80% of the time.</p>\n\n<p>As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution so that it can meet the workload demand in a steady state?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Purchase 80 reserved instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)</strong></p>\n\n<p>As the steady-state workload demand is 80 instances, we can save on costs by purchasing 80 reserved instances. Based on additional workload demand, we can specify a mix of on-demand and spot instances using Application Load Balancer with a launch template to provision the mix of on-demand and spot instances.</p>\n\n<p>Please see this detailed overview of various types of EC2 instances from a pricing perspective:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q33-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Purchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand</strong> - Provisioning 20 on-demand instances implies that there would be a shortfall of 60 instances 80% of the time. Provisioning all of these 60 instances as spot instances is highly risky as there is no guarantee regarding the availability of the spot instances, which means we may not even meet the steady-state requirement for the workload, so this option is incorrect.</p>\n\n<p><strong>Purchase 80 on-demand instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)</strong> - Provisioning 80 on-demand instances would end up costlier than the option where we provision 80 reserved instances. So this option is ruled out.</p>\n\n<p><strong>Purchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand</strong> - The option to purchase 80 spot instances is incorrect, as there is no guarantee regarding the availability of the spot instances, which means we may not even meet the steady-state workload.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n",
                "options": [
                    {
                        "id": 9178,
                        "content": "<p>Purchase 80 on-demand instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9179,
                        "content": "<p>Purchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand</p>",
                        "isValid": false
                    },
                    {
                        "id": 9180,
                        "content": "<p>Purchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand</p>",
                        "isValid": false
                    },
                    {
                        "id": 9181,
                        "content": "<p>Purchase 80 reserved instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2199,
            "attributes": {
                "createdAt": "2023-09-09T20:33:37.623Z",
                "updatedAt": "2023-09-09T20:33:37.623Z",
                "content": "<p>To improve the performance and security of the application, the engineering team at a company has created a CloudFront distribution with an Application Load Balancer as the custom origin. The team has also set up a Web Application Firewall (WAF) with CloudFront distribution. The security team at the company has noticed a surge in malicious attacks from a specific IP address to steal sensitive data stored on the EC2 instances.</p>\n\n<p>As a solutions architect, which of the following actions would you recommend to stop the attacks?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an IP match condition in the WAF to block the malicious IP address</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.</p>\n\n<p>How WAF Works:\n<img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\">\nvia - <a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p>\n\n<p>If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from. So, this option is correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a deny rule for the malicious IP in the NACL associated with each of the instances</strong> - NACLs are not associated with instances. So this option is also ruled out.</p>\n\n<p><strong>Create a deny rule for the malicious IP in the Security Groups associated with each of the instances</strong> - You cannot deny rules in Security Groups. So this option is ruled out.</p>\n\n<p><strong>Create a ticket with AWS support to take action against the malicious IP</strong> - Managing the security of your application is your responsibility, not that of AWS, so you cannot raise a ticket for this issue.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html\">https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html</a></p>\n",
                "options": [
                    {
                        "id": 9182,
                        "content": "<p>Create an IP match condition in the WAF to block the malicious IP address</p>",
                        "isValid": true
                    },
                    {
                        "id": 9183,
                        "content": "<p>Create a deny rule for the malicious IP in the NACL associated with each of the instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9184,
                        "content": "<p>Create a deny rule for the malicious IP in the Security Groups associated with each of the instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9185,
                        "content": "<p>Create a ticket with AWS support to take action against the malicious IP</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2200,
            "attributes": {
                "createdAt": "2023-09-09T20:33:37.716Z",
                "updatedAt": "2023-09-09T20:33:37.716Z",
                "content": "<p>You would like to store a database password in a secure place, and enable automatic rotation of that password every 90 days. What do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>\"Secrets Manager\"</p>\n\n<p>AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB. The correct answer here is Secrets Manager</p>\n\n<p>Incorrect options:</p>\n\n<p>\"KMS\" - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. KMS is an encryption service, it's not a secrets store. So this option is incorrect.</p>\n\n<p>\"CloudHSM\" - AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your encryption keys on the AWS Cloud. With CloudHSM, you can manage your encryption keys using FIPS 140-2 Level 3 validated HSMs. CloudHSM is standards-compliant and enables you to export all of your keys to most other commercially-available HSMs, subject to your configurations. It is a fully-managed service that automates time-consuming administrative tasks for you, such as hardware provisioning, software patching, high-availability, and backups.</p>\n\n<p>CloudHSM is also an encryption service, not a secrets store. So this option is incorrect.</p>\n\n<p>\"SSM Parameter Store\" - AWS Systems Manager Parameter Store (aka SSM Parameter Store) provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, EC2 instance IDs, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter.</p>\n\n<p>SSM Parameter Store can serve as a secrets store, but you must rotate the secrets yourself, it doesn't have an automatic capability for this. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudhsm/\">https://aws.amazon.com/cloudhsm/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/\">https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p>\n",
                "options": [
                    {
                        "id": 9186,
                        "content": "<p>Key Management Service (KMS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9187,
                        "content": "<p>Secrets Manager</p>",
                        "isValid": true
                    },
                    {
                        "id": 9188,
                        "content": "<p>SSM Parameter Store</p>",
                        "isValid": false
                    },
                    {
                        "id": 9189,
                        "content": "<p>CloudHSM</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2201,
            "attributes": {
                "createdAt": "2023-09-09T20:33:37.813Z",
                "updatedAt": "2023-09-09T20:33:37.813Z",
                "content": "<p>An analytics company wants to improve the performance of its big data processing workflows running on Amazon EFS. Which of the following performance modes should be used for EFS to address this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Max I/O</strong></p>\n\n<p>How EFS Works:\n<img src=\"https://d1.awsstatic.com/product-page-diagram_Amazon-EFS-Feb%202021.1936d75b4b5b81b192aada931a1170054e7c196c.png\">\nvia - <a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a></p>\n\n<p>Max I/O performance mode is used to scale to higher levels of aggregate throughput and operations per second. This scaling is done with a tradeoff of slightly higher latencies for file metadata operations. Highly parallelized applications and workloads, such as big data analysis, media processing, and genomic analysis, can benefit from this mode.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q64-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provisioned Throughput</strong></p>\n\n<p><strong>Bursting Throughput</strong></p>\n\n<p>These two options have been added as distractors as these refer to the throughput mode of EFS and not the performance mode. There are two throughput modes to choose from for your file system, Bursting Throughput and Provisioned Throughput. With Bursting Throughput mode, throughput on Amazon EFS scales as the size of your file system in the standard storage class grows. With Provisioned Throughput mode, you can instantly provision the throughput of your file system (in MiB/s) independent of the amount of data stored.</p>\n\n<p><strong>General Purpose</strong> - General Purpose performance mode is ideal for latency-sensitive use cases, like web serving environments, content management systems, home directories, and general file serving. If you don't choose a performance mode when you create your file system, Amazon EFS selects the General Purpose mode for you by default.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a></p>\n",
                "options": [
                    {
                        "id": 9190,
                        "content": "<p>Provisioned Throughput</p>",
                        "isValid": false
                    },
                    {
                        "id": 9191,
                        "content": "<p>Bursting Throughput</p>",
                        "isValid": false
                    },
                    {
                        "id": 9192,
                        "content": "<p>Max I/O</p>",
                        "isValid": true
                    },
                    {
                        "id": 9193,
                        "content": "<p>General Purpose</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2202,
            "attributes": {
                "createdAt": "2023-09-09T20:33:37.911Z",
                "updatedAt": "2023-09-09T20:33:37.911Z",
                "content": "<p>A company has many VPC in various accounts, that need to be connected in a star network with one another and connected with on-premises networks through Direct Connect.</p>\n\n<p>What do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Transit Gateway</strong></p>\n\n<p>AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway into each Amazon VPC, on-premises data center, or remote office across your network. Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks which act like spokes.\nSo, this is a perfect use-case for the Transit Gateway.</p>\n\n<p>Without Transit Gateway\n<img src=\"https://d1.awsstatic.com/product-marketing/transit-gateway/tgw-before.7f287b3bf00bbc4fbdeadef3c8d5910374aec963.png\">\nvia - <a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p>\n\n<p>With Transit Gateway\n<img src=\"https://d1.awsstatic.com/product-marketing/transit-gateway/tgw-after.d85d3e2cb67fd2ed1a3be645d443e9f5910409fd.png\">\nvia - <a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>VPC Peering</strong> - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection).\nVPC Peering helps connect two VPCs and is not transitive. It would require to create many peering connections between all the VPCs to have them connect. This alone wouldn't work, because we would need to also connect the on-premises data center through Direct Connect and Direct Connect Gateway, but that's not mentioned in this answer.</p>\n\n<p><strong>VPN Gateway</strong> - A virtual private gateway (also known as a VPN Gateway) is the endpoint on the VPC side of your VPN connection. You can create a virtual private gateway before creating the VPC itself.\nVPN Gateway is a distractor here because we haven't mentioned a VPN.</p>\n\n<p><strong>Private Link</strong> - AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network.\nPrivate Link is utilized to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering, and allowing the connections between the two to remain within the AWS network.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\">https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateVpnGateway.html\">https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateVpnGateway.html</a></p>\n",
                "options": [
                    {
                        "id": 9194,
                        "content": "<p>Private Link</p>",
                        "isValid": false
                    },
                    {
                        "id": 9195,
                        "content": "<p>Transit Gateway</p>",
                        "isValid": true
                    },
                    {
                        "id": 9196,
                        "content": "<p>VPN Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 9197,
                        "content": "<p>VPC Peering</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2203,
            "attributes": {
                "createdAt": "2023-09-09T20:33:38.035Z",
                "updatedAt": "2023-09-09T20:33:38.035Z",
                "content": "<p>A media company is migrating its flagship application from its on-premises data center to AWS for improving the application's read-scaling capability as well as its availability. The existing architecture leverages a Microsoft SQL Server database that sees a heavy read load. The engineering team does a full copy of the production database at the start of the business day to populate a dev database. During this period, application users face high latency leading to a bad user experience.</p>\n\n<p>The company is looking at alternate database options and migrating database engines if required. What would you suggest?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora</strong></p>\n\n<p>Amazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL. An Amazon Aurora DB cluster consists of one or more DB instances and a cluster volume that manages the data for those DB instances. An Aurora cluster volume is a virtual database storage volume that spans multiple Availability Zones, with each Availability Zone having a copy of the DB cluster data. Aurora supports Multi-AZ Aurora Replicas that improve the application's read-scaling and availability.</p>\n\n<p>Amazon Aurora Overview:\n<img src=\"https://d1.awsstatic.com/Product-Page-Diagram_Amazon-Aurora_How-it-Works.b1c2b37e7548757780b195c6dcceb58511de5b1d.png\">\nvia - <a href=\"https://aws.amazon.com/rds/aurora/\">https://aws.amazon.com/rds/aurora/</a></p>\n\n<p>Aurora backs up your cluster volume automatically and retains restore data for the length of the backup retention period. Aurora backups are continuous and incremental so you can quickly restore to any point within the backup retention period. No performance impact or interruption of database service occurs as backup data is being written.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q52-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html</a></p>\n\n<p>Automated backups occur daily during the preferred backup window. If the backup requires more time than allotted to the backup window, the backup continues after the window ends, until it finishes. The backup window can't overlap with the weekly maintenance window for the DB cluster. Aurora backups are continuous and incremental, but the backup window is used to create a daily system backup that is preserved within the backup retention period. The latest restorable time for a DB cluster is the most recent point at which you can restore your DB cluster, typically within 5 minutes of the current time.</p>\n\n<p>For the given use case, you can create the dev database by restoring from the automated backups of Amazon Aurora.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump</strong> - Restoring the dev database via mysqldump would still result in a significant load on the primary DB, so this option fails to address the given requirement.</p>\n\n<p><strong>Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database</strong> - The standby is there just for handling failover in a Multi-AZ deployment. You cannot access the standby instance and use it as a dev database. Hence this option is incorrect.</p>\n\n<p><strong>Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database</strong> - Amazon RDS supports Multi-AZ deployments for Microsoft SQL Server by using either SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs). Amazon RDS monitors and maintains the health of your Multi-AZ deployment.</p>\n\n<p>Multi-AZ deployments provide increased availability, data durability, and fault tolerance for DB instances. In the event of planned database maintenance or unplanned service disruption, Amazon RDS automatically fails over to the up-to-date secondary DB instance. For SQL Server, I/O activity is suspended briefly during backup for Multi-AZ deployments.</p>\n\n<p>A read replica is only meant to serve read traffic. The primary purpose of the read replica is to replicate the data in the primary DB instance. A read replica cannot be used as a dev database because it does not allow any database write operations.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html</a></p>\n",
                "options": [
                    {
                        "id": 9198,
                        "content": "<p>Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database</p>",
                        "isValid": false
                    },
                    {
                        "id": 9199,
                        "content": "<p>Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora</p>",
                        "isValid": true
                    },
                    {
                        "id": 9200,
                        "content": "<p>Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database</p>",
                        "isValid": false
                    },
                    {
                        "id": 9201,
                        "content": "<p>Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2204,
            "attributes": {
                "createdAt": "2023-09-09T20:33:38.132Z",
                "updatedAt": "2023-09-09T20:33:38.132Z",
                "content": "<p>An IT company is working on client engagement to build a real-time data analytics tool for the Internet of Things (IoT) data. The IoT data is funneled into Kinesis Data Streams which further acts as the source of a delivery stream for Kinesis Firehose. The engineering team has now configured a Kinesis Agent to send IoT data from another set of devices to the same Firehose delivery stream. They noticed that data is not reaching Firehose as expected.</p>\n\n<p>As a solutions architect, which of the following options would you attribute as the MOST plausible root cause behind this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Kinesis Agent cannot write to a Kinesis Firehose for which the delivery stream source is already set as Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</p>\n\n<p>When a Kinesis data stream is configured as the source of a Firehose delivery stream, Firehose’s PutRecord and PutRecordBatch operations are disabled and Kinesis Agent cannot write to Firehose delivery stream directly. Data needs to be added to the Kinesis data stream through the Kinesis Data Streams PutRecord and PutRecords operations instead. Therefore, this option is correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Kinesis Agent can only write to Kinesis Data Streams, not to Kinesis Firehose</strong> - Kinesis Agent is a stand-alone Java software application that offers an easy way to collect and send data to Kinesis Data Streams or Kinesis Firehose. So this option is incorrect.</p>\n\n<p><strong>Kinesis Firehose delivery stream has reached its limit and needs to be scaled manually</strong> - Kinesis Firehose is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. Therefore this option is not correct.</p>\n\n<p>How Kinesis Firehose works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p><strong>The data sent by Kinesis Agent is lost because of a configuration error</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html\">https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/writing-with-agents.html\">https://docs.aws.amazon.com/firehose/latest/dev/writing-with-agents.html</a></p>\n",
                "options": [
                    {
                        "id": 9202,
                        "content": "<p>Kinesis Agent can only write to Kinesis Data Streams, not to Kinesis Firehose</p>",
                        "isValid": false
                    },
                    {
                        "id": 9203,
                        "content": "<p>The data sent by Kinesis Agent is lost because of a configuration error</p>",
                        "isValid": false
                    },
                    {
                        "id": 9204,
                        "content": "<p>Kinesis Firehose delivery stream has reached its limit and needs to be scaled manually</p>",
                        "isValid": false
                    },
                    {
                        "id": 9205,
                        "content": "<p>Kinesis Agent cannot write to a Kinesis Firehose for which the delivery stream source is already set as Kinesis Data Streams</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2205,
            "attributes": {
                "createdAt": "2023-09-09T20:33:38.220Z",
                "updatedAt": "2023-09-09T20:33:38.220Z",
                "content": "<p>A manufacturing company receives unreliable service from its data center provider because the company is located in an area prone to natural disasters. The company is not ready to fully migrate to the AWS Cloud, but it wants a failover environment on AWS in case the on-premises data center fails. The company runs web servers that connect to external vendors. The data available on AWS and on-premises must be uniform.</p>\n\n<p>Which of the following solutions would have the LEAST amount of downtime?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up a Route 53 failover record. Run application servers on EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to S3</strong></p>\n\n<p>If you have multiple resources that perform the same function, you can configure DNS failover so that Route 53 will route your traffic from an unhealthy resource to a healthy resource.</p>\n\n<p>Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group.</p>\n\n<p>AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. It provides low-latency performance by caching frequently accessed data on-premises while storing data securely and durably in Amazon cloud storage services. Storage Gateway optimizes data transfer to AWS by sending only changed data and compressing data. Storage Gateway also integrates natively with Amazon S3 cloud storage which makes your data available for in-cloud processing.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a Route 53 failover record. Execute an AWS CloudFormation template from a script to provision EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to S3</strong></p>\n\n<p><strong>Set up a Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to S3. Set up an AWS Direct Connect connection between a VPC and the data center</strong></p>\n\n<p><strong>Set up a Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer</strong></p>\n\n<p>AWS CloudFormation is a convenient provisioning mechanism for a broad range of AWS and third-party resources. It supports the infrastructure needs of many different types of applications such as existing enterprise applications, legacy applications, applications built using a variety of AWS resources, and container-based solutions.</p>\n\n<p>These three options involve CloudFormation as part of the solution. Now, CloudFormation takes time to provision the resources and hence is not the right solution when LEAST amount of downtime is mandated for the given use case. Therefore, these options are not the right fit for the given requirement.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/route53/\">https://aws.amazon.com/route53/</a></p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/\">https://aws.amazon.com/storagegateway/</a></p>\n",
                "options": [
                    {
                        "id": 9206,
                        "content": "<p>Set up a Route 53 failover record. Execute an AWS CloudFormation template from a script to provision EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9207,
                        "content": "<p>Set up a Route 53 failover record. Run application servers on EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 9208,
                        "content": "<p>Set up a Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to S3. Set up an AWS Direct Connect connection between a VPC and the data center</p>",
                        "isValid": false
                    },
                    {
                        "id": 9209,
                        "content": "<p>Set up a Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2206,
            "attributes": {
                "createdAt": "2023-09-09T20:33:38.312Z",
                "updatedAt": "2023-09-09T20:33:38.312Z",
                "content": "<p>An HTTP application is deployed on an Auto Scaling Group, is accessible from an Application Load Balancer that provides HTTPS termination, and accesses a PostgreSQL database managed by RDS.</p>\n\n<p>How should you configure the security groups? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 5432</strong></p>\n\n<p><strong>The security group of the EC2 instances should have an inbound rule from the security group of the ALB on port 80</strong></p>\n\n<p><strong>The security group of the ALB should have an inbound rule from anywhere on port 443</strong></p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.\nThe following are the characteristics of security group rules:\nBy default, security groups allow all outbound traffic.\nSecurity group rules are always permissive; you can't create rules that deny access.\nSecurity groups are stateful</p>\n\n<p>PostgreSQL port = 5432\nHTTP port = 80\nHTTPS port = 443</p>\n\n<p>The traffic goes like this :\nThe client sends an HTTPS request to ALB on port 443. This is handled by the rule - <strong>The security group of the ALB should have an inbound rule from anywhere on port 443.</strong>\nThe ALB then forwards the request to one of the EC2 instances. This is handled by the rule - <strong>The security group of the EC2 instances should have an inbound rule from the security group of the ALB on port 80.</strong>\nThe EC2 instance further accesses the PostgreSQL database managed by RDS on port 5432. This is handled by the rule - <strong>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 5432.</strong></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The security group of the ALB should have an inbound rule from anywhere on port 80</strong> - The client sends an HTTPS request to ALB on port 443 and not on port 80, so this is incorrect.</p>\n\n<p><strong>The security group of the EC2 instances should have an inbound rule from the security group of the RDS database on port 5432</strong> - The security group of the EC2 instances should have an inbound rule from the security group of the ALB and not from the security group of the RDS database, so this option is incorrect.</p>\n\n<p><strong>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 80</strong> - The EC2 instance further accesses the PostgreSQL database managed by RDS on port 5432 and not on port 80, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 9210,
                        "content": "<p>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 5432</p>",
                        "isValid": true
                    },
                    {
                        "id": 9211,
                        "content": "<p>The security group of the EC2 instances should have an inbound rule from the security group of the RDS database on port 5432</p>",
                        "isValid": false
                    },
                    {
                        "id": 9212,
                        "content": "<p>The security group of the ALB should have an inbound rule from anywhere on port 80</p>",
                        "isValid": false
                    },
                    {
                        "id": 9213,
                        "content": "<p>The security group of the EC2 instances should have an inbound rule from the security group of the ALB on port 80</p>",
                        "isValid": true
                    },
                    {
                        "id": 9214,
                        "content": "<p>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 80</p>",
                        "isValid": false
                    },
                    {
                        "id": 9215,
                        "content": "<p>The security group of the ALB should have an inbound rule from anywhere on port 443</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2207,
            "attributes": {
                "createdAt": "2023-09-09T20:33:38.404Z",
                "updatedAt": "2023-09-09T20:33:38.404Z",
                "content": "<p>A company has historically operated only in the <code>us-east-1</code> region and stores encrypted data in S3 using SSE-KMS. As part of enhancing its security posture as well as improving the backup and recovery architecture, the company wants to store the encrypted data in S3 that is replicated into the <code>us-west-1</code> AWS region. The security policies mandate that the data must be encrypted and decrypted using the same key in both AWS regions.</p>\n\n<p>Which of the following represents the best solution to address these requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a new S3 bucket in the <code>us-east-1</code> region with replication enabled from this new bucket into another bucket in <code>us-west-1</code> region. Enable SSE-KMS encryption on the new bucket in <code>us-east-1</code> region by using an AWS KMS multi-region key. Copy the existing data from the current S3 bucket in <code>us-east-1</code> region into this new S3 bucket in <code>us-east-1</code> region</strong></p>\n\n<p>AWS KMS supports multi-region keys, which are AWS KMS keys in different AWS regions that can be used interchangeably – as though you had the same key in multiple regions. Each set of related multi-region keys has the same key material and key ID, so you can encrypt data in one AWS region and decrypt it in a different AWS region without re-encrypting or making a cross-region call to AWS KMS.</p>\n\n<p>You can use multi-region AWS KMS keys in Amazon S3. However, Amazon S3 currently treats multi-region keys as though they were single-region keys, and does not use the multi-region features of the key.</p>\n\n<p>Multi-region AWS KMS keys:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q46-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html\">https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html</a></p>\n\n<p>For the given use case, you must create a new bucket in the <code>us-east-1</code> region with replication enabled from this new bucket into another bucket in <code>us-west-1</code> region. This would ensure that the data is available in another region for backup and recovery purposes. You should also enable SSE-KMS encryption on the new bucket in <code>us-east-1</code> region by using an AWS KMS multi-region key so that the data can be encrypted and decrypted using the same key in both AWS regions. Since the existing data in the current bucket was encrypted using the AWS KMS key restricted to the <code>us-east-1</code> region, so data must be copied to the new bucket in <code>us-east-1</code> region for replication as well as multi-region KMS key based encryption to kick-in.</p>\n\n<p>To require server-side encryption of all objects in a particular Amazon S3 bucket, you can use a policy. For example, the following bucket policy denies the upload object (s3:PutObject) permission to everyone if the request does not include the x-amz-server-side-encryption header requesting server-side encryption with SSE-KMS.</p>\n\n<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"*\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:kms\"\n            }\n         }\n      }\n   ]\n}\n</code></pre>\n\n<p>The following example IAM policies show statements for using AWS KMS server-side encryption with replication.</p>\n\n<p>In this example, the encryption context is the object ARN. If you use SSE-KMS with an S3 Bucket Key enabled, you must use the bucket ARN as the encryption context.</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n            \"Action\": [\"kms:Decrypt\"],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"List of AWS KMS key ARNs used to encrypt source objects.\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"kms:ViaService\": \"s3.source-bucket-region.amazonaws.com\",\n                    \"kms:EncryptionContext:aws:s3:arn\": \"arn:aws:s3:::source-bucket-name/key-prefix1/*\"\n                }\n            }\n        },\n\n        {\n            \"Action\": [\"kms:Encrypt\"],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"AWS KMS key ARNs (for the AWS Region of the destination bucket 1). Used to encrypt object replicas created in destination bucket 1.\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"kms:ViaService\": \"s3.destination-bucket-1-region.amazonaws.com\",\n                    \"kms:EncryptionContext:aws:s3:arn\": \"arn:aws:s3:::destination-bucket-name-1/key-prefix1/*\"\n                }\n            }\n        },\n        {\n            \"Action\": [\"kms:Encrypt\"],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"AWS KMS key ARNs (for the AWS Region of destination bucket 2). Used to encrypt object replicas created in destination bucket 2.\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"kms:ViaService\": \"s3.destination-bucket-2-region.amazonaws.com\",\n                    \"kms:EncryptionContext:aws:s3:arn\": \"arn:aws:s3:::destination-bucket-2-name/key-prefix1*\"\n                }\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the AWS KMS single region key used for the current S3 bucket into an AWS KMS multi-region key. Enable S3 batch replication for the existing data in the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region</strong> - S3 batch replication can certainly be used to replicate the existing data in the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region.</p>\n\n<p>However, you cannot convert an existing single-Region key to a multi-Region key. This design ensures that all data protected with existing single-Region keys maintain the same data residency and data sovereignty properties. So this option is incorrect.</p>\n\n<p><strong>Enable replication for the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region. Share the existing AWS KMS key from <code>us-east-1</code> region to <code>us-west-1</code> region</strong> - You cannot share an AWS KMS key to another region, so this option is incorrect.</p>\n\n<p><strong>Create a CloudWatch scheduled rule to invoke a Lambda function to copy the daily data from the source bucket in <code>us-east-1</code> region to the destination bucket in <code>us-west-1</code> region. Provide AWS KMS key access to the Lambda function for encryption and decryption operations on the data in the source and destination S3 buckets</strong> - This option is a distractor as the daily frequency of data replication would result in significant data loss in case of a disaster. In addition, this option involves significant development effort to create the functionality to reliably replicate the data from source to destination buckets. So this option is not the best fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html\">https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html</a></p>\n",
                "options": [
                    {
                        "id": 9216,
                        "content": "<p>Create a new S3 bucket in the <code>us-east-1</code> region with replication enabled from this new bucket into another bucket in <code>us-west-1</code> region. Enable SSE-KMS encryption on the new bucket in <code>us-east-1</code> region by using an AWS KMS multi-region key. Copy the existing data from the current S3 bucket in <code>us-east-1</code> region into this new S3 bucket in <code>us-east-1</code> region</p>",
                        "isValid": true
                    },
                    {
                        "id": 9217,
                        "content": "<p>Create a CloudWatch scheduled rule to invoke a Lambda function to copy the daily data from the source bucket in <code>us-east-1</code> region to the destination bucket in <code>us-west-1</code> region. Provide AWS KMS key access to the Lambda function for encryption and decryption operations on the data in the source and destination S3 buckets</p>",
                        "isValid": false
                    },
                    {
                        "id": 9218,
                        "content": "<p>Change the AWS KMS single region key used for the current S3 bucket into an AWS KMS multi-region key. Enable S3 batch replication for the existing data in the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region</p>",
                        "isValid": false
                    },
                    {
                        "id": 9219,
                        "content": "<p>Enable replication for the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region. Share the existing AWS KMS key from <code>us-east-1</code> region to <code>us-west-1</code> region</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2208,
            "attributes": {
                "createdAt": "2023-09-09T20:33:38.496Z",
                "updatedAt": "2023-09-09T20:33:38.496Z",
                "content": "<p>Your company has deployed an application that will perform a lot of overwrites and deletes on data and require the latest information to be available anytime data is read via queries on database tables.</p>\n\n<p>As a Solutions Architect, which database technology will you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Relational Database Service (Amazon RDS)</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. RDS allows you to create, read, update, and delete records without any item lock or ambiguity. All RDS transactions must be ACID compliant or be Atomic, Consistent, Isolated, and Durable to ensure data integrity.</p>\n\n<p>Atomicity requires that either transaction as a whole is successfully executed or if a part of the transaction fails, then the entire transaction be invalidated. Consistency mandates the data written to the database as part of the transaction must adhere to all defined rules, and restrictions including constraints, cascades, and triggers. Isolation is critical to achieving concurrency control and makes sure each transaction is independent unto itself. Durability requires that all of the changes made to the database be permanent once a transaction is completed.\nHence, the best fit is RDS.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. ElastiCache could work but it's a better fit as a caching technology to enhance reads.</p>\n\n<p><strong>Amazon Simple Storage Service (Amazon S3)</strong> - This option is incorrect as S3 is not a database technology that supports queries on database tables out of the box. It is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3.</p>\n\n<p>After a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected. Strong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects.</p>\n\n<p><strong>Amazon Neptune</strong> - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency.</p>\n\n<p>Amazon Neptune is highly available, with read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across Availability Zones. Neptune is secure with support for HTTPS encrypted client connections and encryption at rest. Neptune is fully managed, so you no longer need to worry about database management tasks such as hardware provisioning, software patching, setup, configuration, or backups. Neptune is a graph database so it's not a good fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/relational-database/\">https://aws.amazon.com/relational-database/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/\">https://aws.amazon.com/rds/</a></p>\n\n<p><a href=\"https://aws.amazon.com/neptune/\">https://aws.amazon.com/neptune/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel</a></p>\n",
                "options": [
                    {
                        "id": 9220,
                        "content": "<p>Amazon ElastiCache</p>",
                        "isValid": false
                    },
                    {
                        "id": 9221,
                        "content": "<p>Amazon Relational Database Service (Amazon RDS)</p>",
                        "isValid": true
                    },
                    {
                        "id": 9222,
                        "content": "<p>Amazon Neptune</p>",
                        "isValid": false
                    },
                    {
                        "id": 9223,
                        "content": "<p>Amazon Simple Storage Service (Amazon S3)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2209,
            "attributes": {
                "createdAt": "2023-09-09T20:33:38.589Z",
                "updatedAt": "2023-09-09T20:33:38.589Z",
                "content": "<p>Which of the following IAM policies provides read-only access to the S3 bucket <code>mybucket</code> and its content?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket\"\n      },\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket/*\"\n      }\n   ]\n}\n</code></pre>\n\n<p>You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations SCPs, ACLs, and session policies.</p>\n\n<p><code>s3:ListBucket</code> is applied to buckets, so the ARN is in the form  <code>\"Resource\":\"arn:aws:s3:::mybucket\"</code>, without a trailing <code>/</code>\n<code>s3:GetObject</code> is applied to objects within the bucket, so the ARN is in the form <code>\"Resource\":\"arn:aws:s3:::mybucket/*\"</code>, with a trailing <code>/*</code> to indicate all objects within the bucket <code>mybucket</code></p>\n\n<p>Therefore, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\",\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket\"\n      }\n   ]\n}\n</code></pre>\n\n<p>This option is incorrect as it provides read-only access only to the bucket, not its contents.</p>\n\n<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\",\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket/*\"\n      }\n   ]\n}\n</code></pre>\n\n<p>This option is incorrect as it provides read-only access only to the bucket contents, not to the bucket itself.</p>\n\n<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket/*\"\n      },\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket\"\n      }\n   ]\n}\n</code></pre>\n\n<p>This option is incorrect as it provides listing access only to the bucket contents.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket/\">https://aws.amazon.com/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a></p>\n",
                "options": [
                    {
                        "id": 9224,
                        "content": "<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket\"\n      },\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket/*\"\n      }\n   ]\n}\n</code></pre>",
                        "isValid": true
                    },
                    {
                        "id": 9225,
                        "content": "<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\",\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket\"\n      }\n   ]\n}\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 9226,
                        "content": "<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket/*\"\n      },\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket\"\n      }\n   ]\n}\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 9227,
                        "content": "<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\",\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket/*\"\n      }\n   ]\n}\n</code></pre>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2210,
            "attributes": {
                "createdAt": "2023-09-09T20:33:38.680Z",
                "updatedAt": "2023-09-09T20:33:38.680Z",
                "content": "<p>A developer has configured inbound traffic for the relevant ports in both the Security Group of the EC2 instance as well as the Network Access Control List (NACL) of the subnet for the EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance.</p>\n\n<p>As a solutions architect, how will you fix this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic</strong> - Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic.</p>\n\n<p>To enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as allow outbound traffic from ephemeral ports. When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port.</p>\n\n<p>The designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL.</p>\n\n<p>By default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive, then you need to explicitly allow traffic from the ephemeral port range.</p>\n\n<p>If you accept traffic from the internet, then you also must establish a route through an internet gateway. If you accept traffic over VPN or AWS Direct Connect, then you must establish a route through a virtual private gateway.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic</strong> - This is incorrect as already discussed.</p>\n\n<p><strong>IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs</strong> - This is a made-up option and just added as a distractor.</p>\n\n<p><strong>Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior</strong> - This option is a distractor. AWS does not support modifying rules of Network ACLs from the command line tool.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/\">https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/</a></p>\n",
                "options": [
                    {
                        "id": 9228,
                        "content": "<p>IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs</p>",
                        "isValid": false
                    },
                    {
                        "id": 9229,
                        "content": "<p>Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior</p>",
                        "isValid": false
                    },
                    {
                        "id": 9230,
                        "content": "<p>Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic</p>",
                        "isValid": true
                    },
                    {
                        "id": 9231,
                        "content": "<p>Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2211,
            "attributes": {
                "createdAt": "2023-09-09T20:33:38.772Z",
                "updatedAt": "2023-09-09T20:33:38.772Z",
                "content": "<p>A Hollywood studio is planning a series of promotional events leading up to the launch of the trailer of its next sci-fi thriller. The executives at the studio want to create a static website with lots of animations in line with the theme of the movie. The studio has hired you as a solutions architect to build a scalable serverless solution.</p>\n\n<p>Which of the following represents the MOST cost-optimal and high-performance solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Build the website as a static website hosted on Amazon S3. Create a CloudFront distribution with Amazon S3 as the origin. Use Amazon Route 53 to create an alias record that points to your CloudFront distribution</strong></p>\n\n<p>You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket.</p>\n\n<p>Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.</p>\n\n<p>You can use Amazon CloudFront to improve the performance of your website. CloudFront makes your website files (such as HTML, images, and video) available from data centers around the world (called edge locations). When a visitor requests a file from your website, CloudFront automatically redirects the request to a copy of the file at the nearest edge location. This results in faster download times than if the visitor had requested the content from a data center that is located farther away. Therefore, this option is correct.</p>\n\n<p>Hosting a static website on Amazon S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Host the website on AWS Lambda. Create a CloudFront distribution with Lambda as the origin</strong></p>\n\n<p>With AWS Lambda, you can run code without provisioning or managing servers. You can't host a website on Lambda. Also, you can't have CloudFront in front of Lambda. So this option is incorrect.</p>\n\n<p><strong>Host the website on an EC2 instance. Create a CloudFront distribution with the EC2 instance as the custom origin</strong></p>\n\n<p><strong>Host the website on an instance in the studio's on-premises data center. Create a CloudFront distribution with this instance as the custom origin</strong></p>\n\n<p>Hosting the website on an EC2 instance or a data-center specific instance is ruled out as the studio wants a serverless solution. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-cloudfront-walkthrough.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-cloudfront-walkthrough.html</a></p>\n",
                "options": [
                    {
                        "id": 9232,
                        "content": "<p>Build the website as a static website hosted on Amazon S3. Create a CloudFront distribution with Amazon S3 as the origin. Use Amazon Route 53 to create an alias record that points to your CloudFront distribution</p>",
                        "isValid": true
                    },
                    {
                        "id": 9233,
                        "content": "<p>Host the website on an instance in the studio's on-premises data center. Create a CloudFront distribution with this instance as the custom origin</p>",
                        "isValid": false
                    },
                    {
                        "id": 9234,
                        "content": "<p>Host the website on AWS Lambda. Create a CloudFront distribution with Lambda as the origin</p>",
                        "isValid": false
                    },
                    {
                        "id": 9235,
                        "content": "<p>Host the website on an EC2 instance. Create a CloudFront distribution with the EC2 instance as the custom origin</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2212,
            "attributes": {
                "createdAt": "2023-09-09T20:33:38.861Z",
                "updatedAt": "2023-09-09T20:33:38.861Z",
                "content": "<p>A cybersecurity company uses a fleet of EC2 instances to run a proprietary application. The infrastructure maintenance group at the company wants to be notified via an email whenever the CPU utilization for any of the EC2 instances breaches a certain threshold.</p>\n\n<p>Which of the following services would you use for building a solution with the LEAST amount of development effort? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Amazon SNS</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging.</p>\n\n<p><strong>Amazon CloudWatch</strong> - Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. Amazon CloudWatch allows you to monitor AWS cloud resources and the applications you run on AWS.</p>\n\n<p>You can use CloudWatch Alarms to send an email via SNS whenever any of the EC2 instances breaches a certain threshold. Hence both these options are correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Lambda</strong> - With AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consume—there’s no charge when your code isn’t running. You can run code for virtually any type of application or backend service—all with zero administration. You cannot use AWS Lambda to monitor CPU utilization of EC2 instances or send notification emails, hence this option is incorrect.</p>\n\n<p><strong>Amazon SQS</strong> - Amazon SQS Standard offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. You cannot use SQS to monitor CPU utilization of EC2 instances or send notification emails, hence this option is incorrect.</p>\n\n<p><strong>AWS Step Functions</strong> - AWS Step Functions lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker, into feature-rich applications. You cannot use Step Functions to monitor CPU utilization of EC2 instances or send notification emails, hence this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/faqs/\">https://aws.amazon.com/cloudwatch/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n",
                "options": [
                    {
                        "id": 9236,
                        "content": "<p>Amazon SNS</p>",
                        "isValid": true
                    },
                    {
                        "id": 9237,
                        "content": "<p>Amazon CloudWatch</p>",
                        "isValid": true
                    },
                    {
                        "id": 9238,
                        "content": "<p>AWS Step Functions</p>",
                        "isValid": false
                    },
                    {
                        "id": 9239,
                        "content": "<p>Amazon SQS</p>",
                        "isValid": false
                    },
                    {
                        "id": 9240,
                        "content": "<p>AWS Lambda</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2213,
            "attributes": {
                "createdAt": "2023-09-09T20:33:39.070Z",
                "updatedAt": "2023-09-09T20:33:39.070Z",
                "content": "<p>A company is developing a healthcare application that cannot afford any downtime for database write operations. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution using Amazon Aurora.</p>\n\n<p>Which of the following options would you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up an Aurora multi-master DB cluster</strong></p>\n\n<p>In a multi-master cluster, all DB instances can perform write operations. There isn't any failover when a writer DB instance becomes unavailable, because another writer DB instance is immediately available to take over the work of the failed instance. AWS refers to this type of availability as continuous availability, to distinguish it from the high availability (with brief downtime during failover) offered by a single-master cluster. For applications where you can't afford even brief downtime for database write operations, a multi-master cluster can help to avoid an outage when a writer instance becomes unavailable. The multi-master cluster doesn't use the failover mechanism, because it doesn't need to promote another DB instance to have read/write capability.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q61-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html#aurora-multi-master-workloads\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html#aurora-multi-master-workloads</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Aurora serverless DB cluster</strong></p>\n\n<p><strong>Set up an Aurora provisioned DB cluster</strong></p>\n\n<p><strong>Set up an Aurora Global Database cluster</strong></p>\n\n<p>These three options represent Aurora single-master clusters. In a single-master cluster, a single DB instance performs all write operations and any other DB instances are read-only. If the writer DB instance becomes unavailable, a failover mechanism promotes one of the read-only instances to be the new writer. As there is a brief downtime during this failover, so these three options are incorrect for the given use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html#aurora-multi-master-workloads\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html#aurora-multi-master-workloads</a></p>\n",
                "options": [
                    {
                        "id": 9241,
                        "content": "<p>Set up an Aurora Global Database cluster</p>",
                        "isValid": false
                    },
                    {
                        "id": 9242,
                        "content": "<p>Set up an Aurora serverless DB cluster</p>",
                        "isValid": false
                    },
                    {
                        "id": 9243,
                        "content": "<p>Set up an Aurora multi-master DB cluster</p>",
                        "isValid": true
                    },
                    {
                        "id": 9244,
                        "content": "<p>Set up an Aurora provisioned DB cluster</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2214,
            "attributes": {
                "createdAt": "2023-09-09T20:33:39.205Z",
                "updatedAt": "2023-09-09T20:33:39.205Z",
                "content": "<p>A big data consulting firm needs to set up a data lake on Amazon S3 for a Health-Care client. The data lake is split in raw and refined zones. For compliance reasons, the source data needs to be kept for a minimum of 5 years. The source data arrives in the raw zone and is then processed via an AWS Glue based ETL job into the refined zone. The business analysts run ad-hoc queries only on the data in the refined zone using AWS Athena. The team is concerned about the cost of data storage in both the raw and refined zones as the data is increasing at a rate of 1TB daily in each zone.</p>\n\n<p>As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Setup a lifecycle policy to transition the raw zone data into Glacier Deep Archive after 1 day of object creation</strong></p>\n\n<p>You can manage your objects so that they are stored cost-effectively throughout their lifecycle by configuring their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them.</p>\n\n<p>For the given use-case, the raw zone consists of the source data, so it cannot be deleted due to compliance reasons. Therefore, you should use a lifecycle policy to transition the raw zone data into Glacier Deep Archive after 1 day of object creation.</p>\n\n<p>Please read more about S3 Object Lifecycle Management:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p>\n\n<p><strong>Use Glue ETL job to write the transformed data in the refined zone using a compressed file format</strong></p>\n\n<p>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics.\nYou cannot transition the refined zone data into Glacier Deep Archive because it is used by the business analysts for ad-hoc querying. Therefore, the best optimization is to have the refined zone data stored in a compressed format via the Glue job. The compressed data would reduce the storage cost incurred on the data in the refined zone.</p>\n\n<p>Please see this example for a Glue ETL Pipeline:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Glue_Event-driven-ETL-Pipelines.e24d59bb79a9e24cdba7f43ffd234ec0482a60e2.png\">\nvia - <a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lambda function based job to delete the raw zone data after 1 day</strong> - As mentioned in the use-case, the source data needs to be kept for a minimum of 5 years for compliance reasons. Therefore the data in the raw zone cannot be deleted after 1 day.</p>\n\n<p><strong>Setup a lifecycle policy to transition the refined zone data into Glacier Deep Archive after 1 day of object creation</strong> - You cannot transition the refined zone data into Glacier Deep Archive because it is used by the business analysts for ad-hoc querying. Hence this option is incorrect.</p>\n\n<p><strong>Use Glue ETL job to write the transformed data in the refined zone using CSV format</strong> - It is cost-optimal to write the data in the refined zone using a compressed format instead of CSV format. The compressed data would reduce the storage cost incurred on the data in the refined zone. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p>\n",
                "options": [
                    {
                        "id": 9245,
                        "content": "<p>Create a Lambda function based job to delete the raw zone data after 1 day</p>",
                        "isValid": false
                    },
                    {
                        "id": 9246,
                        "content": "<p>Use Glue ETL job to write the transformed data in the refined zone using CSV format</p>",
                        "isValid": false
                    },
                    {
                        "id": 9247,
                        "content": "<p>Setup a lifecycle policy to transition the raw zone data into Glacier Deep Archive after 1 day of object creation</p>",
                        "isValid": true
                    },
                    {
                        "id": 9248,
                        "content": "<p>Use Glue ETL job to write the transformed data in the refined zone using a compressed file format</p>",
                        "isValid": true
                    },
                    {
                        "id": 9249,
                        "content": "<p>Setup a lifecycle policy to transition the refined zone data into Glacier Deep Archive after 1 day of object creation</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2215,
            "attributes": {
                "createdAt": "2023-09-09T20:33:39.300Z",
                "updatedAt": "2023-09-09T20:33:39.300Z",
                "content": "<p>A retail company wants to share sensitive accounting data that is stored in an Amazon RDS DB instance with an external auditor. The auditor has its own AWS account and needs its own copy of the database.</p>\n\n<p>Which of the following would you recommend to securely share the database with the auditor?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key</strong></p>\n\n<p>You can share the AWS Key Management Service (AWS KMS) customer master key (CMK) that was used to encrypt the snapshot with any accounts that you want to be able to access the snapshot. You can share AWS KMS CMKs with another AWS account by adding the other account to the AWS KMS key policy.</p>\n\n<p>Making an encrypted snapshot of the database will give the auditor a copy of the database, as required for the given use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket</strong> - RDS stores the DB snapshots in the Amazon S3 bucket belonging to the same AWS region where the RDS instance is located. RDS stores these on your behalf and you do not have direct access to these snapshots in S3, so it's not possible to grant access to the snapshot objects in S3.</p>\n\n<p><strong>Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket</strong> - This solution is feasible though not optimal. It requires a lot of unnecessary work and is difficult to audit when such bulk data is exported into text files.</p>\n\n<p><strong>Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access</strong> - Read Replicas make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Creating Read Replicas for audit purposes is overkill. Also, the question mentions that the auditor needs to have their own copy of the database, which is not possible with replicas.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html</a></p>\n",
                "options": [
                    {
                        "id": 9250,
                        "content": "<p>Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access</p>",
                        "isValid": false
                    },
                    {
                        "id": 9251,
                        "content": "<p>Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 9252,
                        "content": "<p>Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key</p>",
                        "isValid": true
                    },
                    {
                        "id": 9253,
                        "content": "<p>Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2216,
            "attributes": {
                "createdAt": "2023-09-09T20:33:39.384Z",
                "updatedAt": "2023-09-09T20:33:39.384Z",
                "content": "<p>A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted EFS file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the EFS file system.</p>\n\n<p>Which of the following represents the MOST operationally efficient way to meet this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up a DataSync scheduled task to send the video files to the EFS file system every 24 hours</strong></p>\n\n<p>AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data between on-premises storage systems and AWS Storage services, as well as between AWS Storage services.</p>\n\n<p>You can use AWS DataSync to migrate data located on-premises, at the edge, or in other clouds to Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon FSx for Lustre, Amazon FSx for OpenZFS, and Amazon FSx for NetApp ONTAP.</p>\n\n<p>AWS DataSync:\n<img src=\"https://d1.awsstatic.com/Digital%20Marketing/House/Editorial/products/DataSync/Product-Page-Diagram_AWS-DataSync_On-Premises-to-AWS%402x.8769b9dea1615c18ee0597b236946cbe0103b2da.png\">\nvia - <a href=\"https://aws.amazon.com/datasync/\">https://aws.amazon.com/datasync/</a></p>\n\n<p>To establish a private connection between your virtual private cloud (VPC) and the Amazon EFS API, you can create an interface VPC endpoint. You can also access the interface VPC endpoint from on-premises environments or other VPCs using AWS VPN, AWS Direct Connect, or VPC peering.</p>\n\n<p>AWS Direct Connect provides three types of virtual interfaces: public, private, and transit.</p>\n\n<p>AWS Direct Connect VIFs:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q11-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/\">https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/</a></p>\n\n<p>For the given use case, you can send data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF.</p>\n\n<p>Using task scheduling in AWS DataSync, you can periodically execute a transfer task from your source storage system to the destination. You can use the DataSync scheduled task to send the video files to the EFS file system every 24 hours.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up a DataSync scheduled task to send the video files to the EFS file system every 24 hours</strong> - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them privately. You cannot use VPC peering to transfer data over the Direct Connect connection from the on-premises systems to AWS. So this option is incorrect.</p>\n\n<p><strong>Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the Direct Connect connection to an S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the EFS file system</strong> - You can use a public virtual interface to connect to AWS resources that are reachable by a public IP address such as an Amazon Simple Storage Service (Amazon S3) bucket or AWS public endpoints. Although it is theoretically possible to set up this solution, however, it is not the most operationally efficient solution, since it involves sending data via DataSync to S3 and then in turn using a Lambda function to finally send data to EFS.</p>\n\n<p><strong>Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the Direct Connect connection to an S3 bucket by using an S3 VPC endpoint. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the EFS file system</strong> - You can access Amazon S3 from your VPC using gateway VPC endpoints. You cannot use the S3 VPC endpoint to transfer data over the Direct Connect connection from the on-premises systems to S3. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/datasync/\">https://aws.amazon.com/datasync/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/storage/transferring-files-from-on-premises-to-aws-and-back-without-leaving-your-vpc-using-aws-datasync/\">https://aws.amazon.com/blogs/storage/transferring-files-from-on-premises-to-aws-and-back-without-leaving-your-vpc-using-aws-datasync/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/efs-vpc-endpoints.html\">https://docs.aws.amazon.com/efs/latest/ug/efs-vpc-endpoints.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/datasync/faqs/\">https://aws.amazon.com/datasync/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/\">https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/datasync/latest/userguide/task-scheduling.html\">https://docs.aws.amazon.com/datasync/latest/userguide/task-scheduling.html</a></p>\n",
                "options": [
                    {
                        "id": 9254,
                        "content": "<p>Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the Direct Connect connection to an S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the EFS file system</p>",
                        "isValid": false
                    },
                    {
                        "id": 9255,
                        "content": "<p>Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up a DataSync scheduled task to send the video files to the EFS file system every 24 hours</p>",
                        "isValid": false
                    },
                    {
                        "id": 9256,
                        "content": "<p>Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the Direct Connect connection to an S3 bucket by using an S3 VPC endpoint. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the EFS file system</p>",
                        "isValid": false
                    },
                    {
                        "id": 9257,
                        "content": "<p>Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up a DataSync scheduled task to send the video files to the EFS file system every 24 hours</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2217,
            "attributes": {
                "createdAt": "2023-09-09T20:33:39.471Z",
                "updatedAt": "2023-09-09T20:33:39.471Z",
                "content": "<p>An application is currently hosted on four EC2 instances (behind Application Load Balancer) deployed in a single Availability Zone (AZ). To maintain an acceptable level of end-user experience, the application needs at least 4 instances to be always available.</p>\n\n<p>As a solutions architect, which of the following would you recommend so that the application achieves high availability with MINIMUM cost?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the instances in three Availability Zones. Launch two instances in each Availability Zone</strong></p>\n\n<p>The correct option is to deploy the instances in three Availability Zones and launch two instances in each Availability Zone. Even if one of the AZs goes out of service, still we shall have 4 instances available and the application can maintain an acceptable level of end-user experience. Therefore, we can achieve high availability with just 6 instances in this case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the instances in two Availability Zones. Launch two instances in each Availability Zone</strong> - When we launch two instances in two AZs, we run the risk of falling below the minimum acceptable threshold of 4 instances if one of the AZs fails. So this option is ruled out.</p>\n\n<p><strong>Deploy the instances in two Availability Zones. Launch four instances in each Availability Zone</strong> - When we launch four instances in two AZs, we have to bear costs for 8 instances which is NOT cost-optimal. So this option is ruled out.</p>\n\n<p><strong>Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone</strong> - We can't have just two instances in a single AZ as that is below the minimum acceptable threshold of 4 instances.</p>\n",
                "options": [
                    {
                        "id": 9258,
                        "content": "<p>Deploy the instances in two Availability Zones. Launch two instances in each Availability Zone</p>",
                        "isValid": false
                    },
                    {
                        "id": 9259,
                        "content": "<p>Deploy the instances in three Availability Zones. Launch two instances in each Availability Zone</p>",
                        "isValid": true
                    },
                    {
                        "id": 9260,
                        "content": "<p>Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone</p>",
                        "isValid": false
                    },
                    {
                        "id": 9261,
                        "content": "<p>Deploy the instances in two Availability Zones. Launch four instances in each Availability Zone</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2218,
            "attributes": {
                "createdAt": "2023-09-09T20:33:39.557Z",
                "updatedAt": "2023-09-09T20:33:39.557Z",
                "content": "<p>The engineering manager for a content management application wants to set up RDS read replicas to provide enhanced performance and read scalability. The manager wants to understand the data transfer charges while setting up RDS read replicas.</p>\n\n<p>Which of the following would you identify as correct regarding the data transfer charges for RDS read replicas?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>There are data transfer charges for replicating data across AWS Regions</strong></p>\n\n<p>RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.</p>\n\n<p>A read replica is billed as a standard DB Instance and at the same rates. You are not charged for the data transfer incurred in replicating data between your source DB instance and read replica within the same AWS Region.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q62-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>There are data transfer charges for replicating data within the same Availability Zone</strong></p>\n\n<p><strong>There are data transfer charges for replicating data within the same AWS Region</strong></p>\n\n<p><strong>There are no data transfer charges for replicating data across AWS Regions</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9262,
                        "content": "<p>There are data transfer charges for replicating data across AWS Regions</p>",
                        "isValid": true
                    },
                    {
                        "id": 9263,
                        "content": "<p>There are no data transfer charges for replicating data across AWS Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 9264,
                        "content": "<p>There are data transfer charges for replicating data within the same Availability Zone</p>",
                        "isValid": false
                    },
                    {
                        "id": 9265,
                        "content": "<p>There are data transfer charges for replicating data within the same AWS Region</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2219,
            "attributes": {
                "createdAt": "2023-09-09T20:33:39.638Z",
                "updatedAt": "2023-09-09T20:33:39.638Z",
                "content": "<p>A health-care solutions company wants to run their applications on single-tenant hardware to meet regulatory guidelines.</p>\n\n<p>Which of the following is the MOST cost-effective way of isolating their Amazon EC2 instances to a single tenant?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Dedicated Instances</strong> - Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.</p>\n\n<p>A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.</p>\n\n<p>Differences between Dedicated Hosts and Dedicated Instances:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Spot Instances</strong> -  A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price.  Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option.</p>\n\n<p><strong>Dedicated Hosts</strong> - An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement.</p>\n\n<p><strong>On-Demand Instances</strong> - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements.</p>\n\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q23-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html</a></p>\n",
                "options": [
                    {
                        "id": 9266,
                        "content": "<p>Spot Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9267,
                        "content": "<p>Dedicated Hosts</p>",
                        "isValid": false
                    },
                    {
                        "id": 9268,
                        "content": "<p>On-Demand Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9269,
                        "content": "<p>Dedicated Instances</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2220,
            "attributes": {
                "createdAt": "2023-09-09T20:33:39.748Z",
                "updatedAt": "2023-09-09T20:33:39.748Z",
                "content": "<p>A startup has just developed a video backup service hosted on a fleet of EC2 instances. The EC2 instances are behind an Application Load Balancer and the instances are using EBS volumes for storage. The service provides authenticated users the ability to upload videos that are then saved on the EBS volume attached to a given instance. On the first day of the beta launch, users start complaining that they can see only some of the videos in their uploaded videos backup. Every time the users log into the website, they claim to see a different subset of their uploaded videos.</p>\n\n<p>Which of the following is the MOST optimal solution to make sure that users can view all the uploaded videos? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Write a one time job to copy the videos from all EBS volumes to S3 and then modify the application to use Amazon S3 standard for storing the videos</strong></p>\n\n<p><strong>Mount EFS on all EC2 instances. Write a one time job to copy the videos from all EBS volumes to EFS. Modify the application to use EFS for storing the videos</strong></p>\n\n<p>Amazon Elastic Block Store (EBS) is an easy to use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale.</p>\n\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth.</p>\n\n<p>Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.</p>\n\n<p>As EBS volumes are attached locally to the EC2 instances, therefore the uploaded videos are tied to specific EC2 instances. Every time the user logs in, they are directed to a different instance and therefore their videos get dispersed across multiple EBS volumes. The correct solution is to use either S3 or EFS to store the user videos.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Write a one time job to copy the videos from all EBS volumes to S3 Glacier Deep Archive and then modify the application to use S3 Glacier Deep Archive for storing the videos</strong> - Glacier Deep Archive is meant to be used for long term data archival. It cannot be used to serve static content such as videos or images via a web application. So this option is incorrect.</p>\n\n<p><strong>Write a one time job to copy the videos from all EBS volumes to RDS and then modify the application to use RDS for storing the videos</strong> - RDS is a relational database and not the right candidate for storing videos.</p>\n\n<p><strong>Write a one time job to copy the videos from all EBS volumes to DynamoDB and then modify the application to use DynamoDB for storing the videos</strong> - DynamoDB is a NoSQL database and not the right candidate for storing videos.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/ebs/\">https://aws.amazon.com/ebs/</a></p>\n",
                "options": [
                    {
                        "id": 9270,
                        "content": "<p>Write a one time job to copy the videos from all EBS volumes to DynamoDB and then modify the application to use DynamoDB for storing the videos</p>",
                        "isValid": false
                    },
                    {
                        "id": 9271,
                        "content": "<p>Write a one time job to copy the videos from all EBS volumes to S3 and then modify the application to use Amazon S3 standard for storing the videos</p>",
                        "isValid": true
                    },
                    {
                        "id": 9272,
                        "content": "<p>Write a one time job to copy the videos from all EBS volumes to RDS and then modify the application to use RDS for storing the videos</p>",
                        "isValid": false
                    },
                    {
                        "id": 9273,
                        "content": "<p>Mount EFS on all EC2 instances. Write a one time job to copy the videos from all EBS volumes to EFS. Modify the application to use EFS for storing the videos</p>",
                        "isValid": true
                    },
                    {
                        "id": 9274,
                        "content": "<p>Write a one time job to copy the videos from all EBS volumes to S3 Glacier Deep Archive and then modify the application to use S3 Glacier Deep Archive for storing the videos</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2221,
            "attributes": {
                "createdAt": "2023-09-09T20:33:39.855Z",
                "updatedAt": "2023-09-09T20:33:39.855Z",
                "content": "<p>A financial services company has deployed its flagship application on EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party SSL/TLS certificates configured on EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort.</p>\n\n<p>What will you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an SNS notification to the security team if any certificate expires within 30 days</strong></p>\n\n<p>AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks.</p>\n\n<p>AWS Config provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q9-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html\">https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html</a></p>\n\n<p>AWS Config provides AWS-managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. You can leverage an AWS Config managed rule to check if any ACM certificates in your account are marked for expiration within the specified number of days. Certificates provided by ACM are automatically renewed. ACM does not automatically renew the certificates that you import. The rule is NON_COMPLIANT if your certificates are about to expire.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q9-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html\">https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html</a></p>\n\n<p>You can configure AWS Config to stream configuration changes and notifications to an Amazon SNS topic. For example, when a resource is updated, you can get a notification sent to your email, so that you can view the changes. You can also be notified when AWS Config evaluates your custom or managed rules against your resources.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Monitor the <code>days to expiry</code> CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the <code>days to expiry</code> metric and then trigger a custom action of notifying the security team</strong> - AWS Certificate Manager (ACM) does not attempt to renew third-party certificates that are imported. Also, an administrator needs to reconfigure missing DNS records for certificates that use DNS validation if the record was removed for any reason after the certificate was issued. Metrics and events provide you visibility into such certificates that require intervention to continue the renewal process. Amazon CloudWatch metrics and Amazon EventBridge events are enabled for all certificates that are managed by ACM. Users can monitor <code>days to expiry</code> as a metric for ACM certificates through Amazon CloudWatch. An Amazon EventBridge expiry event is published for any certificate that is at least 45 days away from expiry by default. Users can build alarms to monitor certificates based on days to expiry and also trigger custom actions such as calling a Lambda function or paging an administrator.</p>\n\n<p>It is certainly possible to use the <code>days to expiry</code> CloudWatch metric to build a CloudWatch alarm to monitor the imported ACM certificates. The alarm will, in turn, trigger a notification to the security team. But this option needs more configuration effort than directly using the AWS Config managed rule that is available off-the-shelf.</p>\n\n<p><strong>Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an SNS notification to the security team if any certificate expires within 30 days</strong></p>\n\n<p><strong>Monitor the <code>days to expiry</code> CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the <code>days to expiry</code> metric and then trigger a custom action of notifying the security team</strong></p>\n\n<p>Any SSL/TLS certificates created via ACM do not need any monitoring/intervention for expiration. ACM automatically renews such certificates. Hence both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html\">https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html\">https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/acm-certificate-expiration-check.html\">https://docs.aws.amazon.com/config/latest/developerguide/acm-certificate-expiration-check.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/\">https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/</a></p>\n",
                "options": [
                    {
                        "id": 9275,
                        "content": "<p>Monitor the <code>days to expiry</code> CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the <code>days to expiry</code> metric and then trigger a custom action of notifying the security team</p>",
                        "isValid": false
                    },
                    {
                        "id": 9276,
                        "content": "<p>Monitor the <code>days to expiry</code> CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the <code>days to expiry</code> metric and then trigger a custom action of notifying the security team</p>",
                        "isValid": false
                    },
                    {
                        "id": 9277,
                        "content": "<p>Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an SNS notification to the security team if any certificate expires within 30 days</p>",
                        "isValid": false
                    },
                    {
                        "id": 9278,
                        "content": "<p>Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an SNS notification to the security team if any certificate expires within 30 days</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2222,
            "attributes": {
                "createdAt": "2023-09-09T20:33:39.958Z",
                "updatedAt": "2023-09-09T20:33:39.958Z",
                "content": "<p>A silicon valley based startup has a two-tier architecture using EC2 instances for its flagship application. The web servers (listening on port 443), which have been assigned security group A, are in public subnets across two Availability Zones and the MSSQL based database instances (listening on port 1433), which have been assigned security group B, are in two private subnets across two Availability Zones. The DevOps team wants to review the security configurations of the application architecture.</p>\n\n<p>As a solutions architect, which of the following options would you select as the MOST secure configuration? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433</strong></p>\n\n<p><strong>For security group B: Add an inbound rule that allows traffic only from security group A on port 1433</strong></p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.</p>\n\n<p>The following are the characteristics of security group rules:</p>\n\n<p>By default, security groups allow all outbound traffic.</p>\n\n<p>Security group rules are always permissive; you can't create rules that deny access.</p>\n\n<p>Security groups are stateful</p>\n\n<p>The MOST secure configuration for the given use case is:</p>\n\n<p>For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433</p>\n\n<p>The above rules make sure that web servers are listening for traffic on all sources on the HTTPS protocol on port 443. The web servers only allow outbound traffic to MSSQL servers in Security Group B on port 1433.</p>\n\n<p>For security group B: Add an inbound rule that allows traffic only from security group A on port 1433.\nThe above rule makes sure that the MSSQL servers only accept traffic from web servers in security group A on port 1433.</p>\n\n<p>Therefore, both of these options are correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443</strong> - As the MSSQL based database instances are listening on port 1433, therefore for security group A, the outbound rule should be added on port 443 with the destination as security group B.</p>\n\n<p><strong>For security group B: Add an inbound rule that allows traffic only from all sources on port 1433</strong> - The inbound rule should allow traffic only from security group A on port 1433. Allowing traffic from all sources will compromise security.</p>\n\n<p><strong>For security group B: Add an inbound rule that allows traffic only from security group A on port 443</strong> -  The inbound rule should allow traffic only from security group A on port 1433 because the MSSQL based database instances are listening on port 1433.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 9279,
                        "content": "<p>For security group B: Add an inbound rule that allows traffic only from security group A on port 443</p>",
                        "isValid": false
                    },
                    {
                        "id": 9280,
                        "content": "<p>For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443</p>",
                        "isValid": false
                    },
                    {
                        "id": 9281,
                        "content": "<p>For security group B: Add an inbound rule that allows traffic only from all sources on port 1433</p>",
                        "isValid": false
                    },
                    {
                        "id": 9282,
                        "content": "<p>For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433</p>",
                        "isValid": true
                    },
                    {
                        "id": 9283,
                        "content": "<p>For security group B: Add an inbound rule that allows traffic only from security group A on port 1433</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2223,
            "attributes": {
                "createdAt": "2023-09-09T20:33:40.093Z",
                "updatedAt": "2023-09-09T20:33:40.093Z",
                "content": "<p>A developer needs to implement a Lambda function in AWS account A that accesses an Amazon S3 bucket in AWS account B.</p>\n\n<p>As a Solutions Architect, which of the following will you recommend to meet this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM role for the Lambda function that grants access to the S3 bucket. Set the IAM role as the Lambda function's execution role. Make sure that the bucket policy also grants access to the Lambda function's execution role</strong></p>\n\n<p>If the IAM role that you create for the Lambda function is in the same AWS account as the bucket, then you don't need to grant Amazon S3 permissions on both the IAM role and the bucket policy. Instead, you can grant the permissions on the IAM role and then verify that the bucket policy doesn't explicitly deny access to the Lambda function role. If the IAM role and the bucket are in different accounts, then you need to grant Amazon S3 permissions on both the IAM role and the bucket policy. Therefore, this is the right way of giving access to AWS Lambda for the given use-case.</p>\n\n<p>Complete list of steps to be followed:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q16-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda</strong> - This is an incorrect statement, used only as a distractor.</p>\n\n<p><strong>Create an IAM role for the Lambda function that grants access to the S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the Lambda function cross-account access to the S3 bucket</strong> - When the execution role of Lambda and S3 bucket to be accessed are from different accounts, then you need to grant S3 bucket access permissions to the IAM role and also ensure that the bucket policy grants access to the Lambda function's execution role.</p>\n\n<p><strong>The S3 bucket owner should make the bucket public so that it can be accessed by the Lambda function in the other AWS account</strong> - Making the S3 bucket public for the given use-case will be considered as a security bad practice. It's usually done for very few use-cases such as hosting a website on S3. Therefore this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/</a></p>\n",
                "options": [
                    {
                        "id": 9284,
                        "content": "<p>AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 9285,
                        "content": "<p>Create an IAM role for the Lambda function that grants access to the S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the Lambda function cross-account access to the S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 9286,
                        "content": "<p>Create an IAM role for the Lambda function that grants access to the S3 bucket. Set the IAM role as the Lambda function's execution role. Make sure that the bucket policy also grants access to the Lambda function's execution role</p>",
                        "isValid": true
                    },
                    {
                        "id": 9287,
                        "content": "<p>The S3 bucket owner should make the bucket public so that it can be accessed by the Lambda function in the other AWS account</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2224,
            "attributes": {
                "createdAt": "2023-09-09T20:33:40.215Z",
                "updatedAt": "2023-09-09T20:33:40.215Z",
                "content": "<p>A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to DNS caching. The company has only two days left for the annual Thanksgiving sale to commence.</p>\n\n<p>As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Blue/green deployment is a technique for releasing applications by shifting traffic between two identical environments running different versions of the application: \"Blue\" is the currently running version and \"green\" the new version. This type of deployment allows you to test features in the green environment without impacting the currently running version of your application. When you’re satisfied that the green version is working properly, you can gradually reroute the traffic from the old blue environment to the new green environment. Blue/green deployments can mitigate common risks associated with deploying software, such as downtime and rollback capability.</p>\n\n<p><strong>Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment</strong> - AWS Global Accelerator is a network layer service that directs traffic to optimal endpoints over the AWS global network, this improves the availability and performance of your internet applications. It provides two static anycast IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers, Elastic IP addresses or Amazon EC2 instances, in a single or in multiple AWS regions.</p>\n\n<p>AWS Global Accelerator uses endpoint weights to determine the proportion of traffic that is directed to endpoints in an endpoint group, and traffic dials to control the percentage of traffic that is directed to an endpoint group (an AWS region where your application is deployed).</p>\n\n<p>While relying on the DNS service is a great option for blue/green deployments, it may not fit use-cases that require a fast and controlled transition of the traffic. Some client devices and internet resolvers cache DNS answers for long periods; this DNS feature improves the efficiency of the DNS service as it reduces the DNS traffic across the Internet, and serves as a resiliency technique by preventing authoritative name-server overloads. The downside of this in blue/green deployments is that you don’t know how long it will take before all of your users receive updated IP addresses when you update a record, change your routing preference or when there is an application failure.</p>\n\n<p>With AWS Global Accelerator, you can shift traffic gradually or all at once between the blue and the green environment and vice-versa without being subject to DNS caching on client devices and internet resolvers, traffic dials and endpoint weights changes are effective within seconds.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Route 53 weighted routing to spread traffic across different deployments</strong> - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software. As discussed earlier, DNS caching is a negative behavior for this use case and hence Route 53 is not a good option.</p>\n\n<p><strong>Use Elastic Load Balancer to distribute traffic across deployments</strong> - An ELB can distribute traffic across healthy instances. You can also use the ALB weighted target groups feature for blue/green deployments as it does not rely on the DNS service. In addition you don’t need to create new ALBs for the green environment. As the use-case refers to a global application, so this option cannot be used for a multi-Region solution which is needed for the given requirement.</p>\n\n<p><strong>Use AWS CodeDeploy deployment options to choose the right deployment</strong> - In CodeDeploy, a deployment is the process, and the components involved in the process, of installing content on one or more instances. This content can consist of code, web and configuration files, executables, packages, scripts, and so on. CodeDeploy deploys content that is stored in a source repository, according to the configuration rules you specify. Blue/Green deployment is one of the deployment types that CodeDeploy supports. CodeDeploy is not meant to distribute traffic across instances, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/using-aws-global-accelerator-to-achieve-blue-green-deployments\">https://aws.amazon.com/blogs/networking-and-content-delivery/using-aws-global-accelerator-to-achieve-blue-green-deployments</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted</a></p>\n",
                "options": [
                    {
                        "id": 9288,
                        "content": "<p>Use Elastic Load Balancer to distribute traffic across deployments</p>",
                        "isValid": false
                    },
                    {
                        "id": 9289,
                        "content": "<p>Use Route 53 weighted routing to spread traffic across different deployments</p>",
                        "isValid": false
                    },
                    {
                        "id": 9290,
                        "content": "<p>Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment</p>",
                        "isValid": true
                    },
                    {
                        "id": 9291,
                        "content": "<p>Use AWS CodeDeploy deployment options to choose the right deployment</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2225,
            "attributes": {
                "createdAt": "2023-09-09T20:33:40.460Z",
                "updatedAt": "2023-09-09T20:33:40.460Z",
                "content": "<p>You would like to mount a network file system on Linux instances, where files will be stored and accessed frequently at first, and then infrequently. What solution is the MOST cost-effective?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>EFS IA</strong></p>\n\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability.</p>\n\n<p>Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files, not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. Therefore, this is the correct option.</p>\n\n<p>How EFS works:\n<img src=\"https://d1.awsstatic.com/r2018/b/EFS/product-page-diagram-Amazon-EFS-Launch_How-It-Works.cf947858f0ef3557b9fc14077bdf3f65b3f9ff43.png\">\nvia - <a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>S3 Intelligent Tiering</strong> - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access.</p>\n\n<p>You can't mount a network file system on S3 Intelligent Tiering as it's an object storage service, so this option is incorrect.</p>\n\n<p><strong>Glacier Deep Archive</strong> - Amazon S3 Glacier and S3 Glacier Deep Archive are a secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n\n<p>You can't mount a network file system on S3 Intelligent Tiering as it's an object storage/archival service, so this option is incorrect.</p>\n\n<p><strong>FSx for Lustre</strong> - Amazon FSx for Lustre makes it easy and cost-effective to launch and run the world’s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. Amazon FSx enables you to use Lustre file systems for any workload where storage speed matters.</p>\n\n<p>FSx for Lustre is a file system better suited for distributed computing for HPC (high-performance computing) and is very expensive</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/efs/features/infrequent-access/\">https://aws.amazon.com/efs/features/infrequent-access/</a></p>\n",
                "options": [
                    {
                        "id": 9292,
                        "content": "<p>FSx for Lustre</p>",
                        "isValid": false
                    },
                    {
                        "id": 9293,
                        "content": "<p>Glacier Deep Archive</p>",
                        "isValid": false
                    },
                    {
                        "id": 9294,
                        "content": "<p>EFS IA</p>",
                        "isValid": true
                    },
                    {
                        "id": 9295,
                        "content": "<p>S3 Intelligent Tiering</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2226,
            "attributes": {
                "createdAt": "2023-09-09T20:33:40.564Z",
                "updatedAt": "2023-09-09T20:33:40.564Z",
                "content": "<p>Amazon EC2 Auto Scaling needs to terminate an instance from Availability Zone (AZ) <code>us-east-1a</code> as it has the most number of instances amongst the AZs being used currently. There are 4 instances in the AZ <code>us-east-1a</code> like so: Instance A has the oldest launch template, Instance B has the oldest launch configuration, Instance C has the newest launch configuration and Instance D is closest to the next billing hour.</p>\n\n<p>Which of the following instances would be terminated per the default termination policy?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Instance B</strong></p>\n\n<p>Per the default termination policy, the first priority is given to any allocation strategy for On-Demand vs Spot instances. As no such information has been provided for the given use-case, so this criterion can be ignored. The next priority is to consider any instance with the oldest launch template unless there is an instance that uses a launch configuration. So this rules out Instance A. Next, you need to consider any instance which has the oldest launch configuration. This implies Instance B will be selected for termination and Instance C will also be ruled out as it has the newest launch configuration. Instance D, which is closest to the next billing hour, is not selected as this criterion is last in the order of priority.</p>\n\n<p>Please see this note for a deep-dive on the default termination policy:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q65-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Instance A</strong></p>\n\n<p><strong>Instance C</strong></p>\n\n<p><strong>Instance D</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>\n",
                "options": [
                    {
                        "id": 9296,
                        "content": "<p>Instance B</p>",
                        "isValid": true
                    },
                    {
                        "id": 9297,
                        "content": "<p>Instance D</p>",
                        "isValid": false
                    },
                    {
                        "id": 9298,
                        "content": "<p>Instance C</p>",
                        "isValid": false
                    },
                    {
                        "id": 9299,
                        "content": "<p>Instance A</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2227,
            "attributes": {
                "createdAt": "2023-09-09T20:33:40.641Z",
                "updatedAt": "2023-09-09T20:33:40.641Z",
                "content": "<p>You have a team of developers in your company, and you would like to ensure they can quickly experiment with AWS Managed Policies by attaching them to their accounts, but you would like to prevent them from doing an escalation of privileges, by granting themselves the <code>AdministratorAccess</code> managed policy. How should you proceed?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves</strong></p>\n\n<p>AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.\nHere we have to use an IAM permission boundary. They can only be applied to roles or users, not IAM groups.</p>\n\n<p>Permissions boundaries for IAM entities:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the <code>AdministratorAccess</code> policy</strong> - Service control policies (SCPs) are one type of policy that you can use to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines. SCPs are available only in an organization that has all features enabled. SCPs aren't available if your organization has enabled only the consolidated billing features. Attaching an SCP to an AWS Organizations entity (root, OU, or account) defines a guardrail for what actions the principals can perform.\nIf you consider this option, since AWS Organizations is not mentioned in this question, so we can't apply an SCP.</p>\n\n<p><strong>Attach an IAM policy to your developers, that prevents them from attaching the <code>AdministratorAccess</code> policy</strong> - This option is incorrect as the developers can remove this policy from themselves and escalate their privileges.</p>\n\n<p><strong>Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves</strong> - IAM permission boundary can only be applied to roles or users, not IAM groups. Hence this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>\n",
                "options": [
                    {
                        "id": 9300,
                        "content": "<p>Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves</p>",
                        "isValid": false
                    },
                    {
                        "id": 9301,
                        "content": "<p>Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the <code>AdministratorAccess</code> policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 9302,
                        "content": "<p>For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves</p>",
                        "isValid": true
                    },
                    {
                        "id": 9303,
                        "content": "<p>Attach an IAM policy to your developers, that prevents them from attaching the <code>AdministratorAccess</code> policy</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2228,
            "attributes": {
                "createdAt": "2023-09-09T20:33:40.717Z",
                "updatedAt": "2023-09-09T20:33:40.717Z",
                "content": "<p>What does this IAM policy do?</p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Mystery Policy\",\n      \"Action\": [\n        \"ec2:RunInstances\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:RequestedRegion\": \"eu-west-1\"\n        }\n      }\n    }\n  ]\n}\n</code></pre>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>It allows running EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world</strong></p>\n\n<p>You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations SCPs, ACLs, and session policies.</p>\n\n<p>You can use the <code>aws:RequestedRegion</code> key to compare the AWS Region that was called in the request with the Region that you specify in the policy. You can use this global condition key to control which Regions can be requested.</p>\n\n<p><code>aws:RequestedRegion</code> represents the target of the API call. So in this example, we can only launch EC2 instances in eu-west-1, and we can do this API call from anywhere.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>It allows running EC2 instances anywhere but in the eu-west-1 region</strong></p>\n\n<p><strong>It allows running EC2 instances in any region when the API call is originating from the eu-west-1 region</strong></p>\n\n<p><strong>It allows running EC2 instances in the eu-west-1 region when the API call is made from the eu-west-1 region</strong></p>\n\n<p>These three options contradict the earlier details provided in the explanation. To summarize, <code>aws:RequestedRegion</code> represents the target of the API call. So, we can only launch EC2 instances in eu-west-1 region and we can do this API call from anywhere. Hence these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html</a></p>\n",
                "options": [
                    {
                        "id": 9304,
                        "content": "<p>It allows running EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world</p>",
                        "isValid": true
                    },
                    {
                        "id": 9305,
                        "content": "<p>It allows running EC2 instances in any region when the API call is originating from the eu-west-1 region</p>",
                        "isValid": false
                    },
                    {
                        "id": 9306,
                        "content": "<p>It allows running EC2 instances anywhere but in the eu-west-1 region</p>",
                        "isValid": false
                    },
                    {
                        "id": 9307,
                        "content": "<p>It allows to run EC2 instances in the eu-west-1 region, when the API call is made from the eu-west-1 region</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2229,
            "attributes": {
                "createdAt": "2023-09-09T20:33:40.813Z",
                "updatedAt": "2023-09-09T20:33:40.813Z",
                "content": "<p>What is true about RDS Read Replicas encryption?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>If the master database is encrypted, the read replicas are encrypted</strong></p>\n\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance. read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.</p>\n\n<p>On a database instance running with Amazon RDS encryption, data stored at rest in the underlying storage is encrypted, as are its automated backups, read replicas, and snapshots. Therefore, this option is correct.</p>\n\n<p>RDS Read Replica Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q44-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If the master database is encrypted, the read replicas can be either encrypted or unencrypted</strong> - If the master database is encrypted, the read replicas are necessarily encrypted, so this option is incorrect.</p>\n\n<p><strong>If the master database is unencrypted, the read replicas can be either encrypted or unencrypted</strong></p>\n\n<p><strong>If the master database is unencrypted, the read replicas are encrypted</strong></p>\n\n<p>If the master database is not encrypted, the read replicas cannot be encrypted, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n",
                "options": [
                    {
                        "id": 9308,
                        "content": "<p>If the master database is unencrypted, the read replicas can be either encrypted or unencrypted</p>",
                        "isValid": false
                    },
                    {
                        "id": 9309,
                        "content": "<p>If the master database is unencrypted, the read replicas are encrypted</p>",
                        "isValid": false
                    },
                    {
                        "id": 9310,
                        "content": "<p>If the master database is encrypted, the read replicas can be either encrypted or unencrypted</p>",
                        "isValid": false
                    },
                    {
                        "id": 9311,
                        "content": "<p>If the master database is encrypted, the read replicas are encrypted</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2230,
            "attributes": {
                "createdAt": "2023-09-09T20:33:40.908Z",
                "updatedAt": "2023-09-09T20:33:40.908Z",
                "content": "<p>Consider the following policy associated with an IAM group containing several users:</p>\n\n<pre><code>{\n    \"Version\":\"2012-10-17\",\n    \"Id\":\"EC2TerminationPolicy\",\n    \"Statement\":[\n        {\n            \"Effect\":\"Deny\",\n            \"Action\":\"ec2:*\",\n            \"Resource\":\"*\",\n            \"Condition\":{\n                \"StringNotEquals\":{\n                    \"ec2:Region\":\"us-west-1\"\n                }\n            }\n        },\n        {\n            \"Effect\":\"Allow\",\n            \"Action\":\"ec2:TerminateInstances\",\n            \"Resource\":\"*\",\n            \"Condition\":{\n                \"IpAddress\":{\n                    \"aws:SourceIp\":\"10.200.200.0/24\"\n                }\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>Which of the following options is correct?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Users belonging to the IAM group can terminate an EC2 instance in the <code>us-west-1</code> region when the user's source IP is 10.200.200.200</strong></p>\n\n<p>The given policy denies all EC2 specification actions on all resources when the region of the underlying resource is not <code>us-west-1</code>. The policy allows the terminate EC2 action on all resources when the source IP address is in the CIDR range 10.200.200.0/24, therefore it would allow the user with the source IP 10.200.200.200 to terminate the EC2 instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Users belonging to the IAM group cannot terminate an EC2 instance in the <code>us-west-1</code> region when the user's source IP is 10.200.200.200</strong></p>\n\n<p><strong>Users belonging to the IAM group can terminate an EC2 instance in the <code>us-west-1</code> region when the EC2 instance's IP address is 10.200.200.200</strong></p>\n\n<p><strong>Users belonging to the IAM group can terminate an EC2 instance belonging to any region except the <code>us-west-1</code> region when the user's source IP is 10.200.200.200</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n",
                "options": [
                    {
                        "id": 9312,
                        "content": "<p>Users belonging to the IAM group can terminate an EC2 instance in the <code>us-west-1</code> region when the EC2 instance's IP address is 10.200.200.200</p>",
                        "isValid": false
                    },
                    {
                        "id": 9313,
                        "content": "<p>Users belonging to the IAM group can terminate an EC2 instance belonging to any region except the <code>us-west-1</code> region when the user's source IP is 10.200.200.200</p>",
                        "isValid": false
                    },
                    {
                        "id": 9314,
                        "content": "<p>Users belonging to the IAM group cannot terminate an EC2 instance in the <code>us-west-1</code> region when the user's source IP is 10.200.200.200</p>",
                        "isValid": false
                    },
                    {
                        "id": 9315,
                        "content": "<p>Users belonging to the IAM group can terminate an EC2 instance in the <code>us-west-1</code> region when the user's source IP is 10.200.200.200</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2231,
            "attributes": {
                "createdAt": "2023-09-09T20:33:41.082Z",
                "updatedAt": "2023-09-09T20:33:41.082Z",
                "content": "<p>A Machine Learning research group uses a proprietary computer vision application hosted on an EC2 instance. Every time the instance needs to be stopped and started again, the application takes about 3 minutes to start as some auxiliary software programs need to be executed so that the application can function. The research group would like to minimize the application boostrap time whenever the system needs to be stopped and then started at a later point in time.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use EC2 Instance Hibernate</strong></p>\n\n<p>When you hibernate an instance, AWS signals the operating system to perform hibernation (suspend-to-disk). Hibernation saves the contents from the instance memory (RAM) to your Amazon EBS root volume. AWS then persists the instance's Amazon EBS root volume and any attached Amazon EBS data volumes.</p>\n\n<p>When you start your instance:</p>\n\n<p>The Amazon EBS root volume is restored to its previous state</p>\n\n<p>The RAM contents are reloaded</p>\n\n<p>The processes that were previously running on the instance are resumed</p>\n\n<p>Previously attached data volumes are reattached and the instance retains its instance ID</p>\n\n<p>Overview of EC2 hibernation:\n<img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/hibernation-flow.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html</a></p>\n\n<p>By using EC2 hibernate, we have the capability to resume it at any point of time, with the application already launched, thus helping us cut the 3 minutes start time.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use EC2 User-Data</strong> - EC2 instance user data is the data that you specified in the form of a configuration script while launching your instance. Here, the problem is that the application takes 3 minutes to launch, no matter what. EC2 user data won't help us because it's just here to help us execute a list of commands, not speed them up.</p>\n\n<p><strong>Use EC2 Meta-Data</strong> - EC2 instance metadata is data about your instance that you can use to configure or manage the running instance. Instance metadata is divided into categories, for example, host name, events, and security groups. The EC2 meta-data is a distractor and can only help us determine some metadata attributes on our EC2 instances.</p>\n\n<p><strong>Create an AMI and launch your EC2 instances from that</strong> - An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. You can launch multiple instances from a single AMI when you need multiple instances with the same configuration. You can use different AMIs to launch instances when you need instances with different configurations.</p>\n\n<p>Creating an AMI may help with all the system dependencies, but it won't help us with speeding up the application start time.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html</a></p>\n",
                "options": [
                    {
                        "id": 9316,
                        "content": "<p>Create an AMI and launch your EC2 instances from that</p>",
                        "isValid": false
                    },
                    {
                        "id": 9317,
                        "content": "<p>Use EC2 User-Data</p>",
                        "isValid": false
                    },
                    {
                        "id": 9318,
                        "content": "<p>Use EC2 Meta-Data</p>",
                        "isValid": false
                    },
                    {
                        "id": 9319,
                        "content": "<p>Use EC2 Instance Hibernate</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2232,
            "attributes": {
                "createdAt": "2023-09-09T20:33:41.197Z",
                "updatedAt": "2023-09-09T20:33:41.197Z",
                "content": "<p>A big-data consulting firm is working on a client engagement where the ETL workloads are currently handled via a Hadoop cluster deployed in the on-premises data center. The client wants to migrate their ETL workloads to AWS Cloud. The AWS Cloud solution needs to be highly available with about 50 EC2 instances per Availability Zone.</p>\n\n<p>As a solutions architect, which of the following EC2 placement groups would you recommend handling the distributed ETL workload?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Partition placement group</strong></p>\n\n<p>You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies:</p>\n\n<p>Partition – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. Therefore, this is the correct option for the given use-case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q12-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Cluster placement group</strong></p>\n\n<p>Cluster – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. This is not suited for distributed and replicated workloads such as Hadoop.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q12-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n\n<p><strong>Spread placement group</strong></p>\n\n<p>Spread – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures. This is not suited for distributed and replicated workloads such as Hadoop.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q12-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n\n<p><strong>Both Spread placement group and Partition placement group</strong> - As mentioned earlier, the spread placement group is not suited for distributed and replicated workloads such as Hadoop. So this option is also incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 9320,
                        "content": "<p>Spread placement group</p>",
                        "isValid": false
                    },
                    {
                        "id": 9321,
                        "content": "<p>Cluster placement group</p>",
                        "isValid": false
                    },
                    {
                        "id": 9322,
                        "content": "<p>Both Spread placement group and Partition placement group</p>",
                        "isValid": false
                    },
                    {
                        "id": 9323,
                        "content": "<p>Partition placement group</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2233,
            "attributes": {
                "createdAt": "2023-09-09T20:33:41.284Z",
                "updatedAt": "2023-09-09T20:33:41.284Z",
                "content": "<p>You would like to migrate an AWS account from an AWS Organization A to an AWS Organization B. What are the steps do to it?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Remove the member account from the old organization. Send an invite to the member account from the new Organization. Accept the invite to the new organization from the member account</strong></p>\n\n<p>AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS. Using AWS Organizations, you can automate account creation, create groups of accounts to reflect your business needs, and apply policies for these groups for governance. You can also simplify billing by setting up a single payment method for all of your AWS accounts. Through integrations with other AWS services, you can use Organizations to define central configurations and resource sharing across accounts in your organization.</p>\n\n<p>To migrate accounts from one organization to another, you must have root or IAM access to both the member and master accounts. Here are the steps to follow:\n1. Remove the member account from the old organization\n2. Send an invite to the member account from the new Organization\n3. Accept the invite to the new organization from the member account</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Send an invite to the new organization. Accept the invite to the new organization from the member account. Remove the member account from the old organization</strong></p>\n\n<p><strong>Send an invite to the new organization. Remove the member account from the old organization. Accept the invite to the new organization from the member account</strong></p>\n\n<p>These two options contradict the steps described earlier for account migration from one organization to another.</p>\n\n<p><strong>Open an AWS Support ticket to ask them to migrate the account</strong> - You don't need to contact AWS support for account migration.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/organizations/\">https://aws.amazon.com/organizations/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/\">https://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/</a></p>\n",
                "options": [
                    {
                        "id": 9324,
                        "content": "<p>Remove the member account from the old organization. Send an invite to the member account from the new Organization. Accept the invite to the new organization from the member account</p>",
                        "isValid": true
                    },
                    {
                        "id": 9325,
                        "content": "<p>Open an AWS Support ticket to ask them to migrate the account</p>",
                        "isValid": false
                    },
                    {
                        "id": 9326,
                        "content": "<p>Send an invite to the new organization. Accept the invite to the new organization from the member account. Remove the member account from the old organization</p>",
                        "isValid": false
                    },
                    {
                        "id": 9327,
                        "content": "<p>Send an invite to the new organization. Remove the member account from the old organization. Accept the invite to the new organization from the member account</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2234,
            "attributes": {
                "createdAt": "2023-09-09T20:33:41.375Z",
                "updatedAt": "2023-09-09T20:33:41.375Z",
                "content": "<p>A financial services company wants a single log processing model for all the log files (consisting of system logs, application logs, database logs, etc) that can be processed in a serverless fashion and then durably stored for downstream analytics. The company wants to use an AWS managed service that automatically scales to match the throughput of the log data and requires no ongoing administration.</p>\n\n<p>As a solutions architect, which of the following AWS services would you recommend solving this problem?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Kinesis Data Firehose</strong></p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. Therefore, this is the correct option.</p>\n\n<p>Please see this overview of how Kinesis Firehose works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Kinesis Data Streams</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. With Amazon Kinesis Data Streams, you can scale up to a sufficient number of shards (note, however, that you'll need to provision enough shards ahead of time). As it requires manual administration of shards, it's not the correct choice for the given use-case.</p>\n\n<p><strong>Amazon EMR</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.</p>\n\n<p>Using an EMR cluster would imply managing the underlying infrastructure so it’s ruled out.</p>\n\n<p><strong>AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used for production-grade serverless log analytics.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n",
                "options": [
                    {
                        "id": 9328,
                        "content": "<p>Amazon EMR</p>",
                        "isValid": false
                    },
                    {
                        "id": 9329,
                        "content": "<p>Kinesis Data Firehose</p>",
                        "isValid": true
                    },
                    {
                        "id": 9330,
                        "content": "<p>Kinesis Data Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 9331,
                        "content": "<p>AWS Lambda</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2235,
            "attributes": {
                "createdAt": "2023-09-09T20:33:41.471Z",
                "updatedAt": "2023-09-09T20:33:41.471Z",
                "content": "<p>An IT company provides S3 bucket access to specific users within the same account for completing project specific work. With changing business requirements, cross-account S3 access requests are also growing every month. The company is looking for a solution that can offer user level as well as account-level access permissions for the data stored in S3 buckets.</p>\n\n<p>As a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 Bucket Policies</strong></p>\n\n<p>Bucket policies in Amazon S3 can be used to add or deny permissions across some or all of the objects within a single bucket. Policies can be attached to users, groups, or Amazon S3 buckets, enabling centralized management of permissions. With bucket policies, you can grant users within your AWS Account or other AWS Accounts access to your Amazon S3 resources.</p>\n\n<p>You can further restrict access to specific resources based on certain conditions. For example, you can restrict access based on request time (Date Condition), whether the request was sent using SSL (Boolean Conditions), a requester’s IP address (IP Address Condition), or based on the requester's client application (String Conditions). To identify these conditions, you use policy keys.</p>\n\n<p>Types of access control in S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q14-i1.jpg\">\nvia - <a href=\"https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf\">https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Identity and Access Management (IAM) policies</strong> - AWS IAM enables organizations with many employees to create and manage multiple users under a single AWS account. IAM policies are attached to the users, enabling centralized control of permissions for users under your AWS Account to access buckets or objects. With IAM policies, you can only grant users within your own AWS account permission to access your Amazon S3 resources. So, this is not the right choice for the current requirement.</p>\n\n<p><strong>Use Access Control Lists (ACLs)</strong> - Within Amazon S3, you can use ACLs to give read or write access on buckets or objects to groups of users. With ACLs, you can only grant other AWS accounts (not specific users) access to your Amazon S3 resources. So, this is not the right choice for the current requirement.</p>\n\n<p><strong>Use Security Groups</strong> - A security group acts as a virtual firewall for EC2 instances to control incoming and outgoing traffic. S3 does not support Security Groups, this option just acts as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf\">https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf</a></p>\n",
                "options": [
                    {
                        "id": 9332,
                        "content": "<p>Use Access Control Lists (ACLs)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9333,
                        "content": "<p>Use Identity and Access Management (IAM) policies</p>",
                        "isValid": false
                    },
                    {
                        "id": 9334,
                        "content": "<p>Use Amazon S3 Bucket Policies</p>",
                        "isValid": true
                    },
                    {
                        "id": 9335,
                        "content": "<p>Use Security Groups</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2236,
            "attributes": {
                "createdAt": "2023-09-09T20:33:41.562Z",
                "updatedAt": "2023-09-09T20:33:41.562Z",
                "content": "<p>A junior DevOps engineer wants to change the default configuration for EBS volume termination. By default, the root volume of an EC2 instance for an EBS-backed AMI is deleted when the instance terminates.</p>\n\n<p>Which option below helps change this default behavior to ensure that the volume persists even after the instance terminates?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set the DeleteOnTermination attribute to false</strong></p>\n\n<p>An EC2 instance can be launched from either an instance store-backed AMI or an Amazon EBS-backed AMI. Instances that use Amazon EBS for the root device automatically have an Amazon EBS volume attached. By default, the root volume for an AMI backed by Amazon EBS is deleted when the instance terminates.&lt;br/&gt;\nThe default behavior can be changed to ensure that the volume persists after the instance terminates. To change the default behavior, set the DeleteOnTermination attribute to false using a block device mapping.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set the TerminateOnDelete attribute to true</strong></p>\n\n<p><strong>Set the TerminateOnDelete attribute to false</strong></p>\n\n<p>Both these options are incorrect as there is no such attribute as TerminateOnDelete. These options have been added as distractors.</p>\n\n<p><strong>Set the DeleteOnTermination attribute to true</strong> - If you set the DeleteOnTermination attribute to true, then the root volume for an AMI backed by Amazon EBS would be deleted when the instance terminates. Therefore, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html</a></p>\n",
                "options": [
                    {
                        "id": 9336,
                        "content": "<p>Set the TerminateOnDelete attribute to false</p>",
                        "isValid": false
                    },
                    {
                        "id": 9337,
                        "content": "<p>Set the DeleteOnTermination attribute to false</p>",
                        "isValid": true
                    },
                    {
                        "id": 9338,
                        "content": "<p>Set the TerminateOnDelete attribute to true</p>",
                        "isValid": false
                    },
                    {
                        "id": 9339,
                        "content": "<p>Set the DeleteOnTermination attribute to true</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2237,
            "attributes": {
                "createdAt": "2023-09-09T20:33:41.651Z",
                "updatedAt": "2023-09-09T20:33:41.651Z",
                "content": "<p>An IT company is working on a client project to build a Supply Chain Management application. The web-tier of the application runs on an EC2 instance and the database tier is on Amazon RDS MySQL. For beta testing, all the resources are currently deployed in a single Availability Zone. The development team wants to improve application availability before the go-live.</p>\n\n<p>Given that all end users of the web application would be located in the US, which of the following would be the MOST resource-efficient solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the web-tier EC2 instances in two Availability Zones, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration</strong></p>\n\n<p>Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Therefore, deploying the web-tier EC2 instances in two Availability Zones, behind an Elastic Load Balancer would improve the availability of the application.</p>\n\n<p>Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.\nDeploying the Amazon RDS MySQL database in Multi-AZ configuration would improve availability and hence this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the web-tier EC2 instances in two Availability Zones, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration</strong></p>\n\n<p><strong>Deploy the web-tier EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration</strong></p>\n\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Read replicas are meant to address scalability issues. You cannot use read replicas for improving availability, so both these options are incorrect.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review this comparison vis-a-vis Multi-AZ vs Read Replica for RDS:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q8-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p><strong>Deploy the web-tier EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration</strong> - As Elastic Load Balancing does not work across regions, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n",
                "options": [
                    {
                        "id": 9340,
                        "content": "<p>Deploy the web-tier EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration</p>",
                        "isValid": false
                    },
                    {
                        "id": 9341,
                        "content": "<p>Deploy the web-tier EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration</p>",
                        "isValid": false
                    },
                    {
                        "id": 9342,
                        "content": "<p>Deploy the web-tier EC2 instances in two Availability Zones, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration</p>",
                        "isValid": false
                    },
                    {
                        "id": 9343,
                        "content": "<p>Deploy the web-tier EC2 instances in two Availability Zones, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2238,
            "attributes": {
                "createdAt": "2023-09-09T20:33:41.755Z",
                "updatedAt": "2023-09-09T20:33:41.755Z",
                "content": "<p>What does this IAM policy do?</p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Mystery Policy\",\n      \"Action\": [\n        \"ec2:RunInstances\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"IpAddress\": {\n          \"aws:SourceIp\": \"34.50.31.0/24\"\n        }\n      }\n    }\n  ]\n}\n</code></pre>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>It allows starting EC2 instances only when the IP where the call originates is within the <code>34.50.31.0/24</code> CIDR block</strong></p>\n\n<p>You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations SCPs, ACLs, and session policies.</p>\n\n<p>Consider the following snippet from the given policy document:</p>\n\n<pre><code>      \"Condition\": {\n        \"IpAddress\": {\n          \"aws:SourceIp\": \"34.50.31.0/24\"\n        }\n      }\n</code></pre>\n\n<p>The <code>aws:SourceIP</code> in this condition always represents the IP of the caller of the API. That is very helpful if you want to restrict access to certain AWS API for example from the public IP of your on-premises infrastructure.</p>\n\n<p>Please see this overview of Elastic vs Public vs Private IP addresses:</p>\n\n<p>Elastic IP address - An Elastic IP address is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.</p>\n\n<p>Private IP address - A private IPv4 address is an IP address that's not reachable over the Internet. You can use private IPv4 addresses for communication between instances in the same VPC.</p>\n\n<p>Public IP address - A public IP address is an IPv4 address that's reachable from the Internet. You can use public addresses for communication between your instances and the Internet.</p>\n\n<p>Please note <code>34.50.31.0/24</code> is a public IP range, not a private IP range. Private IP ranges are:\n192.168.0.0 - 192.168.255.255 (65,536 IP addresses)\n172.16.0.0 - 172.31.255.255 (1,048,576 IP addresses)\n10.0.0.0 - 10.255.255.255 (16,777,216 IP addresses)</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>It allows starting EC2 instances only when they have a Public IP within the <code>34.50.31.0/24</code> CIDR block</strong></p>\n\n<p><strong>It allows starting EC2 instances only when they have an Elastic IP within the <code>34.50.31.0/24</code> CIDR block</strong></p>\n\n<p><strong>It allows starting EC2 instances only when they have a Private IP within the <code>34.50.31.0/24</code> CIDR block</strong></p>\n\n<p>Each of these three options suggests that the IP addresses of the EC2 instances must belong to the <code>34.50.31.0/24</code> CIDR block for the EC2 instances to start. Actually, the policy states that the EC2 instance should start only when the IP where the call originates is within the <code>34.50.31.0/24</code> CIDR block. Hence these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-restrict-calls-ip-addresses/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-restrict-calls-ip-addresses/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html</a></p>\n",
                "options": [
                    {
                        "id": 9344,
                        "content": "<p>It allows starting EC2 instances only when they have an Elastic IP within the <code>34.50.31.0/24</code> CIDR block</p>",
                        "isValid": false
                    },
                    {
                        "id": 9345,
                        "content": "<p>It allows starting EC2 instances only when the IP where the call originates is within the <code>34.50.31.0/24</code> CIDR block</p>",
                        "isValid": true
                    },
                    {
                        "id": 9346,
                        "content": "<p>It allows starting EC2 instances only when they have a Private IP within the <code>34.50.31.0/24</code> CIDR block</p>",
                        "isValid": false
                    },
                    {
                        "id": 9347,
                        "content": "<p>It allows starting EC2 instances only when they have a Public IP within the <code>34.50.31.0/24</code> CIDR block</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2239,
            "attributes": {
                "createdAt": "2023-09-09T20:33:41.836Z",
                "updatedAt": "2023-09-09T20:33:41.836Z",
                "content": "<p>A financial services company wants to store confidential data in Amazon S3 and it needs to meet the following data security and compliance norms:</p>\n\n<ol>\n<li>Encryption key usage must be logged for auditing purposes</li>\n<li>Encryption Keys must be rotated every year</li>\n<li>The data must be encrypted at rest</li>\n</ol>\n\n<p>Which is the MOST operationally efficient solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with automatic key rotation</strong></p>\n\n<p>Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it.</p>\n\n<p>S3 server-side encryption\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q50-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html</a></p>\n\n<p>AWS KMS is a managed service that makes it easy for you to create and control the cryptographic keys that are used to protect your data. AWS KMS keys (KMS keys are also known as customer master key (CMK)) are the primary resource in AWS KMS. You can use a KMS key to encrypt, decrypt, and re-encrypt data. An AWS KMS key is a logical representation of a cryptographic key. A KMS key contains metadata, such as the key ID, key spec, key usage, creation date, description, and key state. Most importantly, it contains a reference to the key material that is used when you run cryptographic operations with the KMS key.</p>\n\n<p>When you enable automatic key rotation for a KMS key, AWS KMS generates new cryptographic material for the KMS key every year.</p>\n\n<p>AWS KMS keys:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q50-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html</a></p>\n\n<p>For the given use case, you can set up server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with automatic key rotation.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with manual key rotation</strong> - Although it is possible to manually rotate the AWS KMS key, it is not the best fit solution as it is not operationally efficient.</p>\n\n<p><strong>Server-side encryption (SSE-S3) with automatic key rotation</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates. However, with SSE-S3, you cannot log the usage of the encryption key for auditing purposes. So this option is incorrect.</p>\n\n<p><strong>Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation</strong> - It is possible to automatically rotate the customer-provided keys but you will need to develop the underlying solution to automate the key rotation. Therefore, this option is not operationally efficient.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html</a></p>\n",
                "options": [
                    {
                        "id": 9348,
                        "content": "<p>Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with automatic key rotation</p>",
                        "isValid": true
                    },
                    {
                        "id": 9349,
                        "content": "<p>Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation</p>",
                        "isValid": false
                    },
                    {
                        "id": 9350,
                        "content": "<p>Server-side encryption (SSE-S3) with automatic key rotation</p>",
                        "isValid": false
                    },
                    {
                        "id": 9351,
                        "content": "<p>Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with manual key rotation</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2240,
            "attributes": {
                "createdAt": "2023-09-09T20:33:41.927Z",
                "updatedAt": "2023-09-09T20:33:41.927Z",
                "content": "<p>An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale.</p>\n\n<p>Which of the following will you recommend as the MOST cost-effective and high-performance solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Aurora Global Database to enable fast local reads with low latency in each region</strong></p>\n\n<p>Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database.</p>\n\n<p>Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case.</p>\n\n<p>Amazon Aurora Global Database Features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q5-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>Global Tables builds upon DynamoDB’s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read, and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions. Given that the use-case wants you to continue with the underlying schema of the relational database, DynamoDB is not the right choice as it's a NoSQL database.</p>\n\n<p>DynamoDB Global Tables Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q5-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n\n<p><strong>Spin up a Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. Redshift is not suited to be used as a transactional relational database, so this option is not correct.</p>\n\n<p><strong>Spin up EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases</strong> - Setting up EC2 instances in multiple regions with manually managed MySQL databases represents a maintenance nightmare and is not the correct choice for this use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n",
                "options": [
                    {
                        "id": 9352,
                        "content": "<p>Use Amazon Aurora Global Database to enable fast local reads with low latency in each region</p>",
                        "isValid": true
                    },
                    {
                        "id": 9353,
                        "content": "<p>Spin up EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases</p>",
                        "isValid": false
                    },
                    {
                        "id": 9354,
                        "content": "<p>Spin up a Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters</p>",
                        "isValid": false
                    },
                    {
                        "id": 9355,
                        "content": "<p>Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2241,
            "attributes": {
                "createdAt": "2023-09-09T20:33:42.036Z",
                "updatedAt": "2023-09-09T20:33:42.036Z",
                "content": "<p>The engineering team at an e-commerce company is working on cost optimizations for EC2 instances. The team wants to manage the workload using a mix of on-demand and spot instances across multiple instance types. They would like to create an Auto Scaling group with a mix of these instances.</p>\n\n<p>Which of the following options would allow the engineering team to provision the instances for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost</strong></p>\n\n<p>A launch template is similar to a launch configuration, in that it specifies instance configuration information such as the ID of the Amazon Machine Image (AMI), the instance type, a key pair, security groups, and the other parameters that you use to launch EC2 instances. Also, defining a launch template instead of a launch configuration allows you to have multiple versions of a template.</p>\n\n<p>With launch templates, you can provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost. Hence this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost</strong></p>\n\n<p><strong>You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost</strong></p>\n\n<p>A launch configuration is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances such as the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping.</p>\n\n<p>You cannot use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances. Therefore both these options are incorrect.</p>\n\n<p><strong>You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost</strong> - You can use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchTemplates.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchTemplates.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html</a></p>\n",
                "options": [
                    {
                        "id": 9356,
                        "content": "<p>You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost</p>",
                        "isValid": false
                    },
                    {
                        "id": 9357,
                        "content": "<p>You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost</p>",
                        "isValid": false
                    },
                    {
                        "id": 9358,
                        "content": "<p>You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost</p>",
                        "isValid": false
                    },
                    {
                        "id": 9359,
                        "content": "<p>You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2242,
            "attributes": {
                "createdAt": "2023-09-09T20:33:42.149Z",
                "updatedAt": "2023-09-09T20:33:42.149Z",
                "content": "<p>A company has recently launched a new mobile gaming application that the users are adopting rapidly. The company uses RDS MySQL as the database. The engineering team wants an urgent solution to this issue where the rapidly increasing workload might exceed the available database storage.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend so that it requires minimum development and systems administration effort to address this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable storage auto-scaling for RDS MySQL</strong></p>\n\n<p>If your workload is unpredictable, you can enable storage autoscaling for an Amazon RDS DB instance. With storage autoscaling enabled, when Amazon RDS detects that you are running out of free database space it automatically scales up your storage. Amazon RDS starts a storage modification for an autoscaling-enabled DB instance when these factors apply:</p>\n\n<p>Free available space is less than 10 percent of the allocated storage.</p>\n\n<p>The low-storage condition lasts at least five minutes.</p>\n\n<p>At least six hours have passed since the last storage modification.</p>\n\n<p>The maximum storage threshold is the limit that you set for autoscaling the DB instance. You can't set the maximum storage threshold for autoscaling-enabled instances to a value greater than the maximum allocated storage.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate RDS MySQL to Aurora which offers storage auto-scaling</strong> - Although Aurora offers automatic storage scaling, this option is ruled out since it involves significant systems administration effort to migrate from RDS MySQL to Aurora. It is much easier to just enable storage auto-scaling for RDS MySQL.</p>\n\n<p><strong>Migrate RDS MySQL database to DynamoDB which automatically allocates storage space when required</strong> - This option is ruled out since DynamoDB is a NoSQL database which implies significant development effort to change the application logic to connect and query data from the underlying database. It is much easier to just enable storage auto-scaling for RDS MySQL.</p>\n\n<p><strong>Create read replica for RDS MySQL</strong> - Read replicas make it easy to take advantage of supported engines' built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create multiple read replicas for a given source DB Instance and distribute your application’s read traffic amongst them. This option acts as a distractor as read replicas cannot help to automatically scale storage for the primary database.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html</a></p>\n",
                "options": [
                    {
                        "id": 9360,
                        "content": "<p>Enable storage auto-scaling for RDS MySQL</p>",
                        "isValid": true
                    },
                    {
                        "id": 9361,
                        "content": "<p>Migrate RDS MySQL database to DynamoDB which automatically allocates storage space when required</p>",
                        "isValid": false
                    },
                    {
                        "id": 9362,
                        "content": "<p>Migrate RDS MySQL database to Aurora which offers storage auto-scaling</p>",
                        "isValid": false
                    },
                    {
                        "id": 9363,
                        "content": "<p>Create read replica for RDS MySQL</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2243,
            "attributes": {
                "createdAt": "2023-09-09T20:33:42.257Z",
                "updatedAt": "2023-09-09T20:33:42.257Z",
                "content": "<p>A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency.</p>\n\n<p>As a solutions architect, which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Lambda</strong> - With AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consume—there’s no charge when your code isn’t running. You can run code for virtually any type of application or backend service—all with zero administration.</p>\n\n<p><strong>DynamoDB</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB is a NoSQL database and it's best suited to store data in key-value pairs.</p>\n\n<p>AWS Lambda can be combined with DynamoDB to process and capture the key-value data from the IoT sources described in the use-case. So both these options are correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. You cannot use Redshift to capture data in key-value pairs from the IoT sources, so this option is not correct.</p>\n\n<p><strong>ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Elasticache is used as a caching layer in front of relational databases. It is not a good fit to store data in key-value pairs from the IoT sources, so this option is not correct.</p>\n\n<p><strong>RDS</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Relational databases are not a good fit to store data in key-value pairs, so this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n\n<p><a href=\"https://aws.amazon.com/lambda/faqs/\">https://aws.amazon.com/lambda/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9364,
                        "content": "<p>DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 9365,
                        "content": "<p>ElastiCache</p>",
                        "isValid": false
                    },
                    {
                        "id": 9366,
                        "content": "<p>Lambda</p>",
                        "isValid": true
                    },
                    {
                        "id": 9367,
                        "content": "<p>Redshift</p>",
                        "isValid": false
                    },
                    {
                        "id": 9368,
                        "content": "<p>RDS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2244,
            "attributes": {
                "createdAt": "2023-09-09T20:33:42.408Z",
                "updatedAt": "2023-09-09T20:33:42.408Z",
                "content": "<p>An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high I/O and adding latency to the write requests against the database.</p>\n\n<p>As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up a read replica and modify the application to use the appropriate endpoint</strong></p>\n\n<p>An Amazon Aurora DB cluster consists of one or more DB instances and a cluster volume that manages the data for those DB instances. An Aurora cluster volume is a virtual database storage volume that spans multiple Availability Zones, with each Availability Zone having a copy of the DB cluster data. Two types of DB instances make up an Aurora DB cluster:</p>\n\n<p>Primary DB instance – Supports read and write operations, and performs all of the data modifications to the cluster volume. Each Aurora DB cluster has one primary DB instance.</p>\n\n<p>Aurora Replica – Connects to the same storage volume as the primary DB instance and supports only read operations. Each Aurora DB cluster can have up to 15 Aurora Replicas in addition to the primary DB instance. Aurora automatically fails over to an Aurora Replica in case the primary DB instance becomes unavailable. You can specify the failover priority for Aurora Replicas. Aurora Replicas can also offload read workloads from the primary DB instance.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/images/AuroraArch001.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html</a></p>\n\n<p>Aurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer.</p>\n\n<p>While setting up a Multi-AZ deployment for Aurora, you create an Aurora replica or reader node in a different AZ.</p>\n\n<p>Multi-AZ for Aurora:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q13-i1.jpg\"></p>\n\n<p>You use the reader endpoint for read-only connections for your Aurora cluster. This endpoint uses a load-balancing mechanism to help your cluster handle a query-intensive workload. The reader endpoint is the endpoint that you supply to applications that do reporting or other read-only operations on the cluster. The reader endpoint load-balances connections to available Aurora Replicas in an Aurora DB cluster.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q13-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provision another Amazon Aurora database and link it to the primary database as a read replica</strong> - You cannot provision another Aurora database and then link it as a read-replica for the primary database. This option is ruled out.</p>\n\n<p><strong>Configure the application to read from the Multi-AZ standby instance</strong> - This option has been added as a distractor as Aurora does not have any entity called standby instance. You create a standby instance while setting up a Multi-AZ deployment for RDS and NOT for Aurora.</p>\n\n<p>Multi-AZ for RDS:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q13-i3.jpg\"></p>\n\n<p><strong>Activate read-through caching on the Amazon Aurora database</strong> - Aurora does not have built-in support for read-through caching, so this option just serves as a distractor. To implement caching, you will need to integrate something like ElastiCache and that would need code changes for the application.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html</a></p>\n",
                "options": [
                    {
                        "id": 9369,
                        "content": "<p>Provision another Amazon Aurora database and link it to the primary database as a read replica</p>",
                        "isValid": false
                    },
                    {
                        "id": 9370,
                        "content": "<p>Set up a read replica and modify the application to use the appropriate endpoint</p>",
                        "isValid": true
                    },
                    {
                        "id": 9371,
                        "content": "<p>Configure the application to read from the Multi-AZ standby instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 9372,
                        "content": "<p>Activate read-through caching on the Amazon Aurora database</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2245,
            "attributes": {
                "createdAt": "2023-09-09T20:33:42.500Z",
                "updatedAt": "2023-09-09T20:33:42.500Z",
                "content": "<p>A systems administrator has created a private hosted zone and associated it with a Virtual Private Cloud (VPC). However, the DNS queries for the private hosted zone remain unresolved.</p>\n\n<p>As a Solutions Architect, can you identify the Amazon VPC options to be configured in order to get the private hosted zone to work?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable DNS hostnames and DNS resolution for private hosted zones</strong> - DNS hostnames and DNS resolution are required settings for private hosted zones. DNS queries for private hosted zones can be resolved by the Amazon-provided VPC DNS server only. As a result, these options must be enabled for your private hosted zone to work.</p>\n\n<p>DNS hostnames: For non-default virtual private clouds that aren't created using the Amazon VPC wizard, this option is disabled by default. If you create a private hosted zone for a domain and create records in the zone without enabling DNS hostnames, private hosted zones aren't enabled. To use a private hosted zone, this option must be enabled.</p>\n\n<p>DNS resolution: Private hosted zones accept DNS queries only from a VPC DNS server. The IP address of the VPC DNS server is the reserved IP address at the base of the VPC IPv4 network range plus two. Enabling DNS resolution allows you to use the VPC DNS server as a Resolver for performing DNS resolution. Keep this option disabled if you're using a custom DNS server in the DHCP Options set, and you're not using a private hosted zone.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Remove any overlapping namespaces for the private and public hosted zones</strong> - If you have private and public hosted zones that have overlapping namespaces, such as example.com and accounting.example.com, then the Resolver routes traffic based on the most specific match. It won't result in unresolved queries, hence this option is wrong.</p>\n\n<p><strong>Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations</strong> - When you create a hosted zone, Amazon Route 53 automatically creates a name server (NS) record and a start of authority (SOA) record for the zone for public hosted zone. However, this issue is about the private hosted zone, hence this is an incorrect option.</p>\n\n<p><strong>Fix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken</strong> - If you have a private hosted zone (example.com) and a Resolver rule that routes traffic to your network for the same domain name, the Resolver rule takes precedence. It won't result in unresolved queries.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/vpc-enable-private-hosted-zone/\">https://aws.amazon.com/premiumsupport/knowledge-center/vpc-enable-private-hosted-zone/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-considerations.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-considerations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-public-considerations.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-public-considerations.html</a></p>\n",
                "options": [
                    {
                        "id": 9373,
                        "content": "<p>Fix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken</p>",
                        "isValid": false
                    },
                    {
                        "id": 9374,
                        "content": "<p>Remove any overlapping namespaces for the private and public hosted zones</p>",
                        "isValid": false
                    },
                    {
                        "id": 9375,
                        "content": "<p>Enable DNS hostnames and DNS resolution for private hosted zones</p>",
                        "isValid": true
                    },
                    {
                        "id": 9376,
                        "content": "<p>Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2246,
            "attributes": {
                "createdAt": "2023-09-09T20:33:42.627Z",
                "updatedAt": "2023-09-09T20:33:42.627Z",
                "content": "<p>The engineering team at a logistics company has noticed that the Auto Scaling group (ASG) is not terminating an unhealthy Amazon EC2 instance.</p>\n\n<p>As a Solutions Architect, which of the following options would you suggest to troubleshoot the issue? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>The health check grace period for the instance has not expired</strong> - Amazon EC2 Auto Scaling doesn't terminate an instance that came into service based on EC2 status checks and ELB health checks until the health check grace period expires.</p>\n\n<p>More on Health check grace period:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q18-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html#health-check-grace-period\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html#health-check-grace-period</a></p>\n\n<p><strong>The instance maybe in Impaired status</strong> - Amazon EC2 Auto Scaling does not immediately terminate instances with an Impaired status. Instead, Amazon EC2 Auto Scaling waits a few minutes for the instance to recover. Amazon EC2 Auto Scaling might also delay or not terminate instances that fail to report data for status checks. This usually happens when there is insufficient data for the status check metrics in Amazon CloudWatch.</p>\n\n<p><strong>The instance has failed the ELB health check status</strong> - By default, Amazon EC2 Auto Scaling doesn't use the results of ELB health checks to determine an instance's health status when the group's health check configuration is set to EC2. As a result, Amazon EC2 Auto Scaling doesn't terminate instances that fail ELB health checks. If an instance's status is OutofService on the ELB console, but the instance's status is Healthy on the Amazon EC2 Auto Scaling console, confirm that the health check type is set to ELB.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EC2 instance could be a spot instance type, which cannot be terminated by ASG</strong> - This is an incorrect statement. Amazon EC2 Auto Scaling terminates Spot instances when capacity is no longer available or the Spot price exceeds your maximum price.</p>\n\n<p><strong>A user might have updated the configuration of ASG and increased the minimum number of instances forcing ASG to keep all instances alive</strong> - This statement is incorrect. If the configuration is updated and ASG needs more number of instances, ASG will launch new, healthy instances and does not keep unhealthy ones alive.</p>\n\n<p><strong>A custom health check might have failed. ASG does not terminate instances that are set unhealthy by custom checks</strong> - This statement is incorrect. You can define custom health checks in Amazon EC2 Auto Scaling. When a custom health check determines that an instance is unhealthy, the check manually triggers SetInstanceHealth and then sets the instance's state to Unhealthy. Amazon EC2 Auto Scaling then terminates the unhealthy instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-instance-how-terminated/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-instance-how-terminated/</a></p>\n",
                "options": [
                    {
                        "id": 9377,
                        "content": "<p>The instance maybe in Impaired status</p>",
                        "isValid": true
                    },
                    {
                        "id": 9378,
                        "content": "<p>A custom health check might have failed. ASG does not terminate instances that are set unhealthy by custom checks</p>",
                        "isValid": false
                    },
                    {
                        "id": 9379,
                        "content": "<p>A user might have updated the configuration of ASG and increased the minimum number of instances forcing ASG to keep all instances alive</p>",
                        "isValid": false
                    },
                    {
                        "id": 9380,
                        "content": "<p>The health check grace period for the instance has not expired</p>",
                        "isValid": true
                    },
                    {
                        "id": 9381,
                        "content": "<p>The EC2 instance could be a spot instance type, which cannot be terminated by ASG</p>",
                        "isValid": false
                    },
                    {
                        "id": 9382,
                        "content": "<p>The instance has failed the ELB health check status</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2247,
            "attributes": {
                "createdAt": "2023-09-09T20:33:42.736Z",
                "updatedAt": "2023-09-09T20:33:42.736Z",
                "content": "<p>You are establishing a monitoring solution for desktop systems, that will be sending telemetry data into AWS every 1 minute. Data for each system must be processed in order, independently, and you would like to scale the number of consumers to be possibly equal to the number of desktop systems that are being monitored.</p>\n\n<p>What do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use an SQS FIFO queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>We, therefore, need to use an SQS FIFO queue. If we don't specify a GroupID, then all the messages are in absolute order, but we can only have 1 consumer at most.\nTo allow for multiple consumers to read data for each Desktop application, and to scale the number of consumers, we should use the \"Group ID\" attribute. So this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an SQS FIFO queue, and send the telemetry data as is</strong> - This is incorrect because if we send the telemetry data as is then we will not be able to scale the number of consumers to be equal to the number of desktop systems. In this case, each message will have its consumer. So we should use the \"Group ID\" attribute so that multiple consumers can read data for each Desktop application.</p>\n\n<p><strong>Use an SQS standard queue, and send the telemetry data as is</strong> - An SQS standard queue has no ordering capability so that's ruled out.</p>\n\n<p><strong>Use a Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\nA Kinesis Data Stream would work and would give us the data for each desktop application within shards, but we can only have as many consumers as shards in Kinesis (which is in practice, much less than the number of producers).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/solving-complex-ordering-challenges-with-amazon-sqs-fifo-queues/\">https://aws.amazon.com/blogs/compute/solving-complex-ordering-challenges-with-amazon-sqs-fifo-queues/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9383,
                        "content": "<p>Use an SQS FIFO queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID</p>",
                        "isValid": true
                    },
                    {
                        "id": 9384,
                        "content": "<p>Use a Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID</p>",
                        "isValid": false
                    },
                    {
                        "id": 9385,
                        "content": "<p>Use an SQS standard queue, and send the telemetry data as is</p>",
                        "isValid": false
                    },
                    {
                        "id": 9386,
                        "content": "<p>Use an SQS FIFO queue, and send the telemetry data as is</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2248,
            "attributes": {
                "createdAt": "2023-09-09T20:33:42.832Z",
                "updatedAt": "2023-09-09T20:33:42.832Z",
                "content": "<p>A financial services company has developed its flagship application on AWS Cloud with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption key.</p>\n\n<p>Which of the following S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Server-Side Encryption with Customer-Provided Keys (SSE-C)</strong></p>\n\n<p>You have the following options for protecting data at rest in Amazon S3:</p>\n\n<p>Server-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.</p>\n\n<p>Client-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>For the given use-case, the company wants to manage the encryption keys via its custom application and let S3 manage the encryption, therefore you must use Server-Side Encryption with Customer-Provided Keys (SSE-C).</p>\n\n<p>Please review these three options for Server Side Encryption on S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect.</p>\n\n<p><strong>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)</strong> - Server-Side Encryption with Customer Master Keys (CMKs) stored in AWS Key Management Service (SSE-KMS) is similar to SSE-S3. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Additionally, you can create and manage customer-managed CMKs or use AWS managed CMKs that are unique to you, your service, and your Region.</p>\n\n<p><strong>Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3</strong> - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n",
                "options": [
                    {
                        "id": 9387,
                        "content": "<p>Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9388,
                        "content": "<p>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9389,
                        "content": "<p>Server-Side Encryption with Customer-Provided Keys (SSE-C)</p>",
                        "isValid": true
                    },
                    {
                        "id": 9390,
                        "content": "<p>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2249,
            "attributes": {
                "createdAt": "2023-09-09T20:33:42.939Z",
                "updatedAt": "2023-09-09T20:33:42.939Z",
                "content": "<p>Your company has an on-premises Distributed File System Replication (DFSR) service to keep files synchronized on multiple Windows servers, and would like to migrate to AWS cloud.</p>\n\n<p>What do you recommend as a replacement for the DFSR?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>FSx for Windows</strong></p>\n\n<p>Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol.  It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration.\nThe Distributed File System Replication (DFSR) service is a new multi-master replication engine that is used to keep folders synchronized on multiple servers. Amazon FSx supports the use of Microsoft’s Distributed File System (DFS) to organize shares into a single folder structure up to hundreds of PB in size.</p>\n\n<p>FSx for Windows is a perfect distributed file system, with replication capability, and can be mounted on Windows.</p>\n\n<p>How FSx for Windows Works:\n<img src=\"https://d1.awsstatic.com/r2018/b/FSx-Windows/FSx_Windows_File_Server_How-it-Works.9396055e727c3903de991e7f3052ec295c86f274.png\">\nvia - <a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>FSx for Lustre</strong> - Amazon FSx for Lustre makes it easy and cost-effective to launch and run the world’s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. The open-source Lustre file system is designed for applications that require fast storage – where you want your storage to keep up with your compute. Amazon FSx enables you to use Lustre file systems for any workload where storage speed matters. FSx for Lustre integrates with Amazon S3, making it easy to process data sets with the Lustre file system.</p>\n\n<p>FSx for Lustre is for Linux only, so this option is incorrect.</p>\n\n<p><strong>EFS</strong> - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth.</p>\n\n<p>EFS is a network file system but for Linux only, so this option is incorrect.</p>\n\n<p><strong>Amazon S3</strong> - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.</p>\n\n<p>Amazon S3 cannot be mounted as a file system on Windows, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.microsoft.com/en-us/previous-versions/windows/desktop/dfsr/dfsr-overview\">https://docs.microsoft.com/en-us/previous-versions/windows/desktop/dfsr/dfsr-overview</a></p>\n\n<p><a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p>\n\n<p><a href=\"https://aws.amazon.com/fsx/lustre/\">https://aws.amazon.com/fsx/lustre/</a></p>\n",
                "options": [
                    {
                        "id": 9391,
                        "content": "<p>EFS</p>",
                        "isValid": false
                    },
                    {
                        "id": 9392,
                        "content": "<p>FSx for Lustre</p>",
                        "isValid": false
                    },
                    {
                        "id": 9393,
                        "content": "<p>Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9394,
                        "content": "<p>FSx for Windows</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2250,
            "attributes": {
                "createdAt": "2023-09-09T20:33:43.073Z",
                "updatedAt": "2023-09-09T20:33:43.073Z",
                "content": "<p>Your company has a monthly big data workload, running for about 2 hours, which can be efficiently distributed across multiple servers of various sizes, with a variable number of CPUs. The solution for the workload should be able to withstand server failures.</p>\n\n<p>Which is the MOST cost-optimal solution for this workload?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Run the workload on a Spot Fleet</strong></p>\n\n<p>The Spot Fleet selects the Spot Instance pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated.</p>\n\n<p>A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Spot Instances provide great cost efficiency, but we need to select an instance type in advance. In this case, we want to use the most cost-optimal option and leave the selection of the cheapest spot instance to a Spot Fleet request, which can be optimized with the <code>lowestPrice</code> strategy. So this is the correct option.</p>\n\n<p>Key Spot Instance Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q49-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Run the workload on Spot Instances</strong> - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. Only spot fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated, so spot instances, by themselves, are not the right fit for this use-case.</p>\n\n<p><strong>Run the workload on Reserved Instances</strong> -  Reserved Instances are less cost-optimized than Spot Instances, and most efficient when used continuously. Here the workload is once a month, so this is not efficient.</p>\n\n<p><strong>Run the workload on Dedicated Hosts</strong> - Amazon EC2 Dedicated Hosts allow you to use your eligible software licenses from vendors such as Microsoft and Oracle on Amazon EC2 so that you get the flexibility and cost-effectiveness of using your licenses, but with the resiliency, simplicity, and elasticity of AWS. An Amazon EC2 Dedicated Host is a physical server fully dedicated for your use, so you can help address corporate compliance requirement. They're not particularly cost-efficient. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html#spot-fleet-allocation-strategy\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html#spot-fleet-allocation-strategy</a></p>\n",
                "options": [
                    {
                        "id": 9395,
                        "content": "<p>Run the workload on a Spot Fleet</p>",
                        "isValid": true
                    },
                    {
                        "id": 9396,
                        "content": "<p>Run the workload on Reserved Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9397,
                        "content": "<p>Run the workload on Spot Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9398,
                        "content": "<p>Run the workload on Dedicated Hosts</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2251,
            "attributes": {
                "createdAt": "2023-09-09T20:33:43.160Z",
                "updatedAt": "2023-09-09T20:33:43.160Z",
                "content": "<p>An e-commerce company operates multiple AWS accounts and has interconnected these accounts in a hub-and-spoke style using the AWS Transit Gateway. VPCs have been provisioned across these AWS accounts to facilitate network isolation.</p>\n\n<p>Which of the following solutions would reduce both the administrative overhead and the costs while providing shared access to services required by workloads in each of the VPCs?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Build a shared services VPC</strong></p>\n\n<p>Consider an organization that has built a hub-and-spoke network with AWS Transit Gateway. VPCs have been provisioned into multiple AWS accounts, perhaps to facilitate network isolation or to enable delegated network administration. When deploying distributed architectures such as this, a popular approach is to build a \"shared services VPC, which provides access to services required by workloads in each of the VPCs. This might include directory services or VPC endpoints. Sharing resources from a central location instead of building them in each VPC may reduce administrative overhead and cost.</p>\n\n<p>Centralized VPC Endpoints (multiple VPCs):\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q21-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/\">https://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/</a></p>\n\n<p>A VPC endpoint allows you to privately connect your VPC to supported AWS services without requiring an Internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Endpoints are virtual devices that are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.</p>\n\n<p>VPC endpoints enable you to reduce data transfer charges resulting from network communication between private VPC resources (such as Amazon Elastic Cloud Compute—or EC2—instances) and AWS Services (such as Amazon Quantum Ledger Database, or QLDB). Without VPC endpoints configured, communications that originate from within a VPC destined for public AWS services must egress AWS to the public Internet in order to access AWS services. This network path incurs outbound data transfer charges. Data transfer charges for traffic egressing from Amazon EC2 to the Internet vary based on volume. With VPC endpoints configured, communication between your VPC and the associated AWS service does not leave the Amazon network. If your workload requires you to transfer significant volumes of data between your VPC and AWS, you can reduce costs by leveraging VPC endpoints.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Transit VPC to reduce cost and share the resources across VPCs</strong> - Transit VPC uses customer-managed Amazon Elastic Compute Cloud (Amazon EC2) VPN instances in a dedicated transit VPC with an Internet gateway. This design requires the customer to deploy, configure, and manage EC2-based VPN appliances, which will result in additional EC2, and potentially third-party product and licensing charges. Note that this design will generate additional data transfer charges for traffic traversing the transit VPC: data is charged when it is sent from a spoke VPC to the transit VPC, and again from the transit VPC to the on-premises network or a different AWS Region. Transit VPC is not the right choice here.</p>\n\n<p>More on Transit VPC:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q21-i2.jpg\">\nvia - <a href=\"https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf\">https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf</a></p>\n\n<p><strong>Use Fully meshed VPC Peers</strong> - This approach creates multiple peering connections to facilitate the sharing of information between resources in different VPCs. This design connects multiple VPCs in a fully meshed configuration, with peering connections between each pair of VPCs. With this configuration, each VPC has access to the resources in all other VPCs. Each peering connection requires modifications to all the other VPCs’ route tables and, as the number of VPCs grows, this can be difficult to maintain. And keep in mind that AWS recommends a maximum of 125 peering connections per VPC. It's complex to manage and isn't a right fit for the current scenario.</p>\n\n<p>More on Fully meshed VPC Peers:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q21-i3.jpg\">\nvia - <a href=\"https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf\">https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf</a></p>\n\n<p><strong>Use VPCs connected with AWS Direct Connect</strong> - This approach is a good alternative for customers who need to connect a high number of VPCs to a central VPC or on-premises resources, or who already have an AWS Direct Connect connection in place. This design also offers customers the ability to incorporate transitive routing into their network design. For example, if VPC A and VPC B are both connected to an on-premises network using AWS Direct Connect connections, then the two VPCs can be connected to each other via AWS Direct Connect. Direct Connect requires physical cables and takes about a month for setting up, this is not an ideal solution for the given scenario.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/\">https://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/</a></p>\n\n<p><a href=\"https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf\">https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf</a></p>\n",
                "options": [
                    {
                        "id": 9399,
                        "content": "<p>Build a shared services VPC</p>",
                        "isValid": true
                    },
                    {
                        "id": 9400,
                        "content": "<p>Use VPCs connected with AWS Direct Connect</p>",
                        "isValid": false
                    },
                    {
                        "id": 9401,
                        "content": "<p>Use Fully meshed VPC Peers</p>",
                        "isValid": false
                    },
                    {
                        "id": 9402,
                        "content": "<p>Use Transit VPC to reduce cost and share the resources across VPCs</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2252,
            "attributes": {
                "createdAt": "2023-09-09T20:33:43.268Z",
                "updatedAt": "2023-09-09T20:33:43.268Z",
                "content": "<p>A silicon valley based startup has a content management application with the web-tier running on EC2 instances and the database tier running on Amazon Aurora. Currently, the entire infrastructure is located in us-east-1 region. The startup has 90% of its customers in the US and Europe. The engineering team is getting reports of deteriorated application performance from customers in Europe with high application load time.</p>\n\n<p>As a solutions architect, which of the following would you recommend addressing these performance issues? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Setup another fleet of EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Route 53</strong> - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Use latency based routing when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server.</p>\n\n<p>As customers in Europe are facing performance issues with high application load time, you can use latency based routing to reduce the latency. Hence this is the correct option.</p>\n\n<p>Route 53 Routing Policy Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q6-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p>\n\n<p><strong>Create Amazon Aurora read replicas in the eu-west-1 region</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance.</p>\n\n<p>Amazon Aurora read replicas can be used to scale out reads across regions. This will improve the application performance for users in Europe. Therefore, this is also a correct option for the given use-case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Setup another fleet of EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Route 53</strong> - Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights. You cannot use geolocation routing to reduce latency, hence this option is incorrect.</p>\n\n<p><strong>Setup another fleet of EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Route 53</strong> - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records. You cannot use failover routing to reduce latency, hence this option is incorrect.</p>\n\n<p><strong>Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region</strong> - Amazon Aurora Multi-AZ enhances the availability and durability for the database, it does not help in read scaling, so it is not a correct option for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-cross-region-read-replicas-for-amazon-aurora/\">https://aws.amazon.com/blogs/aws/new-cross-region-read-replicas-for-amazon-aurora/</a></p>\n",
                "options": [
                    {
                        "id": 9403,
                        "content": "<p>Setup another fleet of EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Route 53</p>",
                        "isValid": true
                    },
                    {
                        "id": 9404,
                        "content": "<p>Create Amazon Aurora read replicas in the eu-west-1 region</p>",
                        "isValid": true
                    },
                    {
                        "id": 9405,
                        "content": "<p>Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region</p>",
                        "isValid": false
                    },
                    {
                        "id": 9406,
                        "content": "<p>Setup another fleet of EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Route 53</p>",
                        "isValid": false
                    },
                    {
                        "id": 9407,
                        "content": "<p>Setup another fleet of EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Route 53</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2253,
            "attributes": {
                "createdAt": "2023-09-09T20:33:43.354Z",
                "updatedAt": "2023-09-09T20:33:43.354Z",
                "content": "<p>An engineering team wants to examine the feasibility of the <code>user data</code> feature of Amazon EC2 for an upcoming project.</p>\n\n<p>Which of the following are true about the EC2 user data configuration? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p>User Data is generally used to perform common automated configuration tasks and even run scripts after the instance starts. When you launch an instance in Amazon EC2, you can pass two types of user data - shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text or as a file.</p>\n\n<p><strong>By default, scripts entered as user data are executed with root user privileges</strong> - Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script. Any files you create will be owned by root; if you need non-root users to have file access, you should modify the permissions accordingly in the script.</p>\n\n<p><strong>By default, user data runs only during the boot cycle when you first launch an instance</strong> - By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>By default, user data is executed every time an EC2 instance is re-started</strong> - As discussed above, this is not a default configuration of the system. But, can be achieved by explicitly configuring the instance.</p>\n\n<p><strong>When an instance is running, you can update user data by using root user credentials</strong> - You can't change the user data if the instance is running (even by using root user credentials), but you can view it.</p>\n\n<p><strong>By default, scripts entered as user data do not have root user privileges for executing</strong> - Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html</a></p>\n",
                "options": [
                    {
                        "id": 9408,
                        "content": "<p>By default, user data runs only during the boot cycle when you first launch an instance</p>",
                        "isValid": true
                    },
                    {
                        "id": 9409,
                        "content": "<p>When an instance is running, you can update user data by using root user credentials</p>",
                        "isValid": false
                    },
                    {
                        "id": 9410,
                        "content": "<p>By default, user data is executed every time an EC2 instance is re-started</p>",
                        "isValid": false
                    },
                    {
                        "id": 9411,
                        "content": "<p>By default, scripts entered as user data are executed with root user privileges</p>",
                        "isValid": true
                    },
                    {
                        "id": 9412,
                        "content": "<p>By default, scripts entered as user data do not have root user privileges for executing</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2254,
            "attributes": {
                "createdAt": "2023-09-09T20:33:43.438Z",
                "updatedAt": "2023-09-09T20:33:43.438Z",
                "content": "<p>You would like to use Snowball to move on-premises backups into a long term archival tier on AWS. Which solution provides the MOST cost savings?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a Snowball job and target an S3 bucket. Create a lifecycle policy to transition this data to Glacier Deep Archive on the same day</strong></p>\n\n<p>AWS Snowball, a part of the AWS Snow Family, is a data migration and edge computing device that comes in two options. Snowball Edge Storage Optimized devices provide both block storage and Amazon S3-compatible object storage, and 40 vCPUs. They are well suited for local storage and large scale data transfer. Snowball Edge Compute Optimized devices provide 52 vCPUs, block and object storage, and an optional GPU for use cases like advanced machine learning and full-motion video analysis in disconnected environments.</p>\n\n<p>Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases.</p>\n\n<p>The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space.</p>\n\n<p>For this scenario, you will want to minimize the time spent in S3 Standard for all files to avoid unintended S3 Standard storage charges. To do this, AWS recommends using a zero-day lifecycle policy. From a cost perspective, when using a zero-day lifecycle policy, you are only charged S3 Glacier Deep Archive rates. When billed, the lifecycle policy is accounted for first, and if the destination is S3 Glacier Deep Archive, you are charged S3 Glacier Deep Archive rates for the transferred files.</p>\n\n<p>You can't move data directly from Snowball into Glacier, you need to go through S3 first, and then use a lifecycle policy. So this option is correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Snowball job and target a Glacier Vault</strong></p>\n\n<p><strong>Create a Snowball job and target a Glacier Deep Archive Vault</strong></p>\n\n<p>Amazon S3 Glacier and S3 Glacier Deep Archive are a secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.\nFinally, Glacier Deep Archive provides more cost savings than Glacier.</p>\n\n<p>Both these options are incorrect as you can't move data directly from Snowball into a Glacier Vault or a Glacier Deep Archive Vault. You need to go through S3 first and then use a lifecycle policy.</p>\n\n<p><strong>Create a Snowball job and target an S3 bucket. Create a lifecycle policy to transition this data to Glacier on the same day</strong> - As Glacier Deep Archive provides more cost savings than Glacier, so you should use Glacier Deep Archive for long term archival for this use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/snowball/features/\">https://aws.amazon.com/snowball/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/glacier/\">https://aws.amazon.com/glacier/</a></p>\n",
                "options": [
                    {
                        "id": 9413,
                        "content": "<p>Create a Snowball job and target an S3 bucket. Create a lifecycle policy to transition this data to Glacier Deep Archive on the same day</p>",
                        "isValid": true
                    },
                    {
                        "id": 9414,
                        "content": "<p>Create a Snowball job and target an S3 bucket. Create a lifecycle policy to transition this data to Glacier on the same day</p>",
                        "isValid": false
                    },
                    {
                        "id": 9415,
                        "content": "<p>Create a Snowball job and target a Glacier Deep Archive Vault</p>",
                        "isValid": false
                    },
                    {
                        "id": 9416,
                        "content": "<p>Create a Snowball job and target a Glacier Vault</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2255,
            "attributes": {
                "createdAt": "2023-09-09T20:33:43.523Z",
                "updatedAt": "2023-09-09T20:33:43.523Z",
                "content": "<p>A social media application is hosted on an EC2 server fleet running behind an Application Load Balancer. The application traffic is fronted by a CloudFront distribution. The engineering team wants to decouple the user authentication process for the application, so that the application servers can just focus on the business logic.</p>\n\n<p>As a Solutions Architect, which of the following solutions would you recommend to the development team so that it requires minimal development effort?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito Authentication via Cognito User Pools for your Application Load Balancer</strong></p>\n\n<p>Application Load Balancer can be used to securely authenticate users for accessing your applications. This enables you to offload the work of authenticating users to your load balancer so that your applications can focus on their business logic. You can use Cognito User Pools to authenticate users through well-known social IdPs, such as Amazon, Facebook, or Google, through the user pools supported by Amazon Cognito or through corporate identities, using SAML, LDAP, or Microsoft AD, through the user pools supported by Amazon Cognito. You configure user authentication by creating an authenticate action for one or more listener rules.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q17-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a></p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt2-q17-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Cognito Authentication via Cognito Identity Pools for your Application Load Balancer</strong> - There is no such thing as using Cognito Authentication via Cognito Identity Pools for managing user authentication for the application. Application-specific user authentication can be provided via Cognito User Pools. Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token.</p>\n\n<p><strong>Use Cognito Authentication via Cognito User Pools for your CloudFront distribution</strong> - You cannot directly integrate Cognito User Pools with CloudFront distribution as you have to create a separate Lambda@Edge function to accomplish the authentication via Cognito User Pools. This involves additional development effort, so this option is not the best fit for the given use-case.</p>\n\n<p><strong>Use Cognito Authentication via Cognito Identity Pools for your CloudFront distribution</strong> - You cannot use Cognito Identity Pools for managing user authentication, so this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/\">https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/</a></p>\n",
                "options": [
                    {
                        "id": 9417,
                        "content": "<p>Use Cognito Authentication via Cognito Identity Pools for your Application Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 9418,
                        "content": "<p>Use Cognito Authentication via Cognito User Pools for your CloudFront distribution</p>",
                        "isValid": false
                    },
                    {
                        "id": 9419,
                        "content": "<p>Use Cognito Authentication via Cognito Identity Pools for your CloudFront distribution</p>",
                        "isValid": false
                    },
                    {
                        "id": 9420,
                        "content": "<p>Use Cognito Authentication via Cognito User Pools for your Application Load Balancer</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2256,
            "attributes": {
                "createdAt": "2023-09-09T20:33:43.642Z",
                "updatedAt": "2023-09-09T20:33:43.642Z",
                "content": "<p>A social media startup uses AWS Cloud to manage its IT infrastructure. The engineering team at the startup wants to perform weekly database rollovers for a MySQL database server using a serverless cron job that typically takes about 5 minutes to execute the database rollover script written in Python. The database rollover will archive the past week’s data from the production database to keep the database small while still keeping its data accessible.</p>\n\n<p>As a solutions architect, which of the following would you recommend as the MOST cost-efficient and reliable solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Schedule a weekly EventBridge event cron expression to invoke a Lambda function that runs the database rollover job</strong></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. AWS Lambda supports standard rate and cron expressions for frequencies of up to once per minute.</p>\n\n<p>Schedule expressions using rate or cron:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q30-i1.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing and it's not the right fit for running a database rollover script. Although AWS Glue is also serverless, Lambda is a more cost-effective option compared to AWS Glue.</p>\n\n<p><strong>Provision an EC2 spot instance to run the database rollover job triggered via an OS-based weekly cron expression</strong> - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly (up to 90% off the On-Demand price).\nAs the Spot Instance runs whenever capacity is available, there is no guarantee that the weekly job will be executed during the defined time window. Additionally, the given use-case requires a serverless solution, therefore this option is incorrect.</p>\n\n<p><strong>Provision an EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression</strong> - Scheduled Reserved Instances run on a part-time basis. Scheduled Reserved Instances option allows you to use reserve capacity on a recurring daily, weekly, and monthly schedules. Scheduled Reserved Instances are available for one-year terms at 5-10% below On-Demand rates. As the given use-case requires a serverless solution, therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents-expressions.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents-expressions.html</a></p>\n",
                "options": [
                    {
                        "id": 9421,
                        "content": "<p>Provision an EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression</p>",
                        "isValid": false
                    },
                    {
                        "id": 9422,
                        "content": "<p>Schedule a weekly EventBridge event cron expression to invoke a Lambda function that runs the database rollover job</p>",
                        "isValid": true
                    },
                    {
                        "id": 9423,
                        "content": "<p>Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script</p>",
                        "isValid": false
                    },
                    {
                        "id": 9424,
                        "content": "<p>Provision an EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2257,
            "attributes": {
                "createdAt": "2023-09-09T20:33:43.739Z",
                "updatedAt": "2023-09-09T20:33:43.739Z",
                "content": "<p>A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS.</p>\n\n<p>As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use EC2 dedicated hosts</strong></p>\n\n<p>You can use Dedicated Hosts to launch Amazon EC2 instances on physical servers that are dedicated for your use. Dedicated Hosts give you additional visibility and control over how instances are placed on a physical server, and you can reliably use the same physical server over time. As a result, Dedicated Hosts enable you to use your existing server-bound software licenses like Windows Server and address corporate compliance and regulatory requirements.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use EC2 dedicated instances</strong> - Dedicated instances are Amazon EC2 instances that run in a VPC on hardware that's dedicated to a single customer. Your dedicated instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. Dedicated instances may share hardware with other instances from the same AWS account that are not dedicated instances. Dedicated instances cannot be used for existing server-bound software licenses.</p>\n\n<p><strong>Use EC2 on-demand instances</strong></p>\n\n<p><strong>Use EC2 reserved instances</strong></p>\n\n<p>Amazon EC2 presents a virtual computing environment, allowing you to use web service interfaces to launch instances with a variety of operating systems, load them with your custom application environment, manage your network’s access permissions, and run your image using as many or few systems as you desire.</p>\n\n<p>Amazon EC2 provides the following purchasing options to enable you to optimize your costs based on your needs:</p>\n\n<p>On-Demand Instances – Pay, by the second, for the instances that you launch.</p>\n\n<p>Reserved Instances – Reduce your Amazon EC2 costs by making a commitment to a consistent instance configuration, including instance type and Region, for a term of 1 or 3 years.</p>\n\n<p>Neither on-demand instances nor reserved instances can be used for existing server-bound software licenses.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/dedicated-hosts/\">https://aws.amazon.com/ec2/dedicated-hosts/</a></p>\n\n<p><a href=\"https://aws.amazon.com/ec2/dedicated-hosts/faqs/\">https://aws.amazon.com/ec2/dedicated-hosts/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/ec2/pricing/dedicated-instances/\">https://aws.amazon.com/ec2/pricing/dedicated-instances/</a></p>\n",
                "options": [
                    {
                        "id": 9425,
                        "content": "<p>Use EC2 dedicated instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9426,
                        "content": "<p>Use EC2 on-demand instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9427,
                        "content": "<p>Use EC2 dedicated hosts</p>",
                        "isValid": true
                    },
                    {
                        "id": 9428,
                        "content": "<p>Use EC2 reserved instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2258,
            "attributes": {
                "createdAt": "2023-09-09T20:33:43.821Z",
                "updatedAt": "2023-09-09T20:33:43.821Z",
                "content": "<p>An IT company hosts windows based applications on its on-premises data center. The company is looking at moving the business to the AWS Cloud. The cloud solution should offer shared storage space that multiple applications can access without a need for replication. Also, the solution should integrate with the company's self-managed Active Directory domain.</p>\n\n<p>Which of the following solutions addresses these requirements with the minimal integration effort?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon FSx for Windows File Server as a shared storage solution</strong> - Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. It offers single-AZ and multi-AZ deployment options, fully managed backups, and encryption of data at rest and in transit. You can optimize cost and performance for your workload needs with SSD and HDD storage options; and you can scale storage and change the throughput performance of your file system at any time.</p>\n\n<p>With Amazon FSx, you get highly available and durable file storage starting from $0.013 per GB-month. Data deduplication enables you to optimize costs even further by removing redundant data. You can increase your file system storage and scale throughput capacity at any time, making it easy to respond to changing business needs. There are no upfront costs or licensing fees.</p>\n\n<p>How Amazon FSx for Windows File Server works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q43-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use File Gateway of AWS Storage Gateway to create a hybrid storage solution</strong> - AWS Storage Gateway connects an on-premises software appliance with cloud-based storage to provide seamless integration between your on-premises IT environment and the AWS storage infrastructure. Storage Gateway uses Amazon S3 to store data on AWS Cloud and from here the on-premises data can seamlessly integrate with Cloud services. It is not suited to be used as a shared storage space that multiple applications can access in parallel.</p>\n\n<p><strong>Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies</strong> - Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance storage for compute workloads. Many workloads such as machine learning, high performance computing (HPC), video rendering, and financial simulations depend on compute instances accessing the same set of data through high-performance shared storage. Lustre is Linux based, hence it is not the right choice since the use case is about Windows-based applications.</p>\n\n<p><strong>Use Amazon Elastic File System (Amazon EFS) as a shared storage solution</strong> - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. EFS is a powerful, shared storage solution that would have been the right answer if the customer systems were Linux based. Amazon EFS is compatible with only Linux-based AMIs for Amazon EC2.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p>\n",
                "options": [
                    {
                        "id": 9429,
                        "content": "<p>Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies</p>",
                        "isValid": false
                    },
                    {
                        "id": 9430,
                        "content": "<p>Use Amazon FSx for Windows File Server as a shared storage solution</p>",
                        "isValid": true
                    },
                    {
                        "id": 9431,
                        "content": "<p>Use Amazon Elastic File System (Amazon EFS) as a shared storage solution</p>",
                        "isValid": false
                    },
                    {
                        "id": 9432,
                        "content": "<p>Use File Gateway of AWS Storage Gateway to create a hybrid storage solution</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2259,
            "attributes": {
                "createdAt": "2023-09-09T20:33:43.920Z",
                "updatedAt": "2023-09-09T20:33:43.920Z",
                "content": "<p>A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters.</p>\n\n<p>As a solutions architect, which of the following AWS services would you recommend addressing this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>VPN CloudHub</strong></p>\n\n<p>If you have multiple AWS Site-to-Site VPN connections, you can provide secure communication between sites using the AWS VPN CloudHub. This enables your remote sites to communicate with each other, and not just with the VPC. Sites that use AWS Direct Connect connections to the virtual private gateway can also be part of the AWS VPN CloudHub. The VPN CloudHub operates on a simple hub-and-spoke model that you can use with or without a VPC. This design is suitable if you have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices.</p>\n\n<p>Per the given use-case, the corporate headquarters has an AWS Direct Connect connection to the VPC and the branch offices have Site-to-Site VPN connections to the VPC. Therefore using the AWS VPN CloudHub, branch offices can send and receive data with each other as well as with their corporate headquarters.</p>\n\n<p>VPN CloudHub\n<img src=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/images/AWS_VPN_CloudHub-diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>VPC Endpoint</strong> - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet.\nWhen you use VPC endpoint, the traffic between your VPC and the other AWS service does not leave the Amazon network, therefore this option cannot be used to send and receive data between the remote branch offices of the company.</p>\n\n<p><strong>VPC Peering</strong> - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network.\nVPC peering facilitates a connection between two VPCs within the AWS network, therefore this option cannot be used to send and receive data between the remote branch offices of the company.</p>\n\n<p><strong>Software VPN</strong> - Amazon VPC offers you the flexibility to fully manage both sides of your Amazon VPC connectivity by creating a VPN connection between your remote network and a software VPN appliance running in your Amazon VPC network. Since Software VPN just handles connectivity between the remote network and Amazon VPC, therefore it cannot be used to send and receive data between the remote branch offices of the company.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-vpn-cloudhub-network-to-amazon.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-vpn-cloudhub-network-to-amazon.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html</a></p>\n",
                "options": [
                    {
                        "id": 9433,
                        "content": "<p>VPN CloudHub</p>",
                        "isValid": true
                    },
                    {
                        "id": 9434,
                        "content": "<p>Software VPN</p>",
                        "isValid": false
                    },
                    {
                        "id": 9435,
                        "content": "<p>VPC Endpoint</p>",
                        "isValid": false
                    },
                    {
                        "id": 9436,
                        "content": "<p>VPC Peering</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2260,
            "attributes": {
                "createdAt": "2023-09-09T20:33:44.014Z",
                "updatedAt": "2023-09-09T20:33:44.014Z",
                "content": "<p>An IT company is looking to move its on-premises infrastructure to AWS Cloud. The company has a portfolio of applications with a few of them using server bound licenses that are valid for the next year. To utilize the licenses, the CTO wants to use dedicated hosts for a one year term and then migrate the given instances to default tenancy thereafter.</p>\n\n<p>As a solutions architect, which of the following options would you identify as CORRECT for changing the tenancy of an instance after you have launched it? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>You can change the tenancy of an instance from dedicated to host</strong></p>\n\n<p><strong>You can change the tenancy of an instance from host to dedicated</strong></p>\n\n<p>Each EC2 instance that you launch into a VPC has a tenancy attribute. This attribute has the following values.\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q16-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html</a></p>\n\n<p>By default, EC2 instances run on a shared-tenancy basis.</p>\n\n<p>Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at the hardware level. However, Dedicated Instances may share hardware with other instances from the same AWS account that is not Dedicated Instances.</p>\n\n<p>A Dedicated Host is also a physical server that's dedicated to your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can change the tenancy of an instance from default to dedicated</strong> - You can only change the tenancy of an instance from dedicated to host, or from host to dedicated after you've launched it. Therefore, this option is incorrect.</p>\n\n<p><strong>You can change the tenancy of an instance from dedicated to default</strong> - You can only change the tenancy of an instance from dedicated to host, or from host to dedicated after you've launched it. Therefore, this option is incorrect.</p>\n\n<p><strong>You can change the tenancy of an instance from default to host</strong> - You can only change the tenancy of an instance from dedicated to host, or from host to dedicated after you've launched it. Therefore, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html</a></p>\n",
                "options": [
                    {
                        "id": 9437,
                        "content": "<p>You can change the tenancy of an instance from default to host</p>",
                        "isValid": false
                    },
                    {
                        "id": 9438,
                        "content": "<p>You can change the tenancy of an instance from dedicated to default</p>",
                        "isValid": false
                    },
                    {
                        "id": 9439,
                        "content": "<p>You can change the tenancy of an instance from host to dedicated</p>",
                        "isValid": true
                    },
                    {
                        "id": 9440,
                        "content": "<p>You can change the tenancy of an instance from default to dedicated</p>",
                        "isValid": false
                    },
                    {
                        "id": 9441,
                        "content": "<p>You can change the tenancy of an instance from dedicated to host</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2261,
            "attributes": {
                "createdAt": "2023-09-09T20:33:44.094Z",
                "updatedAt": "2023-09-09T20:33:44.094Z",
                "content": "<p>The DevOps team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group.</p>\n\n<p>As a solutions architect, which of the following would you identify as an INVALID option for setting up such a configuration?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>You can use an Internet Gateway ID as the custom source for the inbound rule</strong></p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, you can use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group.</p>\n\n<p>Please see this list of allowed source or destination for security group rules:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q65-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n\n<p>Therefore, you cannot use an Internet Gateway ID as the custom source for the inbound rule.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can use a security group as the custom source for the inbound rule</strong></p>\n\n<p><strong>You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule</strong></p>\n\n<p><strong>You can use an IP address as the custom source for the inbound rule</strong></p>\n\n<p>As described in the list of allowed sources or destinations for security group rules, the above options are supported.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</p>\n",
                "options": [
                    {
                        "id": 9442,
                        "content": "<p>You can use an IP address as the custom source for the inbound rule</p>",
                        "isValid": false
                    },
                    {
                        "id": 9443,
                        "content": "<p>You can use a security group as the custom source for the inbound rule</p>",
                        "isValid": false
                    },
                    {
                        "id": 9444,
                        "content": "<p>You can use an Internet Gateway ID as the custom source for the inbound rule</p>",
                        "isValid": true
                    },
                    {
                        "id": 9445,
                        "content": "<p>You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2262,
            "attributes": {
                "createdAt": "2023-09-09T20:33:44.184Z",
                "updatedAt": "2023-09-09T20:33:44.184Z",
                "content": "<p>An AWS Organization is using Service Control Policies (SCP) for central control over the maximum available permissions for all accounts in their organization. This allows the organization to ensure that all accounts stay within the organization’s access control guidelines.</p>\n\n<p>Which of the given scenarios are correct regarding the permissions described below? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can't perform that action</strong></p>\n\n<p><strong>SCPs affect all users and roles in attached accounts, including the root user</strong></p>\n\n<p><strong>SCPs do not affect service-linked role</strong></p>\n\n<p>Service control policies (SCPs) are one type of policy that can be used to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines.</p>\n\n<p>In SCPs, you can restrict which AWS services, resources, and individual API actions the users and roles in each member account can access. You can also define conditions for when to restrict access to AWS services, resources, and API actions. These restrictions even override the administrators of member accounts in the organization.</p>\n\n<p>Please note the following effects on permissions vis-a-vis the SCPs:</p>\n\n<p>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can't perform that action.</p>\n\n<p>SCPs affect all users and roles in the attached accounts, including the root user.</p>\n\n<p>SCPs do not affect any service-linked role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can still perform that action</strong></p>\n\n<p><strong>SCPs affect all users and roles in attached accounts, excluding the root user</strong></p>\n\n<p><strong>SCPs affect service-linked roles</strong></p>\n\n<p>These three options contradict the details provided in the explanation above.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html</a></p>\n",
                "options": [
                    {
                        "id": 9446,
                        "content": "<p>SCPs affect all users and roles in attached accounts, excluding the root user</p>",
                        "isValid": false
                    },
                    {
                        "id": 9447,
                        "content": "<p>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can't perform that action</p>",
                        "isValid": true
                    },
                    {
                        "id": 9448,
                        "content": "<p>SCPs affect all users and roles in attached accounts, including the root user</p>",
                        "isValid": true
                    },
                    {
                        "id": 9449,
                        "content": "<p>SCPs affect service-linked roles</p>",
                        "isValid": false
                    },
                    {
                        "id": 9450,
                        "content": "<p>SCPs do not affect service-linked role</p>",
                        "isValid": true
                    },
                    {
                        "id": 9451,
                        "content": "<p>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can still perform that action</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2263,
            "attributes": {
                "createdAt": "2023-09-09T20:33:44.309Z",
                "updatedAt": "2023-09-09T20:33:44.309Z",
                "content": "<p>An IT training company hosted its website on Amazon S3 a couple of years ago. Due to COVID-19 related travel restrictions, the training website has suddenly gained traction. With an almost 300% increase in the requests served per day, the company's AWS costs have sky-rocketed for just the S3 outbound data costs.</p>\n\n<p>As a Solutions Architect, can you suggest an alternate method to reduce costs while keeping the latency low?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon CloudFront to distribute the data hosted on Amazon S3, cost-effectively</strong> - Storing content with S3 provides a lot of advantages. But to help optimize your application’s performance and security while effectively managing cost, AWS recommends that you also set up Amazon CloudFront to work with your S3 bucket to serve and protect the content.</p>\n\n<p>CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located. CloudFront has edge servers in locations all around the world.</p>\n\n<p>When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content. Then, for the next local request for the same content, it’s already cached nearby and can be served immediately.</p>\n\n<p>By caching your content in Edge Locations, CloudFront reduces the load on your S3 bucket and helps ensure a faster response for your users when they request content. Also, data transfer out for content by using CloudFront is often more cost-effective than serving files directly from S3, and there is no data transfer fee from S3 to CloudFront. You only pay for what is delivered to the internet from CloudFront, plus request fees.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>To reduce S3 cost, the data can be saved on an EBS volume connected to an EC2 instance that can host the application</strong> - EBS volumes are fast and are relatively cheap (though S3 is still a cheaper alternative). But, EBS volumes are accessible only through EC2 instances and are bound to a specific region.</p>\n\n<p><strong>Use Amazon Elastic File System (Amazon EFS), as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data</strong> - EFS is a shareable file system that can be mounted onto EC2 instances. EFS is costlier than EBS and not a solution if the company is looking at reducing costs.</p>\n\n<p><strong>Configure S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to S3 buckets</strong> - This statement is incorrect and given only as a distractor. You can use S3 Batch Operations to perform large-scale batch operations on Amazon S3 objects, and it has nothing to do with content distribution.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n",
                "options": [
                    {
                        "id": 9452,
                        "content": "<p>To reduce S3 cost, the data can be saved on an EBS volume connected to an EC2 instance that can host the application</p>",
                        "isValid": false
                    },
                    {
                        "id": 9453,
                        "content": "<p>Configure S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to S3 buckets</p>",
                        "isValid": false
                    },
                    {
                        "id": 9454,
                        "content": "<p>Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data</p>",
                        "isValid": false
                    },
                    {
                        "id": 9455,
                        "content": "<p>Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2264,
            "attributes": {
                "createdAt": "2023-09-09T20:33:44.481Z",
                "updatedAt": "2023-09-09T20:33:44.481Z",
                "content": "<p>The database backend for a retail company's website is hosted on Amazon RDS for MySQL having a primary instance and three read replicas to support read scalability. The company has mandated that the read replicas should lag no more than 1 second behind the primary instance to provide the best possible user experience. The read replicas are falling further behind during periods of peak traffic spikes, resulting in a bad user experience as the searches produce inconsistent results.</p>\n\n<p>You have been hired as an AWS Certified Solutions Architect Associate to reduce the replication lag as much as possible with minimal changes to the application code or the effort required to manage the underlying resources.</p>\n\n<p>Which of the following will you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up database migration from RDS MySQL to Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling</strong></p>\n\n<p>Aurora features a distributed, fault-tolerant, and self-healing storage system that is decoupled from compute resources and auto-scales up to 128 TiB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon Simple Storage Service (Amazon S3), and replication across three Availability Zones (AZs).</p>\n\n<p>Since Amazon Aurora Replicas share the same data volume as the primary instance in the same AWS Region, there is virtually no replication lag. The replica lag times are in the 10s of milliseconds (compared to the replication lag of seconds in the case of MySQL read replicas). Therefore, this is the right option to ensure that the read replicas lag no more than 1 second behind the primary instance.</p>\n\n<p>Aurora Replicas:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q50-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/aurora/faqs/\">https://aws.amazon.com/rds/aurora/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Host the MySQL primary database on a memory-optimized EC2 instance. Spin up additional compute-optimized EC2 instances to host the read replicas</strong> - Hosting the MySQL primary database and the read replicas on the EC2 instances would result in significant overhead to manage the underlying resources such as OS patching, database patching, etc. So this option is incorrect.</p>\n\n<p><strong>Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas</strong> - Introducing a caching layer would result in significant changes to the application code, so this option is incorrect.</p>\n\n<p><strong>Set up database migration from RDS MySQL to DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling</strong> - Introducing a NoSQL database, such as DynamoDB, would result in significant changes to the application code since the database queries would have to be re-written for DynamoDB. Therefore, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/aurora/faqs/\">https://aws.amazon.com/rds/aurora/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9456,
                        "content": "<p>Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas</p>",
                        "isValid": false
                    },
                    {
                        "id": 9457,
                        "content": "<p>Set up database migration from RDS MySQL to DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling</p>",
                        "isValid": false
                    },
                    {
                        "id": 9458,
                        "content": "<p>Host the MySQL primary database on a memory-optimized EC2 instance. Spin up additional compute-optimized EC2 instances to host the read replicas</p>",
                        "isValid": false
                    },
                    {
                        "id": 9459,
                        "content": "<p>Set up database migration from RDS MySQL to Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2265,
            "attributes": {
                "createdAt": "2023-09-09T20:33:44.574Z",
                "updatedAt": "2023-09-09T20:33:44.574Z",
                "content": "<p>A gaming company uses Application Load Balancers (ALBs) in front of Amazon EC2 instances for different services and microservices. The architecture has now become complex with too many ALBs in multiple AWS Regions. Security updates, firewall configurations, and traffic routing logic have become complex with too many IP addresses and configurations.</p>\n\n<p>The company is looking at an easy and effective way to bring down the number of IP addresses allowed by the firewall and easily manage the entire network infrastructure. Which of these options represents an appropriate solution for this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Launch AWS Global Accelerator and create endpoints for all the Regions. Register the ALBs of each Region to the corresponding endpoints</strong> - AWS Global Accelerator is a networking service that sends your user’s traffic through Amazon Web Service’s global network infrastructure, improving your internet user performance by up to 60%. When the internet is congested, Global Accelerator’s automatic routing optimizations will help keep your packet loss, jitter, and latency consistently low.</p>\n\n<p>With Global Accelerator, you are provided two global static customer-facing IPs to simplify traffic management. On the back end, add or remove your AWS application origins, such as Network Load Balancers, Application Load Balancers, Elastic IPs, and EC2 Instances, without making user-facing changes. To mitigate endpoint failure, Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint.</p>\n\n<p>Simplified and resilient traffic routing for multi-Region applications:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q45-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Elastic IPs for each of the ALBs in each Region</strong> - An Application Load Balancer cannot be assigned an Elastic IP address (static IP address).</p>\n\n<p><strong>Set up a Network Load Balancer (NLB) with Elastic IPs. Register the private IPs of all the ALBs as targets of this NLB</strong> - An NLB can be configured to take an Elastic IP. However, with hundreds of ALBs, the NLB-ALB combination will be equally cumbersome to manage.</p>\n\n<p><strong>Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the ASGs, for each of the Regions</strong> - You cannot assign an Elastic IP to an Auto Scaling Group, since ASG just manages a collection of EC2 instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/using-static-ip-addresses-for-application-load-balancers/\">https://aws.amazon.com/blogs/networking-and-content-delivery/using-static-ip-addresses-for-application-load-balancers/</a></p>\n",
                "options": [
                    {
                        "id": 9460,
                        "content": "<p>Set up a Network Load Balancer (NLB) with Elastic IPs. Register the private IPs of all the ALBs as targets of this NLB</p>",
                        "isValid": false
                    },
                    {
                        "id": 9461,
                        "content": "<p>Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the ASGs, for each of the Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 9462,
                        "content": "<p>Configure Elastic IPs for each of the ALBs in each Region</p>",
                        "isValid": false
                    },
                    {
                        "id": 9463,
                        "content": "<p>Launch AWS Global Accelerator and create endpoints for all the Regions. Register the ALBs of each Region to the corresponding endpoints</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2266,
            "attributes": {
                "createdAt": "2023-09-09T20:33:44.658Z",
                "updatedAt": "2023-09-09T20:33:44.658Z",
                "content": "<p>A media streaming company is looking to migrate its on-premises infrastructure into the AWS Cloud. The engineering team is looking for a fully managed NoSQL persistent data store with in-memory caching to maintain low latency that is critical for real-time scenarios such as video streaming and interactive content. The team expects the number of concurrent users to touch up to a million so the database should be able to scale elastically.</p>\n\n<p>As a solutions architect, which of the following AWS services would you recommend for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>DynamoDB</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. Companies use caching through DynamoDB Accelerator (DAX) when they have high read volumes or need submillisecond read latency.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DocumentDB</strong> - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. Although DocumentDB is fully managed, it does not have an in-memory caching layer.</p>\n\n<p><strong>ElastiCache</strong> - Amazon ElastiCache allows you to set up popular open-Source compatible in-memory data stores in the cloud. You can build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores such as Redis and Memcached. Elasticache is used as a caching layer. It's not a fully managed NoSQL database.</p>\n\n<p><strong>RDS</strong> - RDS makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. It's not a NoSQL database.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n",
                "options": [
                    {
                        "id": 9464,
                        "content": "<p>ElastiCache</p>",
                        "isValid": false
                    },
                    {
                        "id": 9465,
                        "content": "<p>DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 9466,
                        "content": "<p>DocumentDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 9467,
                        "content": "<p>RDS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2267,
            "attributes": {
                "createdAt": "2023-09-09T20:33:44.763Z",
                "updatedAt": "2023-09-09T20:33:44.763Z",
                "content": "<p>A healthcare company has deployed its web application on Amazon ECS container instances running behind an Application Load Balancer (ALB). The website slows down when the traffic spikes and the website availability is also reduced. The development team has configured CloudWatch alarms to receive notifications whenever there is an availability constraint so the team can scale out resources. The company wants an automated solution to respond to such events.</p>\n\n<p>Which of the following addresses the given use case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure AWS Auto Scaling to scale out the ECS cluster when the ECS service's CPU utilization rises above a threshold</strong></p>\n\n<p>You use the Amazon ECS first-run wizard to create a cluster and a service that runs behind an Elastic Load Balancing load balancer. Then you can configure a target tracking scaling policy that scales your service automatically based on the current application load as measured by the service's CPU utilization (from the ECS, ClusterName, and ServiceName category in CloudWatch).</p>\n\n<p>When the average CPU utilization of your service rises above 75% (meaning that more than 75% of the CPU that is reserved for the service is being used), a scale-out alarm triggers Service Auto Scaling to add another task to your service to help out with the increased load. Conversely, when the average CPU utilization of your service drops below the target utilization for a sustained period, a scale-in alarm triggers a decrease in the service's desired count to free up those cluster resources for other tasks and services.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q15-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-configure-auto-scaling.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-configure-auto-scaling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Auto Scaling to scale out the ECS cluster when the ALB target group's CPU utilization rises above a threshold</strong></p>\n\n<p><strong>Configure AWS Auto Scaling to scale out the ECS cluster when the ALB's CPU utilization rises above a threshold</strong></p>\n\n<p><strong>Configure AWS Auto Scaling to scale out the ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-autoscaling-targettracking.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-autoscaling-targettracking.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-configure-auto-scaling.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-configure-auto-scaling.html</a></p>\n",
                "options": [
                    {
                        "id": 9468,
                        "content": "<p>Configure AWS Auto Scaling to scale out the ECS cluster when the ECS service's CPU utilization rises above a threshold</p>",
                        "isValid": true
                    },
                    {
                        "id": 9469,
                        "content": "<p>Configure AWS Auto Scaling to scale out the ECS cluster when the ALB target group's CPU utilization rises above a threshold</p>",
                        "isValid": false
                    },
                    {
                        "id": 9470,
                        "content": "<p>Configure AWS Auto Scaling to scale out the ECS cluster when the ALB's CPU utilization rises above a threshold</p>",
                        "isValid": false
                    },
                    {
                        "id": 9471,
                        "content": "<p>Configure AWS Auto Scaling to scale out the ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2268,
            "attributes": {
                "createdAt": "2023-09-09T20:33:44.837Z",
                "updatedAt": "2023-09-09T20:33:44.837Z",
                "content": "<p>An e-commerce company uses Microsoft Active Directory to provide users and groups with access to resources on the on-premises infrastructure. The company has extended its IT infrastructure to AWS in the form of a hybrid cloud. The engineering team at the company wants to run directory-aware workloads on AWS for a SQL Server-based application. The team also wants to configure a trust relationship to enable single sign-on (SSO) for its users to access resources in either domain.</p>\n\n<p>As a solutions architect, which of the following AWS services would you recommend for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Managed Microsoft AD</strong></p>\n\n<p>AWS Directory Service provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other AWS services.</p>\n\n<p>AWS Directory Service for Microsoft Active Directory (aka AWS Managed Microsoft AD) is powered by an actual Microsoft Windows Server Active Directory (AD), managed by AWS. With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud such as SQL Server-based applications. You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AD Connector</strong> - Use AD Connector if you only need to allow your on-premises users to log in to AWS applications and services with their Active Directory credentials. AD Connector simply connects your existing on-premises Active Directory to AWS. You cannot use it to run directory-aware workloads on AWS, hence this option is not correct.</p>\n\n<p><strong>Simple AD</strong> - Simple AD provides a subset of the features offered by AWS Managed Microsoft AD. Simple AD is a standalone managed directory that is powered by a Samba 4 Active Directory Compatible Server.  Simple AD does not support features such as trust relationships with other domains. Therefore, this option is not correct.</p>\n\n<p><strong>Amazon Cloud Directory</strong> - Amazon Cloud Directory is a cloud-native directory that can store hundreds of millions of application-specific objects with multiple relationships and schemas. Use Amazon Cloud Directory if you need a highly scalable directory store for your application’s hierarchical data. You cannot use it to\nestablish trust relationships with other domains on the on-premises infrastructure. Therefore, this option is not correct.</p>\n\n<p>Exam Alert:</p>\n\n<p>You may see questions on choosing \"AWS Managed Microsoft AD\" vs \"AD Connector\" vs \"Simple AD\" on the exam. Just remember that you should use AD Connector if you only need to allow your on-premises users to log in to AWS applications with their Active Directory credentials. AWS Managed Microsoft AD would also allow you to run directory-aware workloads in the AWS Cloud.  AWS Managed Microsoft AD is your best choice if you have more than 5,000 users and need a trust relationship set up between an AWS hosted directory and your on-premises directories. Simple AD is the least expensive option and your best choice if you have 5,000 or fewer users and don’t need the more advanced Microsoft Active Directory features such as trust relationships with other domains.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html</a></p>\n",
                "options": [
                    {
                        "id": 9472,
                        "content": "<p>Amazon Cloud Directory</p>",
                        "isValid": false
                    },
                    {
                        "id": 9473,
                        "content": "<p>AD Connector</p>",
                        "isValid": false
                    },
                    {
                        "id": 9474,
                        "content": "<p>Simple AD</p>",
                        "isValid": false
                    },
                    {
                        "id": 9475,
                        "content": "<p>AWS Managed Microsoft AD</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2269,
            "attributes": {
                "createdAt": "2023-09-09T20:33:44.921Z",
                "updatedAt": "2023-09-09T20:33:44.921Z",
                "content": "<p>A legacy application is built using a tightly-coupled monolithic architecture. Due to a sharp increase in the number of users, the application performance has degraded. The company now wants to decouple the architecture and adopt AWS microservices architecture. Some of these microservices need to handle fast running processes whereas other microservices need to handle slower processes.</p>\n\n<p>Which of these options would you identify as the right way of connecting these microservices?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon SQS queue to decouple microservices running faster processes from the microservices running slower ones</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.</p>\n\n<p>Use Amazon SQS to transmit any volume of data, at any level of throughput, without losing messages or requiring other services to be available. SQS lets you decouple application components so that they run and fail independently, increasing the overall fault tolerance of the system. Multiple copies of every message are stored redundantly across multiple availability zones so that they are available whenever needed. Being able to store the messages and replay them is a very important feature in decoupling the system architecture, as is needed in the current use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon SNS to decouple microservices running faster processes from the microservices running slower ones</strong> - Amazon SNS follows the \"publish-subscribe\" (pub-sub) messaging paradigm, with notifications being delivered to clients using a \"push\" mechanism. This is an important difference between SNS and SQS. Whereas SQS is a polling mechanism, that gives applications the chance to poll at their own comfort, the push mechanism assumes the other applications are present. For the current requirement, we need messages to be stored till they are processed by the downstream applications. Hence, SQS is the right choice.</p>\n\n<p><strong>Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones</strong> - Amazon Kinesis Data Streams are used for streaming real-time high-volume data. Kinesis is a publish-subscribe model, used when publisher applications need to publish the same data to different consumers in parallel. SQS is the right fit for the current use case.</p>\n\n<p><strong>Add Amazon EventBridge to decouple the complex architecture</strong> - This event-based service is extremely useful for connecting non-AWS SaaS (Software as a Service) services to AWS services. With Eventbridge, the downstream application would need to immediately process the events whenever they arrive, thereby making it a tightly coupled scenario. Hence, this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n",
                "options": [
                    {
                        "id": 9476,
                        "content": "<p>Add Amazon EventBridge to decouple the complex architecture</p>",
                        "isValid": false
                    },
                    {
                        "id": 9477,
                        "content": "<p>Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones</p>",
                        "isValid": false
                    },
                    {
                        "id": 9478,
                        "content": "<p>Use Amazon SNS to decouple microservices running faster processes from the microservices running slower ones</p>",
                        "isValid": false
                    },
                    {
                        "id": 9479,
                        "content": "<p>Configure Amazon SQS queue to decouple microservices running faster processes from the microservices running slower ones</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2270,
            "attributes": {
                "createdAt": "2023-09-09T20:33:45.022Z",
                "updatedAt": "2023-09-09T20:33:45.022Z",
                "content": "<p>An e-commerce company runs its web application on EC2 instances in an Auto Scaling group and it's configured to handle consumer orders in an SQS queue for downstream processing. The DevOps team has observed that the performance of the application goes down in case of a sudden spike in orders received.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use a target tracking scaling policy based on a custom Amazon SQS queue metric</strong></p>\n\n<p>If you use a target tracking scaling policy based on a custom Amazon SQS queue metric, dynamic scaling can adjust to the demand curve of your application more effectively. You may use an existing CloudWatch Amazon SQS metric like ApproximateNumberOfMessagesVisible for target tracking but you could still face an issue so that the number of messages in the queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue. The solution is to use a backlog per instance metric with the target value being the acceptable backlog per instance to maintain.</p>\n\n<p>To calculate your backlog per instance, divide the ApproximateNumberOfMessages queue attribute by the number of instances in the InService state for the Auto Scaling group. Then set a target value for the Acceptable backlog per instance.</p>\n\n<p>To illustrate with an example, let's say that the current ApproximateNumberOfMessages is 1500 and the fleet's running capacity is 10. If the average processing time is 0.1 seconds for each message and the longest acceptable latency is 10 seconds, then the acceptable backlog per instance is 10 / 0.1, which equals 100. This means that 100 is the target value for your target tracking policy. If the backlog per instance is currently at 150 (1500 / 10), your fleet scales out, and it scales out by five instances to maintain proportion to the target value.</p>\n\n<p>Scaling Based on Amazon SQS:\n<img src=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/sqs-as-custom-metric-diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a simple scaling policy based on a custom Amazon SQS queue metric</strong> - With simple scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process. The main issue with simple scaling is that after a scaling activity is started, the policy must wait for the scaling activity or health check replacement to complete and the cooldown period to expire before responding to additional alarms. This implies that the application would not be able to react quickly to sudden spikes in orders.</p>\n\n<p><strong>Use a step scaling policy based on a custom Amazon SQS queue metric</strong> - With step scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process. When step adjustments are applied, they increase or decrease the current capacity of your Auto Scaling group, and the adjustments vary based on the size of the alarm breach. For the given use-case, step scaling would try to approximate the correct number of instances by increasing/decreasing the steps as per the policy. This is not as efficient as the target tracking policy where you can calculate the exact number of instances required to handle the spike in orders.</p>\n\n<p><strong>Use a scheduled scaling policy based on a custom Amazon SQS queue metric</strong> - Scheduled scaling allows you to set your scaling schedule. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the predictable traffic patterns of your web application. Scaling actions are performed automatically as a function of time and date. You cannot use scheduled scaling policies to address the sudden spike in orders.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p>\n",
                "options": [
                    {
                        "id": 9480,
                        "content": "<p>Use a target tracking scaling policy based on a custom Amazon SQS queue metric</p>",
                        "isValid": true
                    },
                    {
                        "id": 9481,
                        "content": "<p>Use a scheduled scaling policy based on a custom Amazon SQS queue metric</p>",
                        "isValid": false
                    },
                    {
                        "id": 9482,
                        "content": "<p>Use a simple scaling policy based on a custom Amazon SQS queue metric</p>",
                        "isValid": false
                    },
                    {
                        "id": 9483,
                        "content": "<p>Use a step scaling policy based on a custom Amazon SQS queue metric</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2271,
            "attributes": {
                "createdAt": "2023-09-09T20:33:45.123Z",
                "updatedAt": "2023-09-09T20:33:45.123Z",
                "content": "<p>A small business has been running its IT systems on the on-premises infrastructure but the business now plans to migrate to AWS Cloud for operational efficiencies.</p>\n\n<p>As a Solutions Architect, can you suggest a cost-effective serverless solution for its flagship application that has both static and dynamic content?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Host the static content on Amazon S3 and use Lambda with DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of Lambda for distribution across diverse regions</strong> - Lambda with DynamoDB is the right answer for a serverless solution. CloudFront will help in enhancing user experience by delivering content, across different geographic locations with low latency. Amazon S3 is a cost-effective and faster way of distributing static content for web applications.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries</strong> - S3 is not the right fit for hosting Dynamic content, so this option is incorrect.</p>\n\n<p><strong>Host the static content on Amazon S3 and use Amazon EC2 with RDS for generating the dynamic content. Amazon CloudFront can be configured in front of EC2 instance, to make global distribution easy</strong> - The company is looking for a serverless solution, and Amazon EC2 is not a serverless service as the EC2 instances have to be managed by AWS customers.</p>\n\n<p><strong>Host both the static and dynamic content of the web application on Amazon EC2 with RDS as the database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions</strong> - This is a possible solution, but not a cost-effective or optimal one. Since static content can be cost-effectively managed on Amazon S3 and can be accessed and distributed faster when compared to fetching the content from the EC2 server.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/\">https://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/</a></p>\n",
                "options": [
                    {
                        "id": 9484,
                        "content": "<p>Host the static content on Amazon S3 and use Amazon EC2 with RDS for generating the dynamic content. Amazon CloudFront can be configured in front of EC2 instance, to make global distribution easy</p>",
                        "isValid": false
                    },
                    {
                        "id": 9485,
                        "content": "<p>Host both the static and dynamic content of the web application on Amazon EC2 with RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 9486,
                        "content": "<p>Host the static content on Amazon S3 and use Lambda with DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of Lambda for distribution across diverse regions</p>",
                        "isValid": true
                    },
                    {
                        "id": 9487,
                        "content": "<p>Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2272,
            "attributes": {
                "createdAt": "2023-09-09T20:33:45.209Z",
                "updatedAt": "2023-09-09T20:33:45.209Z",
                "content": "<p>An online gaming application has a large chunk of its traffic coming from users who download static assets such as historic leaderboard reports and the game tactics for various games. The current infrastructure and design are unable to cope up with the traffic and application freezes on most of the pages.</p>\n\n<p>Which of the following is a cost-optimal solution that does not need provisioning of infrastructure?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon CloudFront with S3 as the storage solution for the static assets</strong></p>\n\n<p>When you put your content in an S3 bucket in the cloud, a lot of things become much easier. First, you don’t need to plan for and allocate a specific amount of storage space because S3 buckets scale automatically. As S3 is a serverless service, you don’t need to manage or patch servers that store files yourself; you just put and get your content. Finally, even if you require a server for your application (for example, because you have a dynamic application), the server can be smaller because it doesn’t have to handle requests for static content.</p>\n\n<p>CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users. CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located.</p>\n\n<p>When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content. Then, for the next local request for the same content, it’s already cached nearby and can be served immediately.</p>\n\n<p>By caching your content in Edge Locations, CloudFront reduces the load on your S3 bucket and helps ensure a faster response for your users when they request content. Also, data transfer out for content by using CloudFront is often more cost-effective than serving files directly from S3, and there is no data transfer fee from S3 to CloudFront. You only pay for what is delivered to the internet from CloudFront, plus request fees.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Lambda with an RDS database to provide a serverless architecture</strong> - RDS is not the right choice for the current scenario because of the overhead of a database management system, as the given use-case can be addressed by using Amazon S3 storage solution.</p>\n\n<p><strong>Use Amazon CloudFront with DynamoDB for greater speed and low latency access to static assets</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. But, DynamoDB is overkill for the given use-case and will prove to be a very costly solution.</p>\n\n<p><strong>Use AWS Lambda with ElastiCache and Amazon RDS for serving static assets at high speed and low latency</strong> - As discussed above, RDS is not needed for this use case where web application needs to display static pages and facilitate downloads of historic data. S3 is much better suited for this requirement.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n",
                "options": [
                    {
                        "id": 9488,
                        "content": "<p>Configure AWS Lambda with an RDS database to provide a serverless architecture</p>",
                        "isValid": false
                    },
                    {
                        "id": 9489,
                        "content": "<p>Use Amazon CloudFront with DynamoDB for greater speed and low latency access to static assets</p>",
                        "isValid": false
                    },
                    {
                        "id": 9490,
                        "content": "<p>Use AWS Lambda with ElastiCache and Amazon RDS for serving static assets at high speed and low latency</p>",
                        "isValid": false
                    },
                    {
                        "id": 9491,
                        "content": "<p>Use Amazon CloudFront with S3 as the storage solution for the static assets</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2273,
            "attributes": {
                "createdAt": "2023-09-09T20:33:45.297Z",
                "updatedAt": "2023-09-09T20:33:45.297Z",
                "content": "<p>The engineering team at a company wants to use Amazon SQS to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing SQS over the public internet.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use VPC endpoint to access Amazon SQS</strong></p>\n\n<p>AWS customers can access Amazon Simple Queue Service (Amazon SQS) from their Amazon Virtual Private Cloud (Amazon VPC) using VPC endpoints, without using public IPs, and without needing to traverse the public internet. VPC endpoints for Amazon SQS are powered by AWS PrivateLink, a highly available, scalable technology that enables you to privately connect your VPC to supported AWS services.</p>\n\n<p>Amazon VPC endpoints are easy to configure. They also provide reliable connectivity to Amazon SQS without requiring an internet gateway, Network Address Translation (NAT) instance, VPN connection, or AWS Direct Connect connection. With VPC endpoints, the data between your Amazon VPC and Amazon SQS queue is transferred within the Amazon network, helping protect your instances from internet traffic.</p>\n\n<p>AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network. AWS PrivateLink makes it easy to connect services across different accounts and VPCs to significantly simplify the network architecture.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Internet Gateway to access Amazon SQS</strong> - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. This option is ruled out as the team does not want to use the public internet to access Amazon SQS.</p>\n\n<p><strong>Use VPN connection to access Amazon SQS</strong> - AWS Site-to-Site VPN (aka VPN Connection) enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. As the existing infrastructure is within AWS Cloud, therefore a VPN connection is not required.</p>\n\n<p><strong>Use Network Address Translation (NAT) instance to access Amazon SQS</strong> - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the Internet. Amazon provides Amazon Linux AMIs that are configured to run as NAT instances. These AMIs include the string amzn-ami-vpc-nat in their names, so you can search for them in the Amazon EC2 console. This option is ruled out because NAT instances are used to provide internet access to any instances in a private subnet.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/privatelink/\">https://aws.amazon.com/privatelink/</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-sqs-vpc-endpoints-aws-privatelink/\">https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-sqs-vpc-endpoints-aws-privatelink/</a></p>\n",
                "options": [
                    {
                        "id": 9492,
                        "content": "<p>Use Internet Gateway to access Amazon SQS</p>",
                        "isValid": false
                    },
                    {
                        "id": 9493,
                        "content": "<p>Use VPN connection to access Amazon SQS</p>",
                        "isValid": false
                    },
                    {
                        "id": 9494,
                        "content": "<p>Use VPC endpoint to access Amazon SQS</p>",
                        "isValid": true
                    },
                    {
                        "id": 9495,
                        "content": "<p>Use Network Address Translation (NAT) instance to access Amazon SQS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2274,
            "attributes": {
                "createdAt": "2023-09-09T20:33:45.383Z",
                "updatedAt": "2023-09-09T20:33:45.383Z",
                "content": "<p>A financial services company has recently migrated from on-premises infrastructure to AWS Cloud. The DevOps team wants to implement a solution that allows all resource configurations to be reviewed and make sure that they meet compliance guidelines. Also, the solution should be able to offer the capability to look into the resource configuration history across the application stack.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend to the team?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>\"Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes\"</p>\n\n<p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”.</p>\n\n<p>How AWS Config Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\">\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</strong> - AWS CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. You cannot use CloudWatch to maintain a history of resource configuration changes.</p>\n\n<p><strong>Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</strong> - With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - “Who made an API call to modify this resource?”. CloudTrail provides an event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You cannot use CloudTrail to maintain a history of resource configuration changes.</p>\n\n<p><strong>Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</strong> - Using AWS Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. You cannot use Systems Manager to maintain a history of resource configuration changes.</p>\n\n<p>Exam Alert:</p>\n\n<p>You may see scenario-based questions asking you to select one of CloudWatch vs CloudTrail vs Config. Just remember this thumb rule -</p>\n\n<p>Think resource performance monitoring, events, and alerts; think CloudWatch.</p>\n\n<p>Think account-specific activity and audit; think CloudTrail.</p>\n\n<p>Think resource-specific history, audit, and compliance; think Config.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/\">https://aws.amazon.com/cloudwatch/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n",
                "options": [
                    {
                        "id": 9496,
                        "content": "<p>Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</p>",
                        "isValid": false
                    },
                    {
                        "id": 9497,
                        "content": "<p>Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</p>",
                        "isValid": true
                    },
                    {
                        "id": 9498,
                        "content": "<p>Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</p>",
                        "isValid": false
                    },
                    {
                        "id": 9499,
                        "content": "<p>Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2275,
            "attributes": {
                "createdAt": "2023-09-09T20:33:45.464Z",
                "updatedAt": "2023-09-09T20:33:45.464Z",
                "content": "<p>A startup has created a new web application for users to complete a risk assessment survey for COVID-19 symptoms via a self-administered questionnaire. The startup has purchased the domain covid19survey.com using Route 53. The web development team would like to create a Route 53 record so that all traffic for covid19survey.com is routed to www.covid19survey.com.</p>\n\n<p>As a solutions architect, which of the following is the MOST cost-effective solution that you would recommend to the web development team?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com</strong></p>\n\n<p>Alias records provide a Route 53–specific extension to DNS functionality. Alias records let you route traffic to selected AWS resources, such as CloudFront distributions and Amazon S3 buckets.</p>\n\n<p>You can create an alias record at the top node of a DNS namespace, also known as the zone apex, however, you cannot create a CNAME record for the top node of the DNS namespace. So, if you register the DNS name covid19survey.com, the zone apex is covid19survey.com. You can't create a CNAME record for covid19survey.com, but you can create an alias record for covid19survey.com that routes traffic to www.covid19survey.com.</p>\n\n<p>Exam Alert:</p>\n\n<p>You should also note that Route 53 doesn't charge for alias queries to AWS resources but Route 53 does charge for CNAME queries. Additionally, an alias record can only redirect queries to selected AWS resources such as S3 buckets, CloudFront distributions, and another record in the same Route 53 hosted zone; however a CNAME record can redirect DNS queries to any DNS record. So, you can create a CNAME record that redirects queries from app.covid19survey.com to app.covid19survey.net.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com</strong> - You cannot create a CNAME record for the top node of the DNS namespace, so this option is incorrect.</p>\n\n<p><strong>Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com</strong> - An MX record specifies the names of your mail servers and, if you have two or more mail servers, the priority order. It cannot be used to create a Route 53 record to route traffic for the top node of the DNS namespace, so this option is incorrect.</p>\n\n<p><strong>Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com</strong> - An NS record identifies the name servers for the hosted zone. It cannot be used to create a Route 53 record to route traffic for the top node of the DNS namespace, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html</a></p>\n",
                "options": [
                    {
                        "id": 9500,
                        "content": "<p>Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com</p>",
                        "isValid": false
                    },
                    {
                        "id": 9501,
                        "content": "<p>Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com</p>",
                        "isValid": false
                    },
                    {
                        "id": 9502,
                        "content": "<p>Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com</p>",
                        "isValid": true
                    },
                    {
                        "id": 9503,
                        "content": "<p>Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2276,
            "attributes": {
                "createdAt": "2023-09-09T20:33:45.565Z",
                "updatedAt": "2023-09-09T20:33:45.565Z",
                "content": "<p>A video conferencing application is hosted on a fleet of EC2 instances which are part of an Auto Scaling group (ASG). The ASG uses a Launch Configuration (LC1) with \"dedicated\" instance placement tenancy but the VPC (V1) used by the Launch Configuration LC1 has the instance tenancy set to default. Later the DevOps team creates a new Launch Configuration (LC2) with \"default\" instance placement tenancy but the VPC (V2) used by the Launch Configuration LC2 has the instance tenancy set to dedicated.</p>\n\n<p>Which of the following is correct regarding the instances launched via Launch Configuration LC1 and Launch Configuration LC2?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The instances launched by both Launch Configuration LC1 and Launch Configuration LC2 will have dedicated instance tenancy</strong></p>\n\n<p>A launch configuration is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances. Include the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping. If you've launched an EC2 instance before, you specified the same information to launch the instance.</p>\n\n<p>When you create a launch configuration, the default value for the instance placement tenancy is null and the instance tenancy is controlled by the tenancy attribute of the VPC.  If you set the Launch Configuration Tenancy to default and the VPC Tenancy is set to dedicated, then the instances have dedicated tenancy. If you set the Launch Configuration Tenancy to dedicated and the VPC Tenancy is set to default, then again the instances have dedicated tenancy.</p>\n\n<p>Launch Configuration Tenancy vs VPC Tenancy\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-in-vpc.html#as-vpc-tenancy\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-in-vpc.html#as-vpc-tenancy</a></p>\n\n<p>Incorrect options:\n<strong>The instances launched by Launch Configuration LC1 will have dedicated instance tenancy while the instances launched by the Launch Configuration LC2 will have default instance tenancy</strong> - If either Launch Configuration Tenancy or VPC Tenancy is set to dedicated, then the instance tenancy is also dedicated. Therefore, this option is incorrect.</p>\n\n<p><strong>The instances launched by Launch Configuration LC1 will have default instance tenancy while the instances launched by the Launch Configuration LC2 will have dedicated instance tenancy</strong> - If either Launch Configuration Tenancy or VPC Tenancy is set to dedicated, then the instance tenancy is also dedicated. Therefore, this option is incorrect.</p>\n\n<p><strong>The instances launched by both Launch Configuration LC1 and Launch Configuration LC2 will have default instance tenancy</strong> - If either Launch Configuration Tenancy or VPC Tenancy is set to dedicated, then the instance tenancy is also dedicated. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-in-vpc.html#as-vpc-tenancy\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-in-vpc.html#as-vpc-tenancy</a></p>\n",
                "options": [
                    {
                        "id": 9504,
                        "content": "<p>The instances launched by Launch Configuration LC1 will have dedicated instance tenancy while the instances launched by the Launch Configuration LC2 will have default instance tenancy</p>",
                        "isValid": false
                    },
                    {
                        "id": 9505,
                        "content": "<p>The instances launched by both Launch Configuration LC1 and Launch Configuration LC2 will have default instance tenancy</p>",
                        "isValid": false
                    },
                    {
                        "id": 9506,
                        "content": "<p>The instances launched by both Launch Configuration LC1 and Launch Configuration LC2 will have dedicated instance tenancy</p>",
                        "isValid": true
                    },
                    {
                        "id": 9507,
                        "content": "<p>The instances launched by Launch Configuration LC1 will have default instance tenancy while the instances launched by the Launch Configuration LC2 will have dedicated instance tenancy</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2277,
            "attributes": {
                "createdAt": "2023-09-09T20:33:45.661Z",
                "updatedAt": "2023-09-09T20:33:45.661Z",
                "content": "<p>A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up \"AWS Organizations\" to manage several departments running their AWS accounts and using resources such as EC2 instances and RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity.</p>\n\n<p>As a solutions architect, which of the following options would you choose to facilitate this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations</strong></p>\n\n<p>VPC sharing (part of Resource Access Manager) allows multiple AWS accounts to create their application resources such as EC2 instances, RDS databases, Redshift clusters, and Lambda functions, into shared and centrally-managed Amazon Virtual Private Clouds (VPCs). To set this up, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner.</p>\n\n<p>You can share Amazon VPCs to leverage the implicit routing within a VPC for applications that require a high degree of interconnectivity and are within the same trust boundaries. This reduces the number of VPCs that you create and manage while using separate accounts for billing and access control.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations</strong> - Using VPC sharing, an account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. The owner account cannot share the VPC itself. Therefore this option is incorrect.</p>\n\n<p><strong>Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations</strong> - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. VPC peering does not facilitate centrally managed VPCs. Therefore this option is incorrect.</p>\n\n<p><strong>Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations</strong> - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. VPC peering does not facilitate centrally managed VPCs. Moreover, an AWS owner account cannot share the VPC itself with another AWS account. Therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\">https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</a></p>\n",
                "options": [
                    {
                        "id": 9508,
                        "content": "<p>Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations</p>",
                        "isValid": true
                    },
                    {
                        "id": 9509,
                        "content": "<p>Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations</p>",
                        "isValid": false
                    },
                    {
                        "id": 9510,
                        "content": "<p>Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations</p>",
                        "isValid": false
                    },
                    {
                        "id": 9511,
                        "content": "<p>Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2278,
            "attributes": {
                "createdAt": "2023-09-09T20:33:45.755Z",
                "updatedAt": "2023-09-09T20:33:45.755Z",
                "content": "<p>The development team at a retail company wants to optimize the cost of EC2 instances. The team wants to move certain nightly batch jobs to spot instances. The team has hired you as a solutions architect to provide the initial guidance.</p>\n\n<p>Which of the following would you identify as CORRECT regarding the capabilities of spot instances? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>If a spot request is persistent, then it is opened again after your Spot Instance is interrupted</strong></p>\n\n<p><strong>Spot blocks are designed not to be interrupted</strong></p>\n\n<p><strong>When you cancel an active spot request, it does not terminate the associated instance</strong></p>\n\n<p>A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone is set by Amazon EC2 and adjusted gradually based on the long-term supply of and demand for Spot Instances.</p>\n\n<p>A Spot Instance request is either one-time or persistent. If the spot request is persistent, the request is opened again after your Spot Instance is interrupted. If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance. Therefore the option - \"If a spot request is persistent, then it is opened again after your Spot Instance is interrupted\" - is correct.</p>\n\n<p>How Spot requests work\n<img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/spot_lifecycle.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html</a></p>\n\n<p>Spot Instances with a defined duration (also known as Spot blocks) are designed not to be interrupted and will run continuously for the duration you select. You can use a duration of 1, 2, 3, 4, 5, or 6 hours. In rare situations, Spot blocks may be interrupted due to Amazon EC2 capacity needs. Therefore, the option - \"Spot blocks are designed not to be interrupted\" - is correct.</p>\n\n<p>If your Spot Instance request is active and has an associated running Spot Instance, or your Spot Instance request is disabled and has an associated stopped Spot Instance, canceling the request does not terminate the instance; you must terminate the running Spot Instance manually. Moreover, to cancel a persistent Spot request and terminate its Spot Instances, you must cancel the Spot request first and then terminate the Spot Instances. Therefore, the option - \"When you cancel an active spot request, it does not terminate the associated instance\" - is correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When you cancel an active spot request, it terminates the associated instance as well</strong> - If your Spot Instance request is active and has an associated running Spot Instance, then canceling the request does not terminate the instance; you must terminate the running Spot Instance manually. So, this option is incorrect.</p>\n\n<p><strong>If a spot request is persistent, then it is opened again after you stop the Spot Instance</strong> - If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance. So, this option is incorrect.</p>\n\n<p><strong>Spot blocks are designed to be interrupted, just like a spot instance</strong> - Spot blocks are designed not to be interrupted. Only in rare situations, spot blocks may be interrupted due to Amazon EC2 capacity needs. So, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html</a></p>\n",
                "options": [
                    {
                        "id": 9512,
                        "content": "<p>When you cancel an active spot request, it does not terminate the associated instance</p>",
                        "isValid": true
                    },
                    {
                        "id": 9513,
                        "content": "<p>If a spot request is persistent, then it is opened again after your Spot Instance is interrupted</p>",
                        "isValid": true
                    },
                    {
                        "id": 9514,
                        "content": "<p>Spot blocks are designed to be interrupted, just like a spot instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 9515,
                        "content": "<p>Spot blocks are designed not to be interrupted</p>",
                        "isValid": true
                    },
                    {
                        "id": 9516,
                        "content": "<p>If a spot request is persistent, then it is opened again after you stop the Spot Instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 9517,
                        "content": "<p>When you cancel an active spot request, it terminates the associated instance as well</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2279,
            "attributes": {
                "createdAt": "2023-09-09T20:33:45.831Z",
                "updatedAt": "2023-09-09T20:33:45.831Z",
                "content": "<p>An e-commerce company is using an Elastic Load Balancer for its fleet of EC2 instances spread across two Availability Zones, with one instance as a target in Availability Zone A and four instances as targets in Availability Zone B. The company is doing benchmarking for server performance when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled.</p>\n\n<p>As a solutions architect, which of the following traffic distribution outcomes would you identify as correct?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each</strong></p>\n\n<p>The nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. Therefore, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. Therefore, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each.</p>\n\n<p>Consider the following diagrams (the scenario illustrated in the diagrams involves 10 target instances split across 2 AZs) to understand the effect of cross-zone load balancing.</p>\n\n<p>If cross-zone load balancing is enabled, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route its 50% of the client traffic to all 10 targets.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/images/cross_zone_load_balancing_enabled.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n\n<p>If cross-zone load balancing is disabled:</p>\n\n<p>Each of the two targets in Availability Zone A receives 25% of the traffic.</p>\n\n<p>Each of the eight targets in Availability Zone B receives 6.25% of the traffic.</p>\n\n<p>This is because each load balancer node can route its 50% of the client traffic only to targets in its Availability Zone</p>\n\n<p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/images/cross_zone_load_balancing_disabled.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each</strong></p>\n\n<p><strong>With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each</strong></p>\n\n<p><strong>With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n",
                "options": [
                    {
                        "id": 9518,
                        "content": "<p>With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each</p>",
                        "isValid": false
                    },
                    {
                        "id": 9519,
                        "content": "<p>With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each</p>",
                        "isValid": false
                    },
                    {
                        "id": 9520,
                        "content": "<p>With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each</p>",
                        "isValid": true
                    },
                    {
                        "id": 9521,
                        "content": "<p>With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2280,
            "attributes": {
                "createdAt": "2023-09-09T20:33:45.921Z",
                "updatedAt": "2023-09-09T20:33:45.921Z",
                "content": "<p>The DevOps team at a multi-national company is helping its subsidiaries standardize EC2 instances by using the same Amazon Machine Image (AMI). Some of these subsidiaries are in the same AWS region but use different AWS accounts whereas others are in different AWS regions but use the same AWS account as the parent company. The DevOps team has hired you as a solutions architect for this project.</p>\n\n<p>Which of the following would you identify as CORRECT regarding the capabilities of AMIs? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>You can copy an AMI across AWS Regions</strong></p>\n\n<p><strong>You can share an AMI with another AWS account</strong></p>\n\n<p><strong>Copying an AMI backed by an encrypted snapshot cannot result in an unencrypted target snapshot</strong></p>\n\n<p>An Amazon Machine Image (AMI) provides the information required to launch an instance. An AMI includes the following:</p>\n\n<p>One or more EBS snapshots, or, for instance-store-backed AMIs, a template for the root volume of the instance.</p>\n\n<p>Launch permissions that control which AWS accounts can use the AMI to launch instances.</p>\n\n<p>A block device mapping that specifies the volumes to attach to the instance when it's launched.</p>\n\n<p>You can copy an AMI within or across AWS Regions using the AWS Management Console, the AWS Command Line Interface or SDKs, or the Amazon EC2 API, all of which support the CopyImage action. You can copy both Amazon EBS-backed AMIs and instance-store-backed AMIs. You can copy AMIs with encrypted snapshots and also change encryption status during the copy process. Therefore, the option - \"You can copy an AMI across AWS Regions\" - is correct.</p>\n\n<p>Copying AMIs across regions:\n<img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/ami_copy.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</a></p>\n\n<p>The following table shows encryption support for various AMI-copying scenarios. While it is possible to copy an unencrypted snapshot to yield an encrypted snapshot, you cannot copy an encrypted snapshot to yield an unencrypted one. Therefore, the option - \"Copying an AMI backed by an encrypted snapshot cannot result in an unencrypted target snapshot\" is correct.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q55-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</a></p>\n\n<p>You can share an AMI with another AWS account. To copy an AMI that was shared with you from another account, the owner of the source AMI must grant you read permissions for the storage that backs the AMI, either the associated EBS snapshot (for an Amazon EBS-backed AMI) or an associated S3 bucket (for an instance store-backed AMI). Therefore, the option - \"You can share an AMI with another AWS account\" - is correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You cannot copy an AMI across AWS Regions</strong></p>\n\n<p><strong>You cannot share an AMI with another AWS account</strong></p>\n\n<p><strong>Copying an AMI backed by an encrypted snapshot results in an unencrypted target snapshot</strong></p>\n\n<p>These three options contradict the details provided in the explanation above.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</a></p>\n",
                "options": [
                    {
                        "id": 9522,
                        "content": "<p>You cannot copy an AMI across AWS Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 9523,
                        "content": "<p>Copying an AMI backed by an encrypted snapshot results in an unencrypted target snapshot</p>",
                        "isValid": false
                    },
                    {
                        "id": 9524,
                        "content": "<p>You can share an AMI with another AWS account</p>",
                        "isValid": true
                    },
                    {
                        "id": 9525,
                        "content": "<p>You cannot share an AMI with another AWS account</p>",
                        "isValid": false
                    },
                    {
                        "id": 9526,
                        "content": "<p>You can copy an AMI across AWS Regions</p>",
                        "isValid": true
                    },
                    {
                        "id": 9527,
                        "content": "<p>Copying an AMI backed by an encrypted snapshot cannot result in an unencrypted target snapshot</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2281,
            "attributes": {
                "createdAt": "2023-09-09T20:33:46.016Z",
                "updatedAt": "2023-09-09T20:33:46.016Z",
                "content": "<p>A data analytics company manages an application that stores user data in a DynamoDB table. The development team has observed that once in a while, the application writes corrupted data in the DynamoDB table. As soon as the issue is detected, the team needs to remove the corrupted data at the earliest.</p>\n\n<p>What do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use DynamoDB point in time recovery to restore the table to the state just before corrupted data was written</strong></p>\n\n<p>Amazon DynamoDB enables you to back up your table data continuously by using point-in-time recovery (PITR). When you enable PITR, DynamoDB backs up your table data automatically with per-second granularity so that you can restore to any given second in the preceding 35 days.</p>\n\n<p>PITR helps protect you against accidental writes and deletes. For example, if a test script writes accidentally to a production DynamoDB table or someone mistakenly issues a \"DeleteItem\" call, PITR has you covered.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use DynamoDB on-demand backup to restore the table to the state just before corrupted data was written</strong> - The on-demand backup and restore process scales without degrading the performance or availability of your applications. It uses a new and unique distributed technology that lets you complete backups in seconds regardless of table size. You can create backups that are consistent within seconds across thousands of partitions without worrying about schedules or long-running backup processes. All on-demand backups are cataloged, discoverable, and retained until they are explicitly deleted.</p>\n\n<p>On-demand backup is created upon request. So this option is not correct since an on-demand backup cannot be created pre-emptively to handle data corruption issues that happen once in a while.</p>\n\n<p><strong>Configure the DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data</strong> - Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications.</p>\n\n<p>Global tables eliminate the difficult work of replicating data between Regions and resolving update conflicts, enabling you to focus on your application's business logic. In addition, global tables enable your applications to stay highly available even in the unlikely event of isolation or degradation of an entire Region.</p>\n\n<p>Any changes made to any item in any replica table are replicated to all the other replicas within the same global table. In a global table, a newly written item is usually propagated to all replica tables within a second. With a global table, each replica table stores the same set of data items. DynamoDB does not support partial replication of only some of the items. If applications update the same item in different Regions at about the same time, conflicts can arise. To help ensure eventual consistency, DynamoDB global tables use a last-writer-wins reconciliation between concurrent updates, in which DynamoDB makes its best effort to determine the last writer. With this conflict resolution mechanism, all replicas agree on the latest update and converge toward a state in which they all have identical data.</p>\n\n<p>Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions. This option has been added as a distractor since you cannot point the application to use the table from another AWS region, since there is no \"other\" table in another region. It's just a single logical Global table.</p>\n\n<p><strong>Use DynamoDB Streams to restore the table to the state just before corrupted data was written</strong> - DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real time. A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.</p>\n\n<p>DynamoDB Streams writes stream records in near-real time so that you can build applications that consume these streams and take action based on the contents. It will take considerable effort and custom coding to reliably rebuild table data to the state just before any corrupted data was written. So this option is not the best fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery_Howitworks.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery_Howitworks.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n",
                "options": [
                    {
                        "id": 9528,
                        "content": "<p>Configure the DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data</p>",
                        "isValid": false
                    },
                    {
                        "id": 9529,
                        "content": "<p>Use DynamoDB point in time recovery to restore the table to the state just before corrupted data was written</p>",
                        "isValid": true
                    },
                    {
                        "id": 9530,
                        "content": "<p>Use DynamoDB on-demand backup to restore the table to the state just before corrupted data was written</p>",
                        "isValid": false
                    },
                    {
                        "id": 9531,
                        "content": "<p>Use DynamoDB Streams to restore the table to the state just before corrupted data was written</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2282,
            "attributes": {
                "createdAt": "2023-09-09T20:33:46.094Z",
                "updatedAt": "2023-09-09T20:33:46.094Z",
                "content": "<p>A company has hired you as an AWS Certified Solutions Architect to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs.</p>\n\n<p>Which solution will you recommend to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong></p>\n\n<p>Amazon Kinesis Data Streams is useful for rapidly moving data off data producers and then continuously processing the data, be it to transform the data before emitting to a data store, run real-time metrics and analytics, or derive more complex data streams for further processing. Kinesis data streams can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>Kinesis Data Streams Overview:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use SNS to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. SNS cannot be used to decouple the producers and consumers for the real-time data processor as described in the given use-case.</p>\n\n<p><strong>Use SQS to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong> - Amazon Simple Queue Service (Amazon SQS) offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. SQS cannot be used to decouple the producers and consumers for the real-time data processor as described in the given use-case.</p>\n\n<p><strong>Use Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. Kinesis Firehose cannot be used to process and analyze the streaming data in custom applications. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics.</p>\n\n<p>Kinesis Data Firehose Overview\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n",
                "options": [
                    {
                        "id": 9532,
                        "content": "<p>Use SQS to process the data streams as well as decouple the producers and consumers for the real-time data processor</p>",
                        "isValid": false
                    },
                    {
                        "id": 9533,
                        "content": "<p>Use Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor</p>",
                        "isValid": true
                    },
                    {
                        "id": 9534,
                        "content": "<p>Use SNS to process the data streams as well as decouple the producers and consumers for the real-time data processor</p>",
                        "isValid": false
                    },
                    {
                        "id": 9535,
                        "content": "<p>Use Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2283,
            "attributes": {
                "createdAt": "2023-09-09T20:33:46.169Z",
                "updatedAt": "2023-09-09T20:33:46.169Z",
                "content": "<p>A company recently experienced a database outage in its on-premises data center. The company now wants to migrate to a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes.</p>\n\n<p>Which of the following solutions meets these requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up an RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data</strong></p>\n\n<p>When you provision an RDS Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption. In the event of a planned or unplanned outage of your DB instance, Amazon RDS automatically switches to a standby replica in another Availability Zone if you have enabled Multi-AZ. The time it takes for the failover to complete depends on the database activity and other conditions at the time the primary DB instance became unavailable. Failover times are typically 60–120 seconds.</p>\n\n<p><img src=\"https://d1.awsstatic.com/asset-repository/multi-az-deployments.bda9d7bf45a74103d0331a985baf2c5fb838a0fa.png\">\nvia - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data</strong></p>\n\n<p><strong>Set up an RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data</strong></p>\n\n<p>Amazon RDS uses the MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL DB engines' built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica. You can reduce the load on your primary DB instance by routing read queries from your applications to the read replica. Using read replicas, you can elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.</p>\n\n<p>Both these options talk about creating a read replica that <strong>synchronously</strong> replicates the data, but in reality, any updates made to the primary DB instance are <strong>asynchronously</strong> copied to the read replica. So both these options are incorrect.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/read-and-standby-replica.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n\n<p><strong>Set up an EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an RDS MySQL DB instance</strong> - Setting up a database on an EC2 instance would not be reliable as you would have to monitor and manage the underlying EC2 instance for any issues or outages. In addition, using AWS Lambda to replicate the data from EC2 based MySQL DB to an RDS MySQL DB would make the solution really complex since the same functionality can be achieved out-of-the-box using RDS Multi-AZ configuration.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n",
                "options": [
                    {
                        "id": 9536,
                        "content": "<p>Set up an RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data</p>",
                        "isValid": false
                    },
                    {
                        "id": 9537,
                        "content": "<p>Set up an EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an RDS MySQL DB instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 9538,
                        "content": "<p>Set up an RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data</p>",
                        "isValid": true
                    },
                    {
                        "id": 9539,
                        "content": "<p>Set up an RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2284,
            "attributes": {
                "createdAt": "2023-09-09T20:33:46.257Z",
                "updatedAt": "2023-09-09T20:33:46.257Z",
                "content": "<p>A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet.</p>\n\n<p>Which of the following represents the correct configuration for the IPSec VPN Connection?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a Virtual Private Gateway on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN</strong></p>\n\n<p>Amazon VPC provides the facility to create an IPsec VPN connection (also known as site-to-site VPN) between remote customer networks and their Amazon VPC over the internet. The following are the key concepts for a site-to-site VPN:</p>\n\n<p>Virtual private gateway: A Virtual Private Gateway (also known as a VPN Gateway) is the endpoint on the AWS VPC side of your VPN connection.</p>\n\n<p>VPN connection: A secure connection between your on-premises equipment and your VPCs.</p>\n\n<p>VPN tunnel: An encrypted link where data can pass from the customer network to or from AWS.</p>\n\n<p>Customer Gateway: An AWS resource that provides information to AWS about your Customer Gateway device.</p>\n\n<p>Customer Gateway device: A physical device or software application on the customer side of the Site-to-Site VPN connection.</p>\n\n<p>AWS Managed IPSec VPN\n<img src=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/images/aws-managed-vpn.png\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn-network-to-amazon.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn-network-to-amazon.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Virtual Private Gateway on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN</strong> - You need to create a Virtual Private Gateway on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN. Therefore, this option is wrong.</p>\n\n<p><strong>Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN</strong> - You need to create a Virtual Private Gateway on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN. Therefore, this option is wrong.</p>\n\n<p><strong>Create a Virtual Private Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN</strong> - You need to create a Virtual Private Gateway on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN. Therefore, this option is wrong.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn-network-to-amazon.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn-network-to-amazon.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html</a></p>\n",
                "options": [
                    {
                        "id": 9540,
                        "content": "<p>Create a Virtual Private Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN</p>",
                        "isValid": false
                    },
                    {
                        "id": 9541,
                        "content": "<p>Create a Virtual Private Gateway on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN</p>",
                        "isValid": true
                    },
                    {
                        "id": 9542,
                        "content": "<p>Create a Virtual Private Gateway on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN</p>",
                        "isValid": false
                    },
                    {
                        "id": 9543,
                        "content": "<p>Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2285,
            "attributes": {
                "createdAt": "2023-09-09T20:33:46.343Z",
                "updatedAt": "2023-09-09T20:33:46.343Z",
                "content": "<p>The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a NAT instance or a NAT gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the NAT instance and the NAT gateway.</p>\n\n<p>As a solutions architect, which of the following options would you identify as CORRECT? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>NAT instance can be used as a bastion server</strong></p>\n\n<p><strong>Security Groups can be associated with a NAT instance</strong></p>\n\n<p><strong>NAT instance supports port forwarding</strong></p>\n\n<p>A NAT instance or a NAT Gateway can be used in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet.</p>\n\n<p>How NAT Gateway works:\n<img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/nat-gateway-diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n\n<p>How NAT Instance works:\n<img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/nat-instance-diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html</a></p>\n\n<p>Please see this high-level summary of the differences between NAT instances and NAT gateways relevant to the options described in the question:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q12-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>NAT gateway supports port forwarding</strong></p>\n\n<p><strong>Security Groups can be associated with a NAT gateway</strong></p>\n\n<p><strong>NAT gateway can be used as a bastion server</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p>\n",
                "options": [
                    {
                        "id": 9544,
                        "content": "<p>NAT instance can be used as a bastion server</p>",
                        "isValid": true
                    },
                    {
                        "id": 9545,
                        "content": "<p>NAT instance supports port forwarding</p>",
                        "isValid": true
                    },
                    {
                        "id": 9546,
                        "content": "<p>Security Groups can be associated with a NAT gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 9547,
                        "content": "<p>NAT gateway can be used as a bastion server</p>",
                        "isValid": false
                    },
                    {
                        "id": 9548,
                        "content": "<p>NAT gateway supports port forwarding</p>",
                        "isValid": false
                    },
                    {
                        "id": 9549,
                        "content": "<p>Security Groups can be associated with a NAT instance</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2286,
            "attributes": {
                "createdAt": "2023-09-09T20:33:46.421Z",
                "updatedAt": "2023-09-09T20:33:46.421Z",
                "content": "<p>An IT consultant is helping a small business revamp their technology infrastructure on the AWS Cloud. The business has two AWS accounts and all resources are provisioned in the us-west-2 region. The IT consultant is trying to launch an EC2 instance in each of the two AWS accounts such that the instances are in the same Availability Zone of the us-west-2 region. Even after selecting the same default subnet (us-west-2a) while launching the instances in each of the AWS accounts, the IT consultant notices that the Availability Zones are still different.</p>\n\n<p>As a solutions architect, which of the following would you suggest resolving this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use AZ ID to uniquely identify the Availability Zones across the two AWS Accounts</strong></p>\n\n<p>An Availability Zone is represented by a region code followed by a letter identifier; for example, us-east-1a. To ensure that resources are distributed across the Availability Zones for a region, AWS maps Availability Zones to names for each AWS account. For example, the Availability Zone us-west-2a for one AWS account might not be the same location as us-west-2a for another AWS account.</p>\n\n<p>To coordinate Availability Zones across accounts, you must use the AZ ID, which is a unique and consistent identifier for an Availability Zone. For example, usw2-az2 is an AZ ID for the us-west-2 region and it has the same location in every AWS account.</p>\n\n<p>Viewing AZ IDs enables you to determine the location of resources in one account relative to the resources in another account. For example, if you share a subnet in the Availability Zone with the AZ ID usw2-az2 with another account, this subnet is available to that account in the Availability Zone whose AZ ID is also usw2-az2.</p>\n\n<p>You can view the AZ IDs by going to the service health section of the EC2 Dashboard via your AWS Management Console.</p>\n\n<p>AZ IDs for Availability Zones\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q22-i1.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts</strong> - A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS Cloud. Since a VPC spans an AWS region, it cannot be used to uniquely identify an Availability Zone. Therefore, this option is incorrect.</p>\n\n<p><strong>Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts</strong> - A subnet is a range of IP addresses in your VPC. A subnet spans an Availability Zone of an AWS region. The default subnet representing the Availability Zone us-west-2a for one AWS account might not be the same location as us-west-2a for another AWS account. Therefore, this option is incorrect.</p>\n\n<p><strong>Reach out to AWS Support for creating the EC2 instances in the same Availability Zone across the two AWS accounts</strong> - Since the AZ ID is a unique and consistent identifier for an Availability Zone, there is no need to contact AWS Support. Therefore, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html</a></p>\n",
                "options": [
                    {
                        "id": 9550,
                        "content": "<p>Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts</p>",
                        "isValid": false
                    },
                    {
                        "id": 9551,
                        "content": "<p>Use AZ ID to uniquely identify the Availability Zones across the two AWS Accounts</p>",
                        "isValid": true
                    },
                    {
                        "id": 9552,
                        "content": "<p>Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts</p>",
                        "isValid": false
                    },
                    {
                        "id": 9553,
                        "content": "<p>Reach out to AWS Support for creating the EC2 instances in the same Availability Zone across the two AWS accounts</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2287,
            "attributes": {
                "createdAt": "2023-09-09T20:33:46.514Z",
                "updatedAt": "2023-09-09T20:33:46.514Z",
                "content": "<p>A company has a hybrid cloud structure for its on-premises data center and AWS Cloud infrastructure. The company wants to build a web log archival solution such that only the most frequently accessed logs are available as cached data locally while backing up all logs on Amazon S3.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket</strong></p>\n\n<p>AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access.\nWith cached volumes, the AWS Volume Gateway stores the full volume in its Amazon S3 service bucket, and just the recently accessed data is retained in the gateway’s local cache for low-latency access.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS direct connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Direct connect cannot be used to store the most frequently accessed logs locally for low-latency access.</p>\n\n<p><strong>Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket</strong> - With stored volumes, your entire data volume is available locally in the gateway, for fast read access. Volume Gateway also maintains an asynchronous copy of your stored volume in the service’s Amazon S3 bucket. This does not fit the requirements per the given use-case, hence this option is not correct.</p>\n\n<p><strong>Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket</strong> - You can use Snowball Edge Storage Optimized device to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. Snowball Edge Storage Optimized device cannot be used to store the most frequently accessed logs locally for low-latency access.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/volume/\">https://aws.amazon.com/storagegateway/volume/</a></p>\n",
                "options": [
                    {
                        "id": 9554,
                        "content": "<p>Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 9555,
                        "content": "<p>Use AWS direct connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 9556,
                        "content": "<p>Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket</p>",
                        "isValid": true
                    },
                    {
                        "id": 9557,
                        "content": "<p>Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2288,
            "attributes": {
                "createdAt": "2023-09-09T20:33:46.597Z",
                "updatedAt": "2023-09-09T20:33:46.597Z",
                "content": "<p>A financial services company is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon SQS. The development team at the company wants to minimize the costs of using SQS.</p>\n\n<p>As a solutions architect, which of the following options would you recommend for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use SQS long polling to retrieve messages from your Amazon SQS queues</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.</p>\n\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.</p>\n\n<p>Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Using long polling can reduce the cost of using SQS because you can reduce the number of empty receives.</p>\n\n<p>Short Polling vs Long Polling\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q34-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use SQS short polling to retrieve messages from your Amazon SQS queues</strong> - With short polling, Amazon SQS sends the response right away, even if the query found no messages. You end up paying more because of the increased number of empty receives.</p>\n\n<p><strong>Use SQS visibility timeout to retrieve messages from your Amazon SQS queues</strong> - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.</p>\n\n<p><strong>Use SQS message timer to retrieve messages from your Amazon SQS queues</strong> - You can use message timers to set an initial invisibility period for a message added to a queue. So, if you send a message with a 60-second timer, the message isn't visible to consumers for its first 60 seconds in the queue. The default (minimum) delay for a message is 0 seconds. The maximum is 15 minutes. You cannot use message timer to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9558,
                        "content": "<p>Use SQS long polling to retrieve messages from your Amazon SQS queues</p>",
                        "isValid": true
                    },
                    {
                        "id": 9559,
                        "content": "<p>Use SQS message timer to retrieve messages from your Amazon SQS queues</p>",
                        "isValid": false
                    },
                    {
                        "id": 9560,
                        "content": "<p>Use SQS short polling to retrieve messages from your Amazon SQS queues</p>",
                        "isValid": false
                    },
                    {
                        "id": 9561,
                        "content": "<p>Use SQS visibility timeout to retrieve messages from your Amazon SQS queues</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2289,
            "attributes": {
                "createdAt": "2023-09-09T20:33:46.690Z",
                "updatedAt": "2023-09-09T20:33:46.690Z",
                "content": "<p>A company wants to improve its gaming application by adding a leaderboard that uses a complex proprietary algorithm based on the participating user's performance metrics to identify the top users on a real-time basis. The technical requirements mandate high elasticity, low latency, and real-time processing to deliver customizable user data for the community of users. The leaderboard would be accessed by millions of users simultaneously.</p>\n\n<p>Which of the following options support the case for using ElastiCache to meet the given requirements? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use ElastiCache to improve latency and throughput for read-heavy application workloads</strong></p>\n\n<p><strong>Use ElastiCache to improve the performance of compute-intensive workloads</strong></p>\n\n<p>Amazon ElastiCache allows you to run in-memory data stores in the AWS cloud. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/images/ElastiCache-Caching.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a></p>\n\n<p>Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, leaderboard, and Q&amp;A portals) or compute-intensive workloads (such as a recommendation engine) by allowing you to store the objects that are often read in the cache.</p>\n\n<p>Overview of Amazon ElastiCache features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q33-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/features/\">https://aws.amazon.com/elasticache/features/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use ElastiCache to improve latency and throughput for write-heavy application workloads</strong> - As mentioned earlier in the explanation, Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads. Caching is not a good fit for write-heavy applications as the cache goes stale at a very fast rate.</p>\n\n<p><strong>Use ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads</strong> - ETL workloads involve reading and transforming high-volume data which is not a good fit for caching. You should use AWS Glue or Amazon EMR to facilitate ETL workloads.</p>\n\n<p><strong>Use ElastiCache to run highly complex JOIN queries</strong> - Complex JSON queries can be run on relational databases such as RDS or Aurora. ElastiCache is not a good fit for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/features/\">https://aws.amazon.com/elasticache/features/</a></p>\n",
                "options": [
                    {
                        "id": 9562,
                        "content": "<p>Use ElastiCache to run highly complex JOIN queries</p>",
                        "isValid": false
                    },
                    {
                        "id": 9563,
                        "content": "<p>Use ElastiCache to improve the performance of compute-intensive workloads</p>",
                        "isValid": true
                    },
                    {
                        "id": 9564,
                        "content": "<p>Use ElastiCache to improve latency and throughput for read-heavy application workloads</p>",
                        "isValid": true
                    },
                    {
                        "id": 9565,
                        "content": "<p>Use ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads</p>",
                        "isValid": false
                    },
                    {
                        "id": 9566,
                        "content": "<p>Use ElastiCache to improve latency and throughput for write-heavy application workloads</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2290,
            "attributes": {
                "createdAt": "2023-09-09T20:33:46.773Z",
                "updatedAt": "2023-09-09T20:33:46.773Z",
                "content": "<p>Which of the following AWS services provides a highly available and fault-tolerant solution to capture the clickstream events from the source and then provide a concurrent feed of the data stream to the downstream applications?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</p>\n\n<p>Amazon Kinesis Data Streams is recommended when you need the ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another application that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.</p>\n\n<p>KDS provides the ability for multiple applications to consume the same stream concurrently\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q6-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect.</p>\n\n<p><strong>AWS Kinesis Data Analytics</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect.</p>\n\n<p><strong>Amazon SQS</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers at the same time, therefore this option is incorrect.</p>\n\n<p>Exam alert:</p>\n\n<p>Please remember that Kinesis Data Firehose is used to load streaming data into data stores (Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk) whereas Kinesis Data Streams provides support for real-time processing of streaming data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple downstream Amazon Kinesis Applications.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9567,
                        "content": "<p>AWS Kinesis Data Analytics</p>",
                        "isValid": false
                    },
                    {
                        "id": 9568,
                        "content": "<p>AWS Kinesis Data Firehose</p>",
                        "isValid": false
                    },
                    {
                        "id": 9569,
                        "content": "<p>AWS Kinesis Data Streams</p>",
                        "isValid": true
                    },
                    {
                        "id": 9570,
                        "content": "<p>Amazon SQS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2291,
            "attributes": {
                "createdAt": "2023-09-09T20:33:46.864Z",
                "updatedAt": "2023-09-09T20:33:46.864Z",
                "content": "<p>A global pharmaceutical company wants to move most of the on-premises data into Amazon S3, Amazon EFS, and Amazon FSx for Windows File Server easily, quickly, and cost-effectively.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend as the BEST fit to automate and accelerate online data transfers to these AWS storage services?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services</strong></p>\n\n<p>AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data to and from AWS storage services over the internet or AWS Direct Connect.</p>\n\n<p>AWS DataSync fully automates and accelerates moving large active datasets to AWS, up to 10 times faster than command-line tools. It is natively integrated with Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon CloudWatch, and AWS CloudTrail, which provides seamless and secure access to your storage services, as well as detailed monitoring of the transfer.</p>\n\n<p>DataSync uses a purpose-built network protocol and scale-out architecture to transfer data. A single DataSync agent is capable of saturating a 10 Gbps network link.</p>\n\n<p>DataSync fully automates the data transfer. It comes with retry and network resiliency mechanisms, network optimizations, built-in task scheduling, monitoring via the DataSync API and Console, and CloudWatch metrics, events, and logs that provide granular visibility into the transfer process. DataSync performs data integrity verification both during the transfer and at the end of the transfer.</p>\n\n<p>How DataSync Works\n<img src=\"https://d1.awsstatic.com/cloud-storage/Storage/aws-datasync-how-it-works-diagram-s3-efs-fsx.c26c66393dc4e433369ee9947f39e9c54cd338bb.png\">\nvia - <a href=\"https://aws.amazon.com/datasync/\">https://aws.amazon.com/datasync/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services</strong> - Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases. As each Snowball Edge Storage Optimized device can handle 80TB of data, you can order 10 such devices to take care of the data transfer for all applications. The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space.</p>\n\n<p>AWS Snowball Edge is suitable for offline data transfers, for customers who are bandwidth constrained or transferring data from remote, disconnected, or austere environments. Therefore, it cannot support automated and accelerated online data transfers.</p>\n\n<p><strong>Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services</strong> - The AWS Transfer Family provides fully managed support for file transfers directly into and out of Amazon S3 and Amazon EFS. Therefore, it cannot support migration into the other AWS storage services mentioned in the given use-case (Amazon FSx for Windows File Server).</p>\n\n<p><strong>Use File Gateway to automate and accelerate online data transfers to the given AWS storage services</strong> - AWS Storage Gateway's file interface, or file gateway, offers you a seamless way to connect to the cloud to store application data files and backup images as durable objects on Amazon S3 cloud storage. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. It can be used for on-premises applications, and for Amazon EC2-based applications that need file protocol access to S3 object storage. Therefore, it cannot support migration into the other AWS storage services mentioned in the given use-case (such as EFS and Amazon FSx for Windows File Server).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/datasync/faqs/\">https://aws.amazon.com/datasync/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p>\n\n<p><a href=\"https://aws.amazon.com/aws-transfer-family/\">https://aws.amazon.com/aws-transfer-family/</a></p>\n",
                "options": [
                    {
                        "id": 9571,
                        "content": "<p>Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services</p>",
                        "isValid": false
                    },
                    {
                        "id": 9572,
                        "content": "<p>Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services</p>",
                        "isValid": true
                    },
                    {
                        "id": 9573,
                        "content": "<p>Use File Gateway to automate and accelerate online data transfers to the given AWS storage services</p>",
                        "isValid": false
                    },
                    {
                        "id": 9574,
                        "content": "<p>Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2292,
            "attributes": {
                "createdAt": "2023-09-09T20:33:46.983Z",
                "updatedAt": "2023-09-09T20:33:46.983Z",
                "content": "<p>A retail company has connected its on-premises data center to the AWS Cloud via AWS Direct Connect. The company wants to be able to resolve DNS queries for any resources in the on-premises network from the AWS VPC and also resolve any DNS queries for resources in the AWS VPC from the on-premises network.</p>\n\n<p>As a solutions architect, which of the following solutions can be combined to address the given use case? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Create an inbound endpoint on Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint</strong></p>\n\n<p><strong>Create an outbound endpoint on Route 53 Resolver and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint</strong></p>\n\n<p>Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Amazon Route 53 effectively connects user requests to infrastructure running in AWS – such as Amazon EC2 instances – and can also be used to route users to infrastructure outside of AWS. By default, Route 53 Resolver automatically answers DNS queries for local VPC domain names for EC2 instances. You can integrate DNS resolution between Resolver and DNS resolvers on your on-premises network by configuring forwarding rules.</p>\n\n<p>To resolve any DNS queries for resources in the AWS VPC from the on-premises network, you can create an inbound endpoint on Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint.</p>\n\n<p>Resolver Inbound Endpoint\n<img src=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/images/Resolver-inbound-endpoint.png\">\nvia - <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html</a></p>\n\n<p>To resolve DNS queries for any resources in the on-premises network from the AWS VPC, you can create an outbound endpoint on Route 53 Resolver and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint. To conditionally forward queries, you need to create Resolver rules that specify the domain names for the DNS queries that you want to forward (such as example.com) and the IP addresses of the DNS resolvers on the on-premises network that you want to forward the queries to.</p>\n\n<p>Resolver Outbound Endpoint\n<img src=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/images/Resolver-outbound-endpoint.png\">\nvia - <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an outbound endpoint on Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint</strong> - DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via an inbound endpoint. Hence, this option is incorrect.</p>\n\n<p><strong>Create an inbound endpoint on Route 53 Resolver and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint</strong> - Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via an outbound endpoint. Hence, this option is incorrect.</p>\n\n<p><strong>Create a universal endpoint on Route 53 Resolver and then Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint</strong> - There is no such thing as a universal endpoint on Route 53 Resolver. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-getting-started.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-getting-started.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html</a></p>\n",
                "options": [
                    {
                        "id": 9575,
                        "content": "<p>Create an outbound endpoint on Route 53 Resolver and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint</p>",
                        "isValid": true
                    },
                    {
                        "id": 9576,
                        "content": "<p>Create an inbound endpoint on Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint</p>",
                        "isValid": true
                    },
                    {
                        "id": 9577,
                        "content": "<p>Create an outbound endpoint on Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint</p>",
                        "isValid": false
                    },
                    {
                        "id": 9578,
                        "content": "<p>Create an inbound endpoint on Route 53 Resolver and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint</p>",
                        "isValid": false
                    },
                    {
                        "id": 9579,
                        "content": "<p>Create a universal endpoint on Route 53 Resolver and then Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2293,
            "attributes": {
                "createdAt": "2023-09-09T20:33:47.065Z",
                "updatedAt": "2023-09-09T20:33:47.065Z",
                "content": "<p>An engineering lead is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow EC2 instances to download software updates.</p>\n\n<p>Which of the following options represents the correct solution to set up internet access for the private subnets?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ</strong></p>\n\n<p>You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances.</p>\n\n<p>To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside. You must also specify an Elastic IP address to associate with the NAT gateway when you create it. The Elastic IP address cannot be changed after you associate it with the NAT Gateway. After you've created a NAT gateway, you must update the route table associated with one or more of your private subnets to point internet-bound traffic to the NAT gateway. This enables instances in your private subnets to communicate with the internet.</p>\n\n<p>Each NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone.</p>\n\n<p>If you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateway’s Availability Zone is down, resources in the other Availability Zones lose internet access. To create an Availability Zone-independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone.</p>\n\n<p>How NAT gateway works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q62-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ</strong> - NAT gateways need to be set up in public subnets, so this option is incorrect.</p>\n\n<p><strong>Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ</strong> - Internet gateways cannot be provisioned in private subnets of a VPC.</p>\n\n<p><strong>Set up three Egress-only Internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Egress-only Internet Gateway in its AZ</strong> - An Egress-only Internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet, and prevents the internet from initiating an IPv6 connection with your instances. The given use-case is for IPv4 traffic, hence an Egress-only Internet gateway is not an option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n",
                "options": [
                    {
                        "id": 9580,
                        "content": "<p>Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ</p>",
                        "isValid": true
                    },
                    {
                        "id": 9581,
                        "content": "<p>Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ</p>",
                        "isValid": false
                    },
                    {
                        "id": 9582,
                        "content": "<p>Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ</p>",
                        "isValid": false
                    },
                    {
                        "id": 9583,
                        "content": "<p>Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2294,
            "attributes": {
                "createdAt": "2023-09-09T20:33:47.143Z",
                "updatedAt": "2023-09-09T20:33:47.143Z",
                "content": "<p>The business analytics team at a company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the business analytics reporting, the engineering team now wants to continuously replicate this data and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.</p>\n\n<p>As a solutions architect, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</strong></p>\n\n<p>AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS Database Migration Service, you can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3.</p>\n\n<p>Continuous Data Replication\n<img src=\"https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram-AWS-DMS_continuous-data-replication.a0e3bd328d2a4bd9b40a83e767199dcc13cf678f.png\">\nvia - <a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p>\n\n<p>You can migrate data to Amazon Redshift databases using AWS Database Migration Service. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. With an Amazon Redshift database as a target, you can migrate data from all of the other supported source databases.</p>\n\n<p>The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance.\nDuring a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database. The AWS DMS replication instance must be located in that same region.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Glue to replicate the data from the databases into Amazon Redshift</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing.</p>\n\n<p>Using AWS Glue involves significant development efforts to write custom migration scripts to copy the database data into Redshift.</p>\n\n<p><strong>Use AWS EMR to replicate the data from the databases into Amazon Redshift</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.</p>\n\n<p>Using EMR involves significant infrastructure management efforts to set up and maintain the EMR cluster. Additionally this option involves a major development effort to write custom migration jobs to copy the database data into Redshift.</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>However, the user is expected to manually provision an appropriate number of shards to process the expected volume of the incoming data stream. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. Therefore Kinesis Data Streams is not the right fit for this use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p>\n",
                "options": [
                    {
                        "id": 9584,
                        "content": "<p>Use AWS EMR to replicate the data from the databases into Amazon Redshift</p>",
                        "isValid": false
                    },
                    {
                        "id": 9585,
                        "content": "<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</p>",
                        "isValid": false
                    },
                    {
                        "id": 9586,
                        "content": "<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</p>",
                        "isValid": true
                    },
                    {
                        "id": 9587,
                        "content": "<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2295,
            "attributes": {
                "createdAt": "2023-09-09T20:33:47.229Z",
                "updatedAt": "2023-09-09T20:33:47.229Z",
                "content": "<p>A DevOps engineer at an IT company just upgraded an EC2 instance type from t2.nano (0.5G of RAM, 1 vCPU) to u-12tb1.metal (12.3 TB of RAM, 448 vCPUs). How would you categorize this upgrade?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>This is a scale-up example of vertical scalability</strong></p>\n\n<p>Vertical scalability means increasing the size of the instance. For example, your application runs on a t2.micro. Scaling up that application vertically means running it on a larger instance such as t2.large. Scaling down that application vertically means running it on a smaller instance such as t2.nano. Scalability is very common for non-distributed systems, such as a database. There’s usually a limit to how much you can vertically scale (hardware limit).\nIn this case, as the instance type was upgraded from t2.nano to u-12tb1.metal, this is a scale-up example of vertical scalability.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>This is a scale-up example of horizontal scalability</strong> - Horizontal Scalability means increasing the number of instances/systems for your application. When you increase the number of instances, it's called scale-out whereas if you decrease the number of instances, it's called scale-in. Scale-up is used in conjunction with vertical scaling and not with horizontal scaling. Hence this is incorrect.</p>\n\n<p><strong>This is a scale-out example of vertical scalability</strong> - Scale-out is used in conjunction with horizontal scaling and not with vertical scaling. Hence this is incorrect.</p>\n\n<p><strong>This is an example of high availability</strong> - High availability means running your application/system in at least 2 data centers (== Availability Zones). The goal of high availability is to survive a data center loss. An example of High Availability is when you run instances for the same application across multi AZ. This option has been added as a distractor.</p>\n",
                "options": [
                    {
                        "id": 9588,
                        "content": "<p>This is a scale-out example of vertical scalability</p>",
                        "isValid": false
                    },
                    {
                        "id": 9589,
                        "content": "<p>This is a scale-up example of vertical scalability</p>",
                        "isValid": true
                    },
                    {
                        "id": 9590,
                        "content": "<p>This is an example of high availability</p>",
                        "isValid": false
                    },
                    {
                        "id": 9591,
                        "content": "<p>This is a scale-up example of horizontal scalability</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2296,
            "attributes": {
                "createdAt": "2023-09-09T20:33:47.310Z",
                "updatedAt": "2023-09-09T20:33:47.310Z",
                "content": "<p>The DevOps team at an IT company has created a custom VPC (V1) and attached an Internet Gateway (I1) to the VPC. The team has also created a subnet (S1) in this custom VPC and added a route to this subnet's route table (R1) that directs internet-bound traffic to the Internet Gateway. Now the team launches an EC2 instance (E1) in the subnet S1 and assigns a public IPv4 address to this instance. Next the team also launches a NAT instance (N1) in the subnet S1.</p>\n\n<p>Under the given infrastructure setup, which of the following entities is doing the Network Address Translation for the EC2 instance E1?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Internet Gateway (I1)</strong></p>\n\n<p>An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet.</p>\n\n<p>An Internet Gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. Therefore, for instance E1, the  Network Address Translation is done by Internet Gateway I1.</p>\n\n<p>Additionally, an Internet Gateway supports IPv4 and IPv6 traffic. It does not cause availability risks or bandwidth constraints on your network traffic.</p>\n\n<p>To enable access to or from the internet for instances in a subnet in a VPC, you must do the following:</p>\n\n<p>Attach an Internet gateway to your VPC.</p>\n\n<p>Add a route to your subnet's route table that directs internet-bound traffic to the internet gateway. If a subnet is associated with a route table that has a route to an internet gateway, it's known as a public subnet. If a subnet is associated with a route table that does not have a route to an internet gateway, it's known as a private subnet.</p>\n\n<p>Ensure that instances in your subnet have a globally unique IP address (public IPv4 address, Elastic IP address, or IPv6 address).</p>\n\n<p>Ensure that your network access control lists and security group rules allow the relevant traffic to flow to and from your instance.</p>\n\n<p>Internet Gateway Overview\n<img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/internet-gateway-overview-diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>NAT instance (N1)</strong> - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the Internet. As the instance E1 is in a public subnet, therefore this option is not correct.</p>\n\n<p><strong>Subnet (S1)</strong></p>\n\n<p><strong>Route Table (R1)</strong></p>\n\n<p>A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. A subnet is a range of IP addresses in your VPC. A route table contains a set of rules, called routes, that are used to determine where network traffic is directed. Therefore neither Subnet nor Route Table can be used for Network Address Translation.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html</p>\n",
                "options": [
                    {
                        "id": 9592,
                        "content": "<p>Route Table (R1)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9593,
                        "content": "<p>NAT instance (N1)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9594,
                        "content": "<p>Internet Gateway (I1)</p>",
                        "isValid": true
                    },
                    {
                        "id": 9595,
                        "content": "<p>Subnet (S1)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2297,
            "attributes": {
                "createdAt": "2023-09-09T20:33:47.398Z",
                "updatedAt": "2023-09-09T20:33:47.398Z",
                "content": "<p>The engineering team at a company is moving the static content from the company's logistics website hosted on EC2 instances to an S3 bucket. The team wants to use a CloudFront distribution to deliver the static content. The security group used by the EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post-migration to CloudFront, access to the static content should only be allowed from the aforementioned IP addresses.</p>\n\n<p>Which options would you combine to build a solution to meet these requirements? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Configure an origin access identity (OAI) and associate it with the CloudFront distribution. Set up the permissions in the S3 bucket policy so that only the OAI can read the objects</strong></p>\n\n<p>When you use CloudFront with an Amazon S3 bucket as the origin, you can configure CloudFront and Amazon S3 in a way that provides the following benefits:</p>\n\n<p>Restricts access to the Amazon S3 bucket so that it's not publicly accessible</p>\n\n<p>Makes sure that viewers (users) can access the content in the bucket only through the specified CloudFront distribution—that is, prevents them from accessing the content directly from the bucket, or through an unintended CloudFront distribution</p>\n\n<p>To do this, configure CloudFront to send authenticated requests to Amazon S3, and configure Amazon S3 to only allow access to authenticated requests from CloudFront. CloudFront provides two ways to send authenticated requests to an Amazon S3 origin: origin access control (OAC) and origin access identity (OAI).</p>\n\n<p>Exam Alert:</p>\n\n<p>Please note that AWS recommends using OAC because it supports:</p>\n\n<p>All Amazon S3 buckets in all AWS Regions, including opt-in Regions launched after December 2022</p>\n\n<p>Amazon S3 server-side encryption with AWS KMS (SSE-KMS)</p>\n\n<p>Dynamic requests (POST, PUT, etc.) to Amazon S3</p>\n\n<p>OAI doesn't work for the scenarios in the preceding list, or it requires extra workarounds in those scenarios. However, you will continue to see answers enlisting OAI as the preferred option in the actual exam as it takes about 6 months/1 year for a new feature to appear in the exam.</p>\n\n<p><strong>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the EC2 security group. Associate this new WAF ACL with the CloudFront distribution</strong></p>\n\n<p>AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to your protected web application resources. You can protect the following resource types:</p>\n\n<p>Amazon CloudFront distribution</p>\n\n<p>Amazon API Gateway REST API</p>\n\n<p>Application Load Balancer</p>\n\n<p>AWS AppSync GraphQL API</p>\n\n<p>Amazon Cognito user pool</p>\n\n<p>AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, your protected resource responds to requests either with the requested content, with an HTTP 403 status code (Forbidden), or with a custom response.</p>\n\n<p>If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions via your AWS WAF. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from.</p>\n\n<p>For the given use case, you should add those IP addresses that are allowed in the EC2 security group into the IP match condition.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the EC2 security group. Associate this new WAF ACL with the S3 bucket policy</strong> - You cannot associate a WAF ACL with an S3 bucket policy.</p>\n\n<p><strong>Create a new NACL that allows traffic from the same IPs as specified in the current EC2 security group. Associate this new NACL with the CloudFront distribution</strong> - NACL is associated with a subnet within a VPC. CloudFront delivers your content through a worldwide network of data centers called edge locations. So a NACL cannot be associated with a CloudFront distribution.</p>\n\n<p><strong>Create a new security group that allows traffic from the same IPs as specified in the current EC2 security group. Associate this new security group with the CloudFront distribution</strong> - A security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance. CloudFront delivers your content through a worldwide network of data centers called edge locations. So a security group cannot be associated with a CloudFront distribution.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html\">https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html</a></p>\n",
                "options": [
                    {
                        "id": 9596,
                        "content": "<p>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the EC2 security group. Associate this new WAF ACL with the S3 bucket policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 9597,
                        "content": "<p>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the EC2 security group. Associate this new WAF ACL with the CloudFront distribution</p>",
                        "isValid": true
                    },
                    {
                        "id": 9598,
                        "content": "<p>Configure an origin access identity (OAI) and associate it with the CloudFront distribution. Set up the permissions in the S3 bucket policy so that only the OAI can read the objects</p>",
                        "isValid": true
                    },
                    {
                        "id": 9599,
                        "content": "<p>Create a new NACL that allows traffic from the same IPs as specified in the current EC2 security group. Associate this new NACL with the CloudFront distribution</p>",
                        "isValid": false
                    },
                    {
                        "id": 9600,
                        "content": "<p>Create a new security group that allows traffic from the same IPs as specified in the current EC2 security group. Associate this new security group with the CloudFront distribution</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2298,
            "attributes": {
                "createdAt": "2023-09-09T20:33:47.478Z",
                "updatedAt": "2023-09-09T20:33:47.478Z",
                "content": "<p>A global manufacturing company with facilities in the US, Europe, and Asia is designing a new distributed application to optimize its procurement workflow. The orders booked in one AWS Region should be visible to all AWS Regions in a second or less. The database should be able to facilitate failover with a short Recovery Time Objective (RTO). The uptime of the application is critical to ensure that the manufacturing processes are not impacted.</p>\n\n<p>As a solutions architect, which of the following will you recommend as the MOST cost-effective solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Provision Amazon Aurora Global Database</strong></p>\n\n<p>An Aurora global database provides more comprehensive failover capabilities than the failover provided by a default Aurora DB cluster. By using an Aurora global database, you can plan for and recover from disaster fairly quickly. Recovery from disaster is typically measured using values for RTO and RPO.</p>\n\n<p>Recovery time objective (RTO) – The time it takes a system to return to a working state after a disaster. In other words, RTO measures downtime. For an Aurora global database, RTO can be in the order of minutes.</p>\n\n<p>Recovery point objective (RPO) – The amount of data that can be lost (measured in time). For an Aurora global database, RPO is typically measured in seconds.</p>\n\n<p>With an Aurora global database, you can choose from two different approaches to failover:</p>\n\n<ol>\n<li><p>Managed planned failover – This feature is intended for controlled environments, such as disaster recovery (DR) testing scenarios, operational maintenance, and other planned operational procedures. Managed planned failover allows you to relocate the primary DB cluster of your Aurora global database to one of the secondary Regions. Because this feature synchronizes secondary DB clusters with the primary before making any other changes, RPO is 0 (no data loss).</p></li>\n<li><p>Unplanned failover (\"detach and promote\") – To recover from an unplanned outage, you can perform a cross-Region failover to one of the secondaries in your Aurora global database. The RTO for this manual process depends on how quickly you can perform the tasks listed in Recovering an Amazon Aurora global database from an unplanned outage. The RPO is typically measured in seconds, but this depends on the Aurora storage replication lag across the network at the time of the failure.</p></li>\n</ol>\n\n<p>Disaster Recovery in Aurora Global Databases:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q61-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provision Amazon RDS for MySQL with a cross-Region read replica</strong></p>\n\n<p><strong>Provision Amazon RDS for PostgreSQL with a cross-Region read replica</strong></p>\n\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For a failover, read replicas have to be manually promoted to a standalone database instance since the process is not automatic. Hence, the RTO will be quite high, so both these options are not correct for this use case.</p>\n\n<p><strong>Provision Amazon DynamoDB global tables</strong> - Aurora Global Database is good for applications that need to support cross-Region reads with low latency updates and the ability to quickly failover between regions. DynamoDB global tables provide cross-region active-active capabilities with high performance, but you lose some of the data access flexibility that comes with SQL-based databases. Due to the active-active configuration of DynamoDB global tables, there is no concept of failover because the application writes to the table in its region, and then the data is replicated to keep the other regions' table in sync. DynamoDB global tables is a much costlier solution than Aurora Global Database for the given requirement.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html#aurora-global-database-failover\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html#aurora-global-database-failover</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/\">https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html</a></p>\n",
                "options": [
                    {
                        "id": 9601,
                        "content": "<p>Provision Amazon RDS for PostgreSQL with a cross-Region read replica</p>",
                        "isValid": false
                    },
                    {
                        "id": 9602,
                        "content": "<p>Provision Amazon Aurora Global Database</p>",
                        "isValid": true
                    },
                    {
                        "id": 9603,
                        "content": "<p>Provision Amazon RDS for MySQL with a cross-Region read replica</p>",
                        "isValid": false
                    },
                    {
                        "id": 9604,
                        "content": "<p>Provision Amazon DynamoDB global tables</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2299,
            "attributes": {
                "createdAt": "2023-09-09T20:33:47.563Z",
                "updatedAt": "2023-09-09T20:33:47.563Z",
                "content": "<p>A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer (NLB) to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB.</p>\n\n<p>As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance</strong></p>\n\n<p>A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration.</p>\n\n<p>Request Routing and IP Addresses -</p>\n\n<p>If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. The load balancer rewrites the destination IP address from the data packet before forwarding it to the target instance.</p>\n\n<p>If you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port. Note that each network interface can have its security group. The load balancer rewrites the destination IP address before forwarding it to the target.</p>\n\n<p>Incorrect options:\n<strong>Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance</strong> - If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. So public IP address cannot be used to route the traffic to the instance.</p>\n\n<p><strong>Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance</strong> - If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. So elastic IP address cannot be used to route the traffic to the instance.</p>\n\n<p><strong>Traffic is routed to instances using the instance ID specified in the primary network interface for the instance</strong> - You cannot use instance ID to route traffic to the instance. This option is just added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 9605,
                        "content": "<p>Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 9606,
                        "content": "<p>Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance</p>",
                        "isValid": true
                    },
                    {
                        "id": 9607,
                        "content": "<p>Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 9608,
                        "content": "<p>Traffic is routed to instances using the instance ID specified in the primary network interface for the instance</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2300,
            "attributes": {
                "createdAt": "2023-09-09T20:33:47.639Z",
                "updatedAt": "2023-09-09T20:33:47.639Z",
                "content": "<p>A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Global Accelerator to provide a low latency way to distribute live sports results</strong></p>\n\n<p>AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. AWS Global Accelerator is easy to set up, configure, and manage. It provides static IP addresses that provide a fixed entry point to your applications and eliminate the complexity of managing specific IP addresses for different AWS Regions and Availability Zones. AWS Global Accelerator always routes user traffic to the optimal endpoint based on performance, reacting instantly to changes in application health, your user’s location, and policies that you configure. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP. Therefore, this option is correct.</p>\n\n<p>How AWS Global Accelerator Works\n<img src=\"https://d1.awsstatic.com/r2018/b/ubiquity/global-accelerator-how-it-works.feb297eb78d8cc55205874a1691e0ea2bc8bdbf1.png\">\nvia - <a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudFront to provide a low latency way to distribute live sports results</strong> - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.</p>\n\n<p>CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content. Regional edge caches help with all types of content, particularly content that tends to become less popular over time. Examples include user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos; and news and event-related content that might suddenly find new popularity. CloudFront supports HTTP/RTMP protocol based requests, therefore this option is incorrect.</p>\n\n<p><strong>Use Elastic Load Balancer to provide a low latency way to distribute live sports results</strong> - Elastic Load Balancer automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Elastic Load Balancer cannot help with decreasing latency of incoming traffic from the source.</p>\n\n<p><strong>Use Auto Scaling group to provide a low latency way to distribute live sports results</strong> - Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. Auto Scaling group cannot help with decreasing latency of incoming traffic from the source.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please note the differences between the capabilities of Global Accelerator and CloudFront -</p>\n\n<p>AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions.</p>\n\n<p>Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudfront/faqs/\">https://aws.amazon.com/cloudfront/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9609,
                        "content": "<p>Use Global Accelerator to provide a low latency way to distribute live sports results</p>",
                        "isValid": true
                    },
                    {
                        "id": 9610,
                        "content": "<p>Use Elastic Load Balancer to provide a low latency way to distribute live sports results</p>",
                        "isValid": false
                    },
                    {
                        "id": 9611,
                        "content": "<p>Use CloudFront to provide a low latency way to distribute live sports results</p>",
                        "isValid": false
                    },
                    {
                        "id": 9612,
                        "content": "<p>Use Auto Scaling group to provide a low latency way to distribute live sports results</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2301,
            "attributes": {
                "createdAt": "2023-09-09T20:33:47.717Z",
                "updatedAt": "2023-09-09T20:33:47.717Z",
                "content": "<p>Your application is hosted by a provider on yourapp.provider.com. You would like to have your users access your application using www.your-domain.com, which you own and manage under Route 53.</p>\n\n<p>What Route 53 record should you create?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a CNAME record</strong></p>\n\n<p>A CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com or example.net) or subdomain (acme.example.com or zenith.example.org).</p>\n\n<p>CNAME records can be used to map one domain name to another. Although you should keep in mind that the DNS protocol does not allow you to create a CNAME record for the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You cannot create a CNAME record for example.com, but you can create CNAME records for www.example.com, newproduct.example.com, and so on.</p>\n\n<p>Please review the major differences between CNAME and Alias Records:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q64-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an A record</strong> - Used to point a domain or subdomain to an IP address. 'A record' cannot be used to map one domain name to another.</p>\n\n<p><strong>Create a PTR record</strong> - A Pointer (PTR) record resolves an IP address to a fully-qualified domain name (FQDN) as an opposite to what A record does. PTR records are also called Reverse DNS records. 'PTR record' cannot be used to map one domain name to another.</p>\n\n<p><strong>Create an Alias Record</strong> - Alias records let you route traffic to selected AWS resources, such as CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record. 3rd party websites do not qualify for these as we have no control over those. 'Alias record' cannot be used to map one domain name to another.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p>\n",
                "options": [
                    {
                        "id": 9613,
                        "content": "<p>Create a CNAME record</p>",
                        "isValid": true
                    },
                    {
                        "id": 9614,
                        "content": "<p>Create an A record</p>",
                        "isValid": false
                    },
                    {
                        "id": 9615,
                        "content": "<p>Create a PTR record</p>",
                        "isValid": false
                    },
                    {
                        "id": 9616,
                        "content": "<p>Create an Alias Record</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2302,
            "attributes": {
                "createdAt": "2023-09-09T20:33:47.802Z",
                "updatedAt": "2023-09-09T20:33:47.802Z",
                "content": "<p>A retail company has its flagship application running on a fleet of EC2 instances behind an Elastic Load Balancer (ELB). The engineering team has been seeing recurrent issues wherein the in-flight requests from the ELB to the EC2 instances are getting dropped when an instance becomes unhealthy.</p>\n\n<p>Which of the following features can be used to address this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Connection Draining</strong></p>\n\n<p>To ensure that an Elastic Load Balancer stops sending requests to instances that are de-registering or unhealthy while keeping the existing connections open, use connection draining. This enables the load balancer to complete in-flight requests made to instances that are de-registering or unhealthy. The maximum timeout value can be set between 1 and 3,600 seconds (the default is 300 seconds). When the maximum time limit is reached, the load balancer forcibly closes connections to the de-registering instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Cross Zone Load Balancing</strong> - The nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. Cross Zone load balancing cannot be used to complete in-flight requests made to instances that are de-registering or unhealthy.</p>\n\n<p><strong>Sticky Sessions</strong> - You can use the sticky session feature (also known as session affinity) to enable the load balancer to bind a user's session to a specific instance. This ensures that all requests from the user during the session are sent to the same instance. Sticky sessions cannot be used to complete in-flight requests made to instances that are de-registering or unhealthy.</p>\n\n<p><strong>Idle Timeout</strong> - For each request that a client makes through an Elastic Load Balancer, the load balancer maintains two connections. The front-end connection is between the client and the load balancer. The back-end connection is between the load balancer and a registered EC2 instance. The load balancer has a configured \"idle timeout\" period that applies to its connections. If no data has been sent or received by the time that the \"idle timeout\" period elapses, the load balancer closes the connection. \"Idle timeout\" can not be used to complete in-flight requests made to instances that are de-registering or unhealthy.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/config-conn-drain.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/config-conn-drain.html</a></p>\n",
                "options": [
                    {
                        "id": 9617,
                        "content": "<p>Connection Draining</p>",
                        "isValid": true
                    },
                    {
                        "id": 9618,
                        "content": "<p>Idle Timeout</p>",
                        "isValid": false
                    },
                    {
                        "id": 9619,
                        "content": "<p>Sticky Sessions</p>",
                        "isValid": false
                    },
                    {
                        "id": 9620,
                        "content": "<p>Cross Zone load balancing</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2303,
            "attributes": {
                "createdAt": "2023-09-09T20:33:47.884Z",
                "updatedAt": "2023-09-09T20:33:47.884Z",
                "content": "<p>A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures.</p>\n\n<p>As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>AWS Schema Conversion Tool</strong></p>\n\n<p><strong>AWS Database Migration Service</strong></p>\n\n<p>AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. AWS Database Migration Service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle or Microsoft SQL Server to Amazon Aurora.</p>\n\n<p>Given the use-case where the CTO at the company wants to move away from license-based, expensive, legacy commercial database solutions deployed at the on-premises data center to more efficient, open-source, and cost-effective options on AWS Cloud, this is an example of heterogeneous database migrations.</p>\n\n<p>For such a scenario, the source and target databases engines are different, like in the case of Oracle to Amazon Aurora, Oracle to PostgreSQL, or Microsoft SQL Server to MySQL migrations. In this case, the schema structure, data types, and database code of source and target databases can be quite different, requiring a schema and code transformation before the data migration starts.</p>\n\n<p>That makes heterogeneous migrations a two-step process. First use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database. All the required data type conversions will automatically be done by the AWS Database Migration Service during the migration. The source database can be located on your on-premises environment outside of AWS, running on an Amazon EC2 instance, or it can be an Amazon RDS database. The target can be a database in Amazon EC2 or Amazon RDS.</p>\n\n<p>Heterogeneous Database Migrations\n<img src=\"https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram_AWS-DMS_heterogeneous-database-migrations-2.3616bac30ab86d4310ddadfdec5d6e6ba4d8b81d.png\">\nvia - <a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Snowball Edge</strong> - Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases. As each Snowball Edge Storage Optimized device can handle 80TB of data, you can order 10 such devices to take care of the data transfer for all applications. The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space. AWS Snowball Edge cannot be used for database migrations.</p>\n\n<p><strong>AWS Glue</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. Therefore, it cannot be used for database migrations.</p>\n\n<p><strong>Basic Schema Copy</strong> - To quickly migrate a database schema to your target instance you can rely on the Basic Schema Copy feature of AWS Database Migration Service. Basic Schema Copy will automatically create tables and primary keys in the target instance if the target does not already contain tables with the same names. Basic Schema Copy is great for doing a test migration, or when you are migrating databases heterogeneously e.g. Oracle to MySQL or SQL Server to Oracle. Basic Schema Copy will not migrate secondary indexes, foreign keys or stored procedures. When you need to use a more customizable schema migration process (e.g. when you are migrating your production database and need to move your stored procedures and secondary database objects), you must use the AWS Schema Conversion Tool.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dms/faqs/\">https://aws.amazon.com/dms/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dms/schema-conversion-tool/\">https://aws.amazon.com/dms/schema-conversion-tool/</a></p>\n",
                "options": [
                    {
                        "id": 9621,
                        "content": "<p>AWS Snowball Edge</p>",
                        "isValid": false
                    },
                    {
                        "id": 9622,
                        "content": "<p>AWS Schema Conversion Tool</p>",
                        "isValid": true
                    },
                    {
                        "id": 9623,
                        "content": "<p>AWS Glue</p>",
                        "isValid": false
                    },
                    {
                        "id": 9624,
                        "content": "<p>Basic Schema Copy</p>",
                        "isValid": false
                    },
                    {
                        "id": 9625,
                        "content": "<p>AWS Database Migration Service</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2304,
            "attributes": {
                "createdAt": "2023-09-09T20:33:47.969Z",
                "updatedAt": "2023-09-09T20:33:47.969Z",
                "content": "<p>A company has set up \"AWS Organizations\" to manage several departments running their own AWS accounts. The departments operate from different countries and are spread across various AWS Regions. The company wants to set up a consistent resource provisioning process across departments so that each resource follows pre-defined configurations such as using a specific type of EC2 instances, specific IAM roles for Lambda functions, etc.</p>\n\n<p>As a solutions architect, which of the following options would you recommend for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions</strong></p>\n\n<p>AWS CloudFormation StackSet extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. A stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. Using an administrator account of an \"AWS Organization\", you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts of an \"AWS Organization\" across specified regions.</p>\n\n<p>AWS CloudFormation StackSets:\n<img src=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/images/stack_set_conceptual_sv.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions</strong> - Cloudformation template is a JSON or YAML-format, text-based file that describes all the AWS resources you need to deploy to run your application. A template acts as a blueprint for a stack. CloudFormation templates cannot be used to deploy the same template across AWS accounts and regions.</p>\n\n<p><strong>Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions</strong> - CloudFormation stack is a set of AWS resources that are created and managed as a single unit when AWS CloudFormation instantiates a template. A stack cannot be used to deploy the same template across AWS accounts and regions.</p>\n\n<p><strong>Use AWS Resource Access Manager (RAM) to deploy the same template across AWS accounts and regions</strong> -  AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. Resource Access Manager cannot be used to deploy the same template across AWS accounts and regions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html</a></p>\n",
                "options": [
                    {
                        "id": 9626,
                        "content": "<p>Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 9627,
                        "content": "<p>Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 9628,
                        "content": "<p>Use AWS Resource Access Manager (RAM) to deploy the same template across AWS accounts and regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 9629,
                        "content": "<p>Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2305,
            "attributes": {
                "createdAt": "2023-09-09T20:33:48.066Z",
                "updatedAt": "2023-09-09T20:33:48.066Z",
                "content": "<p>A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on S3 against any malicious activity.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend to help address the given requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon GuardDuty to monitor any malicious activity on data stored in S3. Use Amazon Macie to identify any sensitive data stored on S3</strong></p>\n\n<p>Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately.</p>\n\n<p>How GuardDuty works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png\">\nvia - <a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n\n<p>Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data on Amazon S3. Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3.</p>\n\n<p>How Macie works:\n<img src=\"https://d1.awsstatic.com/product-marketing/macie/Product-Page-Diagram_AWS-Macie%402x.369dcc5a001e7a44b121d65637ff82b60b809148.png\">\nvia - <a href=\"https://aws.amazon.com/macie/\">https://aws.amazon.com/macie/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon GuardDuty to monitor any malicious activity on data stored in S3 as well as to identify any sensitive data stored on S3</strong></p>\n\n<p><strong>Use Amazon Macie to monitor any malicious activity on data stored in S3 as well as to identify any sensitive data stored on S3</strong></p>\n\n<p><strong>Use Amazon Macie to monitor any malicious activity on data stored in S3. Use Amazon GuardDuty to identify any sensitive data stored on S3</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n\n<p><a href=\"https://aws.amazon.com/macie/\">https://aws.amazon.com/macie/</a></p>\n",
                "options": [
                    {
                        "id": 9630,
                        "content": "<p>Use Amazon GuardDuty to monitor any malicious activity on data stored in S3. Use Amazon Macie to identify any sensitive data stored on S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 9631,
                        "content": "<p>Use Amazon Macie to monitor any malicious activity on data stored in S3 as well as to identify any sensitive data stored on S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9632,
                        "content": "<p>Use Amazon Macie to monitor any malicious activity on data stored in S3. Use Amazon GuardDuty to identify any sensitive data stored on S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9633,
                        "content": "<p>Use Amazon GuardDuty to monitor any malicious activity on data stored in S3 as well as to identify any sensitive data stored on S3</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2306,
            "attributes": {
                "createdAt": "2023-09-09T20:33:48.161Z",
                "updatedAt": "2023-09-09T20:33:48.161Z",
                "content": "<p>A big data analytics company is working on a real-time vehicle tracking solution. The data processing workflow involves both I/O intensive and throughput intensive database workloads. The development team needs to store this real-time data in a NoSQL database hosted on an EC2 instance and needs to support up to 25,000 IOPS per volume.</p>\n\n<p>As a solutions architect, which of the following EBS volume types would you recommend for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Provisioned IOPS SSD (io1)</strong></p>\n\n<p>Provisioned IOPS SSD (io1) is backed by solid-state drives (SSDs) and is a high-performance EBS storage option designed for critical, I/O intensive database and application workloads, as well as throughput-intensive database workloads. io1 is designed to deliver a consistent baseline performance of up to 50 IOPS/GB to a maximum of 64,000 IOPS and provide up to 1,000 MB/s of throughput per volume. Therefore, the io1 volume type would be able to meet the requirement of 25,000 IOPS per volume for the given use-case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>General Purpose SSD (gp2)</strong> - gp2 is backed by solid-state drives (SSDs) and is suitable for a broad range of transactional workloads, including dev/test environments, low-latency interactive applications, and boot volumes. It supports max IOPS/Volume of 16,000.</p>\n\n<p><strong>Cold HDD (sc1)</strong> - sc1 is backed by hard disk drives (HDDs). It is ideal for less frequently accessed workloads with large, cold datasets. It supports max IOPS/Volume of 250.</p>\n\n<p><strong>Throughput Optimized HDD (st1)</strong> - st1 is backed by hard disk drives (HDDs) and is ideal for frequently accessed, throughput-intensive workloads with large datasets and large I/O sizes, such as MapReduce, Kafka, log processing, data warehouse, and ETL workloads. It supports max IOPS/Volume of 500.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/ebs/volume-types/\">https://aws.amazon.com/ebs/volume-types/</a></p>\n",
                "options": [
                    {
                        "id": 9634,
                        "content": "<p>Throughput Optimized HDD (st1)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9635,
                        "content": "<p>Cold HDD (sc1)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9636,
                        "content": "<p>General Purpose SSD (gp2)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9637,
                        "content": "<p>Provisioned IOPS SSD (io1)</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2307,
            "attributes": {
                "createdAt": "2023-09-09T20:33:48.280Z",
                "updatedAt": "2023-09-09T20:33:48.280Z",
                "content": "<p>A company has its application servers in the public subnet that connect to the RDS instances in the private subnet. For regular maintenance, the RDS instances need patch fixes that need to be downloaded from the internet.</p>\n\n<p>Considering the company uses only IPv4 addressing and is looking for a fully managed service, which of the following would you suggest as an optimal solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure a NAT Gateway in the public subnet of the VPC</strong> - You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside.</p>\n\n<p>You must also specify an Elastic IP address to associate with the NAT gateway when you create it. The Elastic IP address cannot be changed after you associate it with the NAT Gateway. After you've created a NAT gateway, you must update the route table associated with one or more of your private subnets to point internet-bound traffic to the NAT gateway. This enables instances in your private subnets to communicate with the internet. If you no longer need a NAT gateway, you can delete it. Deleting a NAT gateway disassociates its Elastic IP address, but does not release the address from your account.</p>\n\n<p>VPC architecture with NAT:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure an Egress-only internet gateway for the resources in the private subnet of the VPC</strong> - An Egress-only internet gateway is an Internet Gateway that supports IPv6 traffic, so this option is not correct for the given use-case.</p>\n\n<p><strong>Configure a NAT instance in the public subnet of the VPC</strong> - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the internet. NAT instances are not a managed service, it has to be managed and maintained by the customer.</p>\n\n<p><strong>Configure the Internet Gateway of the VPC to be accessible to the private subnet resources, by changing the route tables</strong> - Internet Gateway cannot be used directly with a private subnet. It is not possible to set up this configuration, without a NAT instance or a NAT gateway in the public subnet.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html</a></p>\n",
                "options": [
                    {
                        "id": 9638,
                        "content": "<p>Configure a NAT Gateway in the public subnet of the VPC</p>",
                        "isValid": true
                    },
                    {
                        "id": 9639,
                        "content": "<p>Configure a NAT instance in the public subnet of the VPC</p>",
                        "isValid": false
                    },
                    {
                        "id": 9640,
                        "content": "<p>Configure an Egress-only internet gateway for the resources in the private subnet of the VPC</p>",
                        "isValid": false
                    },
                    {
                        "id": 9641,
                        "content": "<p>Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2308,
            "attributes": {
                "createdAt": "2023-09-09T20:33:48.381Z",
                "updatedAt": "2023-09-09T20:33:48.381Z",
                "content": "<p>An IT company is using SQS queues for decoupling the various components of application architecture. As the consuming components need additional time to process SQS messages, the company wants to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p>As a solutions architect, which of the following solutions would you suggest to the company?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use delay queues to postpone the delivery of new messages to the queue for a few seconds</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Delay queues let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/images/sqs-delay-queues-diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use FIFO queues to postpone the delivery of new messages to the queue for a few seconds</strong> - SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. You cannot use FIFO queues to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p><strong>Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds</strong> - Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. You cannot use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p><strong>Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds</strong> - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a></p>\n",
                "options": [
                    {
                        "id": 9642,
                        "content": "<p>Use FIFO queues to postpone the delivery of new messages to the queue for a few seconds</p>",
                        "isValid": false
                    },
                    {
                        "id": 9643,
                        "content": "<p>Use delay queues to postpone the delivery of new messages to the queue for a few seconds</p>",
                        "isValid": true
                    },
                    {
                        "id": 9644,
                        "content": "<p>Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds</p>",
                        "isValid": false
                    },
                    {
                        "id": 9645,
                        "content": "<p>Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2309,
            "attributes": {
                "createdAt": "2023-09-09T20:33:48.467Z",
                "updatedAt": "2023-09-09T20:33:48.467Z",
                "content": "<p>An e-commerce company has deployed its application on several EC2 instances that are configured in a private subnet using IPv4. These EC2 instances read and write a huge volume of data to and from Amazon S3 in the same AWS region. The company has set up subnet routing to direct all the internet-bound traffic through a NAT gateway. The company wants to build the most cost-optimal solution without impacting the application's ability to communicate with Amazon S3 or the internet.</p>\n\n<p>As an AWS Certified Solutions Architect Associate, which of the following would you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint</strong></p>\n\n<p>Gateway endpoints provide reliable connectivity to Amazon S3 without requiring an internet gateway or a NAT device for your VPC. After you create the gateway endpoint, you can add it as a target in your route table for traffic destined from your VPC to Amazon S3. There is no additional charge for using gateway endpoints.</p>\n\n<p>The VPC endpoint policy for the gateway endpoint controls access to Amazon S3 from the VPC through the endpoint. The default policy allows full access.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q47-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html</a></p>\n\n<p>Using the VPC gateway endpoint allows the EC2 instances to reach Amazon S3 without using the public internet. Since the data transfer remains within the same AWS region, so there is no data transfer costs for ingress as well as egress traffic. Hence this is the most cost-optimal solution.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic</strong> - If a subnet is associated with a route table that has a route to an internet gateway, it's known as a public subnet. If a subnet is associated with a route table that does not have a route to an internet gateway, it's known as a private subnet. This option has been added as a distractor as adding a route to the internet gateway in the route table associated with the private subnet would make the subnet public. This would also make the internet-bound routing to the NAT gateway redundant. This option has been added as a distractor.</p>\n\n<p><strong>Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic</strong> - An egress-only internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet, and prevents the internet from initiating an IPv6 connection with your instances. Since the use case talks about only IPv4 traffic, so this option is incorrect.</p>\n\n<p><strong>Set up a gateway load balancer endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the gateway load balancer endpoint</strong> - Gateway Load Balancers use Gateway Load Balancer endpoints to securely exchange traffic across VPC boundaries. A Gateway Load Balancer endpoint is a VPC endpoint that provides private connectivity between virtual appliances in the service provider VPC and application servers in the service consumer VPC. You cannot set up a gateway load balancer endpoint to access Amazon S3. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/vpc-reduce-nat-gateway-transfer-costs/\">https://aws.amazon.com/premiumsupport/knowledge-center/vpc-reduce-nat-gateway-transfer-costs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-gateway-load-balancer.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-gateway-load-balancer.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html</a></p>\n",
                "options": [
                    {
                        "id": 9646,
                        "content": "<p>Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic</p>",
                        "isValid": false
                    },
                    {
                        "id": 9647,
                        "content": "<p>Set up a gateway load balancer endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the gateway load balancer endpoint</p>",
                        "isValid": false
                    },
                    {
                        "id": 9648,
                        "content": "<p>Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic</p>",
                        "isValid": false
                    },
                    {
                        "id": 9649,
                        "content": "<p>Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2310,
            "attributes": {
                "createdAt": "2023-09-09T20:33:48.539Z",
                "updatedAt": "2023-09-09T20:33:48.539Z",
                "content": "<p>A leading bank has moved its IT infrastructure to AWS Cloud and they have been using Amazon EC2 Auto Scaling for their web servers. This has helped them deal with traffic spikes effectively. But, their MySQL relational database has now become a bottleneck and they urgently need a fully managed auto scaling solution for their relational database to address any unpredictable changes in the traffic.</p>\n\n<p>Can you identify the AWS service that is best suited for this use-case?</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Amazon Aurora Serverless</strong></p>\n\n<p>Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start-up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database instances. It's a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads. You pay on a per-second basis for the database capacity you use when the database is active and migrate between standard and serverless configurations with a few clicks in the Amazon RDS Management Console.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon DynamoDB</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second. But, it is a NoSQL database service and hence not a fit for the given use-case.</p>\n\n<p><strong>Amazon ElastiCache</strong> - Amazon ElastiCache allows you to set up popular open-Source compatible in-memory data stores in the cloud. You can build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores such as Redis and Memcached. Elasticache is used as a caching layer. It's not a fully managed MySQL database.</p>\n\n<p><strong>Amazon Aurora</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora is up to five times faster than standard MySQL databases and three times faster than standard PostgreSQL databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 124 TB per database instance. But, its not a complete auto scaling solution and neither is it fully managed like Aurora serverless. Hence is not the right fit for the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/aurora/serverless/\">https://aws.amazon.com/rds/aurora/serverless/</a></p>\n",
                "options": [
                    {
                        "id": 9650,
                        "content": "<p>Amazon ElastiCache</p>",
                        "isValid": false
                    },
                    {
                        "id": 9651,
                        "content": "<p>Amazon Aurora Serverless</p>",
                        "isValid": true
                    },
                    {
                        "id": 9652,
                        "content": "<p>Amazon Aurora</p>",
                        "isValid": false
                    },
                    {
                        "id": 9653,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2311,
            "attributes": {
                "createdAt": "2023-09-09T20:33:48.623Z",
                "updatedAt": "2023-09-09T20:33:48.623Z",
                "content": "<p>A biotechnology company has multiple High Performance Computing (HPC) workflows that quickly and accurately process and analyze genomes for hereditary diseases. The company is looking to migrate these workflows from their on-premises infrastructure to AWS Cloud.</p>\n\n<p>As a solutions architect, which of the following networking components would you recommend on the EC2 instances running these HPC workflows?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Elastic Fabric Adapter</strong></p>\n\n<p>An Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instance to accelerate High Performance Computing (HPC) and machine learning applications. It enhances the performance of inter-instance communication that is critical for scaling HPC and machine learning applications. EFA devices provide all Elastic Network Adapter (ENA) devices functionalities plus a new OS bypass hardware interface that allows user-space applications to communicate directly with the hardware-provided reliable transport functionality.</p>\n\n<p>How Elastic Fabric Adapter Works\n<img src=\"https://d1.awsstatic.com/Product-Page-Diagram_Elastic-Fabric-Adapter_How-it-Works_updated.2a51303e17a203eb094ab098ebc31a61dab66365.png\">\nvia - <a href=\"https://aws.amazon.com/hpc/efa/\">https://aws.amazon.com/hpc/efa/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Elastic Network Interface</strong> - An Elastic Network Interface (ENI) is a logical networking component in a VPC that represents a virtual network card. You can create a network interface, attach it to an instance, detach it from an instance, and attach it to another instance. The ENI is the simplest networking component available on AWS and is insufficient for HPC workflows.</p>\n\n<p><strong>Elastic Network Adapter</strong> - Elastic Network Adapter (ENA) devices support enhanced networking via single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities. Although enhanced networking provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies, still EFA is a better fit for the given use-case because the EFA device provides all the functionality of an ENA device, plus hardware support for applications to communicate directly with the EFA device without involving the instance kernel (OS-bypass communication) using an extended programming interface.</p>\n\n<p><strong>Elastic IP Address</strong> - An Elastic IP address is a static IPv4 address associated with your AWS account. An Elastic IP address is a public IPv4 address, which is reachable from the internet. It is not a networking device that can be used to facilitate HPC workflows.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/hpc/efa/\">https://aws.amazon.com/hpc/efa/</a></p>\n",
                "options": [
                    {
                        "id": 9654,
                        "content": "<p>Elastic IP Address</p>",
                        "isValid": false
                    },
                    {
                        "id": 9655,
                        "content": "<p>Elastic Network Adapter</p>",
                        "isValid": false
                    },
                    {
                        "id": 9656,
                        "content": "<p>Elastic Fabric Adapter</p>",
                        "isValid": true
                    },
                    {
                        "id": 9657,
                        "content": "<p>Elastic Network Interface</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2312,
            "attributes": {
                "createdAt": "2023-09-09T20:33:48.736Z",
                "updatedAt": "2023-09-09T20:33:48.736Z",
                "content": "<p>The application maintenance team at a company has noticed that the production application is very slow when the business reports are run on the RDS database. These reports fetch a large amount of data and have complex queries with multiple joins, spanning across multiple business-critical core tables. CPU, memory, and storage metrics are around 50% of the total capacity.</p>\n\n<p>Can you recommend an improved and cost-effective way of generating the business reports while keeping the production application unaffected?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a read replica and connect the report generation tool/application to it</strong> - Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances.</p>\n\n<p>There are a variety of scenarios where deploying one or more read replicas for a given source DB instance may make sense. Common reasons for deploying a read replica include:</p>\n\n<ol>\n<li>Scaling beyond the compute or I/O capacity of a single DB instance for read-heavy database workloads. This excess read traffic can be directed to one or more read replicas.</li>\n<li>Serving read traffic while the source DB instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your read replica(s). For this use case, keep in mind that the data on the read replica may be “stale” since the source DB Instance is unavailable.</li>\n<li>Business reporting or data warehousing scenarios; you may want business reporting queries to run against a read replica, rather than your primary, production DB Instance.</li>\n<li>You may use a read replica for disaster recovery of the source DB instance, either in the same AWS Region or in another Region.</li>\n</ol>\n\n<p>Comparing Read Replicas with Multi-AZ and Multi-Region RDS deployments:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q41-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the size of RDS instance</strong> - This will not help as it's mentioned that the CPU, memory, and storage are running at only 50% of the total capacity.</p>\n\n<p><strong>Migrate from General Purpose SSD to magnetic storage to enhance IOPS</strong> - This is incorrect. Amazon RDS supports magnetic storage for backward compatibility only. AWS recommends that you use General Purpose SSD or Provisioned IOPS for any storage needs.</p>\n\n<p><strong>Configure the RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ</strong> - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. However, you cannot read from the standby database, making multi-AZ, an incorrect option for the given scenario.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n",
                "options": [
                    {
                        "id": 9658,
                        "content": "<p>Migrate from General Purpose SSD to magnetic storage to enhance IOPS</p>",
                        "isValid": false
                    },
                    {
                        "id": 9659,
                        "content": "<p>Create a read replica and connect the report generation tool/application to it</p>",
                        "isValid": true
                    },
                    {
                        "id": 9660,
                        "content": "<p>Increase the size of RDS instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 9661,
                        "content": "<p>Configure the RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2313,
            "attributes": {
                "createdAt": "2023-09-09T20:33:48.823Z",
                "updatedAt": "2023-09-09T20:33:48.823Z",
                "content": "<p>A leading news aggregation company offers hundreds of digital products and services for customers ranging from law firms to banks to consumers. The company bills its clients based on per unit of clickstream data provided to the clients. As the company operates in a regulated industry, it needs to have the same ordered clickstream data available for auditing within a window of 7 days.</p>\n\n<p>As a solutions architect, which of the following AWS services provides the ability to run the billing process and auditing process on the given clickstream data in the same order?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).\nAmazon Kinesis Data Streams is recommended when you need the ability to consume records in the same order a few hours later.</p>\n\n<p>For example, you have a billing application and an audit application that runs a few hours behind the billing application. Because Amazon Kinesis Data Streams stores data for a maximum of 365 days, you can easily run the audit application up to 7 days behind the billing application.</p>\n\n<p>KDS provides the ability to consume records in the same order a few hours later\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q20-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores , therefore this option is incorrect.</p>\n\n<p><strong>AWS Kinesis Data Analytics</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect.</p>\n\n<p><strong>Amazon SQS</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers in the same order a few hours later, therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9662,
                        "content": "<p>AWS Kinesis Data Firehose</p>",
                        "isValid": false
                    },
                    {
                        "id": 9663,
                        "content": "<p>Amazon SQS</p>",
                        "isValid": false
                    },
                    {
                        "id": 9664,
                        "content": "<p>AWS Kinesis Data Streams</p>",
                        "isValid": true
                    },
                    {
                        "id": 9665,
                        "content": "<p>AWS Kinesis Data Analytics</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2314,
            "attributes": {
                "createdAt": "2023-09-09T20:33:48.915Z",
                "updatedAt": "2023-09-09T20:33:48.915Z",
                "content": "<p>The engineering team at an e-commerce company wants to migrate from SQS Standard queues to FIFO queues with batching.</p>\n\n<p>As a solutions architect, which of the following steps would you have in the migration checklist? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Delete the existing standard queue and recreate it as a FIFO queue</strong></p>\n\n<p><strong>Make sure that the name of the FIFO queue ends with the .fifo suffix</strong></p>\n\n<p><strong>Make sure that the throughput for the target FIFO queue does not exceed 3,000 messages per second</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.</p>\n\n<p>SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>By default, FIFO queues support up to 3,000 messages per second with batching, or up to 300 messages per second (300 send, receive, or delete operations per second) without batching. Therefore, using batching you can meet a throughput requirement of upto 3,000 messages per second.</p>\n\n<p>The name of a FIFO queue must end with the .fifo suffix. The suffix counts towards the 80-character queue name limit. To determine whether a queue is FIFO, you can check whether the queue name ends with the suffix.</p>\n\n<p>If you have an existing application that uses standard queues and you want to take advantage of the ordering or exactly-once processing features of FIFO queues, you need to configure the queue and your application correctly. You can't convert an existing standard queue into a FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Convert the existing standard queue into a FIFO queue</strong> - You can't convert an existing standard queue into a FIFO queue.</p>\n\n<p><strong>Make sure that the name of the FIFO queue is the same as the standard queue</strong> - The name of a FIFO queue must end with the .fifo suffix.</p>\n\n<p><strong>Make sure that the throughput for the target FIFO queue does not exceed 300 messages per second</strong> - By default, FIFO queues support up to 3,000 messages per second with batching.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n",
                "options": [
                    {
                        "id": 9666,
                        "content": "<p>Delete the existing standard queue and recreate it as a FIFO queue</p>",
                        "isValid": true
                    },
                    {
                        "id": 9667,
                        "content": "<p>Make sure that the name of the FIFO queue ends with the .fifo suffix</p>",
                        "isValid": true
                    },
                    {
                        "id": 9668,
                        "content": "<p>Convert the existing standard queue into a FIFO queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 9669,
                        "content": "<p>Make sure that the name of the FIFO queue is the same as the standard queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 9670,
                        "content": "<p>Make sure that the throughput for the target FIFO queue does not exceed 300 messages per second</p>",
                        "isValid": false
                    },
                    {
                        "id": 9671,
                        "content": "<p>Make sure that the throughput for the target FIFO queue does not exceed 3,000 messages per second</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2315,
            "attributes": {
                "createdAt": "2023-09-09T20:33:49.001Z",
                "updatedAt": "2023-09-09T20:33:49.001Z",
                "content": "<p>A startup has recently moved their monolithic web application to AWS Cloud. The application runs on a single EC2 instance. Currently, the user base is small and the startup does not want to spend effort on elaborate disaster recovery strategies or Auto Scaling Group. The application can afford a maximum downtime of 10 minutes.</p>\n\n<p>In case of a failure, which of these options would you suggest as a cost-effective and automatic recovery procedure for the instance?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure an Amazon CloudWatch alarm that triggers the recovery of the EC2 instance, in case the instance fails. The instance, however, should only be configured with an EBS volume</strong> - If your instance fails a system status check, you can use CloudWatch alarm actions to automatically recover it. The recover option is available for over 90% of deployed customer EC2 instances. The CloudWatch recovery option works only for system check failures, not for instance status check failures. Also, if you terminate your instance, then it can't be recovered.</p>\n\n<p>You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically recovers the instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group.</p>\n\n<p>The automatic recovery process attempts to recover your instance for up to three separate failures per day. Your instance may subsequently be retired if automatic recovery fails and a hardware degradation is determined to be the root cause for the original system status check failure.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon EventBridge events that can trigger the recovery of the EC2 instance, in case the instance or the application fails</strong> - You cannot use EventBridge events to directly trigger the recovery of the EC2 instance.</p>\n\n<p><strong>Configure an Amazon CloudWatch alarm that triggers the recovery of the EC2 instance, in case the instance fails. The instance can be configured with EBS volume or with instance store volumes</strong> - The recover action is supported only on instances that have EBS volumes configured on them, instance store volumes are not supported for automatic recovery by CloudWatch alarms.</p>\n\n<p><strong>Configure AWS Trusted Advisor to monitor the health check of EC2 instance and provide a remedial action in case an unhealthy flag is detected</strong> - You can use Amazon EventBridge events to detect and react to changes in the status of Trusted Advisor checks. This support is only available with AWS Business Support and AWS Enterprise Support. Trusted Advisor by itself does not support health checks of EC2 instances or their recovery.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html</a></p>\n",
                "options": [
                    {
                        "id": 9672,
                        "content": "<p>Configure an Amazon CloudWatch alarm that triggers the recovery of the EC2 instance, in case the instance fails. The instance, however, should only be configured with an EBS volume</p>",
                        "isValid": true
                    },
                    {
                        "id": 9673,
                        "content": "<p>Configure AWS Trusted Advisor to monitor the health check of EC2 instance and provide a remedial action in case an unhealthy flag is detected</p>",
                        "isValid": false
                    },
                    {
                        "id": 9674,
                        "content": "<p>Configure an Amazon CloudWatch alarm that triggers the recovery of the EC2 instance, in case the instance fails. The instance can be configured with EBS volume or with instance store volumes</p>",
                        "isValid": false
                    },
                    {
                        "id": 9675,
                        "content": "<p>Configure Amazon EventBridge events that can trigger the recovery of the EC2 instance, in case the instance or the application fails</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2316,
            "attributes": {
                "createdAt": "2023-09-09T20:33:49.088Z",
                "updatedAt": "2023-09-09T20:33:49.088Z",
                "content": "<p>The engineering team at a social media company wants to use Amazon CloudWatch alarms to automatically recover EC2 instances if they become impaired. The team has hired you as a solutions architect to provide subject matter expertise.</p>\n\n<p>As a solutions architect, which of the following statements would you identify as CORRECT regarding this automatic recovery process? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata</strong></p>\n\n<p><strong>If your instance has a public IPv4 address, it retains the public IPv4 address after recovery</strong></p>\n\n<p>You can create an Amazon CloudWatch alarm to automatically recover the Amazon EC2 instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group. If your instance has a public IPv4 address, it retains the public IPv4 address after recovery. During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Terminated EC2 instances can be recovered if they are configured at the launch of instance</strong> - This is incorrect as terminated instances cannot be recovered.</p>\n\n<p><strong>During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained</strong> - As mentioned above, during instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost.</p>\n\n<p><strong>If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery</strong> - As mentioned above, if your instance has a public IPv4 address, it retains the public IPv4 address after recovery.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html</a></p>\n",
                "options": [
                    {
                        "id": 9676,
                        "content": "<p>If your instance has a public IPv4 address, it retains the public IPv4 address after recovery</p>",
                        "isValid": true
                    },
                    {
                        "id": 9677,
                        "content": "<p>Terminated EC2 instances can be recovered if they are configured at the launch of instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 9678,
                        "content": "<p>During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained</p>",
                        "isValid": false
                    },
                    {
                        "id": 9679,
                        "content": "<p>If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery</p>",
                        "isValid": false
                    },
                    {
                        "id": 9680,
                        "content": "<p>A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2317,
            "attributes": {
                "createdAt": "2023-09-09T20:33:49.173Z",
                "updatedAt": "2023-09-09T20:33:49.173Z",
                "content": "<p>A financial services company wants to move the Windows file server clusters out of their datacenters. They are looking for cloud file storage offerings that provide full Windows compatibility. Can you identify the AWS storage services that provide highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol compatible with Windows systems? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Amazon FSx for Windows File Server</strong> - Amazon FSx for Windows File Server is a fully managed, highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration.</p>\n\n<p><strong>File Gateway Configuration of AWS Storage Gateway</strong> - Depending on the use case, Storage Gateway provides 3 types of storage interfaces for on-premises applications: File, Volume, and Tape. The File Gateway enables you to store and retrieve objects in Amazon S3 using file protocols such as Network File System (NFS) and Server Message Block (SMB).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Elastic File System</strong> - Amazon EFS is a file storage service for use with Amazon EC2. Amazon EFS provides a file system interface, file system access semantics, and concurrently-accessible storage for up to thousands of Amazon EC2 instances. Amazon EFS uses the Network File System protocol. EFS does not support SMB protocol.</p>\n\n<p><strong>Elastic Block Storage</strong> - Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest latency access to data from a single EC2 instance. EBS does not support SMB protocol.</p>\n\n<p><strong>Simple Storage Service (Amazon S3)</strong> - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 provides a simple, standards-based REST web services interface that is designed to work with any Internet-development toolkit. S3 does not support SMB protocol.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p>\n",
                "options": [
                    {
                        "id": 9681,
                        "content": "<p>Elastic File System</p>",
                        "isValid": false
                    },
                    {
                        "id": 9682,
                        "content": "<p>Simple Storage Service (Amazon S3)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9683,
                        "content": "<p>Amazon FSx for Windows File Server</p>",
                        "isValid": true
                    },
                    {
                        "id": 9684,
                        "content": "<p>Elastic Block Storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 9685,
                        "content": "<p>File Gateway Configuration of AWS Storage Gateway</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2318,
            "attributes": {
                "createdAt": "2023-09-09T20:33:49.261Z",
                "updatedAt": "2023-09-09T20:33:49.261Z",
                "content": "<p>A health care application processes the real-time health data of the patients into an analytics workflow. With a sharp increase in the number of users, the system has become slow and sometimes even unresponsive as it does not have a retry mechanism. The startup is looking at a scalable solution that has minimal implementation overhead.</p>\n\n<p>Which of the following would you recommend as a scalable alternative to the current solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Kinesis Data Analytics</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service with support for retry mechanism. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>KDS makes sure your streaming data is available to multiple real-time analytics applications, to Amazon S3, or AWS Lambda within 70 milliseconds of the data being collected. Kinesis data streams scale from megabytes to terabytes per hour and scale from thousands to millions of PUT records per second. You can dynamically adjust the throughput of your stream at any time based on the volume of your input data.</p>\n\n<p>How Data Streams work:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q46-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/?nc=sn&amp;loc=2&amp;dn=2\">https://aws.amazon.com/kinesis/data-streams/?nc=sn&amp;loc=2&amp;dn=2</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon SNS for data ingestion and configure Lambda to trigger logic for downstream processing</strong> - Amazon Simple Notification Service (Amazon SNS) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication. SNS is a push mechanism that does not support robust retry mechanisms, as is needed in the current use case.</p>\n\n<p><strong>Use Amazon SQS for data ingestion and configure Lambda to trigger logic for downstream processing</strong> - SQS is a messaging service that helps in decoupling systems and reducing the complexity of architecture. SQS can still work but Kinesis Data streams is custom made for streaming real-time data.</p>\n\n<p><strong>Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture</strong> - Amazon API Gateway is not meant for handling real-time streaming data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/?nc=sn&amp;loc=2&amp;dn=2\">https://aws.amazon.com/kinesis/data-streams/?nc=sn&amp;loc=2&amp;dn=2</a></p>\n",
                "options": [
                    {
                        "id": 9686,
                        "content": "<p>Use Amazon SQS for data ingestion and configure Lambda to trigger logic for downstream processing</p>",
                        "isValid": false
                    },
                    {
                        "id": 9687,
                        "content": "<p>Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture</p>",
                        "isValid": false
                    },
                    {
                        "id": 9688,
                        "content": "<p>Use Amazon SNS for data ingestion and configure Lambda to trigger logic for downstream processing</p>",
                        "isValid": false
                    },
                    {
                        "id": 9689,
                        "content": "<p>Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Kinesis Data Analytics</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2319,
            "attributes": {
                "createdAt": "2023-09-09T20:33:49.343Z",
                "updatedAt": "2023-09-09T20:33:49.343Z",
                "content": "<p>A media startup is looking at hosting their web application on AWS Cloud. The application will be accessed by users from different geographic regions of the world to upload and download video files that can reach a maximum size of 10GB. The startup wants the solution to be cost-effective and scalable with the lowest possible latency for a great user experience.</p>\n\n<p>As a Solutions Architect, which of the following will you suggest as an optimal solution to meet the given requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 for hosting the web application and use S3 Transfer Acceleration to reduce the latency that geographically dispersed users might face</strong></p>\n\n<p>Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Customers who have either web or mobile applications with widespread users or applications hosted far away from their S3 bucket can experience long and variable upload and download speeds over the Internet. S3 Transfer Acceleration (S3TA) reduces the variability in Internet routing, congestion, and speeds that can affect transfers, and logically shortens the distance to S3 for remote applications.</p>\n\n<p>S3TA improves transfer performance by routing traffic through Amazon CloudFront’s globally distributed Edge Locations and over AWS backbone networks, and by using network protocol optimizations.</p>\n\n<p>For applications interacting with your S3 buckets through the S3 API from outside of your bucket’s region, S3TA helps avoid the variability in Internet routing and congestion. It does this by routing your uploads and downloads over the AWS global network infrastructure, so you get the benefit of AWS network optimizations.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users</strong> - Amazon S3 with CloudFront is a very powerful way of distributing static content to geographically dispersed users with low latency speeds. If you have objects that are smaller than 1GB or if the data set is less than 1GB in size, you should consider using Amazon CloudFront's PUT/POST commands for optimal performance. The given use case has data larger than 1GB and hence S3 Transfer Acceleration is a better option.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt4-q37-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n\n<p><strong>Use Amazon EC2 with Global Accelerator for faster distribution of content, while using Amazon S3 as storage service</strong> - AWS Global Accelerator is a networking service that sends your user’s traffic through Amazon Web Service’s global network infrastructure, improving your internet user performance by up to 60%. With Global Accelerator, you are provided two global static customer-facing IPs to simplify traffic management. On the back end, add or remove your AWS application origins, such as Network Load Balancers, Application Load Balancers, Elastic IPs, and EC2 Instances, without making user-facing changes. As discussed, Global Accelerator is meant for a different use case and is not meant for increasing the speed of S3 uploads or downloads.</p>\n\n<p><strong>Use Amazon EC2 with ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service</strong> -\nAmazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. S3 Transfer Acceleration is a better performing option than opting for EC2 with ElastiCache, which is not meant to address the given use-case.</p>\n\n<p>Reference:</p>\n\n<p>[https://aws.amazon.com/s3/transfer-acceleration/](https://aws.amazon.com/s3/transfer-acceleration</p>\n\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9690,
                        "content": "<p>Use Amazon S3 for hosting the web application and use S3 Transfer Acceleration to reduce the latency that geographically dispersed users might face</p>",
                        "isValid": true
                    },
                    {
                        "id": 9691,
                        "content": "<p>Use Amazon EC2 with ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service</p>",
                        "isValid": false
                    },
                    {
                        "id": 9692,
                        "content": "<p>Use Amazon EC2 with Global Accelerator for faster distribution of content, while using Amazon S3 as storage service</p>",
                        "isValid": false
                    },
                    {
                        "id": 9693,
                        "content": "<p>Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2320,
            "attributes": {
                "createdAt": "2023-09-09T20:33:49.477Z",
                "updatedAt": "2023-09-09T20:33:49.477Z",
                "content": "<p>A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on EC2-based web servers and uses RDS PostgreSQL DB as the data store. The PostgreSQL DB is set up in a private subnet that allows inbound traffic from selected EC2 instances. The DB also uses AWS KMS for encrypting data at rest.</p>\n\n<p>Which of the following steps would you recommend to facilitate secure access to the database?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure RDS to use SSL for data in transit</strong></p>\n\n<p>You can use Secure Socket Layer / Transport Layer Security (SSL/TLS) connections to encrypt data in transit. Amazon RDS creates an SSL certificate and installs the certificate on the DB instance when the instance is provisioned. For MySQL, you launch the MySQL client using the --ssl_ca parameter to reference the public key to encrypt connections. Using SSL, you can encrypt a PostgreSQL connection between your applications and your PostgreSQL DB instances. You can also force all connections to your PostgreSQL DB instance to use SSL.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q31-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/security/\">https://aws.amazon.com/rds/features/security/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use IAM authentication to access the DB instead of the database user's access credentials</strong> - You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token.</p>\n\n<p>IAM authentication is just another way to authenticate the user's credentials while accessing the database. It would not significantly enhance the security in a way that enabling SSL does by facilitating the in-transit encryption for the database.</p>\n\n<p><strong>Create a new security group that blocks SSH from the selected EC2 instances into the DB</strong></p>\n\n<p><strong>Create a new Network Access Control List (NACL) that blocks SSH from the entire EC2 subnet into the DB</strong></p>\n\n<p>Both these options are added as distractors. You cannot SSH into an RDS DB instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/security/\">https://aws.amazon.com/rds/features/security/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html#MySQL.Concepts.SSLSupport\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html#MySQL.Concepts.SSLSupport</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.SSL\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.SSL</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/using-iam-authentication-to-connect-with-pgadmin-amazon-aurora-postgresql-or-amazon-rds-for-postgresql/\">https://aws.amazon.com/blogs/database/using-iam-authentication-to-connect-with-pgadmin-amazon-aurora-postgresql-or-amazon-rds-for-postgresql/</a></p>\n",
                "options": [
                    {
                        "id": 9694,
                        "content": "<p>Create a new Network Access Control List (NACL) that blocks SSH from the entire EC2 subnet into the DB</p>",
                        "isValid": false
                    },
                    {
                        "id": 9695,
                        "content": "<p>Create a new security group that blocks SSH from the selected EC2 instances into the DB</p>",
                        "isValid": false
                    },
                    {
                        "id": 9696,
                        "content": "<p>Configure RDS to use SSL for data in transit</p>",
                        "isValid": true
                    },
                    {
                        "id": 9697,
                        "content": "<p>Use IAM authentication to access the DB instead of the database user's access credentials</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2321,
            "attributes": {
                "createdAt": "2023-09-09T20:33:49.564Z",
                "updatedAt": "2023-09-09T20:33:49.564Z",
                "content": "<p>Your company runs a web portal to match developers to clients who need their help. As a solutions architect, you've designed the architecture of the website to be fully serverless with API Gateway &amp; AWS Lambda. The backend uses a DynamoDB table. You would like to automatically congratulate your developers on important milestones, such as - their first paid contract. All the contracts are stored in DynamoDB.</p>\n\n<p>Which DynamoDB feature can you use to implement this functionality such that there is LEAST delay in sending automatic notifications?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p><strong>DynamoDB Streams + Lambda</strong> - A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table.</p>\n\n<p>DynamoDB Streams will contain a stream of all the changes that happen to a DynamoDB table. It can be chained with a Lambda function that will be triggered to react to these changes, one of which is the developer's milestone. Therefore, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DynamoDB DAX + API Gateway</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.</p>\n\n<p>DAX is a caching layer and API Gateway is used to deploy APIs at scale, so this won't help.</p>\n\n<p><strong>Amazon SQS + Lambda</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>SQS and Lambda could also work, but one would need to write extra logic to send messages to SQS, whereas our data already lives on DynamoDB so DynamoDB Streams is a much better choice.</p>\n\n<p><strong>EventBridge events + Lambda</strong> - You cannot use DynamoDB as a target for a EventBridge event, so this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n",
                "options": [
                    {
                        "id": 9698,
                        "content": "<p>DynamoDB DAX + API Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 9699,
                        "content": "<p>EventBridge events + Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 9700,
                        "content": "<p>DynamoDB Streams + Lambda</p>",
                        "isValid": true
                    },
                    {
                        "id": 9701,
                        "content": "<p>Amazon SQS + Lambda</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2322,
            "attributes": {
                "createdAt": "2023-09-09T20:33:49.682Z",
                "updatedAt": "2023-09-09T20:33:49.682Z",
                "content": "<p>A company has noticed that its EBS storage volume (io1) accounts for 90% of the cost and the remaining 10% cost can be attributed to the EC2 instance. The CloudWatch metrics report that both the EC2 instance and the EBS volume are under-utilized. The CloudWatch metrics also show that the EBS volume has occasional I/O bursts. The entire infrastructure is managed by AWS CloudFormation.</p>\n\n<p>As a Solutions Architect, what do you propose to reduce the costs?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Amazon EBS provides the various volume types, that differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications. The volumes types fall into two categories:</p>\n\n<p>SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS.</p>\n\n<p>HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS</p>\n\n<p>Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.</p>\n\n<p><strong>Convert the Amazon EC2 instance EBS volume to gp2</strong> - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for an extended duration. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver a provisioned performance of 99% uptime. A gp2 volume can range in size from 1 GiB to 16 TiB.</p>\n\n<p>Therefore, gp2 is the right choice as it is more cost-effective than io1, and it also allows a burst in performance when needed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Keep the EBS volume to io1 and reduce the IOPS</strong> - Keeping the EBS volume to io1 and reducing the IOPS may interfere with the burst of performance we need, so this option is ruled out.</p>\n\n<p><strong>Change the Amazon EC2 instance type to something much smaller</strong> - Changing the EC2 instance type to something much smaller won't affect 90% of the costs that are incurred, therefore this option is also incorrect.</p>\n\n<p><strong>Don't use a CloudFormation template to create the database as the CloudFormation service incurs greater service charges</strong> - This statement is incorrect as CloudFormation is a free service to use. The resources that are invoked by CloudFormation are charged as per their utilization rates, but using CloudFormation will not cost anything.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops</a></p>\n",
                "options": [
                    {
                        "id": 9702,
                        "content": "<p>Change the Amazon EC2 instance type to something much smaller</p>",
                        "isValid": false
                    },
                    {
                        "id": 9703,
                        "content": "<p>Keep the EBS volume to io1 and reduce the IOPS</p>",
                        "isValid": false
                    },
                    {
                        "id": 9704,
                        "content": "<p>Convert the Amazon EC2 instance EBS volume to gp2</p>",
                        "isValid": true
                    },
                    {
                        "id": 9705,
                        "content": "<p>Don't use a CloudFormation template to create the database as the CloudFormation service incurs greater service charges</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2323,
            "attributes": {
                "createdAt": "2023-09-09T20:33:49.954Z",
                "updatedAt": "2023-09-09T20:33:49.954Z",
                "content": "<p>A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams.</p>\n\n<p>As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams</strong> - You can achieve this by using AWS Database Migration Service (AWS DMS). AWS DMS enables you to seamlessly migrate data from supported sources to relational databases, data warehouses, streaming platforms, and other data stores in AWS cloud.</p>\n\n<p>The given requirement needs the functionality to be implemented in the least possible time. You can use AWS DMS for such data-processing requirements. AWS DMS lets you expand the existing application to stream data from Amazon S3 into Amazon Kinesis Data Streams for real-time analytics without writing and maintaining new code. AWS DMS supports specifying Amazon S3 as the source and streaming services like Kinesis and Amazon Managed Streaming of Kafka (Amazon MSK) as the target. AWS DMS allows migration of full and change data capture (CDC) files to these services. AWS DMS performs this task out of box without any complex configuration or code development. You can also configure an AWS DMS replication instance to scale up or down depending on the workload.</p>\n\n<p>AWS DMS supports Amazon S3 as the source and Kinesis as the target, so data stored in an S3 bucket is streamed to Kinesis. Several consumers, such as AWS Lambda, Amazon Kinesis Data Firehose, Amazon Kinesis Data Analytics, and the Kinesis Consumer Library (KCL), can consume the data concurrently to perform real-time analytics on the dataset. Each AWS service in this architecture can scale independently as needed.</p>\n\n<p>Architecture of the proposed solution:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q27-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/\">https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the EventBridge event that will send the necessary data to Amazon Kinesis Data Streams</strong> - You will need to enable a Cloudtrail trail to use object-level actions as a trigger for EventBridge events. Also, using Lambda functions would require significant custom development to write the data into Kinesis Data Streams, so this option is not the right fit.</p>\n\n<p><strong>Leverage S3 event notification to trigger a Lambda function for the file create event. The Lambda function will then send the necessary data to Amazon Kinesis Data Streams</strong> - Using Lambda functions would require significant custom development to write the data into Kinesis Data Streams, so this option is not the right fit.</p>\n\n<p><strong>Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (SNS). SNS can then be used to send the updates to Amazon Kinesis Data Streams</strong> - S3 cannot directly write data into SNS, although it can certainly use S3 event notifications to send an event to SNS. Also, SNS cannot directly send messages to Kinesis Data Streams. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/\">https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/</a></p>\n",
                "options": [
                    {
                        "id": 9706,
                        "content": "<p>Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (SNS). SNS can then be used to send the updates to Amazon Kinesis Data Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 9707,
                        "content": "<p>Configure EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the EventBridge event that will send the necessary data to Amazon Kinesis Data Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 9708,
                        "content": "<p>Leverage S3 event notification to trigger a Lambda function for the file create event. The Lambda function will then send the necessary data to Amazon Kinesis Data Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 9709,
                        "content": "<p>Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2324,
            "attributes": {
                "createdAt": "2023-09-09T20:33:50.104Z",
                "updatedAt": "2023-09-09T20:33:50.104Z",
                "content": "<p>A company wants to adopt a hybrid cloud infrastructure where it uses some AWS services such as S3 alongside its on-premises data center. The company wants a dedicated private connection between the on-premise data center and AWS. In case of failures though, the company needs to guarantee uptime and is willing to use the public internet for an encrypted connection.</p>\n\n<p>What do you recommend? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use Direct Connect as a primary connection</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC.</p>\n\n<p><strong>Use Site to Site VPN as a backup connection</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.</p>\n\n<p>Direct Connect as a primary connection guarantees great performance and security (as the connection is private). Using Direct Connect as a backup solution would work but probably carries a risk it would fail as well. As we don't mind going over the public internet (which is reliable, but less secure as connections are going over the public route), we should use a Site to Site VPN which offers an encrypted connection to handle failover scenarios.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Egress Only Internet Gateway as a backup connection</strong> - An Egress-Only Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances. Egress-Only Internet Gateway cannot be used to connect on-premises data centers to AWS Cloud.</p>\n\n<p><strong>Use Site to Site VPN as a primary connection</strong> - Site to Site VPN as a primary connection is not advisable since the use of internet-based connection is only for failover scenarios, as stated in the problem.</p>\n\n<p><strong>Use Direct Connect as a backup connection</strong> - Direct Connect is a highly secure, physical connection. It is also a costly solution and hence does not make much sense to set up the connection and keep it only as a backup.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/directconnect/\">https://aws.amazon.com/directconnect/</a></p>\n\n<p><a href=\"https://aws.amazon.com/vpn/\">https://aws.amazon.com/vpn/</a></p>\n",
                "options": [
                    {
                        "id": 9710,
                        "content": "<p>Use Direct Connect as a primary connection</p>",
                        "isValid": true
                    },
                    {
                        "id": 9711,
                        "content": "<p>Use Site to Site VPN as a backup connection</p>",
                        "isValid": true
                    },
                    {
                        "id": 9712,
                        "content": "<p>Use Egress Only Internet Gateway as a backup connection</p>",
                        "isValid": false
                    },
                    {
                        "id": 9713,
                        "content": "<p>Use Site to Site VPN as a primary connection</p>",
                        "isValid": false
                    },
                    {
                        "id": 9714,
                        "content": "<p>Use Direct Connect as a backup connection</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2325,
            "attributes": {
                "createdAt": "2023-09-09T20:33:50.178Z",
                "updatedAt": "2023-09-09T20:33:50.178Z",
                "content": "<p>You are working for a SaaS (Software as a Service) company as a solutions architect and help design solutions for the company's customers. One of the customers is a bank and has a requirement to whitelist up to two public IPs when the bank is accessing external services across the internet.</p>\n\n<p>Which architectural choice do you recommend to maintain high availability, support scaling-up to 10 instances and comply with the bank's requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><em>*Use a Network Load Balancer with an Auto Scaling Group (ASG) *</em> - Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data. A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second.</p>\n\n<p>Network Load Balancers expose a fixed IP to the public web, therefore allowing your application to be predictably reached using these IPs, while allowing you to scale your application behind the Network Load Balancer using an ASG.</p>\n\n<p>Incorrect options:</p>\n\n<p>Classic Load Balancers and Application Load Balancers use the private IP addresses associated with their Elastic network interfaces as the source IP address for requests forwarded to your web servers.</p>\n\n<p>These IP addresses can be used for various purposes, such as allowing the load balancer traffic on the web servers and for request processing. It's a best practice to use security group referencing on the web servers for whitelisting load balancer traffic from Classic Load Balancers or Application Load Balancers.</p>\n\n<p>However, because Network Load Balancers don't support security groups, based on the target group configurations, the IP addresses of the clients or the private IP addresses associated with the Network Load Balancers must be allowed on the web server's security group.</p>\n\n<p><strong>Use a Classic Load Balancer with an Auto Scaling Group (ASG)</strong> - Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network.</p>\n\n<p><strong>Use an Application Load Balancer with an Auto Scaling Group (ASG)</strong> - Application Load Balancer operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at the delivery of modern application architectures, including microservices and container-based applications.</p>\n\n<p>Application and Classic Load Balancers expose a fixed DNS (=URL) rather than the IP address. So these are incorrect options for the given use-case.</p>\n\n<p><strong>Use an Auto Scaling Group (ASG) with Dynamic Elastic IPs attachment</strong> - The option \"Use an ASG with Dynamic Elastic IPs attachment\" has been added as a distractor. ASG does not have a dynamic Elastic IPs attachment feature.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elb-find-load-balancer-IP/\">https://aws.amazon.com/premiumsupport/knowledge-center/elb-find-load-balancer-IP/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html</a></p>\n",
                "options": [
                    {
                        "id": 9715,
                        "content": "<p>Use a Network Load Balancer with an Auto Scaling Group (ASG)</p>",
                        "isValid": true
                    },
                    {
                        "id": 9716,
                        "content": "<p>Use an Application Load Balancer with an Auto Scaling Group (ASG)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9717,
                        "content": "<p>Use an Auto Scaling Group (ASG) with Dynamic Elastic IPs attachment</p>",
                        "isValid": false
                    },
                    {
                        "id": 9718,
                        "content": "<p>Use a Classic Load Balancer with an Auto Scaling Group (ASG)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2326,
            "attributes": {
                "createdAt": "2023-09-09T20:33:50.284Z",
                "updatedAt": "2023-09-09T20:33:50.284Z",
                "content": "<p>An IT company runs a high-performance computing (HPC) workload on AWS. The workload requires high network throughput and low-latency network performance along with tightly coupled node-to-node communications. The EC2 instances are properly sized for compute and storage capacity and are launched using default options.</p>\n\n<p>Which of the following solutions can be used to improve the performance of the workload?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Select a cluster placement group while launching EC2 instances</strong> - When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload.</p>\n\n<p>Cluster placement group packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly coupled node-to-node communication that is typical of HPC applications.</p>\n\n<p>More on Cluster placement groups:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q60-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Select the appropriate capacity reservation while launching EC2 instances</strong> - Capacity Reservations enable you to reserve compute capacity for your Amazon EC2 instances in a specific Availability Zone for any duration. This gives you the ability to create and manage Capacity Reservations independently from the billing discounts offered by Savings Plans or regional Reserved Instances. By creating Capacity Reservations, you ensure that you always have access to EC2 capacity when you need it, for as long as you need it. Capacity Reservations cannot impact the performance of the underlying EC2 instances.</p>\n\n<p><strong>Select dedicated instance tenancy while launching EC2 instances</strong> - Dedicated Instances are Amazon EC2 instances that run in a VPC on hardware that's dedicated to a single customer. Your Dedicated instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. Dedicated Instances are useful from a compliance perspective, but are not meant for performance improvement.</p>\n\n<p><strong>Select an Elastic Inference accelerator while launching EC2 instances</strong> - Amazon Elastic Inference accelerators are GPU-powered hardware devices that are designed to work with any EC2 instance, Sagemaker instance, or ECS task to accelerate deep learning inference workloads at a low cost. They are for workloads that need deep learning. Also, AWS PrivateLink VPC Endpoints are needed for Elastic Inference accelerators, which makes it unsuitable for the current scenario.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 9719,
                        "content": "<p>Select a cluster placement group while launching EC2 instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 9720,
                        "content": "<p>Select an Elastic Inference accelerator while launching EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9721,
                        "content": "<p>Select dedicated instance tenancy while launching EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9722,
                        "content": "<p>Select the appropriate capacity reservation while launching EC2 instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2327,
            "attributes": {
                "createdAt": "2023-09-09T20:33:50.364Z",
                "updatedAt": "2023-09-09T20:33:50.364Z",
                "content": "<p>A development team has configured an Elastic Load Balancer for host-based routing. The idea is to support multiple subdomains and different top-level domains.</p>\n\n<p>The rule *.example.com matches which of the following?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>test.example.com</strong> - You can use host conditions to define rules that route requests based on the hostname in the host header (also known as host-based routing). This enables you to support multiple subdomains and different top-level domains using a single load balancer.</p>\n\n<p>A hostname is not case-sensitive, can be up to 128 characters in length, and can contain any of the following characters:\n1. A–Z, a–z, 0–9\n2. - .\n3. * (matches 0 or more characters)\n4. ? (matches exactly 1 character)</p>\n\n<p>You must include at least one \".\" character. You can include only alphabetical characters after the final \".\" character.</p>\n\n<p>The rule *.example.com matches test.example.com but doesn't match example.com.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>example.com</strong></p>\n\n<p><strong>example.test.com</strong></p>\n\n<p><strong>EXAMPLE.COM</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a></p>\n",
                "options": [
                    {
                        "id": 9723,
                        "content": "<p>EXAMPLE.COM</p>",
                        "isValid": false
                    },
                    {
                        "id": 9724,
                        "content": "<p>example.com</p>",
                        "isValid": false
                    },
                    {
                        "id": 9725,
                        "content": "<p>example.test.com</p>",
                        "isValid": false
                    },
                    {
                        "id": 9726,
                        "content": "<p>test.example.com</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2328,
            "attributes": {
                "createdAt": "2023-09-09T20:33:50.447Z",
                "updatedAt": "2023-09-09T20:33:50.447Z",
                "content": "<p>A ride-sharing company wants to improve the ride-tracking system that stores GPS coordinates for all rides. The engineering team at the company is looking for a NoSQL database that has single-digit millisecond latency, can scale horizontally, and is serverless, so that they can perform high-frequency lookups reliably.</p>\n\n<p>As a Solutions Architect, which database do you recommend for their requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon DynamoDB</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable NoSQL database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second. DynamoDB is serverless, has single-digit millisecond latency and scales horizontally. This is the correct choice for the given requirements.</p>\n\n<p>Sample DynamoDB solution for Real time applications:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q10-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores, compatible with Redis or Memcached. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. The primary use-case for ElastiCache is that of a caching service and it should not be used as the main database.</p>\n\n<p>How ElastCache works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q10-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n\n<p><strong>Amazon Relational Database Service (Amazon RDS)</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. Relational databases are not NoSQL databases and these cannot provide the millisecond latency that the current use case needs, hence it's an incorrect choice.</p>\n\n<p><strong>Amazon Neptune</strong> - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security.</p>\n\n<p>Example Use cases of Neptune:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q10-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/neptune/\">https://aws.amazon.com/neptune/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n\n<p><a href=\"https://aws.amazon.com/neptune/\">https://aws.amazon.com/neptune/</a></p>\n",
                "options": [
                    {
                        "id": 9727,
                        "content": "<p>Amazon Relational Database Service (Amazon RDS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9728,
                        "content": "<p>Amazon ElastiCache</p>",
                        "isValid": false
                    },
                    {
                        "id": 9729,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 9730,
                        "content": "<p>Amazon Neptune</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2329,
            "attributes": {
                "createdAt": "2023-09-09T20:33:50.540Z",
                "updatedAt": "2023-09-09T20:33:50.540Z",
                "content": "<p>As an e-sport tournament hosting company, you have servers that need to scale and be highly available. Therefore you have deployed an Elastic Load Balancer (ELB) with an Auto Scaling group (ASG) across 3 Availability Zones (AZs). When e-sport tournaments are running, the servers need to scale quickly. And when tournaments are done, the servers can be idle. As a general rule, you would like to be highly available, have the capacity to scale and optimize your costs.</p>\n\n<p>What do you recommend? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Set the minimum capacity to 2</strong></p>\n\n<p>An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service.</p>\n\n<p>You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity.</p>\n\n<p>An Auto Scaling group is elastic as long as it has different values for minimum and maximum capacity. All requests to change the Auto Scaling group's desired capacity (either by manual scaling or automatic scaling) must fall within these limits.</p>\n\n<p>Here, even though our ASG is deployed across 3 AZs, the minimum capacity to be highly available is 2. When we specify 2 as the minimum capacity, the ASG would create these 2 instances in separate AZs. If demand goes up, the ASG would spin up a new instance in the third AZ. Later as the demand subsides, the ASG would scale-in and the instance count would be back to 2.</p>\n\n<p><strong>Use Reserved Instances for the minimum capacity</strong></p>\n\n<p>Reserved Instances provide you with significant savings on your Amazon EC2 costs compared to On-Demand Instance pricing. Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account. These On-Demand Instances must match certain attributes, such as instance type and Region, to benefit from the billing discount. Since minimum capacity will always be maintained, it is cost-effective to choose reserved instances than any other option.</p>\n\n<p>In case of an AZ outage, the instance in that AZ would go down however the other instance would still be available. The ASG would provision the replacement instance in the third AZ to keep the minimum count to 2.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set the minimum capacity to 1</strong> - This is not failure proof, since only one instance will be maintained consistently and this will be from only one availability zone.</p>\n\n<p><strong>Set the minimum capacity to 3</strong> - This is not a cost-effective option, as two instances in two different AZs are enough to make the architecture disaster-proof.</p>\n\n<p><strong>Use Dedicated hosts for the minimum capacity</strong> - As there is no use-case to utilize existing per-socket, per-core, or per-VM software licenses or to run the instance on a dedicated physical host, so the option to use dedicated hosts for the minimum capacity is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a></p>\n",
                "options": [
                    {
                        "id": 9731,
                        "content": "<p>Use Dedicated hosts for the minimum capacity</p>",
                        "isValid": false
                    },
                    {
                        "id": 9732,
                        "content": "<p>Set the minimum capacity to 1</p>",
                        "isValid": false
                    },
                    {
                        "id": 9733,
                        "content": "<p>Use Reserved Instances for the minimum capacity</p>",
                        "isValid": true
                    },
                    {
                        "id": 9734,
                        "content": "<p>Set the minimum capacity to 2</p>",
                        "isValid": true
                    },
                    {
                        "id": 9735,
                        "content": "<p>Set the minimum capacity to 3</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2330,
            "attributes": {
                "createdAt": "2023-09-09T20:33:50.627Z",
                "updatedAt": "2023-09-09T20:33:50.627Z",
                "content": "<p>As part of the on-premises data center migration to AWS Cloud, a company is looking at using multiple AWS Snow Family devices to move their on-premises data.</p>\n\n<p>Which Snow Family service offers the feature of storage clustering?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Snowball Edge Compute Optimized</strong> - AWS Snowball is a data migration and edge computing device that comes in two device options: Compute Optimized and Storage Optimized. Snowball Edge Storage Optimized devices provide 40 vCPUs of compute capacity coupled with 80 terabytes of usable block or Amazon S3-compatible object storage. It is well-suited for local storage and large-scale data transfer. Snowball Edge Compute Optimized devices provide 52 vCPUs, 42 terabytes of usable block or object storage, and an optional GPU for use cases such as advanced machine learning and full-motion video analysis in disconnected environments.</p>\n\n<p>Customers can use these two options for data collection, machine learning and processing, and storage in environments with intermittent connectivity (such as manufacturing, industrial, and transportation) or in extremely remote locations (such as military or maritime operations) before shipping it back to AWS. These devices may also be rack mounted and clustered together to build larger, temporary installations.</p>\n\n<p>Therefore, both AWS Snowball Edge Storage Optimized and AWS Snowball Edge Compute Optimized offer the storage clustering feature.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q56-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/snow/#Feature_comparison\">https://aws.amazon.com/snow/#Feature_comparison</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Snowcone</strong> - AWS Snowcone is the smallest member of the AWS Snow Family of edge computing and data transfer devices. Snowcone is portable, rugged, and secure. You can use Snowcone to collect, process, and move data to AWS, either offline by shipping the device or online with AWS DataSync. Snowcone does not offer a storage Clustering option.</p>\n\n<p><strong>AWS Snowmobile</strong> - AWS Snowmobile moves up to 100 PB of data in a 45-foot long ruggedized shipping container and is ideal for multi-petabyte or Exabyte-scale digital media migrations and data centers shutdowns. A Snowmobile arrives at the customer site and appears as a network-attached data store for more secure, high-speed data transfer. After data is transferred to Snowmobile, it is driven back to an AWS Region where the data is loaded into Amazon S3. Snowmobile does not offer a storage Clustering option.</p>\n\n<p><strong>AWS Snowmobile Storage Compute</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/snow/#Feature_comparison\">https://aws.amazon.com/snow/#Feature_comparison</a></p>\n\n<p><a href=\"https://aws.amazon.com/snow/\">https://aws.amazon.com/snow/</a></p>\n",
                "options": [
                    {
                        "id": 9736,
                        "content": "<p>AWS Snowmobile Storage Compute</p>",
                        "isValid": false
                    },
                    {
                        "id": 9737,
                        "content": "<p>AWS Snowball Edge Compute Optimized</p>",
                        "isValid": true
                    },
                    {
                        "id": 9738,
                        "content": "<p>AWS Snowcone</p>",
                        "isValid": false
                    },
                    {
                        "id": 9739,
                        "content": "<p>AWS Snowmobile</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2331,
            "attributes": {
                "createdAt": "2023-09-09T20:33:50.742Z",
                "updatedAt": "2023-09-09T20:33:50.742Z",
                "content": "<p>A niche social media application allows users to connect with sports athletes. As a solutions architect, you've designed the architecture of the application to be fully serverless using API Gateway &amp; AWS Lambda. The backend uses a DynamoDB table. Some of the star athletes using the application are highly popular, and therefore DynamoDB has increased the RCUs. Still, the application is experiencing a hot partition problem.</p>\n\n<p>What can you do to improve the performance of DynamoDB and eliminate the hot partition problem without a lot of application refactoring?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use DynamoDB DAX</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.</p>\n\n<p>DAX will be transparent and won't require an application refactoring, and will cache the \"hotkeys\". Therefore, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use DynamoDB Global Tables</strong> - DynamoDB Global Tables builds upon DynamoDB’s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions. But Global Tables cannot address the hotkey issue.</p>\n\n<p><strong>Use DynamoDB Streams</strong> - A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. DynamoDB Streams cannot address the hotkey issue.</p>\n\n<p><strong>Use Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. ElastiCache could also be a solution, but it will require a lot of refactoring work on the AWS Lambda side.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n",
                "options": [
                    {
                        "id": 9740,
                        "content": "<p>Use Amazon ElastiCache</p>",
                        "isValid": false
                    },
                    {
                        "id": 9741,
                        "content": "<p>Use DynamoDB Global Tables</p>",
                        "isValid": false
                    },
                    {
                        "id": 9742,
                        "content": "<p>Use DynamoDB Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 9743,
                        "content": "<p>Use DynamoDB DAX</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2332,
            "attributes": {
                "createdAt": "2023-09-09T20:33:50.829Z",
                "updatedAt": "2023-09-09T20:33:50.829Z",
                "content": "<p>A company uses Application Load Balancers (ALBs) in multiple AWS Regions. The ALBs receive inconsistent traffic that varies throughout the year. The engineering team at the company needs to allow the IP addresses of the ALBs in the on-premises firewall to enable connectivity.</p>\n\n<p>Which of the following represents the MOST scalable solution with minimal configuration changes?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS Global Accelerator. Register the ALBs in different Regions to the Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the Global Accelerator</strong> - AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. AWS Global Accelerator is easy to set up, configure, and manage. It provides static IP addresses that provide a fixed entry point to your applications and eliminate the complexity of managing specific IP addresses for different AWS Regions and Availability Zones.</p>\n\n<p>Associate the static IP addresses provided by AWS Global Accelerator to regional AWS resources or endpoints, such as Network Load Balancers, Application Load Balancers, EC2 Instances, and Elastic IP addresses. The IP addresses are anycast from AWS edge locations so they provide onboarding to the AWS global network close to your users.</p>\n\n<p>Simplified and resilient traffic routing for multi-Region applications using Global Accelerator:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q58-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate all ALBs in different Regions to the Network Load Balancer (NLBs). Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the NLBs</strong> - Although you could potentially migrate the ALBs to NLBs, this option requires changes to the on-premises firewall's configuration rules, hence this is not the right fit for the given use-case. It is more optimal to manage the two static IPs provided by the Global Accelerator for configuring the firewall.</p>\n\n<p><strong>Set up a Network Load Balancer (NLB) in one Region. Register the private IP addresses of the ALBs in different Regions with the NLB. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the NLB</strong> - Using a single NLB is not possible across AWS regions since an NLB is Region bound. Multiple NLBs have to be registered for the on-premises firewall.</p>\n\n<p><strong>Develop an AWS Lambda script to get the IP addresses of the ALBs in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the ALBs</strong> - This option requires on-going changes to the on-premises firewall's configuration rules because the IP addresses of the ALBs would keep changing. Hence this is not the right fit for the given use-case. It is more optimal to configure the firewall with a one-time change for the two static IPs provided by the Global Accelerator.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/global-accelerator/faqs/\">https://aws.amazon.com/global-accelerator/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9744,
                        "content": "<p>Migrate all ALBs in different Regions to the Network Load Balancer (NLBs). Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the NLBs</p>",
                        "isValid": false
                    },
                    {
                        "id": 9745,
                        "content": "<p>Set up AWS Global Accelerator. Register the ALBs in different Regions to the Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the Global Accelerator</p>",
                        "isValid": true
                    },
                    {
                        "id": 9746,
                        "content": "<p>Develop an AWS Lambda script to get the IP addresses of the ALBs in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the ALBs</p>",
                        "isValid": false
                    },
                    {
                        "id": 9747,
                        "content": "<p>Set up a Network Load Balancer (NLB) in one Region. Register the private IP addresses of the ALBs in different Regions with the NLB. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the NLB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2333,
            "attributes": {
                "createdAt": "2023-09-09T20:33:50.914Z",
                "updatedAt": "2023-09-09T20:33:50.914Z",
                "content": "<p>You have an S3 bucket that contains files in two different folders - <code>s3://my-bucket/images</code> and <code>s3://my-bucket/thumbnails</code>. When an image is first uploaded and new, it is viewed several times. But after 45 days, analytics prove that image files are on average rarely requested, but the thumbnails still are. After 180 days, you would like to archive the image files and the thumbnails. Overall you would like the solution to remain highly available to prevent disasters happening against a whole AZ.</p>\n\n<p>How can you implement an efficient cost strategy for your S3 bucket? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p>To manage your S3 objects, so they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:</p>\n\n<p>Transition actions — Define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them.</p>\n\n<p>Expiration actions — Define when objects expire. Amazon S3 deletes expired objects on your behalf.</p>\n\n<p><strong>Create a Lifecycle Policy to transition objects to S3 Standard IA using a prefix after 45 days</strong></p>\n\n<p>S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.</p>\n\n<p>As the use-case mentions that after 45 days, image files are rarely requested, but the thumbnails still are. So you need to use a prefix while configuring the Lifecycle Policy so that only objects in the <code>s3://my-bucket/images</code> are transitioned to Standard IA and not all the objects in the bucket.</p>\n\n<p><strong>Create a Lifecycle Policy to transition all objects to Glacier after 180 days</strong></p>\n\n<p>Amazon S3 Glacier and S3 Glacier Deep Archive are secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lifecycle Policy to transition all objects to S3 Standard IA after 45 days</strong> - As discussed above, you need to use a prefix while configuring the Lifecycle Policy so that only objects in the <code>s3://my-bucket/images</code> are transitioned to Standard IA and not all the objects in the bucket.</p>\n\n<p><strong>Create a Lifecycle Policy to transition objects to Glacier using a prefix after 180 days</strong> - After 180 days, you can move all the objects to Glacier storage as per the use case. Glacier doesn't need prefixes for the given use-case.</p>\n\n<p><strong>Create a Lifecycle Policy to transition objects to S3 One Zone IA using a prefix after 45 days</strong> - S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.</p>\n\n<p>S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. The minimum storage duration charge is 30 days.</p>\n\n<p>Finally, S3 One Zone IA will not achieve the necessary availability in case an AZ goes down.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p>\n",
                "options": [
                    {
                        "id": 9748,
                        "content": "<p>Create a Lifecycle Policy to transition all objects to Glacier after 180 days</p>",
                        "isValid": true
                    },
                    {
                        "id": 9749,
                        "content": "<p>Create a Lifecycle Policy to transition objects to Glacier using a prefix after 180 days</p>",
                        "isValid": false
                    },
                    {
                        "id": 9750,
                        "content": "<p>Create a Lifecycle Policy to transition all objects to S3 Standard IA after 45 days</p>",
                        "isValid": false
                    },
                    {
                        "id": 9751,
                        "content": "<p>Create a Lifecycle Policy to transition objects to S3 Standard IA using a prefix after 45 days</p>",
                        "isValid": true
                    },
                    {
                        "id": 9752,
                        "content": "<p>Create a Lifecycle Policy to transition objects to S3 One Zone IA using a prefix after 45 days</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2334,
            "attributes": {
                "createdAt": "2023-09-09T20:33:51.014Z",
                "updatedAt": "2023-09-09T20:33:51.014Z",
                "content": "<p>A company runs a popular dating website on the AWS Cloud. As a Solutions Architect, you've designed the architecture of the website to follow a serverless pattern on the AWS Cloud using API Gateway and AWS Lambda. The backend uses an RDS PostgreSQL database. Currently, the application uses a username and password combination to connect the Lambda function to the RDS database.</p>\n\n<p>You would like to improve the security at the authentication level by leveraging short-lived credentials. What will you choose? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use IAM authentication from Lambda to RDS PostgreSQL</strong></p>\n\n<p><strong>Attach an AWS Identity and Access Management (IAM) role to AWS Lambda</strong></p>\n\n<p>You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token.</p>\n\n<p>An authentication token is a unique string of characters that Amazon RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication.</p>\n\n<p>IAM database authentication provides the following benefits:\n    1. Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL).\n    2. You can use IAM to centrally manage access to your database resources, instead of managing access individually on each DB instance.\n    3. For applications running on Amazon EC2, you can use profile credentials specific to your EC2 instance to access your database instead of a password, for greater security.</p>\n\n<p>Incorrect options:</p>\n\n<p>AWS Systems Manager Parameter Store (aka SSM Parameter Store) provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, EC2 instance IDs, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data.</p>\n\n<p><strong>Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM</strong> - Retrieving credentials from SSM is overkill for the expected solution and hence this is not a correct option.</p>\n\n<p><strong>Restrict the RDS database security group to the Lambda's security group</strong></p>\n\n<p><strong>Deploy AWS Lambda in a VPC</strong></p>\n\n<p>This question is very tricky because all answers do indeed increase security. But the question is related to authentication mechanisms, and as such, deploying a Lambda in a VPC or tightening security groups does not change the authentication layer. IAM authentication to RDS is supported, which must be achieved by attaching an IAM role the AWS Lambda function</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p>\n",
                "options": [
                    {
                        "id": 9753,
                        "content": "<p>Restrict the RDS database security group to the Lambda's security group</p>",
                        "isValid": false
                    },
                    {
                        "id": 9754,
                        "content": "<p>Deploy AWS Lambda in a VPC</p>",
                        "isValid": false
                    },
                    {
                        "id": 9755,
                        "content": "<p>Attach an AWS Identity and Access Management (IAM) role to AWS Lambda</p>",
                        "isValid": true
                    },
                    {
                        "id": 9756,
                        "content": "<p>Use IAM authentication from Lambda to RDS PostgreSQL</p>",
                        "isValid": true
                    },
                    {
                        "id": 9757,
                        "content": "<p>Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2335,
            "attributes": {
                "createdAt": "2023-09-09T20:33:51.104Z",
                "updatedAt": "2023-09-09T20:33:51.104Z",
                "content": "<p>An Internet-of-Things (IoT) company would like to have a streaming system that performs real-time analytics on the ingested IoT data. Once the analytics is done, the company would like to send notifications back to the mobile applications of the IoT device owners.</p>\n\n<p>As a solutions architect, which of the following AWS technologies would you recommend to send these notifications to the mobile applications?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Kinesis with Amazon Simple Notification Service (SNS)</strong> - Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application.</p>\n\n<p>With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin.</p>\n\n<p>Kinesis will be great for event streaming from the IoT devices, but not for sending notifications as it doesn't have such a feature.</p>\n\n<p>Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS is a notification service and will be perfect for this use case.</p>\n\n<p>Streaming data with Kinesis and using SNS to send the response notifications is the optimal solution for the current scenario.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Kinesis is better for streaming data since queues aren't meant for real-time streaming of data.</p>\n\n<p><strong>Amazon Kinesis with Simple Email Service (Amazon SES)</strong> - Amazon Simple Email Service (Amazon SES) is a cloud-based email sending service designed to help digital marketers and application developers send marketing, notification, and transactional emails. It is a reliable, cost-effective service for businesses of all sizes that use email to keep in contact with their customers. It is an email service and not a notification service as is the requirement in the current use case.</p>\n\n<p><strong>Amazon Kinesis with Simple Queue Service (SQS)</strong> - As explained above, Kinesis works well for streaming real-time data. SQS is a queuing service that helps decouple system architecture by offering flexibility and ease of maintenance. It cannot send notifications. SQS is paired with SNS to provide this functionality.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/\">https://aws.amazon.com/kinesis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/ses/\">https://aws.amazon.com/ses/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n",
                "options": [
                    {
                        "id": 9758,
                        "content": "<p>Amazon Kinesis with Amazon Simple Notification Service (SNS)</p>",
                        "isValid": true
                    },
                    {
                        "id": 9759,
                        "content": "<p>Amazon Kinesis with Simple Email Service (Amazon SES)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9760,
                        "content": "<p>Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9761,
                        "content": "<p>Amazon Kinesis with Simple Queue Service (SQS)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2336,
            "attributes": {
                "createdAt": "2023-09-09T20:33:51.188Z",
                "updatedAt": "2023-09-09T20:33:51.188Z",
                "content": "<p>A company has migrated its application from a monolith architecture to a microservices based architecture. The development team has updated the Route 53 simple record to point \"myapp.mydomain.com\" from the old Load Balancer to the new one.</p>\n\n<p>The users are still not redirected to the new Load Balancer. What has gone wrong in the configuration?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Amazon Route 53 effectively connects user requests to infrastructure running in AWS – such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets – and can also be used to route users to infrastructure outside of AWS.</p>\n\n<p>You can use Amazon Route 53 to configure DNS health checks to route traffic to healthy endpoints or to independently monitor the health of your application and its endpoints. Amazon Route 53 Traffic Flow makes it easy for you to manage traffic globally through a variety of routing types, including Latency Based Routing, Geo DNS, Geoproximity, and Weighted Round Robin—all of which can be combined with DNS Failover to enable a variety of low-latency, fault-tolerant architectures.</p>\n\n<p><strong>The TTL is still in effect</strong> - TTL (time to live), is the amount of time, in seconds, that you want DNS recursive resolvers to cache information about a record. If you specify a longer value (for example, 172800 seconds, or two days), you reduce the number of calls that DNS recursive resolvers must make to Route 53 to get the latest information for the record. This has the effect of reducing latency and reducing your bill for Route 53 service.</p>\n\n<p>However, if you specify a longer value for TTL, it takes longer for changes to the record (for example, a new IP address) to take effect because recursive resolvers use the values in their cache for longer periods before they ask Route 53 for the latest information. If you're changing settings for a domain or subdomain that's already in use, AWS recommends that you initially specify a shorter value, such as 300 seconds, and increase the value after you confirm that the new settings are correct.</p>\n\n<p>For this use-case, the most likely issue is that the TTL is still in effect so you have to wait until it expires for the new request to perform another DNS query and get the value for the new Load Balancer.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The CNAME Record is misconfigured</strong> - A CNAME record can redirect DNS queries to any DNS record. For example, you can create a CNAME record that redirects queries from acme.example.com to zenith.example.com or to acme.example.org. You don't need to use Route 53 as the DNS service for the domain that you're redirecting queries to.</p>\n\n<p><strong>The Alias Record is misconfigured</strong> - Amazon Route 53 also offers alias records, which are an Amazon Route 53-specific extension to DNS. Alias records let you route traffic to selected AWS resources, such as CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record.\nUnlike a CNAME record, you can create an alias record at the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You can't create a CNAME record for example.com, but you can create an alias record for example.com that routes traffic to www.example.com.</p>\n\n<p><strong>The health checks are failing</strong> - Simple Records do not have health checks, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/route53/\">https://aws.amazon.com/route53/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-basic.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-basic.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p>\n",
                "options": [
                    {
                        "id": 9762,
                        "content": "<p>The TTL is still in effect</p>",
                        "isValid": true
                    },
                    {
                        "id": 9763,
                        "content": "<p>The health checks are failing</p>",
                        "isValid": false
                    },
                    {
                        "id": 9764,
                        "content": "<p>The Alias Record is misconfigured</p>",
                        "isValid": false
                    },
                    {
                        "id": 9765,
                        "content": "<p>The CNAME Record is misconfigured</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2337,
            "attributes": {
                "createdAt": "2023-09-09T20:33:51.266Z",
                "updatedAt": "2023-09-09T20:33:51.266Z",
                "content": "<p>A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure.</p>\n\n<p>As a Solutions Architect, which deployment do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies:</p>\n\n<p>Cluster – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.</p>\n\n<p>Partition – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.</p>\n\n<p>Spread – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.</p>\n\n<p>There is no charge for creating a placement group.</p>\n\n<p><strong>Use a Cluster placement group</strong> - A cluster placement group is a logical grouping of instances within a single Availability Zone. A cluster placement group can span peered VPCs in the same Region. Instances in the same cluster placement group enjoy a higher per-flow throughput limit of up to 10 Gbps for TCP/IP traffic and are placed in the same high-bisection bandwidth segment of the network.</p>\n\n<p>Cluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. They are also recommended when the majority of the network traffic is between the instances in the group. To provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking.</p>\n\n<p>Image of Cluster placement group:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q20-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n\n<p>Image of Partition placement group:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q20-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n\n<p>Image of Spread placement group:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q20-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Spot Instances</strong> - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted. Since performance is the key criteria, this is not the right choice.</p>\n\n<p><strong>Optimize the EC2 kernel using EC2 User Data</strong> -  Optimizing the EC2 kernel won't help with network performance as it's bounded by the EC2 instance type mainly. Therefore, this option is incorrect.</p>\n\n<p><strong>Use a Spread placement group</strong> - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The instances are placed across distinct underlying hardware to reduce correlated failures. A spread placement group can span multiple Availability Zones in the same Region. You can have a maximum of seven running instances per Availability Zone per group.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 9766,
                        "content": "<p>Use Spot Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9767,
                        "content": "<p>Optimize the EC2 kernel using EC2 User Data</p>",
                        "isValid": false
                    },
                    {
                        "id": 9768,
                        "content": "<p>Use a Cluster placement group</p>",
                        "isValid": true
                    },
                    {
                        "id": 9769,
                        "content": "<p>Use a Spread placement group</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2338,
            "attributes": {
                "createdAt": "2023-09-09T20:33:51.356Z",
                "updatedAt": "2023-09-09T20:33:51.356Z",
                "content": "<p>A company has recently created a new department to handle their services workload. An IT team has been asked to create a custom VPC to isolate the resources created in this new department. They have set up the public subnet and internet gateway (IGW). However, they are not able to ping the Amazon EC2 instances with Elastic IP launched in the newly created VPC.</p>\n\n<p>As a Solutions Architect, the team has requested your help. How will you troubleshoot this scenario? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Check if the route table is configured with IGW</strong> - An internet gateway (IGW) is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. An internet gateway supports IPv4 and IPv6 traffic.</p>\n\n<p>To enable access to or from the internet for instances in a subnet in a VPC, you must do the following:\n    1. Attach an internet gateway to your VPC.\n    2. Add a route to your subnet's route table that directs internet-bound traffic to the internet gateway.\n    3. Ensure that instances in your subnet have a globally unique IP address\n    4. Ensure that your network access control lists and security group rules allow the relevant traffic to flow to and from your instance.</p>\n\n<p>A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. After creating an IGW, make sure the route tables are updated. Additionally, ensure the security group allows the ICMP protocol for ping requests.</p>\n\n<p><strong>Check if the security groups allows ping from the source</strong> - A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, AWS uses the default security group. You can add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. To decide whether to allow traffic to reach an instance, all the rules from all the security groups that are associated with the instance are evaluated.</p>\n\n<p>The following are the characteristics of security group rules:\n    1. By default, security groups allow all outbound traffic.\n    2. Security group rules are always permissive; you can't create rules that deny access.\n    3. Security groups are stateful</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Disable Source / Destination check on the EC2 instance</strong> - The Source/Destination Check attribute controls whether source/destination checking is enabled on the instance. Disabling this attribute enables an instance to handle network traffic that isn't specifically destined for the instance. For example, instances running services such as network address translation, routing, or a firewall should set this value to disabled. The default value is enabled. Source/Destination Check is not relevant to the question and it has been added as a distractor.</p>\n\n<p><strong>Create a secondary IGW to attach with public subnet and move the current IGW to private and write route tables</strong> - There is no such thing as a secondary IGW. This option is added as a distractor.</p>\n\n<p><strong>Contact AWS support to map your VPC with subnet</strong> - You cannot contact AWS support to map your VPC with the subnet.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#change_source_dest_check\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#change_source_dest_check</a></p>\n",
                "options": [
                    {
                        "id": 9770,
                        "content": "<p>Check if the route table is configured with IGW</p>",
                        "isValid": true
                    },
                    {
                        "id": 9771,
                        "content": "<p>Contact AWS support to map your VPC with subnet</p>",
                        "isValid": false
                    },
                    {
                        "id": 9772,
                        "content": "<p>Create a secondary IGW to attach with public subnet and move the current IGW to private and write route tables</p>",
                        "isValid": false
                    },
                    {
                        "id": 9773,
                        "content": "<p>Check if the security groups allow ping from the source</p>",
                        "isValid": true
                    },
                    {
                        "id": 9774,
                        "content": "<p>Disable Source / Destination check on the EC2 instance</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2339,
            "attributes": {
                "createdAt": "2023-09-09T20:33:51.455Z",
                "updatedAt": "2023-09-09T20:33:51.455Z",
                "content": "<p>A mobile gaming company is experiencing heavy read traffic to its Amazon Relational Database Service (RDS) database that retrieves player’s scores and stats. The company is using an RDS database instance type that is not cost-effective for their budget. The company would like to implement a strategy to deal with the high volume of read traffic, reduce latency, and also downsize the instance size to cut costs.</p>\n\n<p>Which of the following solutions do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Setup ElastiCache in front of RDS</strong></p>\n\n<p>Amazon ElastiCache is an ideal front-end for data stores such as Amazon RDS, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements. The best part of caching is that it’s minimally invasive to implement and by doing so, your application performance regarding both scale and speed is dramatically improved.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Setup RDS Read Replicas</strong> - Adding read replicas would further add to the database costs and will not help in reducing latency when compared to a caching solution. So this option is ruled out.</p>\n\n<p><strong>Move to Amazon Redshift</strong> - Redshift is optimized for datasets ranging from a few hundred gigabytes to a petabyte or more. If the company is looking at cost-cutting, moving to Redshift from RDS is not an option.</p>\n\n<p><strong>Switch application code to AWS Lambda for better performance</strong> - AWS Lambda can help in running data processing workflows. But, data still needs to be read from RDS and hence we need a solution to speed up the data reads and not before/after processing.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/caching/database-caching/\">https://aws.amazon.com/caching/database-caching/</a></p>\n",
                "options": [
                    {
                        "id": 9775,
                        "content": "<p>Switch application code to AWS Lambda for better performance</p>",
                        "isValid": false
                    },
                    {
                        "id": 9776,
                        "content": "<p>Setup RDS Read Replicas</p>",
                        "isValid": false
                    },
                    {
                        "id": 9777,
                        "content": "<p>Setup ElastiCache in front of RDS</p>",
                        "isValid": true
                    },
                    {
                        "id": 9778,
                        "content": "<p>Move to Amazon Redshift</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2340,
            "attributes": {
                "createdAt": "2023-09-09T20:33:51.537Z",
                "updatedAt": "2023-09-09T20:33:51.537Z",
                "content": "<p>A systems administrator is creating IAM policies and attaching them to IAM identities. After creating the necessary identity-based policies, the administrator is now creating resource-based policies.</p>\n\n<p>Which is the only resource-based policy that the IAM service supports?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions.\nResource-based policies are JSON policy documents that you attach to a resource such as an Amazon S3 bucket. These policies grant the specified principal permission to perform specific actions on that resource and define under what conditions this applies.</p>\n\n<p><strong>Trust policy</strong> - Trust policies define which principal entities (accounts, users, roles, and federated users) can assume the role. An IAM role is both an identity and a resource that supports resource-based policies. For this reason, you must attach both a trust policy and an identity-based policy to an IAM role. The IAM service supports only one type of resource-based policy called a role trust policy, which is attached to an IAM role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Organizations Service Control Policies (SCP)</strong> - If you enable all features of AWS organization, then you can apply service control policies (SCPs) to any or all of your accounts. SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). The SCP limits permissions for entities in member accounts, including each AWS account root user. An explicit deny in any of these policies overrides the allow.</p>\n\n<p><strong>Access control list (ACL)</strong> - Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account. Amazon S3, AWS WAF, and Amazon VPC are examples of services that support ACLs.</p>\n\n<p><strong>Permissions boundary</strong> - AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resource-based\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resource-based</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>\n",
                "options": [
                    {
                        "id": 9779,
                        "content": "<p>Access control list (ACL)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9780,
                        "content": "<p>AWS Organizations Service Control Policies (SCP)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9781,
                        "content": "<p>Trust policy</p>",
                        "isValid": true
                    },
                    {
                        "id": 9782,
                        "content": "<p>Permissions boundary</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2341,
            "attributes": {
                "createdAt": "2023-09-09T20:33:51.620Z",
                "updatedAt": "2023-09-09T20:33:51.620Z",
                "content": "<p>A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements.</p>\n\n<p>Which of the following options represents a valid cost-optimization solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Cost Explorer Resource Optimization to get a report of EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations</strong> - AWS Cost Explorer helps you identify under-utilized EC2 instances that may be downsized on an instance by instance basis within the same instance family, and also understand the potential impact on your AWS bill by taking into account your Reserved Instances and Savings Plans.</p>\n\n<p>AWS Compute Optimizer recommends optimal AWS Compute resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics. Compute Optimizer helps you choose the optimal Amazon EC2 instance types, including those that are part of an Amazon EC2 Auto Scaling group, based on your utilization data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies</strong> -</p>\n\n<p>By using Amazon S3 Analytics Storage Class analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. Storage class analysis does not give recommendations for transitions to the ONEZONE_IA or S3 Glacier storage classes.</p>\n\n<p><strong>Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew Reserved Instances. Trusted advisor also suggests Amazon RDS idle DB instances</strong> - AWS Trusted Advisor checks for Amazon EC2 Reserved Instances that are scheduled to expire within the next 30 days or have expired in the preceding 30 days. Reserved Instances do not renew automatically; you can continue using an EC2 instance covered by the reservation without interruption, but you will be charged On-Demand rates. Trusted advisor does not have a feature to auto-renew Reserved Instances.</p>\n\n<p><strong>Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs</strong> - AWS Compute Optimizer recommends optimal AWS Compute resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics. Over-provisioning compute can lead to unnecessary infrastructure cost and under-provisioning compute can lead to poor application performance. Compute Optimizer helps you choose the optimal Amazon EC2 instance types, including those that are part of an Amazon EC2 Auto Scaling group, based on your utilization data. It does not recommend instance purchase options.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/compute-optimizer/\">https://aws.amazon.com/compute-optimizer/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/analytics-storage-class.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/analytics-storage-class.html</a></p>\n",
                "options": [
                    {
                        "id": 9783,
                        "content": "<p>Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew Reserved Instances. Trusted advisor also suggests Amazon RDS idle DB instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9784,
                        "content": "<p>Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs</p>",
                        "isValid": false
                    },
                    {
                        "id": 9785,
                        "content": "<p>Use AWS Cost Explorer Resource Optimization to get a report of EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations</p>",
                        "isValid": true
                    },
                    {
                        "id": 9786,
                        "content": "<p>Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2342,
            "attributes": {
                "createdAt": "2023-09-09T20:33:51.699Z",
                "updatedAt": "2023-09-09T20:33:51.699Z",
                "content": "<p>An enterprise has decided to move its secondary workloads such as backups and archives to AWS cloud. The CTO wishes to move the data stored on physical tapes to Cloud, without changing their current tape backup workflows. The company holds petabytes of data on tapes and needs a cost-optimized solution to move this data to cloud.</p>\n\n<p>What is an optimal solution that meets these requirements while keeping the costs to a minimum?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years</strong> - Tape Gateway enables you to replace using physical tapes on-premises with virtual tapes in AWS without changing existing backup workflows. Tape Gateway supports all leading backup applications and caches virtual tapes on-premises for low-latency data access. Tape Gateway encrypts data between the gateway and AWS for secure data transfer and compresses data while transitioning virtual tapes between Amazon S3 and Amazon S3 Glacier, or Amazon S3 Glacier Deep Archive, to minimize storage costs.</p>\n\n<p>Tape Gateway compresses and stores archived virtual tapes in the lowest-cost Amazon S3 storage classes, Amazon S3 Glacier and Amazon S3 Glacier Deep Archive. This makes it feasible for you to retain long-term data in the AWS Cloud at a very low cost. With Tape Gateway, you only pay for what you consume, with no minimum commitments and no upfront fees.</p>\n\n<p>Tape Gateway stores your virtual tapes in S3 buckets managed by the AWS Storage Gateway service, so you don’t have to manage your own Amazon S3 storage. Tape Gateway integrates with all leading backup applications allowing you to start using cloud storage for on-premises backup and archive without any changes to your backup and archive workflows.</p>\n\n<p>Tape Gateway Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q36-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/storagegateway/vtl/\">https://aws.amazon.com/storagegateway/vtl/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes</strong> - AWS DataSync supports only NFS and SMB file types and hence is not the right choice for the given use case.</p>\n\n<p><strong>Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs</strong> - AWS Direct Connect is used when customers need to retain on-premises structure because of compliance reasons and have moved the rest of the architecture to AWS Cloud. These businesses generally have an on-going requirement for low latency access to AWS Cloud and hence are willing to spend on installing the physical lines needed for this connection. The given use-case needs a cost-optimized solution and they do not have an ongoing requirement for high availability bandwidth.</p>\n\n<p><strong>Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources</strong> - VPN connection is used when businesses have an on-going requirement for connectivity from the on-premises data center to AWS Cloud. Amazon EFS is a managed file system by AWS and cannot be used for archiving on-premises tape data onto AWS Cloud.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/vtl/\">https://aws.amazon.com/storagegateway/vtl/</a></p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/faqs/\">https://aws.amazon.com/storagegateway/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9787,
                        "content": "<p>Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes</p>",
                        "isValid": false
                    },
                    {
                        "id": 9788,
                        "content": "<p>Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources</p>",
                        "isValid": false
                    },
                    {
                        "id": 9789,
                        "content": "<p>Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs</p>",
                        "isValid": false
                    },
                    {
                        "id": 9790,
                        "content": "<p>Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2343,
            "attributes": {
                "createdAt": "2023-09-09T20:33:51.790Z",
                "updatedAt": "2023-09-09T20:33:51.790Z",
                "content": "<p>You have developed a new REST API leveraging the API Gateway, AWS Lambda and Aurora database services. Most of the workload on the website is read-heavy. The data rarely changes and it is acceptable to serve users outdated data for about 24 hours. Recently, the website has been experiencing high load and the costs incurred on the Aurora database have been very high.</p>\n\n<p>How can you easily reduce the costs while improving performance, with minimal changes?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable API Gateway Caching</strong> - Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.</p>\n\n<p>You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.\nWhen you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of requesting your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled. Using API Gateway Caching feature is the answer for the use case, as we can accept stale data for about 24 hours.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add Aurora Read Replicas</strong> - Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs).</p>\n\n<p>Aurora Read Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region. The DB cluster volume is made up of multiple copies of the data for the DB cluster. However, the data in the cluster volume is represented as a single, logical volume to the primary instance and to Aurora Replicas in the DB cluster. Adding Aurora Read Replicas would greatly increase the cost, therefore this option is ruled out.</p>\n\n<p><strong>Switch to using an Application Load Balancer</strong> - An Application Load Balancer functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI) model. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply, and then selects a target from the target group for the rule action. You can configure listener rules to route requests to different target groups based on the content of the application traffic. Switching to a Load Balancer would not improve the current status as we need a caching mechanism.</p>\n\n<p><strong>Enable AWS Lambda In Memory Caching</strong> - AWS Lambda has no native in-memory caching capability. Lambda is a serverless compute capacity. This option is incorrect and has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p>\n",
                "options": [
                    {
                        "id": 9791,
                        "content": "<p>Enable API Gateway Caching</p>",
                        "isValid": true
                    },
                    {
                        "id": 9792,
                        "content": "<p>Switch to using an Application Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 9793,
                        "content": "<p>Add Aurora Read Replicas</p>",
                        "isValid": false
                    },
                    {
                        "id": 9794,
                        "content": "<p>Enable AWS Lambda In Memory Caching</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2344,
            "attributes": {
                "createdAt": "2023-09-09T20:33:51.867Z",
                "updatedAt": "2023-09-09T20:33:51.867Z",
                "content": "<p>A company wants to grant access to an S3 bucket to users in its own AWS account as well as to users in another AWS account. Which of the following options can be used to meet this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use a bucket policy to grant permission to users in its account as well as to users in another account</strong></p>\n\n<p>A bucket policy is a type of resource-based policy that can be used to grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. For cross-account permissions to other AWS accounts or users in another account, you must use a bucket policy.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q65-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account</strong></p>\n\n<p><strong>Use a user policy to grant permission to users in its account as well as to users in another account</strong></p>\n\n<p>If an AWS account that owns a bucket wants to grant permission to users in its own AWS account, it can use either a bucket policy or a user policy. The user policies are for managing permissions for users in their own AWS account and NOT for users in other AWS accounts. Therefore both these options are incorrect.</p>\n\n<p><strong>Use permissions boundary to grant permission to users in its account as well as to users in another account</strong> - Use a managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions, so this option is not correct for the given use case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q65-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a></p>\n",
                "options": [
                    {
                        "id": 9795,
                        "content": "<p>Use permissions boundary to grant permission to users in its account as well as to users in another account</p>",
                        "isValid": false
                    },
                    {
                        "id": 9796,
                        "content": "<p>Use a user policy to grant permission to users in its account as well as to users in another account</p>",
                        "isValid": false
                    },
                    {
                        "id": 9797,
                        "content": "<p>Use a bucket policy to grant permission to users in its account as well as to users in another account</p>",
                        "isValid": true
                    },
                    {
                        "id": 9798,
                        "content": "<p>Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2345,
            "attributes": {
                "createdAt": "2023-09-09T20:33:51.955Z",
                "updatedAt": "2023-09-09T20:33:51.955Z",
                "content": "<p>A financial services firm has traditionally operated with an on-premise data center and would like to create a disaster recovery strategy leveraging the AWS Cloud.</p>\n\n<p>As a Solutions Architect, you would like to ensure that a scaled-down version of a fully functional environment is always running in the AWS cloud, and in case of a disaster, the recovery time is kept to a minimum. Which disaster recovery strategy is that?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Warm Standby</strong> - The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud. A warm standby solution extends the pilot light elements and preparation. It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems on AWS and have them always on.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Backup and Restore</strong> - In most traditional environments, data is backed up to tape and sent off-site regularly. If you use this method, it can take a long time to restore your system in the event of a disruption or disaster. Amazon S3 is an ideal destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network, and is therefore accessible from any location. Many commercial and open-source backup solutions integrate with Amazon S3.</p>\n\n<p><strong>Pilot Light</strong> - The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in the cloud. The idea of the pilot light is an analogy that comes from the gas heater. In a gas heater, a small flame that’s always on can quickly ignite the entire furnace to heat up a house. This scenario is similar to a backup-and-restore scenario. For example, with AWS you can maintain a pilot light by configuring and running the most critical core elements of your system in AWS. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core.</p>\n\n<p><strong>Multi Site</strong> - A multi-site solution runs in AWS as well as on your existing on-site infrastructure, in an active-active configuration. The data replication method that you employ will be determined by the recovery point that you choose.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/aws-disaster-recovery.pdf\">https://d1.awsstatic.com/whitepapers/aws-disaster-recovery.pdf</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/asset-repository/products/CloudEndure/CloudEndure_Affordable_Enterprise-Grade_Disaster_Recovery_Using_AWS.pdf\">https://d1.awsstatic.com/asset-repository/products/CloudEndure/CloudEndure_Affordable_Enterprise-Grade_Disaster_Recovery_Using_AWS.pdf</a></p>\n",
                "options": [
                    {
                        "id": 9799,
                        "content": "<p>Warm Standby</p>",
                        "isValid": true
                    },
                    {
                        "id": 9800,
                        "content": "<p>Backup and Restore</p>",
                        "isValid": false
                    },
                    {
                        "id": 9801,
                        "content": "<p>Pilot Light</p>",
                        "isValid": false
                    },
                    {
                        "id": 9802,
                        "content": "<p>Multi Site</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2346,
            "attributes": {
                "createdAt": "2023-09-09T20:33:52.054Z",
                "updatedAt": "2023-09-09T20:33:52.054Z",
                "content": "<p>As a solutions architect, you have created a solution that utilizes an Application Load Balancer with stickiness and an Auto Scaling Group (ASG). The ASG spawns across 2 Availability Zones (AZ). AZ-A has 3 EC2 instances and AZ-B has 4 EC2 instances. The ASG is about to go into a scale-in event due to the triggering of a CloudWatch alarm.</p>\n\n<p>What will happen under the default ASG configuration?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The instance with the oldest launch configuration will be terminated in AZ-B</strong> - Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size.</p>\n\n<p>With each Auto Scaling group, you can control when it adds instances (referred to as scaling out) or removes instances (referred to as scaling in) from your network architecture.</p>\n\n<p>The default termination policy is designed to help ensure that your instances span Availability Zones evenly for high availability. The default policy is kept generic and flexible to cover a range of scenarios.</p>\n\n<p>The default termination policy behavior is as follows:\n1. Determine which Availability Zones have the most instances and at least one instance that is not protected from scale-in.\n2. Determine which instances to terminate to align the remaining instances to the allocation strategy for the On-Demand or Spot Instance that is terminating.\n3. Determine whether any of the instances use the oldest launch template or configuration:\n    3.a. Determine whether any of the instances use the oldest launch template unless there are instances that use a launch configuration.\n    3.b. Determine whether any of the instances use the oldest launch configuration.\n4. After applying all of the above criteria, if there are multiple unprotected instances to terminate, determine which instances are closest to the next billing hour.</p>\n\n<p>Per the given use-case, AZs will be balanced first, then the instance with the oldest launch configuration within that AZ will be terminated.</p>\n\n<p>Default Termination policy:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q16-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A random instance in the AZ-A will be terminated</strong></p>\n\n<p><strong>An instance in the AZ-A will be created</strong></p>\n\n<p><strong>A random instance will be terminated in AZ-B</strong></p>\n\n<p>These three options contradict the details provided in the explanation above. Hence these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>\n",
                "options": [
                    {
                        "id": 9803,
                        "content": "<p>A random instance will be terminated in AZ-B</p>",
                        "isValid": false
                    },
                    {
                        "id": 9804,
                        "content": "<p>A random instance in the AZ-A will be terminated</p>",
                        "isValid": false
                    },
                    {
                        "id": 9805,
                        "content": "<p>The instance with the oldest launch configuration will be terminated in AZ-B</p>",
                        "isValid": true
                    },
                    {
                        "id": 9806,
                        "content": "<p>An instance in the AZ-A will be created</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2347,
            "attributes": {
                "createdAt": "2023-09-09T20:33:52.166Z",
                "updatedAt": "2023-09-09T20:33:52.166Z",
                "content": "<p>Amazon Route 53 is configured to route traffic to two Network Load Balancer (NLB) nodes belonging to two Availability Zones (AZs): AZ-A and AZ-B. Cross-zone load balancing is disabled. AZ-A has four targets and AZ-B has six targets.</p>\n\n<p>Which of the below statements is true about traffic distribution to the target instances from Route 53?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Each of the four targets in AZ-A receives 12.5% of the traffic</strong> - The nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone.</p>\n\n<p>Route 53 will distribute traffic such that each load balancer node receives 50% of the traffic from the clients.</p>\n\n<p>If cross-zone load balancing is disabled:\n1. Each of the four targets in AZ-A receives 12.5% of the traffic.\n2. Each of the six targets in AZ-B receives 8.3% of the traffic.</p>\n\n<p>This is because each load balancer node can route its 50% of the client traffic only to targets in its Availability Zone.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Each of the six targets in AZ-B receives 12.5% of the traffic</strong> - As mentioned above in the correct explanation, each of the six targets in AZ-B receives 8.3% of the traffic.</p>\n\n<p><strong>Each of the four targets in AZ-A receives 8% of the traffic</strong> - As mentioned above in the correct explanation, each of the four targets in AZ-A receives 12.5% of the traffic.</p>\n\n<p><strong>Each of the four targets in AZ-A receives 10% of the traffic</strong> - As mentioned above in the correct explanation, each of the four targets in AZ-A receives 12.5% of the traffic.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n",
                "options": [
                    {
                        "id": 9807,
                        "content": "<p>Each of the four targets in AZ-A receives 8% of the traffic</p>",
                        "isValid": false
                    },
                    {
                        "id": 9808,
                        "content": "<p>Each of the six targets in AZ-B receives 10% of the traffic</p>",
                        "isValid": false
                    },
                    {
                        "id": 9809,
                        "content": "<p>Each of the four targets in AZ-A receives 10% of the traffic</p>",
                        "isValid": false
                    },
                    {
                        "id": 9810,
                        "content": "<p>Each of the four targets in AZ-A receives 12.5% of the traffic</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2348,
            "attributes": {
                "createdAt": "2023-09-09T20:33:52.314Z",
                "updatedAt": "2023-09-09T20:33:52.314Z",
                "content": "<p>A small rental company had 5 employees, all working under the same AWS cloud account. These employees deployed their applications built for various functions- including billing, operations, finance, etc. Each of these employees has been operating in their own VPC. Now, there is a need to connect these VPCs so that the applications can communicate with each other.</p>\n\n<p>Which of the following is the MOST cost-effective solution for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection). VPC Peering helps connect two VPCs and is not transitive. To connect VPCs together, the best available option is to use VPC peering.</p>\n\n<p>More on VPC Peering:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q45-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\">https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</a></p>\n\n<p>Incorrect option:</p>\n\n<p><strong>Use an Internet Gateway</strong> - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. Internet Gateway is not meant for connecting between VPCs.</p>\n\n<p><strong>Use a Direct Connect</strong> - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. For the given use-case, direct connect gateway is overkill and is not as cost-optimal as using VPC peering.</p>\n\n<p><strong>Use a NAT Gateway</strong> - You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. You are charged for creating and using a NAT gateway in your account. NAT gateway hourly usage and data processing rates apply. NAT Gateway is not used for connection between VPCs.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\">https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</a></p>\n",
                "options": [
                    {
                        "id": 9811,
                        "content": "<p>Use VPC Peering</p>",
                        "isValid": true
                    },
                    {
                        "id": 9812,
                        "content": "<p>Use an Internet Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 9813,
                        "content": "<p>Use a NAT Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 9814,
                        "content": "<p>Use a Direct Connect</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2349,
            "attributes": {
                "createdAt": "2023-09-09T20:33:52.405Z",
                "updatedAt": "2023-09-09T20:33:52.405Z",
                "content": "<p>A retail company is using AWS Site-to-Site VPN connections for secure connectivity to its AWS cloud resources from its on-premises data center. Due to a surge in traffic across the VPN connections to the AWS cloud, users are experiencing slower VPN connectivity.</p>\n\n<p>Which of the following options will maximize the VPN throughput?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a transit gateway with equal cost multipath routing and add additional VPN tunnels</strong></p>\n\n<p>VPN connection is a secure connection between your on-premises equipment and your VPCs. Each VPN connection has two VPN tunnels which you can use for high availability. A VPN tunnel is an encrypted link where data can pass from the customer network to or from AWS. The following diagram shows the high-level connectivity with virtual private gateways.</p>\n\n<p>With AWS Transit Gateway, you can simplify the connectivity between multiple VPCs and also connect to any VPC attached to AWS Transit Gateway with a single VPN connection. AWS Transit Gateway also enables you to scale the IPsec VPN throughput with equal cost multi-path (ECMP) routing support over multiple VPN tunnels. A single VPN tunnel still has a maximum throughput of 1.25 Gbps. If you establish multiple VPN tunnels to an ECMP-enabled transit gateway, it can scale beyond the default maximum limit of 1.25 Gbps.  You also must enable the dynamic routing option on your transit gateway to be able to take advantage of ECMP for scalability.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q18-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/transit-gateway-ecmp-multiple-tunnels/\">https://aws.amazon.com/premiumsupport/knowledge-center/transit-gateway-ecmp-multiple-tunnels/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Transfer Acceleration for the VPN connection to maximize the throughput</strong> - Transfer Acceleration is an Amazon S3 bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration is designed to optimize transfer speeds from across the world into S3 buckets. Transfer Acceleration takes advantage of the globally distributed edge locations in Amazon CloudFront.</p>\n\n<p>This option has been added as a distractor as it is not relevant to AWS VPN connections.</p>\n\n<p><strong>Use AWS Global Accelerator for the VPN connection to maximize the throughput</strong> - AWS Global Accelerator is a networking service that improves the performance of your users’ traffic by up to 60% using the global network infrastructure of AWS. When the internet is congested, AWS Global Accelerator optimizes the path to your application to keep packet loss, jitter, and latency consistently low. With Global Accelerator, you are provided two global static public IPs that act as a fixed entry point to your application, improving availability. Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint to mitigate endpoint failure.</p>\n\n<p>AWS Global Accelerator can be used to optimize the network path, using the congestion-free AWS global network to route traffic to the endpoint that provides the best application performance . You can use an accelerated VPN connection to avoid network disruptions that might occur when traffic is routed over the public internet. AWS Global Accelerator will not maximize the VPN throughput, so it is not the best fit for the given use case.</p>\n\n<p><strong>Create a virtual private gateway with equal cost multipath routing and multiple channels</strong> - A virtual private gateway is the VPN endpoint on the Amazon side of your Site-to-Site VPN connection that can be attached to a single VPC. A virtual private gateway does not support equal cost multi-path (ECMP) routing, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/transit-gateway-ecmp-multiple-tunnels/\">https://aws.amazon.com/premiumsupport/knowledge-center/transit-gateway-ecmp-multiple-tunnels/</a></p>\n",
                "options": [
                    {
                        "id": 9815,
                        "content": "<p>Create a virtual private gateway with equal cost multipath routing and multiple channels</p>",
                        "isValid": false
                    },
                    {
                        "id": 9816,
                        "content": "<p>Use AWS Global Accelerator for the VPN connection to maximize the throughput</p>",
                        "isValid": false
                    },
                    {
                        "id": 9817,
                        "content": "<p>Use Transfer Acceleration for the VPN connection to maximize the throughput</p>",
                        "isValid": false
                    },
                    {
                        "id": 9818,
                        "content": "<p>Create a transit gateway with equal cost multipath routing and add additional VPN tunnels</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2350,
            "attributes": {
                "createdAt": "2023-09-09T20:33:52.528Z",
                "updatedAt": "2023-09-09T20:33:52.528Z",
                "content": "<p>The engineering team at a global e-commerce company is currently reviewing their disaster recovery strategy. The team has outlined that they need to be able to quickly recover their application stack with a Recovery Time Objective (RTO) of 5 minutes, in all of the AWS Regions that the application runs. The application stack currently takes over 45 minutes to install on a Linux system.</p>\n\n<p>As a Solutions architect, which of the following options would you recommend as the disaster recovery strategy?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an AMI after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions</strong> - An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. You can launch multiple instances from a single AMI when you need multiple instances with the same configuration. You can use different AMIs to launch instances when you need instances with different configurations.</p>\n\n<p>For the current use case, you need to create an AMI such that the application stack is already set up. But AMIs are bound to the Region they are created in. So, you need to copy the AMI across Regions for disaster recovery readiness.</p>\n\n<p>Copying a source AMI results in an identical but distinct target AMI with its own unique identifier. In the case of an Amazon EBS-backed AMI, each of its backing snapshots is, by default, copied to an identical but distinct target snapshot. (The sole exceptions are when you choose to encrypt or re-encrypt the snapshot.) You can change or deregister the source AMI with no effect on the target AMI. The reverse is also true.\nThere are no charges for copying an AMI. However, standard storage and data transfer rates apply. If you copy an EBS-backed AMI, you will incur charges for the storage of any additional EBS snapshots.</p>\n\n<p>AWS does not copy launch permissions, user-defined tags, or Amazon S3 bucket permissions from the source AMI to the new AMI. After the copy operation is complete, you can apply launch permissions, user-defined tags, and Amazon S3 bucket permissions to the new AMI.</p>\n\n<p>AMIs Cross-Region copying:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the installation files in Amazon S3 for quicker retrieval</strong> - Amazon Simple Storage Service (Amazon S3) is an object storage service from AWS. It will not help with speeding up the installation since Amazon S3 is a storage service. You will still need an Amazon EC2 instance to have the necessary installation environment.</p>\n\n<p><strong>Use Amazon EC2 user data to speed up the installation process</strong> - User data of an EC2 instance can be used to perform common automated configuration tasks or run scripts after the instance starts. User data, cannot, however, be used to install the application. EC2 user data would not help as it would run the same installation script for the same duration of 45 minutes.</p>\n\n<p><strong>Create an AMI after installing the software and use this AMI to run the recovery process in other Regions</strong> - As discussed above, AMIs are Region-specific and need to be copied to all Regions they are intended to be used in.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</a></p>\n",
                "options": [
                    {
                        "id": 9819,
                        "content": "<p>Store the installation files in Amazon S3 for quicker retrieval</p>",
                        "isValid": false
                    },
                    {
                        "id": 9820,
                        "content": "<p>Use Amazon EC2 user data to speed up the installation process</p>",
                        "isValid": false
                    },
                    {
                        "id": 9821,
                        "content": "<p>Create an AMI after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions</p>",
                        "isValid": true
                    },
                    {
                        "id": 9822,
                        "content": "<p>Create an AMI after installing the software and use this AMI to run the recovery process in other Regions</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2351,
            "attributes": {
                "createdAt": "2023-09-09T20:33:52.622Z",
                "updatedAt": "2023-09-09T20:33:52.622Z",
                "content": "<p>A media company uses Amazon ElastiCache Redis to enhance the performance of its RDS database layer. The company wants a robust disaster recovery strategy for its caching layer that guarantees minimal downtime as well as minimal data loss while ensuring good application performance.</p>\n\n<p>Which of the following solutions will you recommend to address the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure</strong> - Multi-AZ is the best option when data retention, minimal downtime, and application performance are a priority.</p>\n\n<p>Data-loss potential - Low. Multi-AZ provides fault tolerance for every scenario, including hardware-related issues.</p>\n\n<p>Performance impact - Low. Of the available options, Multi-AZ provides the fastest time to recovery, because there is no manual procedure to follow after the process is implemented.</p>\n\n<p>Cost - Low to high. Multi-AZ is the lowest-cost option. Use Multi-AZ when you can't risk losing data because of hardware failure or you can't afford the downtime required by other options in your response to an outage.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Schedule daily automatic backups at a time when you expect low resource utilization for your cluster</strong> - Data loss potential is high, almost up to a day's worth of data. Hence, this is not the right option.</p>\n\n<p><strong>Schedule manual backups using Redis append-only file (AOF)</strong> - Manual backups using AOF are retained indefinitely and are useful for testing and archiving. You can schedule manual backups to occur up to 20 times per node within any 24-hour period. Although AOF provides a measure of fault tolerance, it can't protect your data from a hardware-related cache node failure, so there is a risk of data loss.</p>\n\n<p><strong>Add read-replicas across multiple availability zones to reduce the risk of potential data loss because of failure</strong> - To scale read capacity, ElastiCache allows you to add up to five read replicas across multiple availability zones. Read replicas are used to ease out read traffic from the primary database and cannot be used as a complete fault-tolerant solution in itself.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/FaultTolerance.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/FaultTolerance.html</a></p>\n",
                "options": [
                    {
                        "id": 9823,
                        "content": "<p>Add read-replicas across multiple availability zones to reduce the risk of potential data loss because of failure</p>",
                        "isValid": false
                    },
                    {
                        "id": 9824,
                        "content": "<p>Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure</p>",
                        "isValid": true
                    },
                    {
                        "id": 9825,
                        "content": "<p>Schedule manual backups using Redis append-only file (AOF)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9826,
                        "content": "<p>Schedule daily automatic backups at a time when you expect low resource utilization for your cluster</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2352,
            "attributes": {
                "createdAt": "2023-09-09T20:33:52.702Z",
                "updatedAt": "2023-09-09T20:33:52.702Z",
                "content": "<p>A CRM web application was written as a monolith in PHP and is facing scaling issues because of performance bottlenecks. The CTO wants to re-engineer towards microservices architecture and expose their application from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, yourcorp.com/profile and yourcorp.com/search. The CTO would like to expose all these URLs as HTTPS endpoints for security purposes.</p>\n\n<p>As a solutions architect, which of the following would you recommend as a solution that requires MINIMAL configuration effort?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use SSL certificates with SNI</strong></p>\n\n<p>You can host multiple TLS secured applications, each with its own TLS certificate, behind a single load balancer. To use SNI, all you need to do is bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client.</p>\n\n<p>ALB’s smart certificate selection goes beyond SNI. In addition to containing a list of valid domain names, certificates also describe the type of key exchange and cryptography that the server supports, as well as the signature algorithm (SHA2, SHA1, MD5) used to sign the certificate.</p>\n\n<p>With SNI support AWS makes it easy to use more than one certificate with the same ALB. The most common reason you might want to use multiple certificates is to handle different domains with the same load balancer. It’s always been possible to use wildcard and subject-alternate-name (SAN) certificates with ALB, but these come with limitations. Wildcard certificates only work for related subdomains that match a simple pattern and while SAN certificates can support many different domains, the same certificate authority has to authenticate each one. That means you have to reauthenticate and reprovision your certificate every time you add a new domain.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a wildcard SSL certificate</strong> - As the use case requires different domain names, so you cannot use a wildcard SSL certificate.</p>\n\n<p><strong>Use an HTTP to HTTPS redirect</strong> - This will not provide multiple secure endpoints for different URLs such as checkout.mycorp.com or www.mycorp.com, therefore it is incorrect for the given use-case.</p>\n\n<p><strong>Change the ELB SSL Security Policy</strong> - ELB SSL Security Policy will not provide multiple secure endpoints for different URLs such as checkout.mycorp.com or www.mycorp.com, therefore it is incorrect for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/\">https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html</a></p>\n",
                "options": [
                    {
                        "id": 9827,
                        "content": "<p>Use SSL certificates with SNI</p>",
                        "isValid": true
                    },
                    {
                        "id": 9828,
                        "content": "<p>Use a wildcard SSL certificate</p>",
                        "isValid": false
                    },
                    {
                        "id": 9829,
                        "content": "<p>Change the ELB SSL Security Policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 9830,
                        "content": "<p>Use an HTTP to HTTPS redirect</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2353,
            "attributes": {
                "createdAt": "2023-09-09T20:33:52.798Z",
                "updatedAt": "2023-09-09T20:33:52.798Z",
                "content": "<p>A company has built a serverless application using API Gateway and AWS Lambda. The backend is leveraging an RDS Aurora MySQL database. The web application was initially launched in the Americas and the company would now like to expand it to Europe, where a read-only version will be available to improve latency. You plan on deploying the API Gateway and AWS Lambda using CloudFormation, but would like to have a read-only copy of your data in Europe as well.</p>\n\n<p>As a Solutions Architect, what do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Aurora Read Replicas</strong></p>\n\n<p>Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs).</p>\n\n<p>Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region. The DB cluster volume is made up of multiple copies of the data for the DB cluster. However, the data in the cluster volume is represented as a single, logical volume to the primary instance and Aurora Replicas in the DB cluster. You can also set up two Aurora MySQL DB clusters in different AWS Regions, by creating an Aurora Read Replica of an Aurora MySQL DB cluster in a different AWS Region. In this way, Aurora Read Replicas can be deployed globally.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Aurora Multi-AZ</strong> - Aurora, stores copies of the data in a DB cluster across multiple Availability Zones in a single AWS Region, regardless of whether the instances in the DB cluster span multiple Availability Zones. When data is written to the primary DB instance, Aurora synchronously replicates the data across Availability Zones to six storage nodes associated with your cluster volume. Doing so provides data redundancy, eliminates I/O freezes, and minimizes latency spikes during system backups. Always remember that the main purpose for multi-AZ is high availability whereas the main purpose of read replicas is read scalability.</p>\n\n<p><strong>Use a DynamoDB Streams</strong> - DynamoDB Streams is a powerful service that you can combine with other AWS services to solve many problems. When enabled, DynamoDB Streams captures a time-ordered sequence of item-level modifications in a DynamoDB table and durably stores the information for up to 24 hours. Applications can access a series of stream records, which contain an item change, from a DynamoDB stream in near real-time.</p>\n\n<p><strong>Create a Lambda function to periodically back up and restore the Aurora database in another region</strong> - Lambda can be used to create backups for RDS. But, the process is not an optimized solution, especially when Aurora already offers the read replica feature.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n",
                "options": [
                    {
                        "id": 9831,
                        "content": "<p>Use a DynamoDB Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 9832,
                        "content": "<p>Create a Lambda function to periodically back up and restore the Aurora database in another region</p>",
                        "isValid": false
                    },
                    {
                        "id": 9833,
                        "content": "<p>Use Aurora Multi-AZ</p>",
                        "isValid": false
                    },
                    {
                        "id": 9834,
                        "content": "<p>Use Aurora Read Replicas</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2354,
            "attributes": {
                "createdAt": "2023-09-09T20:33:52.886Z",
                "updatedAt": "2023-09-09T20:33:52.886Z",
                "content": "<p>As a Solutions Architect, you are tasked to design a distributed application that will run on various EC2 instances. This application needs to have the highest performance local disk to cache data. Also, data is copied through an EC2 to EC2 replication mechanism. It is acceptable if the instance loses its data when stopped or terminated.</p>\n\n<p>Which storage solution do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Instance Store</strong> - An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.</p>\n\n<p>Instance store volumes are included as part of the instance's usage cost. Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Elastic Block Store (EBS)</strong> - Amazon Elastic Block Store (EBS) is an easy to use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale.</p>\n\n<p><strong>Amazon Elastic File System (Amazon EFS)</strong> - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth.</p>\n\n<p><strong>Amazon Simple Storage Service (Amazon S3)</strong> - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3.</p>\n\n<p>Instance Store will have the highest disk performance but you would lose the storage if the instance is terminated, which is acceptable in this case. EBS volumes would provide good performance as far as disk goes, but not as good as Instance Store. EBS data survives instance termination or reboots. EFS is a network drive and it would not match the performance of the Instance Store. Finally, S3 cannot be mounted as a local disk (natively).</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p>\n",
                "options": [
                    {
                        "id": 9835,
                        "content": "<p>Amazon Elastic File System (Amazon EFS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9836,
                        "content": "<p>Instance Store</p>",
                        "isValid": true
                    },
                    {
                        "id": 9837,
                        "content": "<p>Amazon Elastic Block Store (EBS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9838,
                        "content": "<p>Amazon Simple Storage Service (Amazon S3)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2355,
            "attributes": {
                "createdAt": "2023-09-09T20:33:52.983Z",
                "updatedAt": "2023-09-09T20:33:52.983Z",
                "content": "<p>A music-sharing company uses a Network Load Balancer to direct traffic to 5 EC2 instances managed by an Auto Scaling group. When a very popular song is released, the Auto Scaling Group scales to 100 instances and the company incurs high network and compute fees.</p>\n\n<p>The company wants a solution to reduce the costs without changing any of the application code. What do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use a CloudFront distribution</strong> - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.</p>\n\n<p>CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content.</p>\n\n<p>Regional edge caches help with all types of content, particularly content that tends to become less popular over time. Examples include user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos; and news and event-related content that might suddenly find new popularity.</p>\n\n<p>CloudFront is the right answer because we can put it in front of our ASG and leverage a Global Caching feature that will help us distribute the content reliably with dramatically reduced costs (the ASG won't need to scale as much).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage AWS Storage Gateway</strong> - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access. Storage Gateway cannot be used for distributing files to end-users, so this option is ruled out.</p>\n\n<p><strong>Move the songs to S3</strong> - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Using S3 would imply changing the application code, so this option is ruled out.</p>\n\n<p><strong>Move the songs to Glacier</strong> - Amazon S3 Glacier and S3 Glacier Deep Archive are secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. Glacier is not applicable as the files are frequently requested (Glacier has retrieval times ranging from a few minutes to hours), so this option is also ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/\">https://aws.amazon.com/storagegateway/</a></p>\n",
                "options": [
                    {
                        "id": 9839,
                        "content": "<p>Move the songs to Glacier</p>",
                        "isValid": false
                    },
                    {
                        "id": 9840,
                        "content": "<p>Move the songs to S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9841,
                        "content": "<p>Use a CloudFront distribution</p>",
                        "isValid": true
                    },
                    {
                        "id": 9842,
                        "content": "<p>Leverage AWS Storage Gateway</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2356,
            "attributes": {
                "createdAt": "2023-09-09T20:33:53.082Z",
                "updatedAt": "2023-09-09T20:33:53.082Z",
                "content": "<p>A digital media company needs to manage uploads of around 1TB each from an application being used by a partner company.</p>\n\n<p>As a Solutions Architect, how will you handle the upload of these files to Amazon S3?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use multi-part upload feature of Amazon S3</strong> - Multi-part upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object.</p>\n\n<p>AWS recommends that you use multi-part uploading in the following ways:\nIf you're uploading large objects over a stable high-bandwidth network, use multi-part uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance.\nIf you're uploading over a spotty network, use multi-part uploading to increase resiliency to network errors by avoiding upload restarts. When using multi-part uploading, you need to retry uploading only parts that are interrupted during the upload. You don't need to restart uploading your object from the beginning.</p>\n\n<p>In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. If the file is greater than 5GB in size, you must use multi-part upload to upload that file to S3.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon S3 Versioning</strong> - S3 Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures. When you enable versioning for a bucket, if Amazon S3 receives multiple write requests for the same object simultaneously, it stores all of the objects. If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version.</p>\n\n<p><strong>Use Direct Connect connection to provide extra bandwidth</strong> - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.</p>\n\n<p>AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. This dedicated connection can be partitioned into multiple virtual interfaces. This allows you to use the same connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon Virtual Private Cloud (VPC) using private IP space, while maintaining network separation between the public and private environments. Virtual interfaces can be reconfigured at any time to meet your changing needs. This is a physical connection that takes at least a month to set up.</p>\n\n<p><strong>Use AWS Snowball</strong> - Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases.</p>\n\n<p>(The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space.)</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html</a></p>\n",
                "options": [
                    {
                        "id": 9843,
                        "content": "<p>Use multi-part upload feature of Amazon S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 9844,
                        "content": "<p>Use Amazon S3 Versioning</p>",
                        "isValid": false
                    },
                    {
                        "id": 9845,
                        "content": "<p>Use AWS Snowball</p>",
                        "isValid": false
                    },
                    {
                        "id": 9846,
                        "content": "<p>Use Direct Connect to provide extra bandwidth</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2357,
            "attributes": {
                "createdAt": "2023-09-09T20:33:53.171Z",
                "updatedAt": "2023-09-09T20:33:53.171Z",
                "content": "<p>You started a new job as a solutions architect at a company that has both AWS experts and people learning AWS. Recently, a developer misconfigured a newly created RDS database which resulted in a production outage.</p>\n\n<p>How can you ensure that RDS specific best practices are incorporated into a reusable infrastructure template to be used by all your AWS users?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use CloudFormation to manage RDS databases</strong> - AWS CloudFormation provides a common language for you to model and provision AWS and third-party application resources in your cloud environment. AWS CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This gives you a single source of truth for your AWS and third-party resources.</p>\n\n<p>CloudFormation allows you to keep your infrastructure as code and re-use the best practices around your company for configuration parameters. Therefore, this is the correct option for the given use-case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store your recommendations in a custom Trusted Advisor rule</strong> - AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices. Whether establishing new workflows, developing applications, or as part of ongoing improvement, take advantage of the recommendations provided by Trusted Advisor regularly to help keep your solutions provisioned optimally. Trusted Advisor just provides recommendations rather than creating reusable infrastructure templates.</p>\n\n<p><strong>Create a Lambda function that sends emails when it finds misconfigured RDS databases</strong> - Using a Lambda function to scan for a misconfigured RDS database is a reactive mechanism. It does not help in creating reusable infrastructure templates.</p>\n\n<p><strong>Attach an IAM policy to interns preventing them from creating an RDS database</strong> - Using an IAM policy to prevent interns from creating an RDS database does not solve the problem of allowing any user to create resources by leveraging reusable infrastructure templates. So, this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n",
                "options": [
                    {
                        "id": 9847,
                        "content": "<p>Store your recommendations in a custom Trusted Advisor rule</p>",
                        "isValid": false
                    },
                    {
                        "id": 9848,
                        "content": "<p>Use CloudFormation to manage RDS databases</p>",
                        "isValid": true
                    },
                    {
                        "id": 9849,
                        "content": "<p>Attach an IAM policy to interns preventing them from creating an RDS database</p>",
                        "isValid": false
                    },
                    {
                        "id": 9850,
                        "content": "<p>Create a Lambda function which sends emails when it finds misconfigured RDS databases</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2358,
            "attributes": {
                "createdAt": "2023-09-09T20:33:53.287Z",
                "updatedAt": "2023-09-09T20:33:53.287Z",
                "content": "<p>A Pharmaceuticals company is looking for a simple solution to connect its VPCs and on-premises networks through a central hub.</p>\n\n<p>As a Solutions Architect, which of the following would you suggest as the solution that requires the LEAST operational overhead?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks</strong> - The AWS Transit Gateway allows customers to connect their Amazon VPCs and their on-premises networks to a single gateway. As your number of workloads running on AWS increases, you need to be able to scale your networks across multiple accounts and Amazon VPCs to keep up with the growth. With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway into each Amazon VPC, on-premises data center, or remote office across your network. AWS Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks, which act like spokes. This hub and spoke model simplifies management and reduces operational costs because each network only has to connect to the Transit Gateway and not to every other network.</p>\n\n<p>AWS Transit Gateway:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q28-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks</strong> - The Transit VPC can be used to enable connectivity between various VPC’s in different regions and customer data centers. You can use this to connect multiple VPCs that are geographically disparate and/or running in separate AWS accounts, to a common VPC that serves as a global network transit center. This network topology simplifies network management and minimizes the number of connections that you need to set up.</p>\n\n<p>Transit VPC:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q28-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p>\n\n<p>Transit VPC is not the right solution for this use-case as Transit Gateway provides several advantages over Transit VPC:\n1. Transit Gateway abstracts away the complexity of maintaining VPN connections with hundreds of VPCs.\n2. Transit Gateway removes the need to manage and scale EC2 based software appliances. AWS is responsible for managing all resources needed to route traffic.\n3. Transit Gateway removes the need to manage high availability by providing a highly available and redundant Multi-AZ infrastructure.\n4. Transit Gateway improves bandwidth for inter-VPC communication to burst speeds of 50 Gbps per AZ.\n5. Transit Gateway streamlines user costs to a simple per hour per/GB transferred model.\n6. Transit Gateway decreases latency by removing EC2 proxies and the need for VPN encapsulation.</p>\n\n<p><strong>Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks</strong></p>\n\n<p><strong>Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks</strong></p>\n\n<p>The simplest way to connect two VPCs is to use VPC Peering. In this setup, a connection enables full bidirectional connectivity between the VPCs. This peering connection is used to route traffic between the VPCs. VPCs across accounts and AWS Regions can also be peered together. VPC peering only incurs costs for traffic traveling over the connection (there is no hourly infrastructure fee).</p>\n\n<p>VPC peering is point-to-point connectivity, and it does not support transitive routing. If you are using VPC peering, on-premises connectivity (VPN and/or Direct Connect) must be made to each VPC. Resources in a VPC cannot reach on-premises using the hybrid connectivity of a peered VPC. VPC peering is best used when resources in one VPC must communicate with resources in another VPC, the environment of both VPCs is controlled and secured, and the number of VPCs to be connected is less than 10 (to allow for the individual management of each connection). VPC peering offers the lowest overall cost when compared to other options for inter-VPC connectivity.</p>\n\n<p>Network setup using VPC Peering:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q28-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html\">https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html</a></p>\n\n<p>You cannot use VPC Peering to establish on-premises connectivity with AWS Cloud, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway-vs-transit-vpc.html\">https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway-vs-transit-vpc.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html\">https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html</a></p>\n",
                "options": [
                    {
                        "id": 9851,
                        "content": "<p>Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks</p>",
                        "isValid": false
                    },
                    {
                        "id": 9852,
                        "content": "<p>Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks</p>",
                        "isValid": false
                    },
                    {
                        "id": 9853,
                        "content": "<p>Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks</p>",
                        "isValid": true
                    },
                    {
                        "id": 9854,
                        "content": "<p>Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2359,
            "attributes": {
                "createdAt": "2023-09-09T20:33:53.377Z",
                "updatedAt": "2023-09-09T20:33:53.377Z",
                "content": "<p>Your company is deploying a website running on Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process.</p>\n\n<p>As a Solutions Architect, you would like to bring the time to create a new instance in your Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.</p>\n\n<p>You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time.</p>\n\n<p>When you create an AWS Elastic Beanstalk environment, you can specify an Amazon Machine Image (AMI) to use instead of the standard Elastic Beanstalk AMI included in your platform version. A custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn't included in the standard AMIs.</p>\n\n<p><strong>Create a Golden AMI with the static installation components already setup</strong> - A Golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can have the static installation components already setup via the golden AMI.</p>\n\n<p><strong>Use EC2 user data to customize the dynamic installation parts at boot time</strong> - EC2 instance user data is the data that you specified in the form of a configuration script while launching your instance. You can use EC2 user data to customize the dynamic installation parts at boot time, rather than installing the application itself at boot time.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the installation files in S3 so they can be quickly retrieved</strong> - Amazon S3 bucket can be used as a storage location for your source code, logs, and other artifacts that are created when you use Elastic Beanstalk. It cannot be used to run or generate dynamic files since S3 is not an environment but a storage service.</p>\n\n<p><strong>Use EC2 user data to install the application at boot time</strong> - User data of an instance can be used to perform common automated configuration tasks or run scripts after the instance starts. User data, cannot, however, be used to install the application since it takes over 45 minutes for the installation which contains static as well as dynamic files that must be generated during the installation process.</p>\n\n<p><strong>Use Elastic Beanstalk deployment caching feature</strong> - Elastic Beanstalk deployment caching is a made-up option. It is just added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.S3.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.S3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-add-user-data.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-add-user-data.html</a></p>\n",
                "options": [
                    {
                        "id": 9855,
                        "content": "<p>Store the installation files in S3 so they can be quickly retrieved</p>",
                        "isValid": false
                    },
                    {
                        "id": 9856,
                        "content": "<p>Use Elastic Beanstalk deployment caching feature</p>",
                        "isValid": false
                    },
                    {
                        "id": 9857,
                        "content": "<p>Use EC2 user data to install the application at boot time</p>",
                        "isValid": false
                    },
                    {
                        "id": 9858,
                        "content": "<p>Create a Golden AMI with the static installation components already setup</p>",
                        "isValid": true
                    },
                    {
                        "id": 9859,
                        "content": "<p>Use EC2 user data to customize the dynamic installation parts at boot time</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2360,
            "attributes": {
                "createdAt": "2023-09-09T20:33:53.466Z",
                "updatedAt": "2023-09-09T20:33:53.466Z",
                "content": "<p>You are working as a Solutions Architect for a photo processing company that has a proprietary algorithm to compress an image without any loss in quality. Because of the efficiency of the algorithm, your clients are willing to wait for a response that carries their compressed images back. You also want to process these jobs asynchronously and scale quickly, to cater to the high demand. Additionally, you also want the job to be retried in case of failures.</p>\n\n<p>Which combination of choices do you recommend to minimize cost and comply with the requirements? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>EC2 Spot Instances</strong> - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone is set by Amazon EC2 and adjusted gradually based on the long-term supply of and demand for Spot Instances. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price.</p>\n\n<p>To process these jobs, due to the unpredictable nature of their volume, and the desire to save on costs, spot Instances are recommended as compared to on-demand instances. As spot instances are cheaper than reserved instances and do not require long term commitment, spot instances are a better fit for the given use-case.</p>\n\n<p>EC2 Instance purchasing options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q46-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html</a></p>\n\n<p><strong>Amazon Simple Queue Service (SQS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>SQS will allow you to buffer the image compression requests and process them asynchronously. It also has a direct built-in mechanism for retries and scales seamlessly.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS is not the right fit for this use-case, since its not a queuing mechanism.</p>\n\n<p><strong>EC2 Reserved Instances</strong> - Reserved instances reduce your Amazon EC2 costs by making a commitment to a consistent instance configuration, including instance type and Region, for a term of 1 or 3 years. For the given use case, this kind of annual commitment might not be a desirable option.</p>\n\n<p><strong>EC2 On-Demand Instances</strong> - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. There is no long-term commitment required when you purchase On-Demand Instances. You pay only for the seconds that your On-Demand Instances are running. AWS recommends that you use On-Demand Instances for applications with short-term, irregular workloads that cannot be interrupted.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p>\n",
                "options": [
                    {
                        "id": 9860,
                        "content": "<p>EC2 Reserved Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9861,
                        "content": "<p>EC2 Spot Instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 9862,
                        "content": "<p>Amazon Simple Notification Service (SNS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9863,
                        "content": "<p>Amazon Simple Queue Service (SQS)</p>",
                        "isValid": true
                    },
                    {
                        "id": 9864,
                        "content": "<p>EC2 On-Demand Instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2361,
            "attributes": {
                "createdAt": "2023-09-09T20:33:53.556Z",
                "updatedAt": "2023-09-09T20:33:53.556Z",
                "content": "<p>An e-commerce company tracks user clicks on its flagship website and performs analytics to provide near-real-time product recommendations. An EC2 instance receives data from the website and sends the data to an Aurora DB instance. Another EC2 instance continuously checks the changes in the database and executes SQL queries to provide recommendations. Now, the company wants a redesign to decouple and scale the infrastructure. The solution must ensure that data can be analyzed in real-time without any data loss even when the company sees huge traffic spikes.</p>\n\n<p>What would you recommend as an AWS Certified Solutions Architect Associate?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3</strong></p>\n\n<p>You can use Kinesis Data Streams to build custom applications that process or analyze streaming data for specialized needs.\nKinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You don't have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services for your data streams.</p>\n\n<p>For the given use case, you can use Kinesis Data Analytics to transform and analyze incoming streaming data from Kinesis Data Streams in real time. Kinesis Data Analytics takes care of everything required to run streaming applications continuously, and scales automatically to match the volume and throughput of your incoming data. With Kinesis Data Analytics, there are no servers to manage, no minimum fee or setup cost, and you only pay for the resources your streaming applications consume.</p>\n\n<p>Amazon Kinesis Data Analytics:\n<img src=\"https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Amazon-Kinesis-Data-Analytics_HIW.82e3aa53a5c87db03c766218b3d51f1a110c60eb.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/\">https://aws.amazon.com/kinesis/</a></p>\n\n<p>Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms and delivers streaming data to data lakes, data stores, and analytics services.</p>\n\n<p>For the given use case, post the real-time analysis, the output feed from Kinesis Data Analytics is output into Kinesis Data Firehose which dumps the data into Amazon S3 without any data loss.</p>\n\n<p>Amazon Kinesis Data Firehose:\n<img src=\"https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/\">https://aws.amazon.com/kinesis/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3</strong> - QuickSight cannot use Kinesis Data Streams as a source. In addition, QuickSight cannot be used for real-time streaming data analysis from its source. Therefore this option is incorrect.</p>\n\n<p><strong>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena cannot be used to analyze data in real time. Therefore this option is incorrect.</p>\n\n<p><strong>Leverage Amazon SQS to capture the data from the website. Configure a fleet of EC2 instances under an Auto scaling group to process messages from the SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third party library on the EC2 instances</strong> - Even though using SQS with EC2 instances can decouple the architecture, however, performing real-time analytics using a third party library on the EC2 instances is not the best fit solution for the given use case. The Kinesis family of services is the better fit for the given scenario as these services allow streaming data ingestion, real-time analysis, and reliable data delivery to the data sink.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/\">https://aws.amazon.com/kinesis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/quicksight/resources/faqs/\">https://aws.amazon.com/quicksight/resources/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9865,
                        "content": "<p>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 9866,
                        "content": "<p>Leverage Amazon SQS to capture the data from the website. Configure a fleet of EC2 instances under an Auto scaling group to process messages from the SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9867,
                        "content": "<p>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9868,
                        "content": "<p>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2362,
            "attributes": {
                "createdAt": "2023-09-09T20:33:53.654Z",
                "updatedAt": "2023-09-09T20:33:53.654Z",
                "content": "<p>A healthcare company is evaluating storage options on Amazon S3 to meet regulatory guidelines. The data should be stored in such a way on S3 that it cannot be deleted until the regulatory time period has expired.</p>\n\n<p>As a solutions architect, which of the following would you recommend for the given requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use S3 Object Lock</strong></p>\n\n<p>Amazon S3 Object Lock is an Amazon S3 feature that allows you to store objects using a write once, read many (WORM) model. You can use WORM protection for scenarios where it is imperative that data is not changed or deleted after it has been written. Whether your business has a requirement to satisfy compliance regulations in the financial or healthcare sector, or you simply want to capture a golden copy of business records for later auditing and reconciliation, S3 Object Lock is the right tool for you. Object Lock can help prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q64-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q64-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/\">https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use S3 Glacier Vault Lock</strong></p>\n\n<p>A vault is a container for storing archives on Glacier. When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault. Since Vault Lock is only for Glacier and not for S3, so it cannot be used for the given use-case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q64-i3.jpg\"></p>\n\n<p>\"Use S3 cross-Region Replication\" - Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. The object may be replicated to a single destination bucket or multiple destination buckets. Both source and destination buckets must have versioning enabled. By default, when Amazon S3 Replication is enabled and an object is deleted in the source bucket, Amazon S3 adds a delete marker in the source bucket only. This action protects data from malicious deletions. If you have delete marker replication enabled, these markers are copied to the destination buckets, and Amazon S3 behaves as if the object was deleted in both source and destination buckets. However, someone with administrative access to S3 can disable cross-Region replication and then delete all versions from both source as well as the destination, so this option will not be able to safeguard your data compared to S3 Object Lock.</p>\n\n<p><strong>Activate MFA delete on the S3 bucket</strong> - When working with S3 Versioning in Amazon S3 buckets, you can optionally add another layer of security by configuring a bucket to enable MFA (multi-factor authentication) delete. When you do this, the bucket owner must include two forms of authentication in any request to delete a version or change the versioning state of the bucket. Only the root account can enable MFA delete. MFA delete cannot be used for the given use case because it just represents an additional security layer and can be disabled by anyone having access to the root account credentials.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/\">https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/</a></p>\n",
                "options": [
                    {
                        "id": 9869,
                        "content": "<p>Activate MFA delete on the S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 9870,
                        "content": "<p>Use S3 cross-Region Replication</p>",
                        "isValid": false
                    },
                    {
                        "id": 9871,
                        "content": "<p>Use S3 Glacier Vault Lock</p>",
                        "isValid": false
                    },
                    {
                        "id": 9872,
                        "content": "<p>Use S3 Object Lock</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2363,
            "attributes": {
                "createdAt": "2023-09-09T20:33:53.773Z",
                "updatedAt": "2023-09-09T20:33:53.773Z",
                "content": "<p>A company's business logic is built on several microservices that are running in the on-premises data center. They currently communicate using a message broker that supports the MQTT protocol. The company is looking at migrating these applications and the message broker to AWS Cloud without changing the application logic.</p>\n\n<p>Which technology allows you to get a managed message broker that supports the MQTT protocol?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon MQ</strong> - Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Message brokers allow different software systems–often using different programming languages, and on different platforms–to communicate and exchange information. If an organization is using messaging with existing applications and wants to move the messaging service to the cloud quickly and easily, AWS recommends Amazon MQ for such a use case. Connecting your current applications to Amazon MQ is easy because it uses industry-standard APIs and protocols for messaging, including JMS, NMS, AMQP, STOMP, MQTT, and WebSocket.</p>\n\n<p>Therefore, the only possible answer is Amazon MQ.</p>\n\n<p>Incorrect option:</p>\n\n<p><strong>Amazon Simple Queue Service (SQS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p><strong>Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging.</p>\n\n<p>SNS, SQS, and Kinesis are AWS's proprietary technologies and do not come with MQTT compatibility.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/amazon-mq/\">https://aws.amazon.com/amazon-mq/</a></p>\n",
                "options": [
                    {
                        "id": 9873,
                        "content": "<p>Amazon Simple Notification Service (SNS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9874,
                        "content": "<p>Amazon Simple Queue Service (SQS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9875,
                        "content": "<p>Amazon MQ</p>",
                        "isValid": true
                    },
                    {
                        "id": 9876,
                        "content": "<p>Amazon Kinesis Data Streams</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2364,
            "attributes": {
                "createdAt": "2023-09-09T20:33:53.866Z",
                "updatedAt": "2023-09-09T20:33:53.866Z",
                "content": "<p>For security purposes, a development team has decided to deploy the EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints.</p>\n\n<p>As a solutions architect, which of the following services would you suggest for this requirement? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Amazon S3</strong></p>\n\n<p><strong>DynamoDB</strong></p>\n\n<p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p>\n\n<p>Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.</p>\n\n<p>There are two types of VPC endpoints: Interface Endpoints and Gateway Endpoints. An Interface Endpoint is an Elastic Network Interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.</p>\n\n<p>A Gateway Endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported: Amazon S3 and DynamoDB.</p>\n\n<p>You can use two types of VPC endpoints to access Amazon S3: gateway endpoints and interface endpoints. A gateway endpoint is a gateway that you specify in your route table to access Amazon S3 from your VPC over the AWS network. Interface endpoints extend the functionality of gateway endpoints by using private IP addresses to route requests to Amazon S3 from within your VPC, on premises, or from a VPC in another AWS Region using VPC peering or AWS Transit Gateway.</p>\n\n<p>You must remember that these two services use a VPC gateway endpoint. The rest of the AWS services use VPC interface endpoints.</p>\n\n<p>Gateway VPC endpoints:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Queue Service (SQS)</strong></p>\n\n<p><strong>Amazon Simple Notification Service (SNS)</strong></p>\n\n<p><strong>Amazon Kinesis</strong></p>\n\n<p>As mentioned in the description above, these three options use interface endpoints, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a></p>\n",
                "options": [
                    {
                        "id": 9877,
                        "content": "<p>Amazon Simple Queue Service (SQS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9878,
                        "content": "<p>Amazon Simple Notification Service (SNS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 9879,
                        "content": "<p>Amazon Kinesis</p>",
                        "isValid": false
                    },
                    {
                        "id": 9880,
                        "content": "<p>Amazon S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 9881,
                        "content": "<p>DynamoDB</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2365,
            "attributes": {
                "createdAt": "2023-09-09T20:33:53.952Z",
                "updatedAt": "2023-09-09T20:33:53.952Z",
                "content": "<p>The engineering team at a company is running batch workloads on AWS Cloud. The team has embedded RDS database connection strings within each web server hosting the flagship application. After failing a security audit, the team is looking at a different approach to store the database secrets securely and automatically rotate the database credentials.</p>\n\n<p>Which of the following solutions would you recommend to meet this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Secrets Manager</strong></p>\n\n<p>AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.</p>\n\n<p>Benefits of Secrets Manager:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q50-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSM Parameter Store</strong> - AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. SSM Parameter Store cannot be used to automatically rotate the database credentials.</p>\n\n<p><strong>Systems Manager</strong> - AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources. Systems Manager cannot be used to store your secrets securely and automatically rotate the database credentials.</p>\n\n<p><strong>KMS</strong> - AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. KMS cannot be used to store your secrets securely and automatically rotate the database credentials.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html</a></p>\n",
                "options": [
                    {
                        "id": 9882,
                        "content": "<p>SSM Parameter Store</p>",
                        "isValid": false
                    },
                    {
                        "id": 9883,
                        "content": "<p>Systems Manager</p>",
                        "isValid": false
                    },
                    {
                        "id": 9884,
                        "content": "<p>Secrets Manager</p>",
                        "isValid": true
                    },
                    {
                        "id": 9885,
                        "content": "<p>KMS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2366,
            "attributes": {
                "createdAt": "2023-09-09T20:33:54.032Z",
                "updatedAt": "2023-09-09T20:33:54.032Z",
                "content": "<p>A photo hosting service publishes a collection of beautiful mountain images, every month, that aggregate over 50 GB in size and downloaded all around the world. The content is currently hosted on EFS and distributed by Elastic Load Balancing (ELB) and Amazon EC2 instances. The website is experiencing high load each month and very high network costs.</p>\n\n<p>As a Solutions Architect, what can you recommend that won't force an application refactor and reduce network costs and EC2 load drastically?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudFront distribution</strong> - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.</p>\n\n<p>CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content.</p>\n\n<p>Regional edge caches help with all types of content, particularly content that tends to become less popular over time. Examples include user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos; and news and event-related content that might suddenly find new popularity.</p>\n\n<p>For the given use case, you need to create a CloudFront distribution to add a caching layer in front of your ELB. That caching layer will be very effective as the image pack is a static file, and therefore it would save the network costs significantly without requiring an application refactor.</p>\n\n<p>How CloudFront delivers content to your users:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Host the master pack onto Amazon S3 for faster access</strong> - Hosting the master pack into Amazon S3 will result in application code refactoring. So, this option is not correct.</p>\n\n<p><strong>Upgrade the EC2 instances</strong> - Upgrading the EC2 instances won't help save network cost. Hence, this option is incorrect for the given scenario.</p>\n\n<p><strong>Enable Elastic Load Balancer caching</strong> - ELB does not have any caching capability. This option is just added as a distractor.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html</p>\n",
                "options": [
                    {
                        "id": 9886,
                        "content": "<p>Create a CloudFront distribution</p>",
                        "isValid": true
                    },
                    {
                        "id": 9887,
                        "content": "<p>Host the master pack onto Amazon S3 for faster access</p>",
                        "isValid": false
                    },
                    {
                        "id": 9888,
                        "content": "<p>Enable ELB caching</p>",
                        "isValid": false
                    },
                    {
                        "id": 9889,
                        "content": "<p>Upgrade the Amazon EC2 instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2367,
            "attributes": {
                "createdAt": "2023-09-09T20:33:54.134Z",
                "updatedAt": "2023-09-09T20:33:54.134Z",
                "content": "<p>A ride-sharing company wants to use an Amazon DynamoDB table for data storage. The table will not be used during the night hours whereas the read and write traffic will often be unpredictable during day hours. When traffic spikes occur they will happen very quickly.</p>\n\n<p>Which of the following will you recommend as the best-fit solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up a DynamoDB table in the on-demand capacity mode</strong></p>\n\n<p>Amazon DynamoDB has two read/write capacity modes for processing reads and writes on your tables:</p>\n\n<p>On-demand</p>\n\n<p>Provisioned (default, free-tier eligible)</p>\n\n<p>Amazon DynamoDB on-demand is a flexible billing option capable of serving thousands of requests per second without capacity planning. DynamoDB on-demand offers pay-per-request pricing for read and write requests so that you pay only for what you use.</p>\n\n<p>The on-demand mode is a good option if any of the following are true:</p>\n\n<p>You create new tables with unknown workloads.</p>\n\n<p>You have unpredictable application traffic.</p>\n\n<p>You prefer the ease of paying for only what you use.</p>\n\n<p>If you choose provisioned mode, you specify the number of reads and writes per second that you require for your application. You can use auto-scaling to adjust your table’s provisioned capacity automatically in response to traffic changes. This helps you govern your DynamoDB use to stay at or below a defined request rate to obtain cost predictability.</p>\n\n<p>Provisioned mode is a good option if any of the following are true:</p>\n\n<p>You have predictable application traffic.</p>\n\n<p>You run applications whose traffic is consistent or ramps gradually.</p>\n\n<p>You can forecast capacity requirements to control costs.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q62-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/\">https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/</a></p>\n\n<p>With on-demand, DynamoDB instantly allocates capacity as it is needed. There is no concept of provisioned capacity, and there is no delay waiting for CloudWatch thresholds or the subsequent table updates. On-demand is ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or minutes, and when underprovisioned capacity would impact the user experience. On-demand is a perfect solution if your team is moving to a NoOps or serverless environment.</p>\n\n<p>The given use case clearly states that when the traffic spikes occur they happen very quickly, thereby implying an unpredictable traffic pattern, therefore the on-demand capacity mode is the correct option for the given use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a DynamoDB table in the provisioned capacity mode with auto-scaling enabled</strong> - As mentioned in the explanation above, you should use the provisioned capacity mode (even with auto-scaling) only when you have predictable application traffic.</p>\n\n<p>When you create a DynamoDB table, auto-scaling is the default capacity setting, but you can also enable auto-scaling on any table that does not have it active. Behind the scenes, as illustrated in the following diagram, DynamoDB auto scaling uses a scaling policy in Application Auto Scaling. To configure auto-scaling in DynamoDB, you set the minimum and maximum levels of read and write capacity in addition to the target utilization percentage. Auto-scaling uses Amazon CloudWatch to monitor a table’s read and write capacity metrics. To do so, it creates CloudWatch alarms that track consumed capacity.</p>\n\n<p><strong>Set up a DynamoDB table with a global secondary index</strong> - A global secondary index (GSI) is an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions. A global secondary index is stored in its own partition space away from the base table and scales separately from the base table. GSI cannot be used to handle an unpredictable load on a DynamoDB table.</p>\n\n<p><strong>Set up a DynamoDB global table in the provisioned capacity mode</strong> - Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions.</p>\n\n<p>Global tables eliminate the difficult work of replicating data between Regions and resolving update conflicts, enabling you to focus on your application's business logic. In addition, global tables enable your applications to stay highly available even in the unlikely event of isolation or degradation of an entire Region.</p>\n\n<p>DynamoDB global tables:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q62-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n\n<p>DynamoDB global table cannot be used to handle an unpredictable load on a DynamoDB table.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/\">https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n",
                "options": [
                    {
                        "id": 9890,
                        "content": "<p>Set up a DynamoDB global table in the provisioned capacity mode</p>",
                        "isValid": false
                    },
                    {
                        "id": 9891,
                        "content": "<p>Set up a DynamoDB table in the provisioned capacity mode with auto-scaling enabled</p>",
                        "isValid": false
                    },
                    {
                        "id": 9892,
                        "content": "<p>Set up a DynamoDB table with a global secondary index</p>",
                        "isValid": false
                    },
                    {
                        "id": 9893,
                        "content": "<p>Set up a DynamoDB table in the on-demand capacity mode</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2368,
            "attributes": {
                "createdAt": "2023-09-09T20:33:54.227Z",
                "updatedAt": "2023-09-09T20:33:54.227Z",
                "content": "<p>The engineering team at a social media company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on CloudFront capabilities on routing, security, and high availability.</p>\n\n<p>Which of the following would you identify as correct regarding CloudFront? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>CloudFront can route to multiple origins based on the content type</strong></p>\n\n<p>You can configure a single CloudFront web distribution to serve different types of requests from multiple origins. For example, if you are building a website that serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from a load balancer, you can serve both types of content from a CloudFront web distribution.</p>\n\n<p><strong>Use an origin group with primary and secondary origins to configure CloudFront for high availability and failover</strong></p>\n\n<p>You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin.</p>\n\n<p>To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Finally, you create or update a cache behavior to use the origin group.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/images/origingroups-overview.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n\n<p><strong>Use field level encryption in CloudFront to protect sensitive data for specific content</strong></p>\n\n<p>Field-level encryption allows you to enable your users to securely upload sensitive information to your web servers. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack. This encryption ensures that only applications that need the data—and have the credentials to decrypt it—are able to do so.</p>\n\n<p>To use field-level encryption, when you configure your CloudFront distribution, specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them. You can encrypt up to 10 data fields in a request. (You can’t encrypt all of the data in a request with field-level encryption; you must specify individual fields to encrypt.)</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/images/fleoverview.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use KMS encryption in CloudFront to protect sensitive data for specific content</strong> - This option has been added as a distractor. You can use field level encryption in CloudFront to protect sensitive data for specific content.</p>\n\n<p><strong>Use geo restriction to configure CloudFront for high-availability and failover</strong> - You can use geo restriction, also known as geo blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront distribution. Geo restriction is not used to configure CloudFront for high availability and failover.</p>\n\n<p><strong>CloudFront can route to multiple origins based on the price class</strong> - CloudFront edge locations are grouped into geographic regions, and AWS has grouped regions into price classes. The default price class includes all regions. Another price class includes most regions (the United States; Canada; Europe; Hong Kong, Philippines, South Korea, Taiwan, and Singapore; Japan; India; South Africa; and Middle East regions) but excludes the most expensive regions. A third price class includes only the least expensive regions (the United States, Canada, and Europe regions). CloudFront can only route to multiple origins based on content type and not on the basis of the price class.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html</a></p>\n",
                "options": [
                    {
                        "id": 9894,
                        "content": "<p>CloudFront can route to multiple origins based on the price class</p>",
                        "isValid": false
                    },
                    {
                        "id": 9895,
                        "content": "<p>Use an origin group with primary and secondary origins to configure CloudFront for high-availability and failover</p>",
                        "isValid": true
                    },
                    {
                        "id": 9896,
                        "content": "<p>CloudFront can route to multiple origins based on the content type</p>",
                        "isValid": true
                    },
                    {
                        "id": 9897,
                        "content": "<p>Use KMS encryption in CloudFront to protect sensitive data for specific content</p>",
                        "isValid": false
                    },
                    {
                        "id": 9898,
                        "content": "<p>Use geo restriction to configure CloudFront for high-availability and failover</p>",
                        "isValid": false
                    },
                    {
                        "id": 9899,
                        "content": "<p>Use field level encryption in CloudFront to protect sensitive data for specific content</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2369,
            "attributes": {
                "createdAt": "2023-09-09T20:33:54.331Z",
                "updatedAt": "2023-09-09T20:33:54.331Z",
                "content": "<p>A CRM company has a SaaS (Software as a Service) application that feeds updates to other in-house and third-party applications. The SaaS application and the in-house applications are being migrated to use AWS services for this inter-application communication.</p>\n\n<p>As a Solutions Architect, which of the following would you suggest to asynchronously decouple the architecture?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon EventBridge to decouple the system architecture</strong> - Both Amazon EventBridge and Amazon SNS can be used to develop event-driven applications, but for this use case, EventBridge is the right fit.</p>\n\n<p>Amazon EventBridge is recommended when you want to build an application that reacts to events from SaaS applications and/or AWS services. Amazon EventBridge is the only event-based service that integrates directly with third-party SaaS partners. Amazon EventBridge also automatically ingests events from over 90 AWS services without requiring developers to create any resources in their account. Further, Amazon EventBridge uses a defined JSON-based structure for events and allows you to create rules that are applied across the entire event body to select events to forward to a target. Amazon EventBridge currently supports over 15 AWS services as targets, including AWS Lambda, Amazon SQS, Amazon SNS, and Amazon Kinesis Streams and Firehose, among others. At launch, Amazon EventBridge is has limited throughput (see Service Limits) which can be increased upon request, and typical latency of around half a second.</p>\n\n<p>How EventBridge works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q35-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/eventbridge/\">https://aws.amazon.com/eventbridge/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Simple Notification Service (SNS) to communicate between systems and decouple the architecture</strong> - As discussed above, SNS can be used for event-based services. But, our use case needs integration with third-party SaaS services, hence EventBridge is the right choice, as SNS does not support third-party services integration.</p>\n\n<p><strong>Use Amazon Simple Queue Service (SQS) to decouple the architecture</strong> - SQS is a message queuing service from amazon and works well for decoupling applications. It does not directly integrate with third-party SaaS services.</p>\n\n<p><strong>Use Elastic Load Balancing for effective decoupling of system architecture</strong> - Elastic Load Balancing offers a synchronous decoupling of applications, which is not the right fit for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/eventbridge/\">https://aws.amazon.com/eventbridge/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n",
                "options": [
                    {
                        "id": 9900,
                        "content": "<p>Use Amazon Simple Queue Service (SQS) to decouple the architecture</p>",
                        "isValid": false
                    },
                    {
                        "id": 9901,
                        "content": "<p>Use Elastic Load Balancing for effective decoupling of system architecture</p>",
                        "isValid": false
                    },
                    {
                        "id": 9902,
                        "content": "<p>Use Amazon Simple Notification Service (SNS) to communicate between systems and decouple the architecture</p>",
                        "isValid": false
                    },
                    {
                        "id": 9903,
                        "content": "<p>Use Amazon EventBridge to decouple the system architecture</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2370,
            "attributes": {
                "createdAt": "2023-09-09T20:33:54.415Z",
                "updatedAt": "2023-09-09T20:33:54.415Z",
                "content": "<p>The engineering team at a leading e-commerce company is anticipating a surge in the traffic because of a flash sale planned for the weekend. You have estimated the web traffic to be 10x. The content of your website is highly dynamic and changes very often.</p>\n\n<p>As a Solutions Architect, which of the following options would you recommend to make sure your infrastructure scales for that day?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use an Auto Scaling Group</strong> - An Auto Scaling group (ASG) contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service.</p>\n\n<p>The size of an Auto Scaling group depends on the number of instances that you set as the desired capacity. You can adjust its size to meet demand, either manually or by using automatic scaling.</p>\n\n<p>An Auto Scaling group starts by launching enough instances to meet its desired capacity. It maintains this number of instances by performing periodic health checks on the instances in the group. The Auto Scaling group continues to maintain a fixed number of instances even if an instance becomes unhealthy. If an instance becomes unhealthy, the group terminates the unhealthy instance and launches another instance to replace it.</p>\n\n<p>ASG is the correct answer here.</p>\n\n<p>Incorrect option:</p>\n\n<p><strong>Use a CloudFront distribution in front of your website</strong> - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. You can use Amazon CloudFront to improve the performance of your website. CloudFront makes your website files (such as HTML, images, and video) available from data centers around the world (called edge locations). When a visitor requests a file from your website, CloudFront automatically redirects the request to a copy of the file at the nearest edge location. This results in faster download times than if the visitor had requested the content from a data center that is located farther away.</p>\n\n<p>CloudFront is not a good solution here as the content is highly dynamic, and CloudFront will cache things.</p>\n\n<p><strong>Deploy the website on S3</strong> - You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document. Depending on your website requirements, you can also configure other options, including redirects, web traffic logging, and custom error documents.</p>\n\n<p>Dynamic applications cannot be deployed to S3. This option has been added as a distractor.</p>\n\n<p><strong>Use a Route53 Multi Value record</strong> - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Use Multi Value answer routing policy when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random. Route 53 does not help in scaling your application. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p>\n",
                "options": [
                    {
                        "id": 9904,
                        "content": "<p>Use a CloudFront distribution in front of your website</p>",
                        "isValid": false
                    },
                    {
                        "id": 9905,
                        "content": "<p>Deploy the website on S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9906,
                        "content": "<p>Use an Auto Scaling Group</p>",
                        "isValid": true
                    },
                    {
                        "id": 9907,
                        "content": "<p>Use a Route53 Multi Value record</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2371,
            "attributes": {
                "createdAt": "2023-09-09T20:33:54.494Z",
                "updatedAt": "2023-09-09T20:33:54.494Z",
                "content": "<p>The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\".</p>\n\n<p>As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Neptune</strong> - Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security.</p>\n\n<p>Amazon Neptune is highly available, with read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across Availability Zones. Neptune is secure with support for HTTPS encrypted client connections and encryption at rest. Neptune is fully managed, so you no longer need to worry about database management tasks such as hardware provisioning, software patching, setup, configuration, or backups.</p>\n\n<p>Amazon Neptune can quickly and easily process large sets of user-profiles and interactions to build social networking applications. Neptune enables highly interactive graph queries with high throughput to bring social features into your applications. For example, if you are building a social feed into your application, you can use Neptune to provide results that prioritize showing your users the latest updates from their family, from friends whose updates they ‘Like,’ and from friends who live close to them.</p>\n\n<p>Social Networking example with Neptune:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q11-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/neptune/\">https://aws.amazon.com/neptune/</a></p>\n\n<p>Identity graphs example with Neptune:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q11-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/neptune/\">https://aws.amazon.com/neptune/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon ElasticSearch</strong> - Elasticsearch is a search engine based on the Lucene library. Amazon Elasticsearch Service is a fully managed service that makes it easy for you to deploy, secure, and run Elasticsearch cost-effectively at scale. You can build, monitor, and troubleshoot your applications using the tools you love, at the scale you need. The service provides support for open-source Elasticsearch APIs, managed Kibana, integration with Logstash and other AWS services, and built-in alerting and SQL querying.</p>\n\n<p><strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. The given use-case is not about data warehousing, so this is not a correct option.</p>\n\n<p><strong>Amazon Aurora</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database. Here, we need a graph database due to the highly connected datasets and queries, therefore Neptune is the best answer</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/neptune/\">https://aws.amazon.com/neptune/</a></p>\n",
                "options": [
                    {
                        "id": 9908,
                        "content": "<p>Amazon Redshift</p>",
                        "isValid": false
                    },
                    {
                        "id": 9909,
                        "content": "<p>Amazon Aurora</p>",
                        "isValid": false
                    },
                    {
                        "id": 9910,
                        "content": "<p>Amazon ElasticSearch</p>",
                        "isValid": false
                    },
                    {
                        "id": 9911,
                        "content": "<p>Amazon Neptune</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2372,
            "attributes": {
                "createdAt": "2023-09-09T20:33:54.598Z",
                "updatedAt": "2023-09-09T20:33:54.598Z",
                "content": "<p>A company has grown from a small startup to an enterprise employing over 1000 people. As the team size has grown, the company has recently observed some strange behavior, with S3 buckets settings being changed regularly.</p>\n\n<p>How can you figure out what's happening without restricting the rights of the users?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use CloudTrail to analyze API calls</strong> - AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services.</p>\n\n<p>In general, to analyze any API calls made within an AWS account, CloudTrail is used. You can record the actions that are taken by users, roles, or AWS services on Amazon S3 resources and maintain log records for auditing and compliance purposes. To do this, you can use server access logging, AWS CloudTrail logging, or a combination of both. AWS recommends that you use AWS CloudTrail for logging bucket and object-level actions for your Amazon S3 resources.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Implement an IAM policy to forbid users to change S3 bucket settings</strong> - You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, AWS Organizations SCPs, ACLs, and session policies.</p>\n\n<p>Implementing an IAM policy to forbid users would be disruptive and wouldn't go unnoticed.</p>\n\n<p><strong>Use S3 access logs to analyze user access using Athena</strong> - Amazon S3 server access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill. AWS recommends that you use AWS CloudTrail for logging bucket and object-level actions for your Amazon S3 resources, as it provides more options to store, analyze and act on the log information.</p>\n\n<p><strong>Implement a bucket policy requiring MFA for all operations</strong> - Amazon S3 supports MFA-protected API access, a feature that can enforce multi-factor authentication (MFA) for access to your Amazon S3 resources. Multi-factor authentication provides an extra level of security that you can apply to your AWS environment. It is a security feature that requires users to prove the physical possession of an MFA device by providing a valid MFA code. Changing the bucket policy to require MFA would not go unnoticed.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-7\">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-7</a></p>\n",
                "options": [
                    {
                        "id": 9912,
                        "content": "<p>Use S3 access logs to analyze user access using Athena</p>",
                        "isValid": false
                    },
                    {
                        "id": 9913,
                        "content": "<p>Implement a bucket policy requiring MFA for all operations</p>",
                        "isValid": false
                    },
                    {
                        "id": 9914,
                        "content": "<p>Implement an IAM policy to forbid users to change S3 bucket settings</p>",
                        "isValid": false
                    },
                    {
                        "id": 9915,
                        "content": "<p>Use CloudTrail to analyze API calls</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2373,
            "attributes": {
                "createdAt": "2023-09-09T20:33:54.693Z",
                "updatedAt": "2023-09-09T20:33:54.693Z",
                "content": "<p>What does this CloudFormation snippet do? (Select three)</p>\n\n<pre><code>SecurityGroupIngress:\n     - IpProtocol: tcp\n       FromPort: 80\n       ToPort: 80\n       CidrIp: 0.0.0.0/0\n     - IpProtocol: tcp\n       FromPort: 22\n       ToPort: 22\n       CidrIp: 192.168.1.1/32\n\n</code></pre>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>It allows any IP to pass through on the HTTP port</strong></p>\n\n<p><strong>It configures a security group's inbound rules</strong></p>\n\n<p><strong>It lets traffic flow from one IP on port 22</strong></p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.</p>\n\n<p>The following are the characteristics of security group rules:\n    1. By default, security groups allow all outbound traffic.\n    2. Security group rules are always permissive; you can't create rules that deny access.\n    3. Security groups are stateful</p>\n\n<p>AWS CloudFormation provides a common language for you to model and provision AWS and third-party application resources in your cloud environment. AWS CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This gives you a single source of truth for your AWS and third-party resources.</p>\n\n<p>Considering the given CloudFormation snippet, <code>0.0.0.0/0</code> means any IP, not the IP <code>0.0.0.0</code>. Ingress means traffic going into your instance, and Security Groups are different from NACL. Each \"-\" in our security group rule represents a different rule (YAML syntax)</p>\n\n<p>Therefore the CloudFormation snippet creates two Security Group inbound rules that allow any IP to pass through on the HTTP port and lets traffic flow from one source IP (192.168.1.1) on port 22.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>It configures an NACL's inbound rules</strong> - A Network Access Control List (NACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups to add an additional layer of security to your VPC.</p>\n\n<p><strong>It only allows the IP <code>0.0.0.0</code> to reach HTTP</strong></p>\n\n<p><strong>It prevents traffic from reaching on HTTP unless from the IP <code>192.168.1.1</code></strong></p>\n\n<p><strong>It configures a security group's outbound rules</strong></p>\n\n<p>These three options contradict the description provided above. So these are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n",
                "options": [
                    {
                        "id": 9916,
                        "content": "<p>It prevents traffic from reaching on HTTP unless from the IP <code>192.168.1.1</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 9917,
                        "content": "<p>It lets traffic flow from one IP on port 22</p>",
                        "isValid": true
                    },
                    {
                        "id": 9918,
                        "content": "<p>It configures a security group's outbound rules</p>",
                        "isValid": false
                    },
                    {
                        "id": 9919,
                        "content": "<p>It only allows the IP <code>0.0.0.0</code> to reach HTTP</p>",
                        "isValid": false
                    },
                    {
                        "id": 9920,
                        "content": "<p>It configures a security group's inbound rules</p>",
                        "isValid": true
                    },
                    {
                        "id": 9921,
                        "content": "<p>It allows any IP to pass through on the HTTP port</p>",
                        "isValid": true
                    },
                    {
                        "id": 9922,
                        "content": "<p>It configures an NACL's inbound rules</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2374,
            "attributes": {
                "createdAt": "2023-09-09T20:33:54.798Z",
                "updatedAt": "2023-09-09T20:33:54.798Z",
                "content": "<p>The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using Lambda as a backbone for this architecture.</p>\n\n<p>As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>By default, Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once a Lambda function is VPC-enabled, it will need a route through a NAT gateway in a public subnet to access public resources</strong> - Lambda functions always operate from an AWS-owned VPC. By default, your function has the full ability to make network requests to any public internet address — this includes access to any of the public AWS APIs. For example, your function can interact with AWS DynamoDB APIs to PutItem or Query for records. You should only enable your functions for VPC access when you need to interact with a private resource located in a private subnet. An RDS instance is a good example.</p>\n\n<p>Once your function is VPC-enabled, all network traffic from your function is subject to the routing rules of your VPC/Subnet. If your function needs to interact with a public resource, you will need a route through a NAT gateway in a public subnet.</p>\n\n<p>When to VPC-Enable a Lambda Function:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q25-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/\">https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/</a></p>\n\n<p><strong>Since Lambda functions can scale extremely quickly, its a good idea to deploy a CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold</strong> - Since Lambda functions can scale extremely quickly, this means you should have controls in place to notify you when you have a spike in concurrency. A good idea is to deploy a CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds your threshold. You should create an AWS Budget so you can monitor costs on a daily basis.</p>\n\n<p><strong>If you intend to reuse code in more than one Lambda function, you should consider creating a Lambda Layer for the reusable code</strong> - You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package. Layers let you keep your deployment package small, which makes development easier. A function can use up to 5 layers at a time.</p>\n\n<p>You can create layers, or use layers published by AWS and other AWS customers. Layers support resource-based policies for granting layer usage permissions to specific AWS accounts, AWS Organizations, or all accounts. The total unzipped size of the function and all layers can't exceed the unzipped deployment package size limit of 250 MB.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of Lambda functions</strong> - Lambda allocates compute power in proportion to the memory you allocate to your function. This means you can over-provision memory to run your functions faster and potentially reduce your costs. However, AWS recommends that you should not over provision your function time out settings. Always understand your code performance and set a function time out accordingly. Overprovisioning function timeout often results in Lambda functions running longer than expected and unexpected costs.</p>\n\n<p><strong>The bigger your deployment package, the slower your Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual Lambda package</strong> - This statement is incorrect and acts as a distractor. All the dependencies are also packaged into the single Lambda deployment package.</p>\n\n<p><strong>Serverless architecture and containers complement each other but you cannot package and deploy Lambda functions as container images</strong> - This statement is incorrect. You can now package and deploy Lambda functions as container images.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/\">https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/\">https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/</a></p>\n",
                "options": [
                    {
                        "id": 9923,
                        "content": "<p>If you intend to reuse code in more than one Lambda function, you should consider creating a Lambda Layer for the reusable code</p>",
                        "isValid": true
                    },
                    {
                        "id": 9924,
                        "content": "<p>Serverless architecture and containers complement each other but you cannot package and deploy Lambda functions as container images</p>",
                        "isValid": false
                    },
                    {
                        "id": 9925,
                        "content": "<p>The bigger your deployment package, the slower your Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual Lambda package</p>",
                        "isValid": false
                    },
                    {
                        "id": 9926,
                        "content": "<p>Since Lambda functions can scale extremely quickly, it's a good idea to deploy a CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold</p>",
                        "isValid": true
                    },
                    {
                        "id": 9927,
                        "content": "<p>By default, Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once a Lambda function is VPC-enabled, it will need a route through a NAT gateway in a public subnet to access public resources</p>",
                        "isValid": true
                    },
                    {
                        "id": 9928,
                        "content": "<p>Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of Lambda functions</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2375,
            "attributes": {
                "createdAt": "2023-09-09T20:33:54.920Z",
                "updatedAt": "2023-09-09T20:33:54.920Z",
                "content": "<p>An e-commerce company has copied 1 PB of data from its on-premises data center to an Amazon S3 bucket in the us-west-1 Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another S3 bucket in the us-east-1 Region. The on-premises data center does not allow the use of AWS Snowball.</p>\n\n<p>As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Copy data from the source bucket to the destination bucket using the aws S3 sync command</strong></p>\n\n<p>The aws S3 sync command uses the CopyObject APIs to copy objects between S3 buckets. The sync command lists the source and target buckets to identify objects that are in the source bucket but that aren't in the target bucket. The command also identifies objects in the source bucket that have different LastModified dates than the objects that are in the target bucket. The sync command on a versioned bucket copies only the current version of the object—previous versions aren't copied. By default, this preserves object metadata, but the access control lists (ACLs) are set to FULL_CONTROL for your AWS account, which removes any additional ACLs. If the operation fails, you can run the sync command again without duplicating previously copied objects.</p>\n\n<p>You can use the command like so:</p>\n\n<p><code>aws s3 sync s3://DOC-EXAMPLE-BUCKET-SOURCE s3://DOC-EXAMPLE-BUCKET-TARGET</code></p>\n\n<p><strong>Set up S3 batch replication to copy objects across S3 buckets in another Region using S3 console and then delete the replication configuration</strong></p>\n\n<p>S3 Batch Replication provides you a way to replicate objects that existed before a replication configuration was in place, objects that have previously been replicated, and objects that have failed replication. This is done through the use of a Batch Operations job.</p>\n\n<p>You should note that batch replication differs from live replication which continuously and automatically replicates new objects across Amazon S3 buckets. You cannot directly use the AWS S3 console to configure cross-Region replication for existing objects. By default, replication only supports copying new Amazon S3 objects after it is enabled using the AWS S3 console. Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. Object may be replicated to a single destination bucket or multiple destination buckets. Destination buckets can be in different AWS Regions or within the same Region as the source bucket. Once done, you can delete the replication configuration, as it ensures that batch replication is only used for this one-time data copy operation.</p>\n\n<p>If you want to enable live replication for existing objects for your bucket, you must contact AWS Support and raise a support ticket. This is required to ensure that replication is configured correctly.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Snowball Edge device to copy the data from one Region to another Region</strong> - As the given requirement is about copying the data from one AWS Region to another AWS Region, so Snowball Edge cannot be used here. Snowball Edge Storage Optimized is the optimal data transfer choice if you need to securely and quickly transfer terabytes to petabytes of data to AWS. You can use Snowball Edge Storage Optimized if you have a large backlog of data to transfer or if you frequently collect data that needs to be transferred to AWS and your storage is in an area where high-bandwidth internet connections are not available or cost-prohibitive. Snowball Edge can operate in remote locations or harsh operating environments, such as factory floors, oil and gas rigs, mining sites, hospitals, and on moving vehicles.</p>\n\n<p><strong>Copy data from the source S3 bucket to a target S3 bucket using the S3 console</strong> - AWS S3 console cannot be used to copy 1PB of data from one bucket to another as it's not feasible. You should note that this option is different from using the replication options on the AWS console, since here you are using the copy and paste options provided on the AWS console, which is suggested for small or medium data volume. You should use S3 sync for the requirement of one-time copy of data.</p>\n\n<p><strong>Set up S3 Transfer Acceleration to copy objects across S3 buckets in different Regions using S3 console</strong> - S3 Transfer Acceleration is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. You cannot use Transfer Acceleration to copy objects across S3 buckets in different Regions using S3 console.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/move-objects-s3-bucket/\">https://aws.amazon.com/premiumsupport/knowledge-center/move-objects-s3-bucket/</a></p>\n\n<p><a href=\"https://aws.amazon.com/snowball/faqs/\">https://aws.amazon.com/snowball/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html</a></p>\n",
                "options": [
                    {
                        "id": 9929,
                        "content": "<p>Copy data from the source bucket to the destination bucket using the aws S3 sync command</p>",
                        "isValid": true
                    },
                    {
                        "id": 9930,
                        "content": "<p>Set up S3 batch replication to copy objects across S3 buckets in another Region using S3 console and then delete the replication configuration</p>",
                        "isValid": true
                    },
                    {
                        "id": 9931,
                        "content": "<p>Use Snowball Edge device to copy the data from one Region to another Region</p>",
                        "isValid": false
                    },
                    {
                        "id": 9932,
                        "content": "<p>Set up S3 Transfer Acceleration to copy objects across S3 buckets in different Regions using S3 console</p>",
                        "isValid": false
                    },
                    {
                        "id": 9933,
                        "content": "<p>Copy data from the source S3 bucket to a target S3 bucket using the S3 console</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2376,
            "attributes": {
                "createdAt": "2023-09-09T20:33:54.995Z",
                "updatedAt": "2023-09-09T20:33:54.995Z",
                "content": "<p>A developer in your company has set up a classic 2 tier architecture consisting of an Application Load Balancer and an Auto Scaling group (ASG) managing a fleet of EC2 instances. The ALB is deployed in a subnet of size <code>10.0.1.0/24</code> and the ASG is deployed in a subnet of size <code>10.0.4.0/22</code>.</p>\n\n<p>As a solutions architect, you would like to adhere to the security pillar of the well-architected framework. How do you configure the security group of the EC2 instances to only allow traffic coming from the ALB?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>An Auto Scaling group (ASG) contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service.</p>\n\n<p><strong>Add a rule to authorize the security group of the ALB</strong></p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When deciding to allow traffic to reach an instance, all the rules from all the security groups that are associated with the instance are evaluated.</p>\n\n<p>The following are the characteristics of security group rules:\n    1. By default, security groups allow all outbound traffic.\n    2. Security group rules are always permissive; you can't create rules that deny access.\n    3. Security groups are stateful</p>\n\n<p>Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.</p>\n\n<p>Incorrect option:</p>\n\n<p><strong>Add a rule to authorize the CIDR <code>10.0.4.0/22</code></strong></p>\n\n<p><strong>Add a rule to authorize the security group of the ASG</strong></p>\n\n<p><strong>Add a rule to authorize the CIDR <code>10.0.1.0/24</code></strong></p>\n\n<p>Adding the entire CIDR of the ALB would work, but wouldn't guarantee that only the ALB can access the EC2 instances that are part of the ASG. Here, the right solution is to add a rule on the ASG security group to allow incoming traffic only from the security group configured for the ALB.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 9934,
                        "content": "<p>Add a rule to authorize the CIDR <code>10.0.1.0/24</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 9935,
                        "content": "<p>Add a rule to authorize the security group of the ALB</p>",
                        "isValid": true
                    },
                    {
                        "id": 9936,
                        "content": "<p>Add a rule to authorize the security group of the ASG</p>",
                        "isValid": false
                    },
                    {
                        "id": 9937,
                        "content": "<p>Add a rule to authorize the CIDR <code>10.0.4.0/22</code></p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2377,
            "attributes": {
                "createdAt": "2023-09-09T20:33:55.075Z",
                "updatedAt": "2023-09-09T20:33:55.075Z",
                "content": "<p>A social media company wants the capability to dynamically alter the size of a geographic area from which traffic is routed to a specific server resource.</p>\n\n<p>Which feature of Route 53 can help achieve this functionality?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Geoproximity routing</strong> - Geoproximity routing lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources. You can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource.</p>\n\n<p>To optionally change the size of the geographic region from which Route 53 routes traffic to a resource, specify the applicable value for the bias:\n1. To expand the size of the geographic region from which Route 53 routes traffic to a resource, specify a positive integer from 1 to 99 for the bias. Route 53 shrinks the size of adjacent regions.</p>\n\n<ol>\n<li>To shrink the size of the geographic region from which Route 53 routes traffic to a resource, specify a negative bias of -1 to -99. Route 53 expands the size of adjacent regions.</li>\n</ol>\n\n<p>More on how bias works in Geoproximity routing:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q61-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Geolocation routing</strong> - Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region.</p>\n\n<p>When you use geolocation routing, you can localize your content and present some or all of your website in the language of your users. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights. Another possible use is for balancing load across endpoints in a predictable, easy-to-manage way so that each user location is consistently routed to the same endpoint.</p>\n\n<p><strong>Latency-based routing</strong> - If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency.</p>\n\n<p>To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server.</p>\n\n<p><strong>Weighted routing</strong> - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software.</p>\n\n<p>To configure weighted routing, you create records that have the same name and type for each of your resources. You assign each record a relative weight that corresponds with how much traffic you want to send to each resource. Amazon Route 53 sends traffic to a resource based on the weight that you assign to the record as a proportion of the total weight for all records in the group</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p>\n",
                "options": [
                    {
                        "id": 9938,
                        "content": "<p>Geolocation routing</p>",
                        "isValid": false
                    },
                    {
                        "id": 9939,
                        "content": "<p>Weighted routing</p>",
                        "isValid": false
                    },
                    {
                        "id": 9940,
                        "content": "<p>Geoproximity routing</p>",
                        "isValid": true
                    },
                    {
                        "id": 9941,
                        "content": "<p>Latency-based routing</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2378,
            "attributes": {
                "createdAt": "2023-09-09T20:33:55.214Z",
                "updatedAt": "2023-09-09T20:33:55.214Z",
                "content": "<p>You are working as an AWS architect for a weather tracking facility. You are asked to set up a Disaster Recovery (DR) mechanism with minimum costs. In case of failure, the facility can only bear data loss of a few minutes without jeopardizing the forecasting models.</p>\n\n<p>As a Solutions Architect, which DR method will you suggest?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Pilot Light</strong> - The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in the cloud. The idea of the pilot light is an analogy that comes from the gas heater. In a gas heater, a small flame that’s always on can quickly ignite the entire furnace to heat up a house. This scenario is similar to a backup-and-restore scenario. For example, with AWS you can maintain a pilot light by configuring and running the most critical core elements of your system in AWS. For the given use-case, a small part of the backup infrastructure is always running simultaneously syncing mutable data (such as databases or documents) so that there is no loss of critical data. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core. For Pilot light, RPO is in minutes, so this is the correct solution.</p>\n\n<p>Four Disaster Recovery scenarios:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html\">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Backup and Restore</strong> - In most traditional environments, data is backed up to tape and sent off-site regularly. If you use this method, it can take a long time to restore your system in the event of a disruption or disaster. Amazon S3 is an ideal destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network and is therefore accessible from any location. There are many commercial and open-source backup solutions that integrate with Amazon S3. You can use AWS Import/Export to transfer very large data sets by shipping storage devices directly to AWS. For longer-term data storage where retrieval times of several hours are adequate, there is Amazon Glacier, which has the same durability model as Amazon S3. Amazon Glacier is a low-cost alternative starting from $0.01/GB per month. Amazon Glacier and Amazon S3 can be used in conjunction to produce a tiered backup solution. Even though Backup and Restore method is cheaper, it has an RPO in hours, so this option is not the right fit.</p>\n\n<p><strong>Warm Standby</strong> - The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud. A warm standby solution extends the pilot light elements and preparation. It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems on AWS and have them always on. This option is more costly compared to Pilot Light.</p>\n\n<p><strong>Multi-Site</strong> - A multi-site solution runs on AWS as well as on your existing on-site infrastructure in an active-active configuration. The data replication method that you employ will be determined by the recovery point that you choose, either Recovery Time Objective (the maximum allowable downtime before degraded operations are restored) or Recovery Point Objective (the maximum allowable time window whereby you will accept the loss of transactions during the DR process). This option is more costly compared to Pilot Light.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/\">https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html\">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/disaster-recovery-dr-objectives.html\">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/disaster-recovery-dr-objectives.html</a></p>\n",
                "options": [
                    {
                        "id": 9942,
                        "content": "<p>Pilot Light</p>",
                        "isValid": true
                    },
                    {
                        "id": 9943,
                        "content": "<p>Backup and Restore</p>",
                        "isValid": false
                    },
                    {
                        "id": 9944,
                        "content": "<p>Multi-Site</p>",
                        "isValid": false
                    },
                    {
                        "id": 9945,
                        "content": "<p>Warm Standby</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2379,
            "attributes": {
                "createdAt": "2023-09-09T20:33:55.307Z",
                "updatedAt": "2023-09-09T20:33:55.307Z",
                "content": "<p>A leading e-commerce company runs its IT infrastructure on AWS Cloud. The company has a batch job running at 7 am daily on an RDS database. It processes shipping orders for the past day, and usually gets around 2000 records that need to be processed sequentially in a batch job via a shell script. The processing of each record takes about 3 seconds.</p>\n\n<p>What platform do you recommend to run this batch job?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EC2</strong></p>\n\n<p>Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers. Amazon EC2’s simple web service interface allows you to obtain and configure capacity with minimal friction. It provides you with complete control of your computing resources and lets you run on Amazon’s proven computing environment. AWS Batch can be used to plan, schedule, and execute your batch computing workloads on Amazon EC2 Instances. Amazon EC2 is the right choice as it can accommodate batch processing and run customized scripts, as is the needed requirement.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Glue</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. Glue is for performing ETL, but cannot run custom shell scripts and hence not the right choice here.</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service.  KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. However, Kinesis works great with real-time data, we are looking at batch processing, so Kinesis is not an option.</p>\n\n<p><strong>AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. AWS Lambda functions can be configured to run up to 15 minutes per execution. You can set the timeout to any value between 1 second and 15 minutes. The total runtime for the given use-case is 100 minutes (2000*3=6000 seconds = 100 minutes) but the Lambda would time out after 15 minutes, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/features/\">https://aws.amazon.com/ec2/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/lambda/faqs/\">https://aws.amazon.com/lambda/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 9946,
                        "content": "<p>Amazon EC2</p>",
                        "isValid": true
                    },
                    {
                        "id": 9947,
                        "content": "<p>AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 9948,
                        "content": "<p>AWS Glue</p>",
                        "isValid": false
                    },
                    {
                        "id": 9949,
                        "content": "<p>Amazon Kinesis Data Streams</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2380,
            "attributes": {
                "createdAt": "2023-09-09T20:33:55.389Z",
                "updatedAt": "2023-09-09T20:33:55.389Z",
                "content": "<p>An e-commerce company wants to migrate its on-premises application to AWS. The application consists of application servers and a Microsoft SQL Server database. The solution should result in the maximum possible availability for the database layer while minimizing operational and management overhead.</p>\n\n<p>As a solutions architect, which of the following would you recommend to meet the given requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment</strong></p>\n\n<p>Amazon RDS supports Multi-AZ deployments for Microsoft SQL Server by using either SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs). Amazon RDS monitors and maintains the health of your Multi-AZ deployment. If problems occur, RDS automatically repairs unhealthy DB instances, reestablishes synchronization, and initiates failovers.</p>\n\n<p>Multi-AZ deployments provide increased availability, data durability, and fault tolerance for DB instances. In the event of planned database maintenance or unplanned service disruption, Amazon RDS automatically fails over to the up-to-date secondary DB instance. This functionality lets database operations resume quickly without manual intervention. The primary and standby instances use the same endpoint, whose physical network address transitions to the secondary replica as part of the failover process. You don't have to reconfigure your application when a failover occurs.</p>\n\n<p>This option provides the maximum possible availability for the database layer while minimizing operational and management overhead.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the data to EC2 instance hosted SQL Server database. Deploy the EC2 instances in a Multi-AZ configuration</strong> - Hosting SQL Server database on EC2 instance involves significant operational and management overhead in terms of OS patching, database patching, etc. So this option is incorrect.</p>\n\n<p><strong>Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration</strong> - Amazon RDS Read Replicas enable you to create one or more read-only copies of your database instance within the same AWS Region or in a different AWS Region. Read replicas are used to enhance the read scalability of a database. You cannot use read replicas to improve the availability of a database. Therefore this option is incorrect.</p>\n\n<p><strong>Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment</strong> - Amazon RDS Multi-AZ deployments provide enhanced availability for database instances within a single AWS Region. There is no such thing as a cross-region Multi-AZ deployment. Hence this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/\">https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/</a></p>\n",
                "options": [
                    {
                        "id": 9950,
                        "content": "<p>Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment</p>",
                        "isValid": false
                    },
                    {
                        "id": 9951,
                        "content": "<p>Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment</p>",
                        "isValid": true
                    },
                    {
                        "id": 9952,
                        "content": "<p>Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration</p>",
                        "isValid": false
                    },
                    {
                        "id": 9953,
                        "content": "<p>Migrate the data to EC2 instance hosted SQL Server database. Deploy the EC2 instances in a Multi-AZ configuration</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2381,
            "attributes": {
                "createdAt": "2023-09-09T20:33:55.473Z",
                "updatedAt": "2023-09-09T20:33:55.473Z",
                "content": "<p>An IT company has a large number of clients opting to build their APIs by using Docker containers. To facilitate the hosting of these containers, the company is looking at various orchestration services available with AWS.</p>\n\n<p>As a Solutions Architect, which of the following solutions will you suggest? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use Amazon EKS with AWS Fargate for serverless orchestration of the containerized services</strong></p>\n\n<p><strong>Use Amazon ECS with AWS Fargate for serverless orchestration of the containerized services</strong></p>\n\n<p>Building APIs with Docker containers has been gaining momentum over the years. For hosting and exposing these container-based APIs, they need a solution which supports HTTP requests routing, autoscaling, and high availability. In some cases, user authorization is also needed.</p>\n\n<p>For this purpose, many organizations are orchestrating their containerized services with Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS), while hosting their containers on Amazon EC2 or AWS Fargate. Then, they can add scalability and high availability with Service Auto Scaling (in Amazon ECS) or Horizontal Pod Auto Scaler (in Amazon EKS), and they expose the services through load balancers.</p>\n\n<p>When you use Amazon ECS as an orchestrator (with EC2 or Fargate launch type), you also have the option to expose your services with Amazon API Gateway and AWS Cloud Map instead of a load balancer. AWS Cloud Map is used for service discovery: no matter how Amazon ECS tasks scale, AWS Cloud Map service names would point to the right set of Amazon ECS tasks. Then, API Gateway HTTP APIs can be used to define API routes and point them to the corresponding AWS Cloud Map services.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon ECS with Amazon EC2 for serverless orchestration of the containerized services</strong> - Amazon EC2 can be used to host the container services. EC2 needs hosting and management of the instance, hence does not come under serverless solution. Fargate can be used for serverless container solutions.</p>\n\n<p><strong>Use Amazon EMR for serverless orchestration of the containerized services</strong> - Amazon EMR is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data. It utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon EC2 and Amazon S3. EMR is not a docker orchestration service, as required for the use case.</p>\n\n<p><strong>Use Amazon SageMaker for serverless orchestration of the containerized services</strong> - Amazon SageMaker helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML. A powerful tool, SageMaker is not a docker orchestration service, as required for the use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/architecture/field-notes-serverless-container-based-apis-with-amazon-ecs-and-amazon-api-gateway/\">https://aws.amazon.com/blogs/architecture/field-notes-serverless-container-based-apis-with-amazon-ecs-and-amazon-api-gateway/</a></p>\n",
                "options": [
                    {
                        "id": 9954,
                        "content": "<p>Use Amazon EKS with AWS Fargate for serverless orchestration of the containerized services</p>",
                        "isValid": true
                    },
                    {
                        "id": 9955,
                        "content": "<p>Use Amazon ECS with Amazon EC2 for serverless orchestration of the containerized services</p>",
                        "isValid": false
                    },
                    {
                        "id": 9956,
                        "content": "<p>Use Amazon SageMaker for serverless orchestration of the containerized services</p>",
                        "isValid": false
                    },
                    {
                        "id": 9957,
                        "content": "<p>Use Amazon EMR for serverless orchestration of the containerized services</p>",
                        "isValid": false
                    },
                    {
                        "id": 9958,
                        "content": "<p>Use Amazon ECS with AWS Fargate for serverless orchestration of the containerized services</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2382,
            "attributes": {
                "createdAt": "2023-09-09T20:33:55.558Z",
                "updatedAt": "2023-09-09T20:33:55.558Z",
                "content": "<p>A company has developed a popular photo-sharing website using a serverless pattern on the AWS Cloud using API Gateway and AWS Lambda. The backend uses an RDS PostgreSQL database. The website is experiencing high read traffic and the Lambda functions are putting an increased read load on the RDS database.</p>\n\n<p>The architecture team is planning to increase the read throughput of the database, without changing the application's core logic. As a Solutions Architect, what do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon RDS Read Replicas</strong> - Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances.</p>\n\n<p>More on RDS Read Replicas:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q13-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon RDS Multi-AZ feature</strong> - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete.</p>\n\n<p><strong>Use Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.</p>\n\n<p><strong>Use Amazon DynamoDB</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second.</p>\n\n<p>RDS Multi-AZ helps with disaster recovery in case of an AZ failure. ElastiCache would definitely help with the read load, but would require a refactor of the application's core logic. DynamoDB with DAX would also probably help with the read load, but once again it would require a refactor of the application's core logic. Here, our only option to scale reads is to use RDS Read Replicas</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n",
                "options": [
                    {
                        "id": 9959,
                        "content": "<p>Use Amazon RDS Multi-AZ feature</p>",
                        "isValid": false
                    },
                    {
                        "id": 9960,
                        "content": "<p>Use Amazon DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 9961,
                        "content": "<p>Use Amazon RDS Read Replicas</p>",
                        "isValid": true
                    },
                    {
                        "id": 9962,
                        "content": "<p>Use Amazon ElastiCache</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2383,
            "attributes": {
                "createdAt": "2023-09-09T20:33:55.646Z",
                "updatedAt": "2023-09-09T20:33:55.646Z",
                "content": "<p>An Elastic Load Balancer has marked all the EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the EC2 instances in the web browser, he can access the website.</p>\n\n<p>What could be the reason the instances are being marked as unhealthy? (Select two)</p>",
                "answerExplanation": "<p>Correct options</p>\n\n<p><strong>The security group of the EC2 instance does not allow for traffic from the security group of the Application Load Balancer</strong></p>\n\n<p><strong>The route for the health check is misconfigured</strong></p>\n\n<p>An Application Load Balancer periodically sends requests to its registered targets to test their status. These tests are called health checks.</p>\n\n<p>Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. If a target group contains only unhealthy registered targets, the load balancer nodes route requests across its unhealthy targets.</p>\n\n<p>You must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port. Whenever you add a listener to your load balancer or update the health check port for a target group used by the load balancer to route requests, you must verify that the security groups associated with the load balancer allow traffic on the new port in both directions.</p>\n\n<p>Application Load Balancer Configuration for Security Groups and Health Check Routes:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q26-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EBS volumes have been improperly mounted</strong> - You can access the website using the IP address which means there is no issue with the EBS volumes. So this option is not correct.</p>\n\n<p><strong>Your web-app has a runtime that is not supported by the Application Load Balancer</strong> - There is no connection between a web app runtime and the application load balancer. This option has been added as a distractor.</p>\n\n<p><strong>You need to attach Elastic IP to the EC2 instances</strong> - This option is a distractor as Elastic IPs do not need to be assigned to EC2 instances while using an Application Load Balancer.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n",
                "options": [
                    {
                        "id": 9963,
                        "content": "<p>Your web-app has a runtime that is not supported by the Application Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 9964,
                        "content": "<p>The route for the health check is misconfigured</p>",
                        "isValid": true
                    },
                    {
                        "id": 9965,
                        "content": "<p>The EBS volumes have been improperly mounted</p>",
                        "isValid": false
                    },
                    {
                        "id": 9966,
                        "content": "<p>The security group of the EC2 instance does not allow for traffic from the security group of the Application Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 9967,
                        "content": "<p>You need to attach Elastic IP to the EC2 instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2384,
            "attributes": {
                "createdAt": "2023-09-09T20:33:55.754Z",
                "updatedAt": "2023-09-09T20:33:55.754Z",
                "content": "<p>A junior developer has downloaded a sample Amazon S3 bucket policy to make changes to it based on new company-wide access policies. He has requested your help in understanding this bucket policy.</p>\n\n<p>As a Solutions Architect, which of the following would you identify as the correct description for the given policy?</p>\n\n<pre><code>{\n \"Version\": \"2012-10-17\",\n \"Id\": \"S3PolicyId1\",\n \"Statement\": [\n   {\n     \"Sid\": \"IPAllow\",\n     \"Effect\": \"Allow\",\n     \"Principal\": \"*\",\n     \"Action\": \"s3:*\",\n     \"Resource\": \"arn:aws:s3:::examplebucket/*\",\n     \"Condition\": {\n        \"IpAddress\": {\"aws:SourceIp\": \"54.240.143.0/24\"},\n        \"NotIpAddress\": {\"aws:SourceIp\": \"54.240.143.188/32\"}\n     }\n   }\n ]\n}\n</code></pre>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>It authorizes an entire CIDR except one IP address to access the S3 bucket</strong> -</p>\n\n<p>You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, AWS Organizations SCPs, ACLs, and session policies.</p>\n\n<p>Let's analyze the bucket policy one step at a time:</p>\n\n<p>The snippet <code>\"Effect\": \"Allow\"</code> implies an allow effect.\nThe snippet <code>\"Principal\": \"*\"</code> implies any Principal.\nThe snippet <code>\"Action\": \"s3:*\"</code> implies any S3 API.\nThe snippet <code>\"Resource\": \"arn:aws:s3:::examplebucket/*\"</code> implies that the resource can be the bucket <code>examplebucket</code> and its contents.\nConsider the last snippet of the given bucket policy:\n<code>\"Condition\": {\n         \"IpAddress\": {\"aws:SourceIp\": \"54.240.143.0/24\"},\n         \"NotIpAddress\": {\"aws:SourceIp\": \"54.240.143.188/32\"}\n      }</code>\nThis snippet implies that if the source IP is in the CIDR block \"54.240.143.0/24\" (== 54.240.143.0 - 54.240.143.255), then it is allowed to access the <code>examplebucket</code> and its contents.\nHowever, the source IP cannot be in the CIDR \"54.240.143.188/32\" (== 1 IP, 54.240.143.188/32), which means one IP address is explicitly blocked from accessing the <code>examplebucket</code> and its contents.</p>\n\n<p>Example Bucket policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt3-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>It ensures the S3 bucket is exposing an external IP within the CIDR range specified, except one IP</strong></p>\n\n<p><strong>It ensures EC2 instances that have inherited a security group can access the bucket</strong></p>\n\n<p><strong>It authorizes an IP address and a CIDR to access the S3 bucket</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html</a></p>\n",
                "options": [
                    {
                        "id": 9968,
                        "content": "<p>It ensures the S3 bucket is exposing an external IP within the CIDR range specified, except one IP</p>",
                        "isValid": false
                    },
                    {
                        "id": 9969,
                        "content": "<p>It authorizes an IP address and a CIDR to access the S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 9970,
                        "content": "<p>It authorizes an entire CIDR except one IP address to access the S3 bucket</p>",
                        "isValid": true
                    },
                    {
                        "id": 9971,
                        "content": "<p>It ensures EC2 instances that have inherited a security group can access the bucket</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2385,
            "attributes": {
                "createdAt": "2023-09-09T20:33:55.865Z",
                "updatedAt": "2023-09-09T20:33:55.865Z",
                "content": "<p>A company wants to publish an event into an SQS queue whenever a new object is uploaded on S3.</p>\n\n<p>Which of the following statements are true regarding this functionality?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Only Standard SQS queue is allowed as an Amazon S3 event notification destination, whereas FIFO SQS queue is not allowed</strong></p>\n\n<p>The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications.</p>\n\n<p>Amazon S3 supports the following destinations where it can publish events:</p>\n\n<p>Amazon Simple Notification Service (Amazon SNS) topic</p>\n\n<p>Amazon Simple Queue Service (Amazon SQS) queue</p>\n\n<p>AWS Lambda</p>\n\n<p>Currently, the Standard SQS queue is only allowed as an Amazon S3 event notification destination, whereas the FIFO SQS queue is not allowed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Both Standard SQS queue and FIFO SQS queue are allowed as an Amazon S3 event notification destination</strong></p>\n\n<p><strong>Neither Standard SQS queue nor FIFO SQS queue is allowed as an Amazon S3 event notification destination</strong></p>\n\n<p><strong>Only FIFO SQS queue is allowed as an Amazon S3 event notification destination, whereas Standard SQS queue is not allowed</strong></p>\n\n<p>These three options contradict the details provided in the explanation above. To summarize, the Standard SQS queue is only allowed as an Amazon S3 event notification destination, whereas the FIFO SQS queue is not allowed. Hence these three options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n",
                "options": [
                    {
                        "id": 9972,
                        "content": "<p>Neither Standard SQS queue nor FIFO SQS queue are allowed as an Amazon S3 event notification destination</p>",
                        "isValid": false
                    },
                    {
                        "id": 9973,
                        "content": "<p>Only Standard SQS queue is allowed as an Amazon S3 event notification destination, whereas FIFO SQS queue is not allowed</p>",
                        "isValid": true
                    },
                    {
                        "id": 9974,
                        "content": "<p>Only FIFO SQS queue is allowed as an Amazon S3 event notification destination, whereas Standard SQS queue is not allowed</p>",
                        "isValid": false
                    },
                    {
                        "id": 9975,
                        "content": "<p>Both Standard SQS queue and FIFO SQS queue are allowed as an Amazon S3 event notification destination</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2386,
            "attributes": {
                "createdAt": "2023-09-09T20:33:55.952Z",
                "updatedAt": "2023-09-09T20:33:55.952Z",
                "content": "<p>A company wants to store business-critical data on EBS volumes which provide persistent storage independent of EC2 instances. During a test run, the development team found that on terminating an EC2 instance, the attached EBS volume was also lost, which was contrary to their assumptions.</p>\n\n<p>As a solutions architect, could you explain this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The EBS volume was configured as the root volume of the Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume</strong></p>\n\n<p>Amazon Elastic Block Store (EBS) is an easy to use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale.</p>\n\n<p>When you launch an instance, the root device volume contains the image used to boot the instance. You can choose between AMIs backed by Amazon EC2 instance store and AMIs backed by Amazon EBS.</p>\n\n<p>By default, the root volume for an AMI backed by Amazon EBS is deleted when the instance terminates. You can change the default behavior to ensure that the volume persists after the instance terminates. Non-root EBS volumes remain available even after you terminate an instance to which the volumes were attached. Therefore, this option is correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume</strong></p>\n\n<p><strong>The EBS volumes were not backed up on EFS file system storage, resulting in the loss of volume</strong></p>\n\n<p>EBS volumes do not need to back up the data on Amazon S3 or EFS filesystem. Both these options are added as distractors.</p>\n\n<p><strong>On termination of an EC2 instance, all the attached EBS volumes are always terminated</strong> - As mentioned earlier, non-root EBS volumes remain available even after you terminate an instance to which the volumes were attached. Hence this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html</a></p>\n",
                "options": [
                    {
                        "id": 9976,
                        "content": "<p>The EBS volumes were not backed up on EFS file system storage, resulting in the loss of volume</p>",
                        "isValid": false
                    },
                    {
                        "id": 9977,
                        "content": "<p>The EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume</p>",
                        "isValid": false
                    },
                    {
                        "id": 9978,
                        "content": "<p>On termination of an EC2 instance, all the attached EBS volumes are always terminated</p>",
                        "isValid": false
                    },
                    {
                        "id": 9979,
                        "content": "<p>The EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2387,
            "attributes": {
                "createdAt": "2023-09-09T20:33:56.051Z",
                "updatedAt": "2023-09-09T20:33:56.051Z",
                "content": "<p>Your company is evolving towards a microservice approach for their website. The company plans to expose the website from the same load balancer, linked to different target groups with different URLs, that are similar to these - checkout.mycorp.com, www.mycorp.com, mycorp.com/profile, and mycorp.com/search.</p>\n\n<p>As a Solutions Architect, which Load Balancer type do you recommend to achieve this routing feature with MINIMUM configuration and development effort?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an Application Load Balancer</strong></p>\n\n<p>Application Load Balancer can automatically distribute incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.</p>\n\n<p>If your application is composed of several individual services, an Application Load Balancer can route a request to a service based on the content of the request.</p>\n\n<p>Here are the different types -</p>\n\n<p>Host-based Routing:\nYou can route a client request based on the Host field of the HTTP header allowing you to route to multiple domains from the same load balancer. You can use host conditions to define rules that route requests based on the hostname in the host header (also known as host-based routing). This enables you to support multiple domains using a single load balancer. Example hostnames:\nexample.com\ntest.example.com\n*.example.com\nThe rule *.example.com matches test.example.com but doesn't match example.com.</p>\n\n<p>Path-based Routing:\nYou can route a client request based on the URL path of the HTTP header. You can use path conditions to define rules that route requests based on the URL in the request (also known as path-based routing). Example path patterns:\n/img/*\n/img/<em>/pics\nThe path pattern is used to route requests but does not alter them. For example, if a rule has a path pattern of /img/</em>, the rule would forward a request for /img/picture.jpg to the specified target group as a request for /img/picture.jpg.\nThe path pattern is applied only to the path of the URL, not to its query parameters.</p>\n\n<p>HTTP header-based routing:\nYou can route a client request based on the value of any standard or custom HTTP header.</p>\n\n<p>HTTP method-based routing:\nYou can route a client request based on any standard or custom HTTP method.</p>\n\n<p>Query string parameter-based routing:\nYou can route a client request based on query string or query parameters.</p>\n\n<p>Source IP address CIDR-based routing:\nYou can route a client request based on source IP address CIDR from where the request originates.</p>\n\n<p>Path based routing and host based routing are only available for the Application Load Balancer (ALB). Therefore this is the correct option for the given use-case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an NGINX based load balancer on an EC2 instance to have advanced routing capabilities</strong> - Although it is technically possible to set up NGINX based load balancer, however, this option involves a lot of configuration effort, so this option is ruled out for the given use-case. So,  deploying an NGINX load balancer on EC2 would work but would suffer management and scaling issues.</p>\n\n<p><strong>Create a Network Load Balancer</strong> - Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.</p>\n\n<p><strong>Create a Classic Load Balancer</strong> - Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network.</p>\n\n<p>As mentioned in the description above, these two options are incorrect for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/\">https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/</a></p>\n",
                "options": [
                    {
                        "id": 9980,
                        "content": "<p>Create an NGINX based load balancer on an EC2 instance to have advanced routing capabilities</p>",
                        "isValid": false
                    },
                    {
                        "id": 9981,
                        "content": "<p>Create an Application Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 9982,
                        "content": "<p>Create a Network Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 9983,
                        "content": "<p>Create a Classic Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2388,
            "attributes": {
                "createdAt": "2023-09-09T20:33:56.199Z",
                "updatedAt": "2023-09-09T20:33:56.199Z",
                "content": "<p>The engineering team at an online fashion retailer uses AWS Cloud to manage its technology infrastructure. The EC2 server fleet is behind an Application Load Balancer and the fleet strength is managed by an Auto Scaling group. Based on the historical data, the team is anticipating a huge traffic spike during the upcoming Thanksgiving sale.</p>\n\n<p>As an AWS solutions architect, what feature of the Auto Scaling group would you leverage so that the potential surge in traffic can be preemptively addressed?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Auto Scaling group scheduled action</strong></p>\n\n<p>The engineering team can create a scheduled action for the Auto Scaling group to pre-emptively provision additional instances for the sale duration. This makes sure that adequate instances are ready before the sale goes live.\nThe scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect, and the new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto Scaling updates the group with the values for minimum, maximum, and desired size that are specified by the scaling action.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Auto Scaling group target tracking scaling policy</strong> - With target tracking scaling policies, you choose a scaling metric and set a target value. Application Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value.</p>\n\n<p><strong>Auto Scaling group step scaling policy</strong> - With step scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process as well as define how your scalable target should be scaled when a threshold is in breach for a specified number of evaluation periods.</p>\n\n<p>Both the target tracking as well as step scaling policies entail a lag wherein the instances will be provisioned only when the underlying CloudWatch alarms go off. Therefore these two options are not pre-emptive in nature and ruled out for the given use-case.</p>\n\n<p><strong>Auto Scaling group lifecycle hook</strong> - Auto Scaling group lifecycle hooks enable you to perform custom actions as the Auto Scaling group launches or terminates instances. For example, you could install or configure software on newly launched instances, or download log files from an instance before it terminates. Lifecycle hooks cannot be used to pre-emptively provision additional instances for a specific period such as the sale duration.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p>\n",
                "options": [
                    {
                        "id": 9984,
                        "content": "<p>Auto Scaling group target tracking scaling policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 9985,
                        "content": "<p>Auto Scaling group scheduled action</p>",
                        "isValid": true
                    },
                    {
                        "id": 9986,
                        "content": "<p>Auto Scaling group lifecycle hook</p>",
                        "isValid": false
                    },
                    {
                        "id": 9987,
                        "content": "<p>Auto Scaling group step scaling policy</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2389,
            "attributes": {
                "createdAt": "2023-09-09T20:33:56.335Z",
                "updatedAt": "2023-09-09T20:33:56.335Z",
                "content": "<p>A company's cloud architect has set up a solution that uses Route 53 to configure the DNS records for the primary website with the domain pointing to the Application Load Balancer (ALB). The company wants a solution where users will be directed to a static error page, configured as a backup, in case of unavailability of the primary website.</p>\n\n<p>Which configuration will meet the company's requirements, while keeping the changes to a bare minimum?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up a Route 53 active-passive type of failover routing policy. If Route 53 health check determines the ALB endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket</strong></p>\n\n<p>Use an active-passive failover configuration when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. When responding to queries, Route 53 includes only healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only the healthy secondary resources in response to DNS queries.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a Route 53 active-active type of failover routing policy. If Route 53 health check determines the ALB endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket</strong> - This option has been added as a distractor as there is no such thing as an active-active failover routing policy in Route 53. You can configure active-active failover using any routing policy (or combination of routing policies) other than failover routing policy and you configure active-passive failover only using the failover routing policy. In active-active failover configuration, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Route 53 considers them unhealthy. Route 53 can respond to a DNS query using any healthy record.</p>\n\n<p><strong>Use Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed</strong> - If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency - this is Latency-based routing and is not helpful for the current use case.</p>\n\n<p><strong>Use Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page</strong> - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software. This is not useful for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-passive\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-passive</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency</a></p>\n",
                "options": [
                    {
                        "id": 9988,
                        "content": "<p>Use Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed</p>",
                        "isValid": false
                    },
                    {
                        "id": 9989,
                        "content": "<p>Set up a Route 53 active-active type of failover routing policy. If Route 53 health check determines the ALB endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 9990,
                        "content": "<p>Use Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page</p>",
                        "isValid": false
                    },
                    {
                        "id": 9991,
                        "content": "<p>Set up a Route 53 active-passive type of failover routing policy. If Route 53 health check determines the ALB endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2390,
            "attributes": {
                "createdAt": "2023-09-09T20:33:56.432Z",
                "updatedAt": "2023-09-09T20:33:56.432Z",
                "content": "<p>A development team has deployed a microservice to the ECS. The application layer is in a Docker container that provides both static and dynamic content through an Application Load Balancer. With increasing load, the ECS cluster is experiencing higher network usage. The development team has looked into the network usage and found that 90% of it is due to distributing static content of the application.</p>\n\n<p>As a Solutions Architect, what do you recommend to improve the application's network usage and decrease costs?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Distribute the static content through Amazon S3</strong> -</p>\n\n<p>You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you must enable website hosting, set permissions, and create and add an index document. Depending on your website requirements, you can also configure redirects, web traffic logging, and a custom error document.</p>\n\n<p>Distributing the static content through S3 allows us to offload most of the network usage to S3 and free up our applications running on ECS.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Distribute the dynamic content through Amazon S3</strong> - By contrast, a dynamic website relies on server-side processing, including server-side scripts such as PHP, JSP, or ASP.NET. Amazon S3 does not support server-side scripting, but AWS has other resources for hosting dynamic websites.</p>\n\n<p><strong>Distribute the static content through Amazon EFS</strong></p>\n\n<p><strong>Distribute the dynamic content through Amazon EFS</strong></p>\n\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Using EFS for static or dynamic content will not change anything as static content on EFS would still have to be distributed by the ECS instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p>\n",
                "options": [
                    {
                        "id": 9992,
                        "content": "<p>Distribute the dynamic content through Amazon EFS</p>",
                        "isValid": false
                    },
                    {
                        "id": 9993,
                        "content": "<p>Distribute the dynamic content through Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 9994,
                        "content": "<p>Distribute the static content through Amazon EFS</p>",
                        "isValid": false
                    },
                    {
                        "id": 9995,
                        "content": "<p>Distribute the static content through Amazon S3</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2391,
            "attributes": {
                "createdAt": "2023-09-09T20:33:56.538Z",
                "updatedAt": "2023-09-09T20:33:56.538Z",
                "content": "<p>An online gaming company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on EC2 instances running under an Application Load Balancer (ALB) with AWS WAF.</p>\n\n<p>As a solutions architect, which of the following solutions can be combined to address the given use-case? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use WAF geo match statement listing the countries that you want to block</strong></p>\n\n<p><strong>Use WAF IP set statement that specifies the IP addresses that you want to allow through</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns and rules that filter out specific traffic patterns you define.</p>\n\n<p>You can deploy AWS WAF on Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts your web servers or origin servers running on EC2, or Amazon API Gateway for your APIs.</p>\n\n<p>AWS WAF - How it Works\n<img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\">\nvia - <a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p>\n\n<p>To block specific countries, you can create a WAF geo match statement listing the countries that you want to block, and to allow traffic from IPs of the remote development team, you can create a WAF IP set statement that specifies the IP addresses that you want to allow through. You can combine the two rules as shown below:</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a deny rule for the blocked countries in the NACL associated to each of the EC2 instances</strong> - A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. NACL does not have the capability to block traffic based on geographic match conditions.</p>\n\n<p><strong>Use ALB geo match statement listing the countries that you want to block</strong></p>\n\n<p><strong>Use ALB IP set statement that specifies the IP addresses that you want to allow through</strong></p>\n\n<p>An Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.</p>\n\n<p>An ALB cannot block or allow traffic based on geographic match conditions or IP based conditions. Both these options have been added as distractors.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/\">https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/</a></p>\n",
                "options": [
                    {
                        "id": 9996,
                        "content": "<p>Use ALB IP set statement that specifies the IP addresses that you want to allow through</p>",
                        "isValid": false
                    },
                    {
                        "id": 9997,
                        "content": "<p>Use WAF geo match statement listing the countries that you want to block</p>",
                        "isValid": true
                    },
                    {
                        "id": 9998,
                        "content": "<p>Create a deny rule for the blocked countries in the NACL associated with each of the EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 9999,
                        "content": "<p>Use ALB geo match statement listing the countries that you want to block</p>",
                        "isValid": false
                    },
                    {
                        "id": 10000,
                        "content": "<p>Use WAF IP set statement that specifies the IP addresses that you want to allow through</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2392,
            "attributes": {
                "createdAt": "2023-09-09T20:33:56.666Z",
                "updatedAt": "2023-09-09T20:33:56.666Z",
                "content": "<p>A financial services company is moving its IT infrastructure to AWS Cloud and wants to enforce adequate data protection mechanisms on Amazon S3 to meet compliance guidelines. The engineering team has hired you as a solutions architect to build a solution for this requirement.</p>\n\n<p>Can you help the team identify the INCORRECT option from the choices below?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>S3 can encrypt object metadata by using Server-Side Encryption</strong></p>\n\n<p>Amazon S3 is a simple key-value store designed to store as many objects as you want. You store these objects in one or more buckets, and each object can be up to 5 TB in size.</p>\n\n<p>An object consists of the following:</p>\n\n<p>Key – The name that you assign to an object. You use the object key to retrieve the object.</p>\n\n<p>Version ID – Within a bucket, a key and version ID uniquely identify an object.</p>\n\n<p>Value – The content that you are storing.</p>\n\n<p>Metadata – A set of name-value pairs with which you can store information regarding the object.</p>\n\n<p>Subresources – Amazon S3 uses the subresource mechanism to store object-specific additional information.</p>\n\n<p>Access Control Information – You can control access to the objects you store in Amazon S3.</p>\n\n<p>Metadata, which can be included with the object, is not encrypted while being stored on Amazon S3. Therefore, AWS recommends that customers not place sensitive information in Amazon S3 metadata.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>S3 can protect data at rest using Server-Side Encryption</strong>  - This is possible and AWS provides three different ways of doing this - Server-side encryption with Amazon S3‐managed keys (SSE-S3), Server-side encryption with customer master keys stored in AWS Key Management Service (SSE-KMS), Server-side encryption with customer-provided keys (SSE-C).</p>\n\n<p><strong>S3 can protect data at rest using Client-Side Encryption</strong> - This is a possible scenario too. You can encrypt data on the client-side and upload the encrypted data to Amazon S3. In this case, the client manages the encryption process, the encryption keys, and related tools.</p>\n\n<p><strong>S3 can encrypt data in transit using HTTPS (TLS)</strong> - This is also possible and you can use HTTPS (TLS) to help prevent potential attackers from eavesdropping on or manipulating network traffic using person-in-the-middle or similar attacks.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html#server-side\">https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html#server-side</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf?did=wp_card&amp;trk=wp_card\">https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf?did=wp_card&amp;trk=wp_card</a></p>\n",
                "options": [
                    {
                        "id": 10001,
                        "content": "<p>S3 can encrypt data in transit using HTTPS (TLS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10002,
                        "content": "<p>S3 can encrypt object metadata by using Server-Side Encryption</p>",
                        "isValid": true
                    },
                    {
                        "id": 10003,
                        "content": "<p>S3 can protect data at rest using Client-Side Encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 10004,
                        "content": "<p>S3 can protect data at rest using Server-Side Encryption</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2393,
            "attributes": {
                "createdAt": "2023-09-09T20:33:56.787Z",
                "updatedAt": "2023-09-09T20:33:56.787Z",
                "content": "<p>The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connections between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs.</p>\n\n<p>As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use a transit gateway to interconnect the VPCs</strong></p>\n\n<p>A transit gateway is a network transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks.</p>\n\n<p>Transit Gateway Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q14-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html\">https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html</a></p>\n\n<p>A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Transitive Peering does not work for VPC peering connections. So, if you have a VPC peering connection between VPC A and VPC B (pcx-aaaabbbb), and between VPC A and VPC C (pcx-aaaacccc). Then, there is no VPC peering connection between VPC B and VPC C. Instead of using VPC peering, you can use an AWS Transit Gateway that acts as a network transit hub, to interconnect your VPCs or connect your VPCs with on-premises networks. Therefore this is the correct option.</p>\n\n<p>VPC Peering Connections Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q14-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-basics.html\">https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-basics.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an internet gateway to interconnect the VPCs</strong> - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. You cannot use an internet gateway to interconnect your VPCs and on-premises networks, hence this option is incorrect.</p>\n\n<p><strong>Use a VPC endpoint to interconnect the VPCs</strong> - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. You cannot use a VPC endpoint to interconnect your VPCs and on-premises networks, hence this option is incorrect.</p>\n\n<p><strong>Establish VPC peering connections between all VPCs</strong> - Establishing VPC peering between all VPCs is an inelegant and clumsy way to establish connectivity between all VPCs. Instead, you should use a Transit Gateway that acts as a network transit hub to interconnect your VPCs and on-premises networks.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\">https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html\">https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html</a></p>\n",
                "options": [
                    {
                        "id": 10005,
                        "content": "<p>Use an internet gateway to interconnect the VPCs</p>",
                        "isValid": false
                    },
                    {
                        "id": 10006,
                        "content": "<p>Establish VPC peering connections between all VPCs</p>",
                        "isValid": false
                    },
                    {
                        "id": 10007,
                        "content": "<p>Use a transit gateway to interconnect the VPCs</p>",
                        "isValid": true
                    },
                    {
                        "id": 10008,
                        "content": "<p>Use a VPC endpoint to interconnect the VPCs</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2394,
            "attributes": {
                "createdAt": "2023-09-09T20:33:56.914Z",
                "updatedAt": "2023-09-09T20:33:56.914Z",
                "content": "<p>A leading video streaming provider is migrating to AWS Cloud infrastructure for delivering its content to users across the world. The company wants to make sure that the solution supports at least a million requests per second for its EC2 server farm.</p>\n\n<p>As a solutions architect, which type of Elastic Load Balancer would you recommend as part of the solution stack?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Network Load Balancer</strong></p>\n\n<p>Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Application Load Balancer</strong> - Application Load Balancer operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.\nApplication Load Balancer is not a good fit for the low latency and high throughput scenario mentioned in the given use-case.</p>\n\n<p><strong>Classic Load Balancer</strong> - Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network. Classic Load Balancer is not a good fit for the low latency and high throughput scenario mentioned in the given use-case.</p>\n\n<p><strong>Infrastructure Load Balancer</strong> - There is no such thing as Infrastructure Load Balancer and this option just acts as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p>\n",
                "options": [
                    {
                        "id": 10009,
                        "content": "<p>Infrastructure Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 10010,
                        "content": "<p>Network Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 10011,
                        "content": "<p>Classic Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 10012,
                        "content": "<p>Application Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2395,
            "attributes": {
                "createdAt": "2023-09-09T20:33:57.008Z",
                "updatedAt": "2023-09-09T20:33:57.008Z",
                "content": "<p>A pharma company is working on developing a vaccine for the COVID-19 virus. The researchers at the company want to process the reference healthcare data in a highly available as well as HIPAA compliant in-memory database that supports caching results of SQL queries.</p>\n\n<p>As a solutions architect, which of the following AWS services would you recommend for this task?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>ElastiCache for Redis/Memcached</strong></p>\n\n<p>ElastiCache Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q16-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store. ElastiCache for Redis supports replication, high availability, and cluster sharding right out of the box.</p>\n\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database. Session stores are easy to create with Amazon ElastiCache for Memcached.</p>\n\n<p>Both ElastiCache for Redis and ElastiCache for Memcached are HIPAA Eligible. Therefore, this is the correct option.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review this comparison sheet for Redis vs Memcached features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q16-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p>Incorrect Options:</p>\n\n<p><strong>DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. DAX does not support SQL query caching.</p>\n\n<p><strong>DynamoDB</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching (via DAX) for internet-scale applications. DynamoDB is not an in-memory database, so this option is incorrect.</p>\n\n<p><strong>DocumentDB</strong> - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. DocumentDB is not an in-memory database, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-elasticache-for-redis-is-now-hipaa-eligible-to-help-you-power-secure-healthcare-applications-with-sub-millisecond-latency/\">https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-elasticache-for-redis-is-now-hipaa-eligible-to-help-you-power-secure-healthcare-applications-with-sub-millisecond-latency/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2022/08/amazon-elasticache-memcached-hipaa-eligible/\">https://aws.amazon.com/about-aws/whats-new/2022/08/amazon-elasticache-memcached-hipaa-eligible/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/automating-sql-caching-for-amazon-elasticache-and-amazon-rds/\">https://aws.amazon.com/blogs/database/automating-sql-caching-for-amazon-elasticache-and-amazon-rds/</a></p>\n",
                "options": [
                    {
                        "id": 10013,
                        "content": "<p>DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 10014,
                        "content": "<p>DynamoDB Accelerator (DAX)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10015,
                        "content": "<p>ElastiCache for Redis/Memcached</p>",
                        "isValid": true
                    },
                    {
                        "id": 10016,
                        "content": "<p>DocumentDB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2396,
            "attributes": {
                "createdAt": "2023-09-09T20:33:57.194Z",
                "updatedAt": "2023-09-09T20:33:57.194Z",
                "content": "<p>A health-care company manages its web application on Amazon EC2 instances running behind Auto Scaling group (ASG). The company provides ambulances for critical patients and needs the application to be reliable. The workload of the company can be managed on 2 EC2 instances and can peak up to 6 instances when traffic increases.</p>\n\n<p>As a Solutions Architect, which of the following configurations would you select as the best fit for these requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The ASG should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the ASG should be set to 6</strong> - You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity.</p>\n\n<p>Amazon EC2 Auto Scaling enables you to take advantage of the safety and reliability of geographic redundancy by spanning Auto Scaling groups across multiple Availability Zones within a Region. When one Availability Zone becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected Availability Zone. When the unhealthy Availability Zone returns to a healthy state, Auto Scaling automatically redistributes the application instances evenly across all of the designated Availability Zones. Since the application is extremely critical and needs to have a reliable architecture to support it, the EC2 instances should be maintained in at least two Availability Zones (AZs) for uninterrupted service.</p>\n\n<p>Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group. This is why the minimum capacity should be 4 instances and not 2. ASG will launch 2 instances each in both the AZs and this redundancy is needed to keep the service available always.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The ASG should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the ASG should be set to 6</strong></p>\n\n<p><strong>The ASG should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone</strong></p>\n\n<p>The explanation above gives the correct rationale for minimum capacity as well as the instance distribution across AZs, so both these options are incorrect.</p>\n\n<p><strong>The ASG should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the ASG should be set to 6</strong> - An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same region. However, Auto Scaling groups cannot span multiple Regions.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p>\n",
                "options": [
                    {
                        "id": 10017,
                        "content": "<p>The ASG should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the ASG should be set to 6</p>",
                        "isValid": false
                    },
                    {
                        "id": 10018,
                        "content": "<p>The ASG should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the ASG should be set to 6</p>",
                        "isValid": false
                    },
                    {
                        "id": 10019,
                        "content": "<p>The ASG should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone</p>",
                        "isValid": false
                    },
                    {
                        "id": 10020,
                        "content": "<p>The ASG should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the ASG should be set to 6</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2397,
            "attributes": {
                "createdAt": "2023-09-09T20:33:57.277Z",
                "updatedAt": "2023-09-09T20:33:57.277Z",
                "content": "<p>An Internet-of-Things (IoT) company is planning on distributing a master sensor in people's homes to measure the key metrics from its smart devices. In order to provide adjustment commands for these devices, the company would like to have a streaming system that supports ordered data based on the sensor's key, and also sustains high throughput messages (thousands of messages per second).</p>\n\n<p>As a solutions architect, which of the following AWS services would you recommend for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Kinesis Data Streams (KDS)</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service.  KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream.</p>\n\n<p>However, there are certain limits you should keep in mind while using Amazon Kinesis Data Streams:</p>\n\n<p>A Kinesis data stream stores records from 24 hours by default, up to 8760 hours (365 days).</p>\n\n<p>The maximum size of a data blob (the data payload before Base64-encoding) within one record is 1 megabyte (MB).\nEach shard can support up to 1000 PUT records per second.</p>\n\n<p>Kinesis is the right answer here, as by providing a partition key in your message, you can guarantee ordered messages for a specific sensor, even if your stream is sharded.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Queue Service (SQS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware, and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Kinesis is better for streaming data since queues aren't meant for real-time streaming of data.</p>\n\n<p><strong>Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS cannot be used for data streaming. Therefore this option is not the best fit for the given use-case.</p>\n\n<p><strong>AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used for production-grade serverless log analytics. Lambda isn't meant to retain data either. Therefore this option is not the best fit for the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 10021,
                        "content": "<p>AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 10022,
                        "content": "<p>Amazon Kinesis Data Streams (KDS)</p>",
                        "isValid": true
                    },
                    {
                        "id": 10023,
                        "content": "<p>Amazon Simple Notification Service (SNS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10024,
                        "content": "<p>Amazon Simple Queue Service (SQS)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2398,
            "attributes": {
                "createdAt": "2023-09-09T20:33:57.372Z",
                "updatedAt": "2023-09-09T20:33:57.372Z",
                "content": "<p>A big data analytics company is using Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams.</p>\n\n<p>As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Enhanced Fanout feature of Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream. You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream.</p>\n\n<p>Kinesis Data Streams Fanout\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q38-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Swap out Kinesis Data Streams with Kinesis Data Firehose</strong> -  Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Kinesis Data Firehose can only write to S3, Redshift, Elasticsearch or Splunk. You can't have applications consuming data streams from Kinesis Data Firehose, that's the job of Kinesis Data Streams. Therefore this option is not correct.</p>\n\n<p><strong>Swap out Kinesis Data Streams with SQS Standard queues</strong></p>\n\n<p><strong>Swap out Kinesis Data Streams with SQS FIFO queues</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both SQS Standard and SQS FIFO are not the right fit for the given use-case.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please understand the differences between the capabilities of Kinesis Data Streams vs SQS, as you may be asked scenario-based questions on this topic in the exam.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q38-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 10025,
                        "content": "<p>Swap out Kinesis Data Streams with SQS Standard queues</p>",
                        "isValid": false
                    },
                    {
                        "id": 10026,
                        "content": "<p>Swap out Kinesis Data Streams with Kinesis Data Firehose</p>",
                        "isValid": false
                    },
                    {
                        "id": 10027,
                        "content": "<p>Use Enhanced Fanout feature of Kinesis Data Streams</p>",
                        "isValid": true
                    },
                    {
                        "id": 10028,
                        "content": "<p>Swap out Kinesis Data Streams with SQS FIFO queues</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2399,
            "attributes": {
                "createdAt": "2023-09-09T20:33:57.455Z",
                "updatedAt": "2023-09-09T20:33:57.455Z",
                "content": "<p>A mobile chat application uses DynamoDB as its database service to provide low latency chat updates. A new developer has joined the team and is reviewing the configuration settings for DynamoDB which have been tweaked for certain technical requirements. CloudTrail service has been enabled on all the resources used for the project. Yet, DynamoDB encryption details are nowhere to be found.</p>\n\n<p>Which of the following options can explain the root cause for the given issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>By default, all DynamoDB tables are encrypted under an AWS owned customer master key (CMK), which do not write to CloudTrail logs</strong> - AWS owned CMKs are a collection of CMKs that an AWS service owns and manages for use in multiple AWS accounts. Although AWS owned CMKs are not in your AWS account, an AWS service can use its AWS owned CMKs to protect the resources in your account.</p>\n\n<p>You do not need to create or manage the AWS owned CMKs. However, you cannot view, use, track, or audit them. You are not charged a monthly fee or usage fee for AWS owned CMKs and they do not count against the AWS KMS quotas for your account.</p>\n\n<p>The key rotation strategy for an AWS owned CMK is determined by the AWS service that creates and manages the CMK.</p>\n\n<p>All DynamoDB tables are encrypted. There is no option to enable or disable encryption for new or existing tables. By default, all tables are encrypted under an AWS owned customer master key (CMK) in the DynamoDB service account. However, you can select an option to encrypt some or all of your tables under a customer-managed CMK or the AWS managed CMK for DynamoDB in your account.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>By default, all DynamoDB tables are encrypted under AWS managed CMKs, which do not write to CloudTrail logs</strong></p>\n\n<p><strong>By default, all DynamoDB tables are encrypted under Customer managed CMKs, which do not write to CloudTrail logs</strong></p>\n\n<p><strong>By default, all DynamoDB tables are encrypted using Data keys, which do not write to CloudTrail logs</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-dynamodb.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-dynamodb.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys</a></p>\n",
                "options": [
                    {
                        "id": 10029,
                        "content": "<p>By default, all DynamoDB tables are encrypted under an AWS owned customer master key (CMK), which do not write to CloudTrail logs</p>",
                        "isValid": true
                    },
                    {
                        "id": 10030,
                        "content": "<p>By default, all DynamoDB tables are encrypted under Customer managed CMKs, which do not write to CloudTrail logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 10031,
                        "content": "<p>By default, all DynamoDB tables are encrypted under AWS managed CMKs, which do not write to CloudTrail logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 10032,
                        "content": "<p>By default, all DynamoDB tables are encrypted using Data keys, which do not write to CloudTrail logs</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2400,
            "attributes": {
                "createdAt": "2023-09-09T20:33:57.550Z",
                "updatedAt": "2023-09-09T20:33:57.550Z",
                "content": "<p>Which of the following is true regarding cross-zone load balancing as seen in Application Load Balancer versus Network Load Balancer?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>By default, cross-zone load balancing is enabled for Application Load Balancer and disabled for Network Load Balancer</strong></p>\n\n<p>By default, cross-zone load balancing is enabled for Application Load Balancer and disabled for Network Load Balancer.\nWhen cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all the enabled Availability Zones.\nWhen cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone.</p>\n\n<p>How cross-zone load balancing works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n\n<p>Incorrect Options:</p>\n\n<p><strong>By default, cross-zone load balancing is disabled for both Application Load Balancer and Network Load Balancer</strong></p>\n\n<p><strong>By default, cross-zone load balancing is enabled for both Application Load Balancer and Network Load Balancer</strong></p>\n\n<p><strong>By default, cross-zone load balancing is disabled for Application Load Balancer and enabled for Network Load Balancer</strong></p>\n\n<p>Per the default cross-zone load balancing settings described earlier in the explanation, these three options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n",
                "options": [
                    {
                        "id": 10033,
                        "content": "<p>By default, cross-zone load balancing is disabled for Application Load Balancer and enabled for Network Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 10034,
                        "content": "<p>By default, cross-zone load balancing is enabled for both Application Load Balancer and Network Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 10035,
                        "content": "<p>By default, cross-zone load balancing is enabled for Application Load Balancer and disabled for Network Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 10036,
                        "content": "<p>By default, cross-zone load balancing is disabled for both Application Load Balancer and Network Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2401,
            "attributes": {
                "createdAt": "2023-09-09T20:33:57.698Z",
                "updatedAt": "2023-09-09T20:33:57.698Z",
                "content": "<p>A streaming solutions company is building a video streaming product by using an Application Load Balancer (ALB) that routes the requests to the underlying EC2 instances. The engineering team has noticed a peculiar pattern. The ALB removes an instance from its pool of healthy instances whenever it is detected as unhealthy but the Auto Scaling group fails to kick-in and provision the replacement instance.</p>\n\n<p>What could explain this anomaly?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The Auto Scaling group is using EC2 based health check and the Application Load Balancer is using ALB based health check</strong></p>\n\n<p>An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management.</p>\n\n<p>Auto Scaling Group Overview:\n<img src=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/as-basic-diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n\n<p>Application Load Balancer automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.</p>\n\n<p>If the Auto Scaling group (ASG) is using EC2 as the health check type and the Application Load Balancer (ALB) is using its in-built health check, there may be a situation where the ALB health check fails because the health check pings fail to receive a response from the instance. At the same time, ASG health check can come back as successful because it is based on EC2 based health check.\nTherefore, in this scenario, the ALB will remove the instance from its inventory, however, the ASG will fail to provide the replacement instance. This can lead to the scaling issues mentioned in the problem statement.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Auto Scaling group is using ALB based health check and the Application Load Balancer is using EC2 based health check</strong> - ALB cannot use EC2 based health checks, so this option is incorrect.</p>\n\n<p><strong>Both the Auto Scaling group and Application Load Balancer are using ALB based health check</strong> - It is recommended to use ALB based health checks for both Auto Scaling group and Application Load Balancer. If both the Auto Scaling group and Application Load Balancer use ALB based health checks, then you will be able to avoid the scenario mentioned in the question.</p>\n\n<p><strong>Both the Auto Scaling group and Application Load Balancer are using EC2 based health check</strong> - ALB cannot use EC2 based health checks, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html</a></p>\n",
                "options": [
                    {
                        "id": 10037,
                        "content": "<p>The Auto Scaling group is using ALB based health check and the Application Load Balancer is using EC2 based health check</p>",
                        "isValid": false
                    },
                    {
                        "id": 10038,
                        "content": "<p>Both the Auto Scaling group and Application Load Balancer are using EC2 based health check</p>",
                        "isValid": false
                    },
                    {
                        "id": 10039,
                        "content": "<p>Both the Auto Scaling group and Application Load Balancer are using ALB based health check</p>",
                        "isValid": false
                    },
                    {
                        "id": 10040,
                        "content": "<p>The Auto Scaling group is using EC2 based health check and the Application Load Balancer is using ALB based health check</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2402,
            "attributes": {
                "createdAt": "2023-09-09T20:33:57.791Z",
                "updatedAt": "2023-09-09T20:33:57.791Z",
                "content": "<p>While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the internet using the Internet Gateway.</p>\n\n<p>Which conditions should be met for internet connectivity to be established? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic</strong> - The network access control lists (ACLs) that are associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivity.</p>\n\n<p><strong>The route table in the instance’s subnet should have a route to an Internet Gateway</strong> - A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instance’s subnet should have a route defined to the Internet Gateway.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The instance's subnet is not associated with any route table</strong> - This is an incorrect statement. A subnet is implicitly associated with the main route table if it is not explicitly associated with a particular route table. So, a subnet is always associated with some route table.</p>\n\n<p><strong>The instance's subnet is associated with multiple route tables with conflicting configurations</strong> - This is an incorrect statement. A subnet can only be associated with one route table at a time.</p>\n\n<p><strong>The subnet has been configured to be public and has no access to internet</strong> - This is an incorrect statement. Public subnets have access to the internet via Internet Gateway.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html</a></p>\n",
                "options": [
                    {
                        "id": 10041,
                        "content": "<p>The subnet has been configured to be public and has no access to the internet</p>",
                        "isValid": false
                    },
                    {
                        "id": 10042,
                        "content": "<p>The instance's subnet is associated with multiple route tables with conflicting configurations</p>",
                        "isValid": false
                    },
                    {
                        "id": 10043,
                        "content": "<p>The route table in the instance’s subnet should have a route to an Internet Gateway</p>",
                        "isValid": true
                    },
                    {
                        "id": 10044,
                        "content": "<p>The instance's subnet is not associated with any route table</p>",
                        "isValid": false
                    },
                    {
                        "id": 10045,
                        "content": "<p>The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2403,
            "attributes": {
                "createdAt": "2023-09-09T20:33:57.888Z",
                "updatedAt": "2023-09-09T20:33:57.888Z",
                "content": "<p>A junior developer is learning to build websites using HTML, CSS, and JavaScript. He has created a static website and then deployed it on Amazon S3. Now he can't seem to figure out the endpoint for his super cool website.</p>\n\n<p>As a solutions architect, can you help him figure out the allowed formats for the Amazon S3 website endpoints? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>http://bucket-name.s3-website.Region.amazonaws.com</strong></p>\n\n<p><strong>http://bucket-name.s3-website-Region.amazonaws.com</strong></p>\n\n<p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document. Depending on your website requirements, you can also configure other options, including redirects, web traffic logging, and custom error documents.</p>\n\n<p>When you configure your bucket as a static website, the website is available at the AWS Region-specific website endpoint of the bucket.</p>\n\n<p>Depending on your Region, your Amazon S3 website endpoints follow one of these two formats.</p>\n\n<p>s3-website dash (-) Region ‐ http://bucket-name.s3-website.Region.amazonaws.com</p>\n\n<p>s3-website dot (.) Region ‐ http://bucket-name.s3-website-Region.amazonaws.com</p>\n\n<p>These URLs return the default index document that you configure for the website.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>http://s3-website-Region.bucket-name.amazonaws.com</strong></p>\n\n<p><strong>http://s3-website.Region.bucket-name.amazonaws.com</strong></p>\n\n<p><strong>http://bucket-name.Region.s3-website.amazonaws.com</strong></p>\n\n<p>These three options do not meet the specifications for the Amazon S3 website endpoints format, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteEndpoints.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteEndpoints.html</a></p>\n",
                "options": [
                    {
                        "id": 10046,
                        "content": "<p>http://s3-website-Region.bucket-name.amazonaws.com</p>",
                        "isValid": false
                    },
                    {
                        "id": 10047,
                        "content": "<p>http://bucket-name.s3-website.Region.amazonaws.com</p>",
                        "isValid": true
                    },
                    {
                        "id": 10048,
                        "content": "<p>http://bucket-name.s3-website-Region.amazonaws.com</p>",
                        "isValid": true
                    },
                    {
                        "id": 10049,
                        "content": "<p>http://s3-website.Region.bucket-name.amazonaws.com</p>",
                        "isValid": false
                    },
                    {
                        "id": 10050,
                        "content": "<p>http://bucket-name.Region.s3-website.amazonaws.com</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2404,
            "attributes": {
                "createdAt": "2023-09-09T20:33:58.025Z",
                "updatedAt": "2023-09-09T20:33:58.025Z",
                "content": "<p>A financial services company has to retain the activity logs for each of their customers to meet compliance guidelines. Depending on the business line, the company wants to retain the logs for 5-10 years in highly available and durable storage on AWS. The overall data size is expected to be in PetaBytes. In case of an audit, the data would need to be accessible within a timeframe of up to 48 hours.</p>\n\n<p>Which AWS storage option is the MOST cost-effective for the given compliance requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon S3 Glacier Deep Archive</strong></p>\n\n<p>Amazon S3 Glacier and S3 Glacier Deep Archive are secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n\n<p>S3 Glacier Deep Archive is a new Amazon S3 storage class that provides secure and durable object storage for long-term retention of data that is accessed once or twice in a year. From just $0.00099 per GB-month (less than one-tenth of one cent, or about $1 per TB-month), S3 Glacier Deep Archive offers the lowest cost storage in the cloud, at prices significantly lower than storing and maintaining data in on-premises magnetic tape libraries or archiving data off-site.</p>\n\n<p>S3 Glacier Deep Archive is up to 75% less expensive than S3 Glacier and provides retrieval within 12 hours using the Standard retrieval speed. You may also reduce retrieval costs by selecting Bulk retrieval, which will return data within 48 hours.</p>\n\n<p>Therefore, Amazon S3 Glacier Deep Archive is the correct choice.</p>\n\n<p>S3 Glacier vs S3 Glacier Deep Archive:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q24-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 Glacier</strong> - As mentioned earlier, S3 Glacier Deep Archive is up to 75% less expensive than S3 Glacier and provides retrieval within 12 hours. So using Amazon S3 Glacier is not the correct choice.</p>\n\n<p><strong>Third-party tape storage</strong></p>\n\n<p><strong>Amazon S3 Standard storage</strong></p>\n\n<p>Given the relaxed retrieval times, S3 standard storage would be much costlier than the S3 Glacier Deep Archive, so S3 standard storage is not the correct option. Using Third-party tape storage is ruled out as the company wants to use an AWS storage service. Therefore, both of these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 10051,
                        "content": "<p>Amazon S3 Glacier</p>",
                        "isValid": false
                    },
                    {
                        "id": 10052,
                        "content": "<p>Third party tape storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 10053,
                        "content": "<p>Amazon S3 Standard storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 10054,
                        "content": "<p>Amazon S3 Glacier Deep Archive</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2405,
            "attributes": {
                "createdAt": "2023-09-09T20:33:58.136Z",
                "updatedAt": "2023-09-09T20:33:58.136Z",
                "content": "<p>A leading media company wants to do an accelerated online migration of hundreds of terabytes of files from their on-premises data center to Amazon S3 and then establish a mechanism to access the migrated data for ongoing updates from the on-premises applications.</p>\n\n<p>As a solutions architect, which of the following would you select as the MOST performant solution for the given use-case?</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications</strong></p>\n\n<p>AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data to and from AWS storage services over the internet or AWS Direct Connect.</p>\n\n<p>AWS DataSync fully automates and accelerates moving large active datasets to AWS, up to 10 times faster than command-line tools. It is natively integrated with Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon CloudWatch, and AWS CloudTrail, which provides seamless and secure access to your storage services, as well as detailed monitoring of the transfer.\nDataSync uses a purpose-built network protocol and scale-out architecture to transfer data. A single DataSync agent is capable of saturating a 10 Gbps network link.</p>\n\n<p>DataSync fully automates the data transfer. It comes with retry and network resiliency mechanisms, network optimizations, built-in task scheduling, monitoring via the DataSync API and Console, and CloudWatch metrics, events, and logs that provide granular visibility into the transfer process. DataSync performs data integrity verification both during the transfer and at the end of the transfer.</p>\n\n<p>How DataSync Works:\n<img src=\"https://d1.awsstatic.com/cloud-storage/Storage/aws-datasync-how-it-works-diagram-s3-efs-fsx.c26c66393dc4e433369ee9947f39e9c54cd338bb.png\">\nvia - <a href=\"https://aws.amazon.com/datasync/\">https://aws.amazon.com/datasync/</a></p>\n\n<p>AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching.</p>\n\n<p>The combination of DataSync and File Gateway is the correct solution. AWS DataSync enables you to automate and accelerate online data transfers to AWS storage services. File Gateway then provides your on-premises applications with low latency access to the migrated data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS DataSync to migrate existing data to Amazon S3 as well as access the S3 data for ongoing updates</strong> - AWS DataSync is used to easily transfer data to and from AWS with up to 10x faster speeds. It is used to transfer data and cannot be used to facilitate ongoing updates to the migrated files from the on-premises applications.</p>\n\n<p><strong>Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use S3 Transfer Acceleration for ongoing updates from the on-premises applications</strong> - File Gateway can be used to move on-premises data to AWS Cloud, but it not an optimal solution for high volumes. Migration services such as DataSync are best suited for this purpose. S3 Transfer Acceleration cannot facilitate ongoing updates to the migrated files from the on-premises applications.</p>\n\n<p><strong>Use S3 Transfer Acceleration to migrate existing data to Amazon S3 and then use DataSync for ongoing updates from the on-premises applications</strong> -  If your application is already integrated with the Amazon S3 API, and you want higher throughput for transferring large files to S3, S3 Transfer Acceleration can be used. However DataSync cannot be used to facilitate ongoing updates to the migrated files from the on-premises applications.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/datasync/features/\">https://aws.amazon.com/datasync/features/</a></p>\n",
                "options": [
                    {
                        "id": 10055,
                        "content": "<p>Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications</p>",
                        "isValid": true
                    },
                    {
                        "id": 10056,
                        "content": "<p>Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use S3 Transfer Acceleration for ongoing updates from the on-premises applications</p>",
                        "isValid": false
                    },
                    {
                        "id": 10057,
                        "content": "<p>Use AWS DataSync to migrate existing data to Amazon S3 as well as access the S3 data for ongoing updates</p>",
                        "isValid": false
                    },
                    {
                        "id": 10058,
                        "content": "<p>Use S3 Transfer Acceleration to migrate existing data to Amazon S3 and then use DataSync for ongoing updates from the on-premises applications</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2406,
            "attributes": {
                "createdAt": "2023-09-09T20:33:58.250Z",
                "updatedAt": "2023-09-09T20:33:58.250Z",
                "content": "<p>The engineering team at an e-commerce company uses a Lambda function to write the order data into a single DB instance Aurora cluster. The team has noticed that many order- writes to its Aurora cluster are getting missed during peak load times. The diagnostics data has revealed that the DB is experiencing high CPU and memory consumption during traffic spikes. The team also wants to enhance the availability of the Aurora DB.</p>\n\n<p>Which of the following steps would you combine to address the given scenario? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Handle all read operations for your application by connecting to the reader endpoint of the Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica</strong></p>\n\n<p>When you create a second, third, and so on DB instance in an Aurora-provisioned DB cluster, Aurora automatically sets up replication from the writer DB instance to all the other DB instances. These other DB instances are read-only and are known as Aurora Replicas.</p>\n\n<p>Aurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q3-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p>\n\n<p><strong>Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target</strong></p>\n\n<p>If the primary instance in a DB cluster using single-master replication fails, Aurora automatically fails over to a new primary instance in one of two ways:</p>\n\n<p>By promoting an existing Aurora Replica to the new primary instance\nBy creating a new primary instance</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q3-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target</strong> - There are no standby instances in Aurora. Aurora performs an automatic failover to a read replica when a problem is detected. So this option is incorrect.</p>\n\n<p>Read replicas, Multi-AZ deployments, and multi-region deployments:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q3-i4.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n\n<p><strong>Increase the concurrency of the Lambda function so that the order-writes do not get missed during traffic spikes</strong> - Increasing the concurrency of the Lambda function would not resolve the issue since the bottleneck is at the database layer, as exhibited by the high CPU and memory consumption for the Aurora instance. This option has been added as a distractor.</p>\n\n<p><strong>Use EC2 instances behind an Application Load Balancer to write the order data into the Aurora cluster</strong> - Using EC2 instances behind an Application Load Balancer would not resolve the issue since the bottleneck is at the database layer, as exhibited by the high CPU and memory consumption for the Aurora instance. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n",
                "options": [
                    {
                        "id": 10059,
                        "content": "<p>Handle all read operations for your application by connecting to the reader endpoint of the Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica</p>",
                        "isValid": true
                    },
                    {
                        "id": 10060,
                        "content": "<p>Use EC2 instances behind an Application Load Balancer to write the order data into the Aurora cluster</p>",
                        "isValid": false
                    },
                    {
                        "id": 10061,
                        "content": "<p>Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target</p>",
                        "isValid": false
                    },
                    {
                        "id": 10062,
                        "content": "<p>Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target</p>",
                        "isValid": true
                    },
                    {
                        "id": 10063,
                        "content": "<p>Increase the concurrency of the Lambda function so that the order-writes do not get missed during traffic spikes</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2407,
            "attributes": {
                "createdAt": "2023-09-09T20:33:58.342Z",
                "updatedAt": "2023-09-09T20:33:58.342Z",
                "content": "<p>A silicon valley based startup helps its users legally sign highly confidential contracts. To meet the compliance guidelines, the startup must ensure that the signed contracts are encrypted using the AES-256 algorithm via an encryption key that is generated internally. The startup is now migrating to AWS Cloud and would like the data to be encrypted on AWS. The startup wants to continue using their existing encryption key generation mechanism.</p>\n\n<p>What do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>SSE-C</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects. With SSE-C, the startup can still provide the encryption key but let AWS do the encryption. Therefore, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-KMS</strong> - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. But, you never get to know the actual key here.</p>\n\n<p><strong>SSE-S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. However, this option does not provide the ability to audit trail the usage of the encryption keys.</p>\n\n<p><strong>Client-Side Encryption</strong> - Client-side encryption is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options: Use a customer master key (CMK) stored in AWS Key Management Service (AWS KMS), Use a master key you store within your application. Since the customer wants to use AWS provided facility, this is not an option.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n",
                "options": [
                    {
                        "id": 10064,
                        "content": "<p>SSE-KMS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10065,
                        "content": "<p>Client-Side Encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 10066,
                        "content": "<p>SSE-S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 10067,
                        "content": "<p>SSE-C</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2408,
            "attributes": {
                "createdAt": "2023-09-09T20:33:58.462Z",
                "updatedAt": "2023-09-09T20:33:58.462Z",
                "content": "<p>A gaming company is doing pre-launch testing for its new product. The company runs its production database on an Aurora MySQL DB cluster and the performance testing team wants access to multiple test databases that must be re-created from production data. The company has hired you as an AWS Certified Solutions Architect Associate to deploy a solution to create these test databases quickly with the LEAST required effort.</p>\n\n<p>What would you suggest to address this use case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use database cloning to create multiple clones of the production DB and use each clone as a test DB</strong></p>\n\n<p>You can quickly create clones of an Aurora DB by using the database cloning feature. In addition, database cloning uses a copy-on-write protocol, in which data is copied only at the time the data changes, either on the source database or the clone database. Cloning is much faster than a manual snapshot of the DB cluster.</p>\n\n<p>For the given use case, the most optimal solution is to clone the DB cluster. This would allow the performance testing team to have quick access to the production data in an isolated way. The team can iterate over the various test phases by deleting existing test databases and then cloning the production DB to create new test DBs.</p>\n\n<p>You cannot clone databases across AWS regions. The clone databases must be created in the same region as the source databases. Currently, you are limited to 15 clones based on a copy, including clones based on other clones. After that, only copies can be created. However, each copy can also have up to 15 clones.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q59-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable database Backtracking on the production DB and let the testing team use the production DB</strong> - Using Backtracking, you can \"rewind\" the DB cluster to any time you specify. One of the major advantages of backtracking is that it can rewind the DB cluster much faster compared to restoring a DB cluster via point-in-time restore (PITR) or via a manual DB cluster snapshot, which can take hours. Backtracking a DB cluster doesn't require a new DB cluster and rewinds the DB cluster in minutes.</p>\n\n<p>However, as the given use-case is around pre-release testing, it does not make sense to use production DB itself for testing even if backtracking is enabled. The right solution is to use clones of the production DB for testing.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q59-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html</a></p>\n\n<p><strong>Take a backup of the Aurora MySQL DB instance using the mysqldump utility, create multiple new test DB instances and restore each test DB from the backup</strong> - As the use-case mandates the least effort for database administration, therefore this option is not correct since using the mysqldump utility requires several manual steps to take a backup of a DB and restore into another DB.</p>\n\n<p><strong>Create additional Read Replicas of the Aurora MySQL production DB and use the Read Replicas for testing by promoting each Replica to be its own independent standalone instance</strong> - This option has been added as a distractor as you cannot promote an Aurora Read Replica to a standalone DB instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html</a></p>\n",
                "options": [
                    {
                        "id": 10068,
                        "content": "<p>Use database cloning to create multiple clones of the production DB and use each clone as a test DB</p>",
                        "isValid": true
                    },
                    {
                        "id": 10069,
                        "content": "<p>Create additional Read Replicas of the Aurora MySQL production DB and use the Read Replicas for testing by promoting each Replica to be its own independent standalone instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10070,
                        "content": "<p>Take a backup of the Aurora MySQL DB instance using the mysqldump utility, create multiple new test DB instances and restore each test DB from the backup</p>",
                        "isValid": false
                    },
                    {
                        "id": 10071,
                        "content": "<p>Enable database Backtracking on the production DB and let the testing team use the production DB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2409,
            "attributes": {
                "createdAt": "2023-09-09T20:33:58.538Z",
                "updatedAt": "2023-09-09T20:33:58.538Z",
                "content": "<p>A global media company uses a fleet of EC2 instances (behind an Application Load Balancer) to power its video streaming application. To improve the performance of the application, the engineering team has also created a CloudFront distribution with the Application Load Balancer as the custom origin. The security team at the company has noticed a spike in the number and types of SQL injection and cross-site scripting attack vectors on the application.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend as the MOST effective in countering these malicious attacks?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Web Application Firewall (WAF) with CloudFront distribution</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.</p>\n\n<p>How WAF Works:\n<img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\">\nvia - <a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p>\n\n<p>A web access control list (web ACL) gives you fine-grained control over the web requests that your Amazon CloudFront distribution, Amazon API Gateway API, or Application Load Balancer responds to.</p>\n\n<p>When you create a web ACL, you can specify one or more CloudFront distributions that you want AWS WAF to inspect. AWS WAF starts to allow, block, or count web requests for those distributions based on the conditions that you identify in the web ACL. Therefore, combining WAF with CloudFront can prevent SQL injection and cross-site scripting attacks. So this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Route 53 with CloudFront distribution</strong> - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. You cannot use Route 53 to prevent SQL injection and cross-site scripting attacks. So this option is incorrect.</p>\n\n<p><strong>Use Security Hub with CloudFront distribution</strong> - AWS Security Hub gives you a comprehensive view of your high-priority security alerts and security posture across your AWS accounts. With Security Hub, you have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, Amazon Macie, AWS Identity and Access Management (IAM) Access Analyzer, and AWS Firewall Manager, as well as from AWS Partner solutions. You cannot use Security Hub to prevent SQL injection and cross-site scripting attacks. So this option is incorrect.</p>\n\n<p><strong>Use AWS Firewall Manager with CloudFront distribution</strong> - AWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organization. You cannot use Firewall Manager to prevent SQL injection and cross-site scripting attacks. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/waf/features/\">https://aws.amazon.com/waf/features/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html\">https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p>\n",
                "options": [
                    {
                        "id": 10072,
                        "content": "<p>Use AWS Firewall Manager with CloudFront distribution</p>",
                        "isValid": false
                    },
                    {
                        "id": 10073,
                        "content": "<p>Use Route 53 with CloudFront distribution</p>",
                        "isValid": false
                    },
                    {
                        "id": 10074,
                        "content": "<p>Use Web Application Firewall (WAF) with CloudFront distribution</p>",
                        "isValid": true
                    },
                    {
                        "id": 10075,
                        "content": "<p>Use Security Hub with CloudFront distribution</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2410,
            "attributes": {
                "createdAt": "2023-09-09T20:33:58.638Z",
                "updatedAt": "2023-09-09T20:33:58.638Z",
                "content": "<p>A company is looking for a technology that allows its mobile app users to connect through a Google login and have the capability to turn on MFA (Multi-Factor Authentication) to have maximum security. Ideally, the solution should be fully managed by AWS.</p>\n\n<p>Which technology do you recommend for managing the users' accounts?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Cognito</strong> - Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0. Here Cognito is the best technology choice for managing mobile user accounts.</p>\n\n<p>Amazon Cognito Features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q30-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/cognito/details/\">https://aws.amazon.com/cognito/details/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Write a Lambda function with Auth0 3rd party integration</strong> - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Using Lambda would require code maintenance for user management functionality, therefore this option is ruled out.</p>\n\n<p><strong>AWS Identity and Access Management (IAM)</strong> - AWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. IAM cannot be used to manage mobile user accounts.</p>\n\n<p><strong>Enable the AWS Google Login Service</strong> - There is no such thing as AWS Google Login service. This option is just added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/cognito/\">https://aws.amazon.com/cognito/</a></p>\n",
                "options": [
                    {
                        "id": 10076,
                        "content": "<p>AWS Identity and Access Management (IAM)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10077,
                        "content": "<p>Write a Lambda function with Auth0 3rd party integration</p>",
                        "isValid": false
                    },
                    {
                        "id": 10078,
                        "content": "<p>Amazon Cognito</p>",
                        "isValid": true
                    },
                    {
                        "id": 10079,
                        "content": "<p>Enable the AWS Google Login Service</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2411,
            "attributes": {
                "createdAt": "2023-09-09T20:33:58.734Z",
                "updatedAt": "2023-09-09T20:33:58.734Z",
                "content": "<p>A solutions architect has been tasked to design a low-latency solution for a static, single-page application, accessed by users through a custom domain name. The solution must be serverless, provide in-transit data encryption and needs to be cost-effective.</p>\n\n<p>Which AWS services can be combined to build the simplest possible solution for the company's requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 to host the static website and Amazon CloudFront to distribute the content for low latency access</strong></p>\n\n<p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you must enable website hosting, set permissions, and create and add an index document. Depending on your website requirements, you can also configure redirects, web traffic logging, and a custom error document.</p>\n\n<p>After you configure your bucket as a static website, you can access the bucket through the AWS Region-specific Amazon S3 website endpoints for your bucket. Website endpoints are different from the endpoints where you send REST API requests. Amazon S3 doesn't support HTTPS access for website endpoints. If you want to use HTTPS, you can use CloudFront to serve a static website hosted on Amazon S3.</p>\n\n<p>You can use Amazon CloudFront to improve the performance of your website. CloudFront makes your website files (such as HTML, images, and video) available from data centers around the world (called edge locations). When a visitor requests a file from your website, CloudFront automatically redirects the request to a copy of the file at the nearest edge location. This results in faster download times than if the visitor had requested the content from a data center that is located farther away.</p>\n\n<p>CloudFront caches content at edge locations for a period of time that you specify. If a visitor requests content that has been cached for longer than the expiration date, CloudFront checks the origin server to see if a newer version of the content is available. If a newer version is available, CloudFront copies the new version to the edge location. Changes that you make to the original content are replicated to edge locations as visitors request the content.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Host the application on Amazon EC2 instance with instance store volume for high performance and low latency access to users</strong> - Since the use case speaks about a serverless solution, Amazon EC2 cannot be the answer, since EC2 is not serverless.</p>\n\n<p><strong>Host the application on AWS Fargate and front it with an Elastic Load Balancer for an improved performance</strong> - AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Elastic Load Balancer can spread the incoming requests across a fleet of EC2 instances. This added complexity is not needed since we are looking at a static single-page webpage.</p>\n\n<p><strong>Configure Amazon S3 to store the static data and use AWS Fargate for hosting the application</strong> - Fargate is overkill for hosting a static single-page webpage.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p>\n",
                "options": [
                    {
                        "id": 10080,
                        "content": "<p>Use Amazon S3 to host the static website and Amazon CloudFront to distribute the content for low latency access</p>",
                        "isValid": true
                    },
                    {
                        "id": 10081,
                        "content": "<p>Host the application on Amazon EC2 instance with instance store volume for high performance and low latency access to users</p>",
                        "isValid": false
                    },
                    {
                        "id": 10082,
                        "content": "<p>Host the application on AWS Fargate and front it with an Elastic Load Balancer for an improved performance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10083,
                        "content": "<p>Configure Amazon S3 to store the static data and use AWS Fargate for hosting the application</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2412,
            "attributes": {
                "createdAt": "2023-09-09T20:33:58.863Z",
                "updatedAt": "2023-09-09T20:33:58.863Z",
                "content": "<p>A DevOps engineer at an IT company was recently added to the admin group of the company's AWS account. The <code>AdministratorAccess</code> managed policy is attached to this group.</p>\n\n<p>Can you identify the AWS tasks that the DevOps engineer CANNOT perform even though he has full Administrator privileges (Select two)?</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Configure an Amazon S3 bucket to enable MFA (Multi Factor Authentication) delete</strong></p>\n\n<p><strong>Close the company's AWS account</strong></p>\n\n<p>An IAM user with full administrator access can perform almost all AWS tasks except a few tasks designated only for the root account user. Some of the AWS tasks that only a root account user can do are as follows: change account name or root password or root email address, change AWS support plan, close AWS account, enable MFA on S3 bucket delete, create Cloudfront key pair, register for GovCloud. Even though the DevOps engineer is part of the admin group, he cannot configure an Amazon S3 bucket to enable MFA delete or close the company's AWS account.</p>\n\n<p>Incorrect Options:</p>\n\n<p><strong>Delete the IAM user for his manager</strong></p>\n\n<p><strong>Delete an S3 bucket from the production environment</strong></p>\n\n<p><strong>Change the password for his own IAM user account</strong></p>\n\n<p>The DevOps engineer is part of the admin group, so he can delete any IAM user, delete the S3 bucket, and change the password for his own IAM user account.</p>\n\n<p>For the complete list of AWS tasks that require AWS account root user credentials, please review this reference link:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/general/latest/gr/aws_tasks-that-require-root.html\">https://docs.aws.amazon.com/general/latest/gr/aws_tasks-that-require-root.html</a></p>\n",
                "options": [
                    {
                        "id": 10084,
                        "content": "<p>Change the password for his own IAM user account</p>",
                        "isValid": false
                    },
                    {
                        "id": 10085,
                        "content": "<p>Close the company's AWS account</p>",
                        "isValid": true
                    },
                    {
                        "id": 10086,
                        "content": "<p>Delete the IAM user for his manager</p>",
                        "isValid": false
                    },
                    {
                        "id": 10087,
                        "content": "<p>Configure an Amazon S3 bucket to enable MFA (Multi Factor Authentication) delete</p>",
                        "isValid": true
                    },
                    {
                        "id": 10088,
                        "content": "<p>Delete an S3 bucket from the production environment</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2413,
            "attributes": {
                "createdAt": "2023-09-09T20:33:58.958Z",
                "updatedAt": "2023-09-09T20:33:58.958Z",
                "content": "<p>A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on EC2 instances with storage on EBS volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on EBS.</p>\n\n<p>Which of the following options outline the correct capabilities of an encrypted EBS volume? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Data at rest inside the volume is encrypted</strong></p>\n\n<p><strong>Any snapshot created from the volume is encrypted</strong></p>\n\n<p><strong>Data moving between the volume and the instance is encrypted</strong></p>\n\n<p>Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with EC2 instances. When you create an encrypted EBS volume and attach it to a supported instance type, data stored at rest on the volume, data moving between the volume and the instance, snapshots created from the volume and volumes created from those snapshots are all encrypted. It uses AWS Key Management Service (AWS KMS) customer master keys (CMK) when creating encrypted volumes and snapshots. Encryption operations occur on the servers that host EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached EBS storage.</p>\n\n<p>Therefore, the incorrect options are:</p>\n\n<p><strong>Data moving between the volume and the instance is NOT encrypted</strong></p>\n\n<p><strong>Any snapshot created from the volume is NOT encrypted</strong></p>\n\n<p><strong>Data at rest inside the volume is NOT encrypted</strong></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n",
                "options": [
                    {
                        "id": 10089,
                        "content": "<p>Any snapshot created from the volume is NOT encrypted</p>",
                        "isValid": false
                    },
                    {
                        "id": 10090,
                        "content": "<p>Any snapshot created from the volume is encrypted</p>",
                        "isValid": true
                    },
                    {
                        "id": 10091,
                        "content": "<p>Data moving between the volume and the instance is NOT encrypted</p>",
                        "isValid": false
                    },
                    {
                        "id": 10092,
                        "content": "<p>Data at rest inside the volume is NOT encrypted</p>",
                        "isValid": false
                    },
                    {
                        "id": 10093,
                        "content": "<p>Data at rest inside the volume is encrypted</p>",
                        "isValid": true
                    },
                    {
                        "id": 10094,
                        "content": "<p>Data moving between the volume and the instance is encrypted</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2414,
            "attributes": {
                "createdAt": "2023-09-09T20:33:59.050Z",
                "updatedAt": "2023-09-09T20:33:59.050Z",
                "content": "<p>An e-commerce company uses a two-tier architecture with application servers in the public subnet and an RDS MySQL DB in a private subnet. The development team can use a bastion host in the public subnet to access the MySQL DB and run queries from the bastion host. However, end-users are reporting application errors. Upon inspecting application logs, the team notices several \"could not connect to server: connection timed out\" error messages.</p>\n\n<p>Which of the following options represent the root cause for this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The security group configuration for the DB instance does not have the correct rules to allow inbound connections from the application servers</strong></p>\n\n<p>You should use security groups to control the inbound and outbound traffic for your DB instance. For your application servers, create a security group with inbound rules that use the IP addresses of the client application as the source. This security group allows your client application to connect to your application servers. Then create a second security group for your DB instance and create a new rule by specifying the security group that you created earlier as the source for this DB-specific security group.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q29-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The security group configuration for the application servers does not have the correct rules to allow inbound connections from the DB instance</strong> - As mentioned in the explanation above, the application servers don't need inbound connections from the DB instance, rather the DB instance needs the correct inbound rule with application servers' security group as the source.</p>\n\n<p><strong>The database user credentials (username and password) configured for the application are incorrect</strong></p>\n\n<p><strong>The database user credentials (username and password) configured for the application do not have the required privilege for the given database</strong></p>\n\n<p>These two options have been added as a distractor since the error mentions a \"connection timeout\" issue rather than an \"access denied\" error.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/</a></p>\n",
                "options": [
                    {
                        "id": 10095,
                        "content": "<p>The security group configuration for the application servers does not have the correct rules to allow inbound connections from the DB instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10096,
                        "content": "<p>The security group configuration for the DB instance does not have the correct rules to allow inbound connections from the application servers</p>",
                        "isValid": true
                    },
                    {
                        "id": 10097,
                        "content": "<p>The database user credentials (username and password) configured for the application do not have the required privilege for the given database</p>",
                        "isValid": false
                    },
                    {
                        "id": 10098,
                        "content": "<p>The database user credentials (username and password) configured for the application are incorrect</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2415,
            "attributes": {
                "createdAt": "2023-09-09T20:33:59.147Z",
                "updatedAt": "2023-09-09T20:33:59.147Z",
                "content": "<p>A medium-sized business has a taxi dispatch application deployed on an EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console.</p>\n\n<p>Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Setup a CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance</strong></p>\n\n<p>Using Amazon CloudWatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or recover your EC2 instances. You can use the stop or terminate actions to help you save money when you no longer need an instance to be running. You can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs.</p>\n\n<p>You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically reboots the instance. The reboot alarm action is recommended for Instance Health Check failures (as opposed to the recover alarm action, which is suited for System Health Check failures).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Setup a CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, CloudWatch Alarm can publish to an SNS event which can then trigger a lambda function. The lambda function can use AWS EC2 API to reboot the instance</strong></p>\n\n<p><strong>Use EventBridge events to trigger a Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the lambda function can use AWS EC2 API to reboot the instance</strong></p>\n\n<p><strong>Use EventBridge events to trigger a Lambda function to reboot the instance status every 5 minutes</strong></p>\n\n<p>Using EventBridge event or CloudWatch alarm to trigger a lambda function, directly or indirectly, is wasteful of resources. You should just use the EC2 Reboot CloudWatch Alarm Action to reboot the instance. So all the options that trigger the lambda function are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html</a></p>\n",
                "options": [
                    {
                        "id": 10099,
                        "content": "<p>Setup a CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, CloudWatch Alarm can publish to an SNS event which can then trigger a lambda function. The lambda function can use AWS EC2 API to reboot the instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10100,
                        "content": "<p>Setup a CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance</p>",
                        "isValid": true
                    },
                    {
                        "id": 10101,
                        "content": "<p>Use EventBridge events to trigger a Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the lambda function can use AWS EC2 API to reboot the instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10102,
                        "content": "<p>Use EventBridge events to trigger a Lambda function to reboot the instance status every 5 minutes</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2416,
            "attributes": {
                "createdAt": "2023-09-09T20:33:59.243Z",
                "updatedAt": "2023-09-09T20:33:59.243Z",
                "content": "<p>A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5PB of data in its on-premises data center to durable long term storage.</p>\n\n<p>As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into AWS Glacier</strong></p>\n\n<p>Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases.\nThe data stored on the Snowball Edge device can be copied into the S3 bucket and later transitioned into AWS Glacier via a lifecycle policy. You can't directly copy data from Snowball Edge devices into AWS Glacier.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into AWS Glacier</strong> - As mentioned earlier, you can't directly copy data from Snowball Edge devices into AWS Glacier. Hence, this option is incorrect.</p>\n\n<p><strong>Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into AWS Glacier</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. Direct Connect involves significant monetary investment and takes more than a month to set up, therefore it's not the correct fit for this use-case where just a one-time data transfer has to be done.</p>\n\n<p><strong>Setup Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into AWS Glacier</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). VPN Connections are a good solution if you have an immediate need, and have low to modest bandwidth requirements. Because of the high data volume for the given use-case, Site-to-Site VPN is not the correct choice.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p>\n",
                "options": [
                    {
                        "id": 10103,
                        "content": "<p>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into AWS Glacier</p>",
                        "isValid": false
                    },
                    {
                        "id": 10104,
                        "content": "<p>Setup Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into AWS Glacier</p>",
                        "isValid": false
                    },
                    {
                        "id": 10105,
                        "content": "<p>Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into AWS Glacier</p>",
                        "isValid": false
                    },
                    {
                        "id": 10106,
                        "content": "<p>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into AWS Glacier</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2417,
            "attributes": {
                "createdAt": "2023-09-09T20:33:59.331Z",
                "updatedAt": "2023-09-09T20:33:59.331Z",
                "content": "<p>A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon S3. The system will also read these log files in parallel on a near real-time basis. The engineering team wants to address any data discrepancies that might arise when the trading system overwrites an existing log file and then tries to read that specific log file.</p>\n\n<p>Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object</strong></p>\n\n<p>Amazon S3 delivers strong read-after-write consistency automatically, without changes to performance or availability, without sacrificing regional isolation for applications, and at no additional cost.</p>\n\n<p>After a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected.</p>\n\n<p>Strong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects.</p>\n\n<p>To summarize, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are strongly consistent. What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data</strong></p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data</strong></p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data</strong></p>\n\n<p>These three options contradict the earlier details provided in the explanation.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 10107,
                        "content": "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data</p>",
                        "isValid": false
                    },
                    {
                        "id": 10108,
                        "content": "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data</p>",
                        "isValid": false
                    },
                    {
                        "id": 10109,
                        "content": "<p>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object</p>",
                        "isValid": true
                    },
                    {
                        "id": 10110,
                        "content": "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2418,
            "attributes": {
                "createdAt": "2023-09-09T20:33:59.422Z",
                "updatedAt": "2023-09-09T20:33:59.422Z",
                "content": "<p>You have just terminated an instance in the us-west-1a availability zone. The attached EBS volume is now available for attachment to other instances. An intern launches a new Linux EC2 instance in the us-west-1b availability zone and is attempting to attach the EBS volume. The intern informs you that it is not possible and needs your help.</p>\n\n<p>Which of the following explanations would you provide to them?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>EBS volumes are AZ locked</strong></p>\n\n<p>An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. For current-generation volumes attached to current-generation instance types, you can dynamically increase size, modify the provisioned IOPS capacity, and change volume type on live production volumes.</p>\n\n<p>When you create an EBS volume, it is automatically replicated within its Availability Zone to prevent data loss due to the failure of any single hardware component. You can attach an EBS volume to an EC2 instance in the same Availability Zone.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EBS volumes are region locked</strong> - It's confined to an Availability Zone and not by region.</p>\n\n<p><strong>The required IAM permissions are missing</strong> - This is a possibility as well but if permissions are not an issue then you are still confined to an availability zone.</p>\n\n<p><strong>The EBS volume is encrypted</strong> - This doesn't affect the ability to attach an EBS volume.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html</a></p>\n",
                "options": [
                    {
                        "id": 10111,
                        "content": "<p>The EBS volume is encrypted</p>",
                        "isValid": false
                    },
                    {
                        "id": 10112,
                        "content": "<p>EBS volumes are AZ locked</p>",
                        "isValid": true
                    },
                    {
                        "id": 10113,
                        "content": "<p>The required IAM permissions are missing</p>",
                        "isValid": false
                    },
                    {
                        "id": 10114,
                        "content": "<p>EBS volumes are region locked</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2419,
            "attributes": {
                "createdAt": "2023-09-09T20:33:59.505Z",
                "updatedAt": "2023-09-09T20:33:59.505Z",
                "content": "<p>The content division at a digital media agency has an application that generates a large number of files on Amazon S3, each approximately 10 MB in size. The agency mandates that the files be stored for 5 years before they can be deleted. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days. The files contain critical business data that is not easy to reproduce, therefore, immediate accessibility is always required.</p>\n\n<p>Which solution is the MOST cost-effective for the given use case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation</strong></p>\n\n<p>S3 Standard-IA class is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval charge.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q51-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n\n<p>For the given use case, you can set up an S3 lifecycle configuration and create a transition action to move objects from S3 Standard to S3 Standard-IA 30 days after object creation. You can set up an expiration action to delete the object 5 years after object creation.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q51-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q51-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation</strong> - S3 Glacier Flexible Retrieval storage class has the best case retrieval time of the order of minutes, so this option is incorrect for the given requirement.</p>\n\n<p><strong>Set up an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-IA 30 days after object creation. Archive the files to S3 Glacier Deep Archive 5 years after object creation</strong> - The files can simply be deleted 5 years after object creation instead of archiving the files to S3 Glacier Deep Archive. There is no need to incur the cost of archival.</p>\n\n<p><strong>Set up an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation</strong> - Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. S3 One Zone-IA is a good choice for storing secondary backup copies of on-premises data or easily re-creatable data. The given scenario clearly states that the business-critical data is not easy to reproduce, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/glacier/\">https://aws.amazon.com/s3/storage-classes/glacier/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html</a></p>\n",
                "options": [
                    {
                        "id": 10115,
                        "content": "<p>Set up an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation</p>",
                        "isValid": false
                    },
                    {
                        "id": 10116,
                        "content": "<p>Set up an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation</p>",
                        "isValid": true
                    },
                    {
                        "id": 10117,
                        "content": "<p>Set up an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-IA 30 days after object creation. Archive the files to S3 Glacier Deep Archive 5 years after object creation</p>",
                        "isValid": false
                    },
                    {
                        "id": 10118,
                        "content": "<p>Set up an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2420,
            "attributes": {
                "createdAt": "2023-09-09T20:33:59.609Z",
                "updatedAt": "2023-09-09T20:33:59.609Z",
                "content": "<p>A company needs a massive PostgreSQL database and the engineering team would like to retain control over managing the patches, version upgrades for the database, and consistent performance with high IOPS. The team wants to install the database on an EC2 instance with the optimal storage type on the attached EBS volume.</p>\n\n<p>As a solutions architect, which of the following configurations would you suggest to the engineering team?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EC2 with EBS volume of Provisioned IOPS SSD (io1) type</strong></p>\n\n<p>Amazon EBS provides the following volume types, which differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications.</p>\n\n<p>The volumes types fall into two categories:</p>\n\n<p>SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS</p>\n\n<p>HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS</p>\n\n<p>Provision IOPS type supports critical business applications that require sustained IOPS performance, or more than 16,000 IOPS or 250 MiB/s of throughput per volume. Examples are large database workloads, such as:\nMongoDB\nCassandra\nMicrosoft SQL Server\nMySQL\nPostgreSQL\nOracle</p>\n\n<p>Therefore, Amazon EC2 with EBS volume of Provisioned IOPS SSD (io1) type is the right fit for the given use-case.</p>\n\n<p>Please see this detailed overview of the volume types for EBS volumes.\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q22-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon EC2 with EBS volume of General Purpose SSD (gp2) type</strong></p>\n\n<p><strong>Amazon EC2 with EBS volume of Throughput Optimized HDD (st1) type</strong></p>\n\n<p><strong>Amazon EC2 with EBS volume of cold HDD (sc1) type</strong></p>\n\n<p>Per the explanation in the detailed overview provided above, these three options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n",
                "options": [
                    {
                        "id": 10119,
                        "content": "<p>Amazon EC2 with EBS volume of cold HDD (sc1) type</p>",
                        "isValid": false
                    },
                    {
                        "id": 10120,
                        "content": "<p>Amazon EC2 with EBS volume of Throughput Optimized HDD (st1) type</p>",
                        "isValid": false
                    },
                    {
                        "id": 10121,
                        "content": "<p>Amazon EC2 with EBS volume of General Purpose SSD (gp2) type</p>",
                        "isValid": false
                    },
                    {
                        "id": 10122,
                        "content": "<p>Amazon EC2 with EBS volume of Provisioned IOPS SSD (io1) type</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2421,
            "attributes": {
                "createdAt": "2023-09-09T20:33:59.722Z",
                "updatedAt": "2023-09-09T20:33:59.722Z",
                "content": "<p>A medical devices company uses S3 buckets to store critical data. Hundreds of buckets are used to keep the data segregated and well organized. Recently, the development team noticed that the lifecycle policies on the S3 buckets have not been applied optimally, resulting in higher costs.</p>\n\n<p>As a Solutions Architect, can you recommend a solution to reduce storage costs on S3 while keeping the IT team's involvement to a minimum?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use S3 Intelligent-Tiering storage class to optimize the S3 storage costs</strong> - The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access.</p>\n\n<p>For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. There are no retrieval fees when using the S3 Intelligent-Tiering storage class, and no additional tiering fees when objects are moved between access tiers. It is the ideal storage class for long-lived data with access patterns that are unknown or unpredictable.</p>\n\n<p>S3 Storage Classes can be configured at the object level and a single bucket can contain objects stored in S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA. You can upload objects directly to S3 Intelligent-Tiering, or use S3 Lifecycle policies to transfer objects from S3 Standard and S3 Standard-IA to S3 Intelligent-Tiering. You can also archive objects from S3 Intelligent-Tiering to S3 Glacier.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon Elastic File System (Amazon EFS) to provide a fast, cost-effective and sharable storage service</strong> - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. EFS offers sharable service, unlike Amazon Elastic Block Storage (EBS) that cannot be shared by instances. EFS is costlier than storing data in Amazon S3. Also, EFS needs an Amazon EC2 instance or an AWS Direct Connect network connection. Hence, this is not the correct option.</p>\n\n<p><strong>Use S3 One Zone-Infrequent Access, to reduce the costs on S3 storage</strong> - S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of S3 Standard or S3 Standard-IA. Not a right option, since data stored is business-critical and cannot be risked by using S3 One Zone-IA.</p>\n\n<p><strong>Use S3 Outposts storage class to reduce the costs on S3 storage by storing the data on-premises</strong> - This is a distractor as Amazon S3 Outposts delivers object storage to your on-premises AWS Outposts environment. It is used in conjunction with AWS Outposts and has no relevance to the current use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n",
                "options": [
                    {
                        "id": 10123,
                        "content": "<p>Configure Amazon EFS to provide a fast, cost-effective and sharable storage service</p>",
                        "isValid": false
                    },
                    {
                        "id": 10124,
                        "content": "<p>Use S3 One Zone-Infrequent Access, to reduce the costs on S3 storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 10125,
                        "content": "<p>Use S3 Outposts storage class to reduce the costs on S3 storage by storing the data on-premises</p>",
                        "isValid": false
                    },
                    {
                        "id": 10126,
                        "content": "<p>Use S3 Intelligent-Tiering storage class to optimize the S3 storage costs</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2422,
            "attributes": {
                "createdAt": "2023-09-09T20:33:59.825Z",
                "updatedAt": "2023-09-09T20:33:59.825Z",
                "content": "<p>You have built an application that is deployed with an Elastic Load Balancer and an Auto Scaling Group. As a Solutions Architect, you have configured aggressive CloudWatch alarms, making your Auto Scaling Group (ASG) scale in and out very quickly, renewing your fleet of Amazon EC2 instances on a daily basis. A production bug appeared two days ago, but the team is unable to SSH into the instance to debug the issue, because the instance has already been terminated by the ASG. The log files are saved on the EC2 instance.</p>\n\n<p>How will you resolve the issue and make sure it doesn't happen again?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Install a CloudWatch Logs agents on the EC2 instances to send logs to CloudWatch</strong></p>\n\n<p>You can use the CloudWatch Logs agent installer on an existing EC2 instance to install and configure the CloudWatch Logs agent. After installation is complete, logs automatically flow from the instance to the log stream you create while installing the agent. The agent confirms that it has started and it stays running until you disable it.</p>\n\n<p>Here, the natural and by far the easiest solution would be to use the CloudWatch Logs agents on the EC2 instances to automatically send log files into CloudWatch, so we can analyze them in the future easily should any problem arise.</p>\n\n<p>To control whether an Auto Scaling group can terminate a particular instance when scaling in, use instance scale-in protection. You can enable the instance scale-in protection setting on an Auto Scaling group or on an individual Auto Scaling instance. When the Auto Scaling group launches an instance, it inherits the instance scale-in protection setting of the Auto Scaling group. You can change the instance scale-in protection setting for an Auto Scaling group or an Auto Scaling instance at any time.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Disable the Termination from the ASG any time a user reports an issue</strong> - Disabling the Termination from the ASG would prevent our ASG from being Elastic and impact our costs. Therefore this option is incorrect.</p>\n\n<p><strong>Make a snapshot of the EC2 instance just before it gets terminated</strong> - Making a snapshot of the EC2 instance before it gets terminated <em>could</em> work but it's tedious, not elastic and very expensive, since our interest is just the log files. Therefore this option is not the best fit for the given use-case.</p>\n\n<p>You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. This minimizes the time required to create the snapshot and saves on storage costs by not duplicating data.</p>\n\n<p><strong>Use AWS Lambda to regularly SSH into the EC2 instances and copy the log files to S3</strong> - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used for production-grade serverless log analytics. Using AWS Lambda would be extremely hard to use for this task. Therefore this option is not the best fit for the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartEC2Instance.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartEC2Instance.html</a></p>\n",
                "options": [
                    {
                        "id": 10127,
                        "content": "<p>Install a CloudWatch Logs agents on the EC2 instances to send logs to CloudWatch</p>",
                        "isValid": true
                    },
                    {
                        "id": 10128,
                        "content": "<p>Disable the Termination from the ASG any time a user reports an issue</p>",
                        "isValid": false
                    },
                    {
                        "id": 10129,
                        "content": "<p>Make a snapshot of the EC2 instance just before it gets terminated</p>",
                        "isValid": false
                    },
                    {
                        "id": 10130,
                        "content": "<p>Use AWS Lambda to regularly SSH into the EC2 instances and copy the log files to S3</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2423,
            "attributes": {
                "createdAt": "2023-09-09T20:34:00.115Z",
                "updatedAt": "2023-09-09T20:34:00.115Z",
                "content": "<p>An IT company has built a custom data warehousing solution for a retail organization by using Amazon Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older than a year) into S3, as the daily analytical reports consume data for just the last one year. However the analysts want to retain the ability to cross-reference this historical data along with the daily reports.</p>\n\n<p>The company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a solutions architect, which option would you recommend to facilitate this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Redshift Spectrum to create Redshift cluster tables pointing to the underlying historical data in S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift</strong></p>\n\n<p>Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis.</p>\n\n<p>Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables.</p>\n\n<p>Amazon Redshift Spectrum resides on dedicated Amazon Redshift servers that are independent of your cluster. Redshift Spectrum pushes many compute-intensive tasks, such as predicate filtering and aggregation, down to the Redshift Spectrum layer. Thus, Redshift Spectrum queries use much less of your cluster's processing capacity than other queries.</p>\n\n<p>Redshift Spectrum Overview\n<img src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2017/07/18/redshift_spectrum-1.gif\">\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Setup access to the historical data via Athena. The analytics team can run historical data queries on Athena and continue the daily reporting on Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis</strong>\nAmazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.\nProviding access to historical data via Athena would mean that historical data reconciliation would become difficult as the daily report would still be produced via Redshift. Such a setup is cumbersome to maintain on a day to day basis. Hence the option to use Athena is ruled out.</p>\n\n<p><strong>Use the Redshift COPY command to load the S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</strong></p>\n\n<p><strong>Use Glue ETL job to load the S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</strong></p>\n\n<p>Loading historical data into Redshift via COPY command or Glue ETL job would cost heavy for a one-time ad-hoc process. The same result can be achieved more cost-efficiently by using Redshift Spectrum. Therefore both these options to load historical data into Redshift are also incorrect for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html#c-spectrum-overview\nhttps://aws.amazon.com/blogs/big-data/\">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html#c-spectrum-overview\nhttps://aws.amazon.com/blogs/big-data/</a></p>\n\n<p><a href=\"#\">amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n",
                "options": [
                    {
                        "id": 10131,
                        "content": "<p>Use Redshift Spectrum to create Redshift cluster tables pointing to the underlying historical data in S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift</p>",
                        "isValid": true
                    },
                    {
                        "id": 10132,
                        "content": "<p>Setup access to the historical data via Athena. The analytics team can run historical data queries on Athena and continue the daily reporting on Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis</p>",
                        "isValid": false
                    },
                    {
                        "id": 10133,
                        "content": "<p>Use Glue ETL job to load the S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</p>",
                        "isValid": false
                    },
                    {
                        "id": 10134,
                        "content": "<p>Use the Redshift COPY command to load the S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2424,
            "attributes": {
                "createdAt": "2023-09-09T20:34:00.284Z",
                "updatedAt": "2023-09-09T20:34:00.284Z",
                "content": "<p>A company hires experienced specialists to analyze the customer service calls attended by its call center representatives. Now, the company wants to move to AWS Cloud and is looking at an automated solution to analyze customer service calls for sentiment analysis via ad-hoc SQL queries.</p>\n\n<p>As a Solutions Architect, which of the following solutions would you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Transcribe to convert audio files to text and Amazon Athena to understand the underlying customer sentiments</strong> - Amazon Transcribe is an automatic speech recognition (ASR) service that makes it easy to convert audio to text. One key feature of the service is called speaker identification, which you can use to label each individual speaker when transcribing multi-speaker audio files. You can specify Amazon Transcribe to identify 2–10 speakers in the audio clip.</p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. To leverage Athena, you can simply point to your data in Amazon S3, define the schema, and start querying using standard SQL. Most results are delivered within seconds.</p>\n\n<p>Analyzing multi-speaker audio files using Amazon Transcribe and Amazon Athena:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q58-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena\">https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis</strong> - Amazon Kinesis can be used to stream real-time data for further analysis and storage. Kinesis Data Streams cannot read audio files. You will still need to use AWS Transcribe for ASR services.</p>\n\n<p><strong>Use Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output</strong> -  Kinesis Data Streams cannot read audio files. Amazon Alexa cannot be used as an Automatic Speech Recognition (ASR) service, though Alexa internally uses ASR for its working.</p>\n\n<p><strong>Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to run analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for human analysis</strong> - Amazon Quicksight is for the visual representation of data through Dashboards, graphs and various other modes. It has a rich feature set that helps analyze data and the complex relationships that exist between different data features. However, it is not an SQL query based analysis tool like Amazon Athena.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena\">https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena</a></p>\n\n<p><a href=\"https://aws.amazon.com/athena\">https://aws.amazon.com/athena</a></p>\n",
                "options": [
                    {
                        "id": 10135,
                        "content": "<p>Use Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis</p>",
                        "isValid": false
                    },
                    {
                        "id": 10136,
                        "content": "<p>Use Amazon Transcribe to convert audio files to text and Amazon Athena to understand the underlying customer sentiments</p>",
                        "isValid": true
                    },
                    {
                        "id": 10137,
                        "content": "<p>Use Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output</p>",
                        "isValid": false
                    },
                    {
                        "id": 10138,
                        "content": "<p>Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to run analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for human analysis</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2425,
            "attributes": {
                "createdAt": "2023-09-09T20:34:00.381Z",
                "updatedAt": "2023-09-09T20:34:00.381Z",
                "content": "<p>A cyber security company is running a mission critical application using a single Spread placement group of EC2 instances. The company needs 15 Amazon EC2 instances for optimal performance.</p>\n\n<p>How many Availability Zones (AZs) will the company need to deploy these EC2 instances per the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>3</strong></p>\n\n<p>When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies:</p>\n\n<p>Cluster placement group</p>\n\n<p>Partition placement group</p>\n\n<p>Spread placement group.</p>\n\n<p>A Spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source.</p>\n\n<p>Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Launching instances in a spread placement group reduces the risk of simultaneous failures that might occur when instances share the same racks.</p>\n\n<p>A spread placement group can span multiple Availability Zones in the same Region. You can have a maximum of seven running instances per Availability Zone per group. Therefore, to deploy 15 EC2 instances in a single Spread placement group, the company needs to use 3 AZs.</p>\n\n<p>Spread placement group overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q11-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>7</strong></p>\n\n<p><strong>14</strong></p>\n\n<p><strong>15</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 10139,
                        "content": "<p>7</p>",
                        "isValid": false
                    },
                    {
                        "id": 10140,
                        "content": "<p>15</p>",
                        "isValid": false
                    },
                    {
                        "id": 10141,
                        "content": "<p>14</p>",
                        "isValid": false
                    },
                    {
                        "id": 10142,
                        "content": "<p>3</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2426,
            "attributes": {
                "createdAt": "2023-09-09T20:34:00.493Z",
                "updatedAt": "2023-09-09T20:34:00.493Z",
                "content": "<p>The engineering team at a weather tracking company wants to enhance the performance of its relation database and is looking for a caching solution that supports geospatial data.</p>\n\n<p>As a solutions architect, which of the following solutions will you suggest?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon ElastiCache for Redis</strong> - Amazon ElastiCache is a web service that makes it easy to set up, manage, and scale a distributed in-memory data store or cache environment in the cloud. Redis, which stands for Remote Dictionary Server, is a fast, open-source, in-memory key-value data store for use as a database, cache, message broker, and queue. Redis now delivers sub-millisecond response times enabling millions of requests per second for real-time applications in Gaming, Ad-Tech, Financial Services, Healthcare, and IoT. Redis is a popular choice for caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing, chat/messaging, media streaming, and pub/sub apps.</p>\n\n<p>All Redis data resides in the server’s main memory, in contrast to databases such as PostgreSQL, Cassandra, MongoDB and others that store most data on disk or on SSDs. In comparison to traditional disk based databases where most operations require a roundtrip to disk, in-memory data stores such as Redis don’t suffer the same penalty. They can therefore support an order of magnitude more operations and faster response times. The result is – blazing fast performance with average read or write operations taking less than a millisecond and support for millions of operations per second.</p>\n\n<p>Redis has purpose-built commands for working with real-time geospatial data at scale. You can perform operations like finding the distance between two elements (for example people or places) and finding all elements within a given distance of a point.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon ElastiCache for Memcached</strong> - Both Redis and MemCached are in-memory, open-source data stores. Memcached, a high-performance distributed memory cache service, is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases. Memcached does not offer support for geospatial data.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q65-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p><strong>Amazon DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB. DAX does not support relational databases.</p>\n\n<p><strong>AWS Global Accelerator</strong> - AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. This option has been added as a distractor, it has nothing to do with database caching.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n",
                "options": [
                    {
                        "id": 10143,
                        "content": "<p>Use Amazon ElastiCache for Redis</p>",
                        "isValid": true
                    },
                    {
                        "id": 10144,
                        "content": "<p>Use Amazon DynamoDB Accelerator (DAX)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10145,
                        "content": "<p>Use AWS Global Accelerator</p>",
                        "isValid": false
                    },
                    {
                        "id": 10146,
                        "content": "<p>Use Amazon ElastiCache for Memcached</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2427,
            "attributes": {
                "createdAt": "2023-09-09T20:34:00.587Z",
                "updatedAt": "2023-09-09T20:34:00.587Z",
                "content": "<p>As a Solutions Architect, you have been hired to work with the engineering team at a company to create a REST API using the serverless architecture.</p>\n\n<p>Which of the following solutions will you recommend to move the company to the serverless architecture?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>API Gateway exposing Lambda Functionality</strong></p>\n\n<p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\">\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p>API Gateway can expose Lambda functionality through RESTful APIs. Both are serverless options offered by AWS and hence the right choice for this scenario, considering all the functionality they offer.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Fargate with Lambda at the front</strong> - Lambda cannot directly handle RESTful API requests. You can invoke a Lambda function over HTTPS by defining a custom RESTful API using Amazon API Gateway. So, Fargate with Lambda as the front-facing service is a wrong combination, though both Fargate and Lambda are serverless.</p>\n\n<p><strong>Public-facing Application Load Balancer with ECS on Amazon EC2</strong> - ECS on Amazon EC2 does not come under serverless and hence cannot be considered for this use case.</p>\n\n<p><strong>Route 53 with EC2 as backend</strong> - Amazon EC2 is not a serverless service and hence cannot be considered for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/serverless/\">https://aws.amazon.com/serverless/</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n",
                "options": [
                    {
                        "id": 10147,
                        "content": "<p>Fargate with Lambda at the front</p>",
                        "isValid": false
                    },
                    {
                        "id": 10148,
                        "content": "<p>API Gateway exposing Lambda Functionality</p>",
                        "isValid": true
                    },
                    {
                        "id": 10149,
                        "content": "<p>Route 53 with EC2 as backend</p>",
                        "isValid": false
                    },
                    {
                        "id": 10150,
                        "content": "<p>Public-facing Application Load Balancer with ECS on Amazon EC2</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2428,
            "attributes": {
                "createdAt": "2023-09-09T20:34:00.681Z",
                "updatedAt": "2023-09-09T20:34:00.681Z",
                "content": "<p>A startup has created a cost-effective backup solution in another AWS Region. The application is running in warm standby mode and has Application Load Balancer (ALB) to support it from the front. The current failover process is manual and requires updating the DNS alias record to point to the secondary ALB in another Region in case of failure of the primary ALB.</p>\n\n<p>As a Solutions Architect, what will you recommend to automate the failover process?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable an Amazon Route 53 health check</strong> - Determining the health of an ELB endpoint is more complex than health checking a single IP address. For example, what if your application is running fine on EC2, but the load balancer itself isn't reachable? Or if your load balancer and your EC2 instances are working correctly, but a bug in your code causes your application to crash? Or how about if the EC2 instances in one Availability Zone of a multi-AZ ELB are experiencing problems?</p>\n\n<p>Route 53 DNS Failover handles all of these failure scenarios by integrating with ELB behind the scenes. Once enabled, Route 53 automatically configures and manages health checks for individual ELB nodes. Route 53 also takes advantage of the EC2 instance health checking that ELB performs (information on configuring your ELB health checks is available here). By combining the results of health checks of your EC2 instances and your ELBs, Route 53 DNS Failover can evaluate the health of the load balancer and the health of the application running on the EC2 instances behind it. In other words, if any part of the stack goes down, Route 53 detects the failure and routes traffic away from the failed endpoint.</p>\n\n<p>Using Route 53 DNS Failover, you can run your primary application simultaneously in multiple AWS regions around the world and failover across regions. Your end-users will be routed to the closest (by latency), healthy region for your application. Route 53 automatically removes from service any region where your application is unavailable - it will pull an endpoint out of service if there is region-wide connectivity or operational issue, if your application goes down in that region, or if your ELB or EC2 instances go down in that region.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable an ALB health check</strong> - ELB health check verifies that a specified TCP port on an instance is accepting connections or a specified page has returned an error code of 200. It is not useful for the given failover scenario.</p>\n\n<p><strong>Enable an EC2 instance health check</strong> - Instance status checks monitor the software and network configuration of your instance. It is not intelligent enough to understand if the application on the instance is working correctly. Hence, this is not the right choice for the given use-case.</p>\n\n<p><strong>Configure Trusted Advisor to check on unhealthy instances</strong> - AWS Trusted Advisor examines the health check configuration for Auto Scaling groups. If Elastic Load Balancing is being used for an Auto Scaling group, the recommended configuration is to enable an Elastic Load Balancing health check. Trusted Advisor recommends certain configuration changes by comparing your system configurations to AWS Best practices. It cannot handle a failover the way Route 53 does.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-route-53-elb-integration-dns-failover/\">https://aws.amazon.com/blogs/aws/amazon-route-53-elb-integration-dns-failover/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>\n",
                "options": [
                    {
                        "id": 10151,
                        "content": "<p>Configure Trusted Advisor to check on unhealthy instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10152,
                        "content": "<p>Enable an Amazon Route 53 health check</p>",
                        "isValid": true
                    },
                    {
                        "id": 10153,
                        "content": "<p>Enable an EC2 instance health check</p>",
                        "isValid": false
                    },
                    {
                        "id": 10154,
                        "content": "<p>Enable an ALB health check</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2429,
            "attributes": {
                "createdAt": "2023-09-09T20:34:00.819Z",
                "updatedAt": "2023-09-09T20:34:00.819Z",
                "content": "<p>An application hosted on Amazon EC2 contains sensitive personal information about all its customers and needs to be protected from all types of cyber-attacks. The company is considering using the AWS Web Application Firewall (WAF) to handle this requirement.</p>\n\n<p>Can you identify the correct solution leveraging the capabilities of WAF?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures</strong></p>\n\n<p>When you use AWS WAF with CloudFront, you can protect your applications running on any HTTP webserver, whether it's a webserver that's running in Amazon Elastic Compute Cloud (Amazon EC2) or a web server that you manage privately. You can also configure CloudFront to require HTTPS between CloudFront and your own webserver, as well as between viewers and CloudFront.</p>\n\n<p>AWS WAF is tightly integrated with Amazon CloudFront and the Application Load Balancer (ALB), services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn’t come at the expense of performance. Blocked requests are stopped before they reach your web servers. When you use AWS WAF on Application Load Balancer, your rules run in the region and can be used to protect internet-facing as well as internal load balancers.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure an Application Load Balancer (ALB) to balance the workload for all the EC2 instances. Configure CloudFront to distribute from an ALB since WAF cannot be directly configured on ALBs. This configuration not only provides necessary safety but is scalable too</strong> - This statement is wrong. You can configure WAF on Application Load Balancers (ALB).</p>\n\n<p><em>AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data</em>* - AWS WAF can be deployed on Amazon CloudFront, the Application Load Balancer (ALB), and Amazon API Gateway. It cannot be configured directly on an EC2 instance.</p>\n\n<p><strong>AWS WAF can be directly configured only on an Application Load Balancer (ALB) or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture</strong> - This statement is only partially correct. WAF can also be deployed on Amazon CloudFront service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/waf/faqs/\">https://aws.amazon.com/waf/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p>\n",
                "options": [
                    {
                        "id": 10155,
                        "content": "<p>AWS WAF can be directly configured only on an ALB or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture</p>",
                        "isValid": false
                    },
                    {
                        "id": 10156,
                        "content": "<p>AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data</p>",
                        "isValid": false
                    },
                    {
                        "id": 10157,
                        "content": "<p>Configure an ALB to balance the workload for all the EC2 instances. Configure CloudFront to distribute from an ALB since WAF cannot be directly configured on ALBs. This configuration not only provides necessary safety but is scalable too</p>",
                        "isValid": false
                    },
                    {
                        "id": 10158,
                        "content": "<p>Create a CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2430,
            "attributes": {
                "createdAt": "2023-09-09T20:34:00.949Z",
                "updatedAt": "2023-09-09T20:34:00.949Z",
                "content": "<p>An Internet-of-Things (IoT) company is looking for a database solution on AWS Cloud that has Auto Scaling capabilities and is highly available. The database should be able to handle any changes in data attributes over time, in case the company updates the data feed from its IoT devices. The database must provide the capability to output a continuous stream with details of any changes to the underlying data.</p>\n\n<p>As a Solutions Architect, which database will you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon DynamoDB</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second. DynamoDB is serverless with no servers to provision, patch, or manage and no software to install, maintain, or operate.</p>\n\n<p>A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.</p>\n\n<p>Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the \"before\" and \"after\" images of modified items.</p>\n\n<p>DynamoDB is horizontally scalable, has a DynamoDB streams capability and is multi-AZ by default. On top of it, we can adjust the RCU and WCU automatically using Auto Scaling. This is the right choice for current requirements.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Relational Database Service (Amazon RDS)</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. Schema changes on relational databases are not straight forward and are hard to maintain if the schema requirements change often.</p>\n\n<p><strong>Amazon Aurora</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database. Schema changes on relational databases are not straight forward and are hard to maintain if the schema requirements change often.</p>\n\n<p><strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud based data warehouse product designed for large scale data set storage and analysis. It is a powerful warehousing service from Amazon. The current requirement, however, is not looking for a warehousing solution and hence Redshift is not an option here.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n",
                "options": [
                    {
                        "id": 10159,
                        "content": "<p>Amazon Relational Database Service (Amazon RDS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10160,
                        "content": "<p>Amazon Aurora</p>",
                        "isValid": false
                    },
                    {
                        "id": 10161,
                        "content": "<p>Amazon Redshift</p>",
                        "isValid": false
                    },
                    {
                        "id": 10162,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2431,
            "attributes": {
                "createdAt": "2023-09-09T20:34:01.046Z",
                "updatedAt": "2023-09-09T20:34:01.046Z",
                "content": "<p>Reporters at a news agency upload/download video files (about 500MB each) to/from an S3 bucket as part of their daily work. As the agency has started offices in remote locations, it has resulted in poor latency for uploading and accessing data to/from the given S3 bucket. The agency wants to continue using a serverless storage solution such as S3 but wants to improve the performance.</p>\n\n<p>As a solutions architect, which of the following solutions do you propose to address this issue? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use Amazon CloudFront distribution with origin as the S3 bucket. This would speed up uploads as well as downloads for the video files</strong></p>\n\n<p>Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, within a developer-friendly environment.\nWhen an object from S3 that is set up with CloudFront CDN is requested, the request would come through the Edge Location transfer paths only for the first request. Thereafter, it would be served from the nearest edge location to the users until it expires. So in this way, you can speed up uploads as well as downloads for the video files.</p>\n\n<p>Following is a good reference blog for a deep-dive:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/\">https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/</a></p>\n\n<p><strong>Enable Amazon S3 Transfer Acceleration for the S3 bucket. This would speed up uploads as well as downloads for the video files</strong></p>\n\n<p>Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. So this option is also correct.</p>\n\n<p>Transfer Acceleration Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q21-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/s3/transfer-acceleration/\">https://aws.amazon.com/s3/transfer-acceleration/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create new S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets</strong> - Creating new S3 buckets in every region is not an option, since the agency maintains centralized storage. Hence this option is incorrect.</p>\n\n<p><strong>Move S3 data into EFS file system created in a US region, connect to EFS file system from EC2 instances in other AWS regions using an inter-region VPC peering connection</strong></p>\n\n<p><strong>Spin up EC2 instances in each region where the agency has a remote office. Create a daily job to transfer S3 data into EBS volumes attached to the EC2 instances</strong></p>\n\n<p>Both these options using EC2 instances are not correct for the given use-case, as the agency wants a serverless storage solution.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/transfer-acceleration/\">https://aws.amazon.com/s3/transfer-acceleration/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/\">https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/</a></p>\n",
                "options": [
                    {
                        "id": 10163,
                        "content": "<p>Use Amazon CloudFront distribution with origin as the S3 bucket. This would speed up uploads as well as downloads for the video files</p>",
                        "isValid": true
                    },
                    {
                        "id": 10164,
                        "content": "<p>Move S3 data into EFS file system created in a US region, connect to EFS file system from EC2 instances in other AWS regions using an inter-region VPC peering connection</p>",
                        "isValid": false
                    },
                    {
                        "id": 10165,
                        "content": "<p>Create new S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets</p>",
                        "isValid": false
                    },
                    {
                        "id": 10166,
                        "content": "<p>Spin up EC2 instances in each region where the agency has a remote office. Create a daily job to transfer S3 data into EBS volumes attached to the EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10167,
                        "content": "<p>Enable Amazon S3 Transfer Acceleration for the S3 bucket. This would speed up uploads as well as downloads for the video files</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2432,
            "attributes": {
                "createdAt": "2023-09-09T20:34:01.154Z",
                "updatedAt": "2023-09-09T20:34:01.154Z",
                "content": "<p>A multi-national company is looking at optimizing their AWS resources across various countries and regions. They want to understand the best practices on cost optimization, performance, and security for their system architecture spanning across multiple business units.</p>\n\n<p>Which AWS service is the best fit for their requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Trusted Advisor</strong></p>\n\n<p>AWS Trusted Advisor is an online tool that draws upon best practices learned from AWS’s aggregated operational history of serving hundreds of thousands of AWS customers. Trusted Advisor inspects your AWS environment and makes recommendations for saving money, improving system performance, or closing security gaps. It scans your AWS infrastructure and compares it to AWS Best practices in five categories (Cost Optimization, Performance, Security, Fault Tolerance, Service limits) and then provides recommendations.</p>\n\n<p>How Trusted Advisor Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/AWS%20Support/AWS-trusted-advisor.5b9909d5f29f680eeb12ccff536e8d88d8701304.png\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Config</strong> - AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”. It does not offer any feedback about architectural best practices.</p>\n\n<p><strong>AWS Management Console</strong> - The AWS Management Console is a web application that comprises and refers to a broad collection of service consoles for managing Amazon Web Services. You log into your AWS account using the AWS Management console. It does not offer any feedback about architectural best practices.</p>\n\n<p><strong>AWS Systems Manager</strong> - AWS Systems Manager is an AWS service that you can use to view and control your infrastructure on AWS. Using the Systems Manager console, you can view operational data from multiple AWS services and automate operational tasks across your AWS resources. With Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. It does not offer any feedback about architectural best practices.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/</a></p>\n",
                "options": [
                    {
                        "id": 10168,
                        "content": "<p>AWS Config</p>",
                        "isValid": false
                    },
                    {
                        "id": 10169,
                        "content": "<p>AWS Management Console</p>",
                        "isValid": false
                    },
                    {
                        "id": 10170,
                        "content": "<p>AWS Trusted Advisor</p>",
                        "isValid": true
                    },
                    {
                        "id": 10171,
                        "content": "<p>AWS Systems Manager</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2433,
            "attributes": {
                "createdAt": "2023-09-09T20:34:01.267Z",
                "updatedAt": "2023-09-09T20:34:01.267Z",
                "content": "<p>A DevOps engineer at an organization is debugging issues related to an Amazon EC2 instance. The engineer has SSH'ed into the instance and he needs to retrieve the instance public IP from within a shell script running on the instance command line.</p>\n\n<p>Can you identify the correct URL path to get the instance public IP?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>http://169.254.169.254/latest/meta-data/public-ipv4</strong></p>\n\n<p>Instance metadata is the data about your instance that you can use to configure or manage the running instance.</p>\n\n<p>Instance user data is the data that you specified in the form of a configuration script while launching your instance.</p>\n\n<p>The following URL paths can be used to get the instance meta data and user data from within the instance:\nhttp://169.254.169.254/latest/meta-data/</p>\n\n<p>http://169.254.169.254/latest/user-data/</p>\n\n<p>Further, you can get the instance public IP via the URL - http://169.254.169.254/latest/meta-data/public-ipv4</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>http://169.254.169.254/latest/user-data/public-ipv4</strong></p>\n\n<p><strong>http://254.169.254.169/latest/meta-data/public-ipv4</strong></p>\n\n<p><strong>http://254.169.254.169/latest/user-data/public-ipv4</strong></p>\n\n<p>These three options do not meet the specification for the URL path to get the instance public IP, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-add-user-data.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-add-user-data.html</a></p>\n",
                "options": [
                    {
                        "id": 10172,
                        "content": "<p>http://169.254.169.254/latest/meta-data/public-ipv4</p>",
                        "isValid": true
                    },
                    {
                        "id": 10173,
                        "content": "<p>http://254.169.254.169/latest/meta-data/public-ipv4</p>",
                        "isValid": false
                    },
                    {
                        "id": 10174,
                        "content": "<p>http://169.254.169.254/latest/user-data/public-ipv4</p>",
                        "isValid": false
                    },
                    {
                        "id": 10175,
                        "content": "<p>http://254.169.254.169/latest/user-data/public-ipv4</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2434,
            "attributes": {
                "createdAt": "2023-09-09T20:34:01.378Z",
                "updatedAt": "2023-09-09T20:34:01.378Z",
                "content": "<p>A company wants to ensure high availability for its RDS database. The development team wants to opt for Multi-AZ deployment and they would like to understand what happens when the primary instance of the Multi-AZ configuration goes down.</p>\n\n<p>As a Solutions Architect, which of the following will you identify as the outcome of the scenario?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The CNAME record will be updated to point to the standby DB</strong> - Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology. SQL Server DB instances use SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs).</p>\n\n<p>In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption.</p>\n\n<p>Failover is automatically handled by Amazon RDS so that you can resume database operations as quickly as possible without administrative intervention. When failing over, Amazon RDS simply flips the canonical name record (CNAME) for your DB instance to point at the standby, which is in turn promoted to become the new primary. Multi-AZ means the URL is the same, the failover is automated, and the CNAME will automatically be updated to point to the standby database.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The URL to access the database will change to the standby DB</strong> - As discussed above, URL remains the same.</p>\n\n<p><strong>An email will be sent to the System Administrator asking for manual intervention</strong> - This option is incorrect and it has been added as a distractor.</p>\n\n<p><strong>The application will be down until the primary database has recovered itself</strong> - This option is incorrect and it has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 10176,
                        "content": "<p>The CNAME record will be updated to point to the standby DB</p>",
                        "isValid": true
                    },
                    {
                        "id": 10177,
                        "content": "<p>An email will be sent to the System Administrator asking for manual intervention</p>",
                        "isValid": false
                    },
                    {
                        "id": 10178,
                        "content": "<p>The URL to access the database will change to the standby DB</p>",
                        "isValid": false
                    },
                    {
                        "id": 10179,
                        "content": "<p>The application will be down until the primary database has recovered itself</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2435,
            "attributes": {
                "createdAt": "2023-09-09T20:34:01.499Z",
                "updatedAt": "2023-09-09T20:34:01.499Z",
                "content": "<p>The engineering team at a retail company manages 3 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for the PostgreSQL DB instance. As an AWS Certified Solutions Architect Associate, you have been tasked to make the database instance resilient from a disaster recovery perspective.</p>\n\n<p>Which of the following features will help you in disaster recovery of the database? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use cross-Region Read Replicas</strong></p>\n\n<p>In addition to using Read Replicas to reduce the load on your source DB instance, you can also use Read Replicas to implement a DR solution for your production DB environment. If the source DB instance fails, you can promote your Read Replica to a standalone source server. Read Replicas can also be created in a different Region than the source database. Using a cross-Region Read Replica can help ensure that you get back up and running if you experience a regional availability issue.</p>\n\n<p><strong>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions</strong></p>\n\n<p>Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology.</p>\n\n<p>The automated backup feature of Amazon RDS enables point-in-time recovery for your database instance. Amazon RDS will back up your database and transaction logs and store both for a user-specified retention period. If it’s a Multi-AZ configuration, backups occur on standby to reduce the I/O impact on the primary. Amazon RDS supports Cross-Region Automated Backups. Manual snapshots and Read Replicas are also supported across multiple Regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region</strong> - This is an incorrect statement. Automated backups can be created across AWS Regions.</p>\n\n<p><strong>Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</strong> - Amazon RDS Provisioned IOPS Storage is an SSD-backed storage option designed to deliver fast, predictable, and consistent I/O performance. This storage type enhances the performance of the RDS database, but this isn't a disaster recovery option.</p>\n\n<p><strong>Use the database cloning feature of the RDS DB cluster</strong> - This option has been added as a distractor. Database cloning is only available for Aurora and not for RDS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/\">https://aws.amazon.com/rds/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2021/07/amazon-rds-cross-region-automated-backups-regional-expansion/\">https://aws.amazon.com/about-aws/whats-new/2021/07/amazon-rds-cross-region-automated-backups-regional-expansion/</a></p>\n",
                "options": [
                    {
                        "id": 10180,
                        "content": "<p>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions</p>",
                        "isValid": true
                    },
                    {
                        "id": 10181,
                        "content": "<p>Use cross-Region Read Replicas</p>",
                        "isValid": true
                    },
                    {
                        "id": 10182,
                        "content": "<p>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region</p>",
                        "isValid": false
                    },
                    {
                        "id": 10183,
                        "content": "<p>Use the database cloning feature of the RDS DB cluster</p>",
                        "isValid": false
                    },
                    {
                        "id": 10184,
                        "content": "<p>Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2436,
            "attributes": {
                "createdAt": "2023-09-09T20:34:01.662Z",
                "updatedAt": "2023-09-09T20:34:01.662Z",
                "content": "<p>A retail company maintains a Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 MB and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 KB.</p>\n\n<p>Which of the following options offers the LOWEST data transfer egress cost for the company?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region</strong></p>\n\n<p>AWS Direct Connect is a networking service that provides an alternative to using the internet to connect to AWS. Using AWS Direct Connect, data that would have previously been transported over the internet is delivered through a private network connection between your on-premises data center and AWS.</p>\n\n<p>For the given use case, the main pricing parameter while using the Direct Connect connection is the Data Transfer Out (DTO) from AWS to the on-premises data center. DTO refers to the cumulative network traffic that is sent through AWS Direct Connect to destinations outside of AWS. This is charged per gigabyte (GB), and unlike capacity measurements, DTO refers to the amount of data transferred, not the speed.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q2-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/directconnect/pricing/\">https://aws.amazon.com/directconnect/pricing/</a></p>\n\n<p>Each query response is 60MB in size and each webpage for the visualization tool is 600KB in size. If you deploy the visualization tool in the same AWS region as the data warehouse, then you only need to pay for the 600KB of DTO charges for the webpage. Therefore this option is correct.</p>\n\n<p>However, if you deploy the visualization tool on-premises, then you need to pay for the 60 MB of DTO charges for the query response from the data warehouse to the visualization tool.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region</strong></p>\n\n<p><strong>Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region</strong></p>\n\n<p>Data transfer pricing over Direct Connect is lower than data transfer pricing over the internet, so both of these options are incorrect.</p>\n\n<p><strong>Deploy the visualization tool on-premises. Query the data warehouse directly over a Direct Connect connection at a location in the same AWS region</strong> - As mentioned in the explanation above, if you deploy the visualization tool on-premises, then you need to pay for the 60 MB of DTO charges for the query response from the data warehouse to the visualization tool. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/directconnect/pricing/\">https://aws.amazon.com/directconnect/pricing/</a></p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/hands-on/connect-data-center-to-aws/services-costs/\">https://aws.amazon.com/getting-started/hands-on/connect-data-center-to-aws/services-costs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/directconnect/faqs/\">https://aws.amazon.com/directconnect/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 10185,
                        "content": "<p>Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region</p>",
                        "isValid": false
                    },
                    {
                        "id": 10186,
                        "content": "<p>Deploy the visualization tool on-premises. Query the data warehouse directly over a Direct Connect connection at a location in the same AWS region</p>",
                        "isValid": false
                    },
                    {
                        "id": 10187,
                        "content": "<p>Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region</p>",
                        "isValid": false
                    },
                    {
                        "id": 10188,
                        "content": "<p>Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2437,
            "attributes": {
                "createdAt": "2023-09-09T20:34:01.847Z",
                "updatedAt": "2023-09-09T20:34:01.847Z",
                "content": "<p>A CRM application is facing user experience issues with users reporting frequent sign-in requests from the application. The application is currently hosted on multiple EC2 instances behind an Application Load Balancer. The engineering team has identified the root cause as unhealthy servers causing session data to be lost. The team would like to implement a distributed in-memory cache-based session management solution.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Elasticache for distributed cache-based session management</strong></p>\n\n<p>Amazon ElastiCache can be used as a distributed in-memory cache for session management. Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Session stores can be set up using both Memcached or Redis for ElastiCache.</p>\n\n<p>Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.</p>\n\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Session stores are easy to create with Amazon ElastiCache for Memcached.</p>\n\n<p>How ElastiCache Works:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_how-it-works.ec509f8b878f549b7fb8a49669bf2547878303f6.png\">\nvia - <a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use RDS for distributed in-memory cache-based session management</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It cannot be used as a distributed in-memory cache for session management, hence this option is incorrect.</p>\n\n<p><strong>Use DynamoDB for distributed in-memory cache-based session management</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. DynamoDB is a NoSQL database and is not the right fit for a distributed in-memory cache-based session management solution.</p>\n\n<p><strong>Use Application Load Balancer sticky sessions</strong> - Although sticky sessions enable each user to interact with one server and one server only, however, in case of an unhealthy server, all the session data is gone as well. Therefore Elasticache powered distributed in-memory cache-based session management is a better solution.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/\">https://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n",
                "options": [
                    {
                        "id": 10189,
                        "content": "<p>Use Application Load Balancer sticky sessions</p>",
                        "isValid": false
                    },
                    {
                        "id": 10190,
                        "content": "<p>Use DynamoDB for distributed in-memory cache based session management</p>",
                        "isValid": false
                    },
                    {
                        "id": 10191,
                        "content": "<p>Use Elasticache for distributed in-memory cache based session management</p>",
                        "isValid": true
                    },
                    {
                        "id": 10192,
                        "content": "<p>Use RDS for distributed in-memory cache based session management</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2438,
            "attributes": {
                "createdAt": "2023-09-09T20:34:01.941Z",
                "updatedAt": "2023-09-09T20:34:01.941Z",
                "content": "<p>An e-commerce company uses Amazon SQS queues to decouple their application architecture. The engineering team has observed message processing failures for some customer orders.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend for handling such message failures?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use a dead-letter queue to handle message processing failures</strong></p>\n\n<p>Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.\nSometimes, messages can’t be processed because of a variety of possible issues, such as when a user comments on a story but it remains unprocessed because the original story itself is deleted by the author while the comments were being posted. In such a case, the dead-letter queue can be used to handle message processing failures.</p>\n\n<p>How do dead-letter queues work?\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a temporary queue to handle message processing failures</strong> - The most common use case for temporary queues is the request-response messaging pattern (for example, processing a login request), where a requester creates a temporary queue for receiving each response message. To avoid creating an Amazon SQS queue for each response message, the Temporary Queue Client lets you create and delete multiple temporary queues without making any Amazon SQS API calls. Temporary queues cannot be used to handle message processing failures.</p>\n\n<p><strong>Use short polling to handle message processing failures</strong></p>\n\n<p><strong>Use long polling to handle message processing failures</strong></p>\n\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.\nNeither short polling nor long polling can be used to handle message processing failures.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n",
                "options": [
                    {
                        "id": 10193,
                        "content": "<p>Use a temporary queue to handle message processing failures</p>",
                        "isValid": false
                    },
                    {
                        "id": 10194,
                        "content": "<p>Use short polling to handle message processing failures</p>",
                        "isValid": false
                    },
                    {
                        "id": 10195,
                        "content": "<p>Use a dead-letter queue to handle message processing failures</p>",
                        "isValid": true
                    },
                    {
                        "id": 10196,
                        "content": "<p>Use long polling to handle message processing failures</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2439,
            "attributes": {
                "createdAt": "2023-09-09T20:34:02.044Z",
                "updatedAt": "2023-09-09T20:34:02.044Z",
                "content": "<p>The data engineering team at an e-commerce company has set up a workflow to ingest the clickstream data into the raw zone of the S3 data lake. The team wants to run some SQL based data sanity checks on the raw zone of the data lake.</p>\n\n<p>What AWS services would you recommend for this use-case such that the solution is cost-effective and easy to maintain?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Athena to run SQL based analytics against S3 data</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>AWS Athena Benefits:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q12-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Load the incremental raw zone data into Redshift on an hourly basis and run the SQL based sanity checks</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis.\nAs the development team would have to maintain and monitor the Redshift cluster size and would require significant development time to set up the processes to consume the data periodically, so this option is ruled out.</p>\n\n<p><strong>Load the incremental raw zone data into an EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nUsing an EMR cluster would imply managing the underlying infrastructure so it’s ruled out because the correct solution for the given use-case should require the least amount of development effort and ongoing maintenance.</p>\n\n<p><strong>Load the incremental raw zone data into RDS on an hourly basis and run the SQL based sanity checks</strong> - Loading the incremental data into RDS implies data migration jobs will have to be written via a Lambda function or an EC2 based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance. Hence this option is not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n",
                "options": [
                    {
                        "id": 10197,
                        "content": "<p>Load the incremental raw zone data into RDS on an hourly basis and run the SQL based sanity checks</p>",
                        "isValid": false
                    },
                    {
                        "id": 10198,
                        "content": "<p>Use Athena to run SQL based analytics against S3 data</p>",
                        "isValid": true
                    },
                    {
                        "id": 10199,
                        "content": "<p>Load the incremental raw zone data into an EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks</p>",
                        "isValid": false
                    },
                    {
                        "id": 10200,
                        "content": "<p>Load the incremental raw zone data into Redshift on an hourly basis and run the SQL based sanity checks</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2440,
            "attributes": {
                "createdAt": "2023-09-09T20:34:02.126Z",
                "updatedAt": "2023-09-09T20:34:02.126Z",
                "content": "<p>You are a cloud architect at an IT company. The company has multiple enterprise customers that manage their own mobile apps that capture and send data to Amazon Kinesis Data Streams. They have been getting a <code>ProvisionedThroughputExceededException</code> exception. You have been contacted to help and upon analysis, you notice that messages are being sent one by one at a high rate.</p>\n\n<p>Which of the following options will help with the exception while keeping costs at a minimum?</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use batch messages</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Kinesis Data Streams Overview:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p>When a host needs to send many records per second (RPS) to Amazon Kinesis, simply calling the basic PutRecord API action in a loop is inadequate. To reduce overhead and increase throughput, the application must batch records and implement parallel HTTP requests. This will increase the efficiency overall and ensure you are optimally using the shards.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Exponential Backoff</strong>: While this may help in the short term, as soon as the request rate increases, you will see the <code>ProvisionedThroughputExceededException</code> exception again.</p>\n\n<p><strong>Increase the number of shards</strong> - Increasing shards could be a short term fix but will substantially increase the cost, so this option is ruled out.</p>\n\n<p><strong>Decrease the Stream retention duration</strong> - This operation may result in data loss and won't help with the exceptions, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/\">https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n",
                "options": [
                    {
                        "id": 10201,
                        "content": "<p>Increase the number of shards</p>",
                        "isValid": false
                    },
                    {
                        "id": 10202,
                        "content": "<p>Use batch messages</p>",
                        "isValid": true
                    },
                    {
                        "id": 10203,
                        "content": "<p>Decrease the Stream retention duration</p>",
                        "isValid": false
                    },
                    {
                        "id": 10204,
                        "content": "<p>Use Exponential Backoff</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2441,
            "attributes": {
                "createdAt": "2023-09-09T20:34:02.234Z",
                "updatedAt": "2023-09-09T20:34:02.234Z",
                "content": "<p>A developer in your team has set up a classic 3 tier architecture composed of an Application Load Balancer, an Auto Scaling group managing a fleet of EC2 instances, and an Aurora database. As a Solutions Architect, you would like to adhere to the security pillar of the well-architected framework.</p>\n\n<p>How do you configure the security group of the Aurora database to only allow traffic coming from the EC2 instances?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Add a rule authorizing the EC2 security group</strong></p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.</p>\n\n<p>The following are the characteristics of security group rules:</p>\n\n<p>By default, security groups allow all outbound traffic.</p>\n\n<p>Security group rules are always permissive; you can't create rules that deny access.</p>\n\n<p>Security groups are stateful.</p>\n\n<p>For the given scenario, the EC2 instances that are part of the ASG are the ones accessing the database layer. The correct response is to add a rule to the security group attached to Aurora authorizing the EC2 instance's security group.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add a rule authorizing the Aurora security group</strong> - Adding a rule, authorizing the Aurora security group, is just a distractor. Since it has no bearing on traffic allowed from the EC2 instances.</p>\n\n<p><strong>Add a rule authorizing the ASG's subnets CIDR</strong> - Authorizing the entire CIDR of the ASG's subnets is overkill and would allow non-ASG instances, access Aurora if they were part of the same CIDR.</p>\n\n<p><strong>Add a rule authorizing the ELB security group</strong> - Adding a rule authorizing the ELB security group would dilute the security for the Aurora databases because only the EC2 instances that are part of the ASG are the ones accessing the database layer. Therefore, it is not the correct option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 10205,
                        "content": "<p>Add a rule authorizing the ELB security group</p>",
                        "isValid": false
                    },
                    {
                        "id": 10206,
                        "content": "<p>Add a rule authorizing the EC2 security group</p>",
                        "isValid": true
                    },
                    {
                        "id": 10207,
                        "content": "<p>Add a rule authorizing the Aurora security group</p>",
                        "isValid": false
                    },
                    {
                        "id": 10208,
                        "content": "<p>Add a rule authorizing the ASG's subnets CIDR</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2442,
            "attributes": {
                "createdAt": "2023-09-09T20:34:02.329Z",
                "updatedAt": "2023-09-09T20:34:02.329Z",
                "content": "<p>Your firm has implemented a multi-tiered networking structure within the VPC - with two public and two private subnets. The public subnets are used to deploy the Application Load Balancers, while the two private subnets are used to deploy the application on Amazon EC2 instances. The development team wants the EC2 instances to have access to the internet. The solution has to be fully managed by AWS and needs to work over IPv4.</p>\n\n<p>What will you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>NAT Gateways deployed in your public subnet</strong> - You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. A NAT gateway has the following characteristics and limitations:</p>\n\n<ol>\n<li>A NAT gateway supports 5 Gbps of bandwidth and automatically scales up to 45 Gbps.</li>\n<li>You can associate exactly one Elastic IP address with a NAT gateway.</li>\n<li>A NAT gateway supports the following protocols: TCP, UDP, and ICMP.</li>\n<li>You cannot associate a security group with a NAT gateway.</li>\n<li>You can use a network ACL to control the traffic to and from the subnet in which the NAT gateway is located.</li>\n<li>A NAT gateway can support up to 55,000 simultaneous connections to each unique destination.</li>\n</ol>\n\n<p>Therefore you must use a NAT Gateway in your public subnet in order to provide internet access to your instances in your private subnets. You are charged for creating and using a NAT gateway in your account. NAT gateway hourly usage and data processing rates apply.</p>\n\n<p>Comparison of NAT instances and NAT gateways:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>NAT Instances deployed in your public subnet</strong> - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the Internet. Amazon provides Amazon Linux AMIs that are configured to run as NAT instances. These AMIs include the string amzn-ami-vpc-nat in their names, so you can search for them in the Amazon EC2 console. NAT Instances would work but won't scale and you would have to manage them (as they're nothing but EC2 instances).</p>\n\n<p><strong>Internet Gateways deployed in your private subnet</strong> - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. Internet Gateways must be deployed in a public subnet, hence not an option here.</p>\n\n<p><strong>Egress-Only Internet Gateways deployed in your private subnet</strong> - An Egress-Only Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances. Egress-Only Internet Gateways are for IPv6, not IPv4. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html</a></p>\n",
                "options": [
                    {
                        "id": 10209,
                        "content": "<p>Internet Gateways deployed in your private subnet</p>",
                        "isValid": false
                    },
                    {
                        "id": 10210,
                        "content": "<p>Egress-Only Internet Gateways deployed in your private subnet</p>",
                        "isValid": false
                    },
                    {
                        "id": 10211,
                        "content": "<p>NAT Instances deployed in your public subnet</p>",
                        "isValid": false
                    },
                    {
                        "id": 10212,
                        "content": "<p>NAT Gateways deployed in your public subnet</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2443,
            "attributes": {
                "createdAt": "2023-09-09T20:34:02.557Z",
                "updatedAt": "2023-09-09T20:34:02.557Z",
                "content": "<p>A retail company wants to establish encrypted network connectivity between its on-premises data center and AWS Cloud. The company wants to get the solution up and running in the fastest possible time and it should also support encryption in transit.</p>\n\n<p>As a solutions architect, which of the following solutions would you suggest to the company?</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud</strong></p>\n\n<p>AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your on-premises network and Amazon VPC over the Internet. IPsec is a protocol suite for securing IP communications by authenticating and encrypting each IP packet in a data stream.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not encrypt your traffic that is in transit. To encrypt the data in transit that traverses AWS Direct Connect, you must use the transit encryption options for that service. As AWS Direct Connect does not support encrypted network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect.</p>\n\n<p><strong>Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud</strong> - AWS DataSync makes it simple and fast to move large amounts of data online between on-premises storage and AWS. DataSync eliminates or automatically handles many of these tasks, including scripting copy jobs, scheduling, and monitoring transfers, validating data, and optimizing network utilization. As AWS Data Sync cannot be used to establish network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect.</p>\n\n<p><strong>Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud</strong> - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. As AWS Secrets Manager cannot be used to establish network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/internetwork-traffic-privacy.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/internetwork-traffic-privacy.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html</a></p>\n",
                "options": [
                    {
                        "id": 10213,
                        "content": "<p>Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud</p>",
                        "isValid": false
                    },
                    {
                        "id": 10214,
                        "content": "<p>Use Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud</p>",
                        "isValid": true
                    },
                    {
                        "id": 10215,
                        "content": "<p>Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud</p>",
                        "isValid": false
                    },
                    {
                        "id": 10216,
                        "content": "<p>Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2444,
            "attributes": {
                "createdAt": "2023-09-09T20:34:03.597Z",
                "updatedAt": "2023-09-09T20:34:03.597Z",
                "content": "<p>A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company needs at least 10 TB of storage with the maximum possible I/O performance for processing certain files which are mostly large videos. The company also needs close to 450 TB of very durable storage for storing media content and almost double of it, i.e. 900 TB for archival of legacy data.</p>\n\n<p>As a Solutions Architect, which set of services will you recommend to meet these requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage</strong> - An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.</p>\n\n<p>You can specify instance store volumes for an instance only when you launch it. You can't detach an instance store volume from one instance and attach it to a different instance.</p>\n\n<p>Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures.</p>\n\n<p>S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.</p>\n\n<p>S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with or cheaper than on-premises solutions. To keep costs low yet suitable for varying needs, S3 Glacier provides three retrieval options that range from a few minutes to hours. You can upload objects directly to S3 Glacier, or use S3 Lifecycle policies to transfer data between any of the S3 Storage Classes for active data (S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA) and S3 Glacier.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage</strong> - Amazon EC2 instance store volumes provide the best I/O performance for low latency requirement, as in the current use case. The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead.</p>\n\n<p>S3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice a year. It is designed for customers — particularly those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors — that retain data sets for 7-10 years or longer to meet regulatory compliance requirements.</p>\n\n<p><strong>Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage</strong> - Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with EC2 instances. EBS volumes are particularly well-suited for use as the primary storage for file systems, databases, or for any applications that require fine granular updates and access to raw, unformatted, block-level storage. For high I/O performance, instance store volumes are a better option.</p>\n\n<p><strong>Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage</strong> - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. Storage Gateway will be the right answer if the customer wanted to retain the on-premises data storage and just move the applications to AWS Cloud. In the absence of such requirements, instance store is a better option for high performance and Amazon S3 for durable storage.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n",
                "options": [
                    {
                        "id": 10217,
                        "content": "<p>Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 10218,
                        "content": "<p>Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 10219,
                        "content": "<p>Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 10220,
                        "content": "<p>Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2445,
            "attributes": {
                "createdAt": "2023-09-09T20:34:03.719Z",
                "updatedAt": "2023-09-09T20:34:03.719Z",
                "content": "<p>An application with global users across AWS Regions had suffered an issue when the Elastic Load Balancer (ELB) in a Region malfunctioned thereby taking down the traffic with it. The manual intervention cost the company significant time and resulted in major revenue loss.</p>\n\n<p>What should a solutions architect recommend to reduce internet latency and add automatic failover across AWS Regions?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations</strong></p>\n\n<p>As your application architecture grows, so does the complexity, with longer user-facing IP lists and more nuanced traffic routing logic. AWS Global Accelerator solves this by providing you with two static IPs that are anycast from our globally distributed edge locations, giving you a single entry point to your application, regardless of how many AWS Regions it’s deployed in. This allows you to add or remove origins, Availability Zones or Regions without reducing your application availability. Your traffic routing is managed manually, or in console with endpoint traffic dials and weights. If your application endpoint has a failure or availability issue, AWS Global Accelerator will automatically redirect your new connections to a healthy endpoint within seconds.</p>\n\n<p>By using AWS Global Accelerator, you can:</p>\n\n<ol>\n<li><p>Associate the static IP addresses provided by AWS Global Accelerator to regional AWS resources or endpoints, such as Network Load Balancers, Application Load Balancers, EC2 Instances, and Elastic IP addresses. The IP addresses are anycast from AWS edge locations so they provide onboarding to the AWS global network close to your users.</p></li>\n<li><p>Easily move endpoints between Availability Zones or AWS Regions without needing to update your DNS configuration or change client-facing applications.</p></li>\n<li><p>Dial traffic up or down for a specific AWS Region by configuring a traffic dial percentage for your endpoint groups. This is especially useful for testing performance and releasing updates.</p></li>\n<li><p>Control the proportion of traffic directed to each endpoint within an endpoint group by assigning weights across the endpoints.</p></li>\n</ol>\n\n<p>AWS Global Accelerator for Multi-Region applications:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q55-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed</strong> - AWS Direct Connect can reduce latency to great extent. Direct Connect is used to connect on-premises systems to AWS Cloud for extremely low latency use cases. It cannot be used to serve users directly.</p>\n\n<p><strong>Create S3 buckets in different AWS Regions and configure CloudFront to pick the nearest edge location to the user</strong> - If most of the content is static, we can configure CloudFront to improve performance. In the current scenario, the architecture has ELBs, EC2 instances too that need to be covered in the automatic failover plan.</p>\n\n<p><em>Set up an Amazon Route 53 geoproximity routing policy to route traffic</em>* - Geoproximity routing lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources. Unlike Global Accelerator, managing and routing to different instances, ELBs and other AWS resources will become an operational overhead as the resource count reaches into the hundreds. With inbuilt features like Static anycast IP addresses, fault tolerance using network zones, Global performance-based routing, TCP Termination at the Edge - Global Accelerator is the right choice for multi-region, low latency use cases.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/global-accelerator/features/\">https://aws.amazon.com/global-accelerator/features/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geoproximity\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geoproximity</a></p>\n",
                "options": [
                    {
                        "id": 10221,
                        "content": "<p>Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations</p>",
                        "isValid": true
                    },
                    {
                        "id": 10222,
                        "content": "<p>Create S3 buckets in different AWS Regions and configure CloudFront to pick the nearest edge location to the user</p>",
                        "isValid": false
                    },
                    {
                        "id": 10223,
                        "content": "<p>Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed</p>",
                        "isValid": false
                    },
                    {
                        "id": 10224,
                        "content": "<p>Set up an Amazon Route 53 geoproximity routing policy to route traffic</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2446,
            "attributes": {
                "createdAt": "2023-09-09T20:34:03.836Z",
                "updatedAt": "2023-09-09T20:34:03.836Z",
                "content": "<p>A silicon valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores patient health records on Amazon S3. The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls</strong></p>\n\n<p>Amazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n\n<p>An S3 Glacier vault is a container for storing archives. When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault. S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy. You can specify controls such as “write once read many” (WORM) in a vault lock policy and lock the policy from future edits. Therefore, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use S3 Glacier to store the sensitive archived data and then use an S3 lifecycle policy to enforce compliance controls</strong> - You can use lifecycle policy to define actions you want Amazon S3 to take during an object's lifetime. For example, use a lifecycle policy to transition objects to another storage class, archive them, or delete them after a specified period. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p><strong>Use S3 Glacier vault to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p><strong>Use S3 Glacier to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html</a></p>\n",
                "options": [
                    {
                        "id": 10225,
                        "content": "<p>Use S3 Glacier vault to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</p>",
                        "isValid": false
                    },
                    {
                        "id": 10226,
                        "content": "<p>Use S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls</p>",
                        "isValid": true
                    },
                    {
                        "id": 10227,
                        "content": "<p>Use S3 Glacier to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</p>",
                        "isValid": false
                    },
                    {
                        "id": 10228,
                        "content": "<p>Use S3 Glacier to store the sensitive archived data and then use an S3 lifecycle policy to enforce compliance controls</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2447,
            "attributes": {
                "createdAt": "2023-09-09T20:34:03.965Z",
                "updatedAt": "2023-09-09T20:34:03.965Z",
                "content": "<p>A financial services company runs its flagship web application on AWS. The application serves thousands of users during peak hours. The company needs a scalable near-real-time solution to share hundreds of thousands of financial transactions with multiple internal applications. The solution should also remove sensitive details from the transactions before storing the cleansed transactions in a document database for low-latency retrieval.</p>\n\n<p>As an AWS Certified Solutions Architect Associate, which of the following would you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage Amazon Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in DynamoDB. The internal applications can consume the raw transactions off the Kinesis Data Stream</strong></p>\n\n<p>You can use Kinesis Data Streams to build custom applications that process or analyze streaming data for specialized needs.\nKinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You don't have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services for your data streams.</p>\n\n<p>How Kinesis Data Streams Work:\n<img src=\"https://d1.awsstatic.com/Digital%20Marketing/House/1up/products/kinesis/Product-Page-Diagram_Amazon-Kinesis-Data-Streams.e04132af59c6aa1e9372cabf44a17749f4a81b16.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p>Kinesis Data Streams Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q35-i1.jpg\">\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q35-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>For the given use case, you can stream the raw financial transactions into Kinesis Data Streams, which in turn, are processed by the Lambda function that is set up as one of the consumers of the data stream. The Lambda would remove sensitive data from every transaction and then store the cleansed transactions in DynamoDB. The internal applications can be configured as the other consumers of the data stream and ingest the raw transactions</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an Amazon Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications</strong> - The use case requires a near-real-time solution for cleansing, processing and storing the transactions, so using a batch process would be incorrect.</p>\n\n<p><strong>Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage Amazon Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in DynamoDB. The internal applications can consume the raw transactions off the Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms, and delivers streaming data to data lakes, data stores, and analytics services.</p>\n\n<p><img src=\"https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p>You cannot set up multiple consumers for Kinesis Data Firehose delivery streams as it can dump data in a single data repository at a time, so this option is incorrect.</p>\n\n<p><strong>Persist the raw transactions into Amazon DynamoDB. Configure a rule in DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage DynamoDB Streams to share the transactions data with the internal applications</strong> - There is no such rule within DynamoDB that can auto-update every time a new item is written in a DynamoDB table. You would need to use a DynamoDB trigger to invoke an external service like a Lambda function on every new write, which can then cleanse and update the item. In addition, this process introduces inefficiency in the workflow as the same item is written and then updated for cleansing purposes. Therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n",
                "options": [
                    {
                        "id": 10229,
                        "content": "<p>Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage Amazon Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in DynamoDB. The internal applications can consume the raw transactions off the Kinesis Data Firehose</p>",
                        "isValid": false
                    },
                    {
                        "id": 10230,
                        "content": "<p>Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage Amazon Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in DynamoDB. The internal applications can consume the raw transactions off the Kinesis Data Stream</p>",
                        "isValid": true
                    },
                    {
                        "id": 10231,
                        "content": "<p>Persist the raw transactions into Amazon DynamoDB. Configure a rule in DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage DynamoDB Streams to share the transactions data with the internal applications</p>",
                        "isValid": false
                    },
                    {
                        "id": 10232,
                        "content": "<p>Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an Amazon Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2448,
            "attributes": {
                "createdAt": "2023-09-09T20:34:04.130Z",
                "updatedAt": "2023-09-09T20:34:04.130Z",
                "content": "<p>Computer vision researchers at a university are trying to optimize the I/O bound processes for a proprietary algorithm running on EC2 instances. The ideal storage would facilitate high-performance IOPS when doing file processing in a temporary storage space before uploading the results back into Amazon S3.</p>\n\n<p>As a solutions architect, which of the following AWS storage options would you recommend as the MOST performant as well as cost-optimal?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use EC2 instances with Instance Store as the storage type</strong></p>\n\n<p>An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures.</p>\n\n<p>As Instance Store delivers high random I/O performance, it can act as a temporary storage space, and these volumes are included as part of the instance's usage cost, therefore this is the correct option.</p>\n\n<p>Amazon EC2 Instance Store\n<img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/instance_storage.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use EC2 instances with EBS General Purpose SSD (gp2) as the storage option</strong> - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver its provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB.\nEBS gp2 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the EC2 instance), therefore this option is not correct.</p>\n\n<p><strong>Use EC2 instances with EBS Provisioned IOPS SSD (io1) as the storage option</strong> - Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.\nEBS io1 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the EC2 instance), therefore this option is not correct.</p>\n\n<p><strong>Use EC2 instances with EBS Throughput Optimized HDD (st1) as the storage option</strong> - Throughput Optimized HDD (st1) are low-cost HDD volumes designed for frequently accessed, throughput-intensive workloads such as Big data and Data warehouses. EBS st1 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the EC2 instance), therefore this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n",
                "options": [
                    {
                        "id": 10233,
                        "content": "<p>Use EC2 instances with EBS General Purpose SSD (gp2) as the storage option</p>",
                        "isValid": false
                    },
                    {
                        "id": 10234,
                        "content": "<p>Use EC2 instances with Instance Store as the storage option</p>",
                        "isValid": true
                    },
                    {
                        "id": 10235,
                        "content": "<p>Use EC2 instances with EBS Provisioned IOPS SSD (io1) as the storage option</p>",
                        "isValid": false
                    },
                    {
                        "id": 10236,
                        "content": "<p>Use EC2 instances with EBS Throughput Optimized HDD (st1) as the storage option</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2449,
            "attributes": {
                "createdAt": "2023-09-09T20:34:04.266Z",
                "updatedAt": "2023-09-09T20:34:04.266Z",
                "content": "<p>A company has noticed that its application performance has deteriorated after a new Auto Scaling group was deployed a few days back. Upon investigation, the team found out that the Launch Configuration selected for the Auto Scaling group is using the incorrect instance type that is not optimized to handle the application workflow.</p>\n\n<p>As a solutions architect, what would you recommend to provide a long term resolution for this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed</strong></p>\n\n<p>A launch configuration is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances. Include the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping.</p>\n\n<p>It is not possible to modify a launch configuration once it is created. The correct option is to create a new launch configuration to use the correct instance type. Then modify the Auto Scaling group to use this new launch configuration. Lastly to clean-up, just delete the old launch configuration as it is no longer needed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group</strong> - As mentioned earlier, it is not possible to modify a launch configuration once it is created. Hence, this option is incorrect.</p>\n\n<p><strong>No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type</strong> - You cannot use an Auto Scaling group to directly modify the instance type of the underlying instances. Hence, this option is incorrect.</p>\n\n<p><strong>No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance</strong> - Using the Auto Scaling group to increase the number of instances to cover up for the performance loss is not recommended as it does not address the root cause of the problem. The Machine Learning workflow requires a certain instance type that is optimized to handle Machine Learning computations. Hence, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html</a></p>\n",
                "options": [
                    {
                        "id": 10237,
                        "content": "<p>Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group</p>",
                        "isValid": false
                    },
                    {
                        "id": 10238,
                        "content": "<p>Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed</p>",
                        "isValid": true
                    },
                    {
                        "id": 10239,
                        "content": "<p>No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10240,
                        "content": "<p>No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2450,
            "attributes": {
                "createdAt": "2023-09-09T20:34:04.381Z",
                "updatedAt": "2023-09-09T20:34:04.381Z",
                "content": "<p>A healthcare company wants to run its applications on single-tenant hardware to meet compliance guidelines.</p>\n\n<p>Which of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single tenant?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Dedicated Instances</strong> - Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.</p>\n\n<p>A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.</p>\n\n<p>Differences between Dedicated Hosts and Dedicated Instances:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Spot Instances</strong> -  A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price.  Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option.</p>\n\n<p><strong>Dedicated Hosts</strong> - An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement.</p>\n\n<p><strong>On-Demand Instances</strong> - With On-Demand Instances, you pay for the compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements.</p>\n\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q21-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html</a></p>\n",
                "options": [
                    {
                        "id": 10241,
                        "content": "<p>Dedicated Hosts</p>",
                        "isValid": false
                    },
                    {
                        "id": 10242,
                        "content": "<p>On-Demand Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10243,
                        "content": "<p>Spot Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10244,
                        "content": "<p>Dedicated Instances</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2451,
            "attributes": {
                "createdAt": "2023-09-09T20:34:04.495Z",
                "updatedAt": "2023-09-09T20:34:04.495Z",
                "content": "<p>A company has multiple EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a DynamoDB table.</p>\n\n<p>How would you go about providing private access to these AWS resources which are not part of this custom VPC?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</strong></p>\n\n<p>Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.</p>\n\n<p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p>\n\n<p>There are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.</p>\n\n<p>A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported:</p>\n\n<p>Amazon S3</p>\n\n<p>DynamoDB</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB service using the private IP address</strong></p>\n\n<p><strong>Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address</strong></p>\n\n<p>DynamoDB does not support interface endpoints, so these two options are incorrect.</p>\n\n<p><strong>Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for S3 and then connect to the S3 service using the private IP address</strong> - Origin Access Identity (OAI) is used within the context of CloudFront. To restrict access to content that you serve from Amazon S3 buckets, you can create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution. You cannot use OAI to facilitate access to S3 from a VPC.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a></p>\n",
                "options": [
                    {
                        "id": 10245,
                        "content": "<p>Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB service using the private IP address</p>",
                        "isValid": false
                    },
                    {
                        "id": 10246,
                        "content": "<p>Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for S3 and then connect to the S3 service using the private IP address</p>",
                        "isValid": false
                    },
                    {
                        "id": 10247,
                        "content": "<p>Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address</p>",
                        "isValid": false
                    },
                    {
                        "id": 10248,
                        "content": "<p>Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2452,
            "attributes": {
                "createdAt": "2023-09-09T20:34:04.599Z",
                "updatedAt": "2023-09-09T20:34:04.599Z",
                "content": "<p>During a review, a security team has flagged concerns over an Amazon EC2 instance querying IP addresses used for cryptocurrency mining. The EC2 instance does not host any authorized application related to cryptocurrency mining.</p>\n\n<p>Which AWS service can be used to protect the EC2 instances from such unauthorized behavior in the future?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon GuardDuty</strong> - Amazon GuardDuty continuously monitors for malicious or unauthorized behavior to help protect your AWS resources, including your AWS accounts and access keys. GuardDuty identifies any unusual or unauthorized activity, like cryptocurrency mining or infrastructure deployments in a region that has never been used. Powered by threat intelligence and machine learning, GuardDuty is continuously evolving to help you protect your AWS environment.</p>\n\n<p>The cryptocurrency finding expands the service’s ability to detect Amazon EC2 instances querying IP addresses associated with the cryptocurrency-related activity. The finding type is: CryptoCurrency:EC2/BitcoinTool.B, CryptoCurrency:EC2/BitcoinTool.B!DNS.</p>\n\n<p>This finding informs you that the listed EC2 instance in your AWS environment is querying a domain name that is associated with Bitcoin or other cryptocurrency-related activity. Bitcoin is a worldwide cryptocurrency and digital payment system that can be exchanged for other currencies, products, and services. Bitcoin is a reward for bitcoin mining and is highly sought after by threat actors.</p>\n\n<p>If you use the EC2 instance to mine or manage cryptocurrency, or this instance is otherwise involved in blockchain activity, this finding could represent expected activity for your environment. If this is the case in your AWS environment, AWS recommends that you set up a suppression rule for this finding.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Web Application Firewall (AWS WAF)</strong> - AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting.</p>\n\n<p><strong>AWS Shield Advanced</strong> - For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall. AWS Shield Advanced also gives you 24x7 access to the AWS DDoS Response Team (DRT) and protection against DDoS-related spikes in your Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 charges.</p>\n\n<p><strong>AWS Firewall Manager</strong> - AWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure, from a central administrator account.</p>\n\n<p>None of these three services can detect unauthorized cryptocurrency mining activity on EC2 instances, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#cryptocurrency-ec2-bitcointoolbdns\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#cryptocurrency-ec2-bitcointoolbdns</a></p>\n",
                "options": [
                    {
                        "id": 10249,
                        "content": "<p>AWS Shield Advanced</p>",
                        "isValid": false
                    },
                    {
                        "id": 10250,
                        "content": "<p>AWS Web Application Firewall (AWS WAF)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10251,
                        "content": "<p>AWS Firewall Manager</p>",
                        "isValid": false
                    },
                    {
                        "id": 10252,
                        "content": "<p>Amazon GuardDuty</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2453,
            "attributes": {
                "createdAt": "2023-09-09T20:34:04.722Z",
                "updatedAt": "2023-09-09T20:34:04.722Z",
                "content": "<p>A digital media streaming company wants to use AWS Cloudfront to distribute its content only to its service subscribers. As a solutions architect, which of the following solutions would you suggest to deliver restricted content to the bona fide end users? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use CloudFront signed URLs</strong></p>\n\n<p>Many companies that distribute content over the internet want to restrict access to documents, business data, media streams, or content that is intended for selected users, for example, users who have paid a fee.</p>\n\n<p>To securely serve this private content by using CloudFront, you can do the following:</p>\n\n<p>Require that your users access your private content by using special CloudFront signed URLs or signed cookies.</p>\n\n<p>A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content. So this is a correct option.</p>\n\n<p><strong>Use CloudFront signed cookies</strong></p>\n\n<p>CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website. So this is also a correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Require HTTPS for communication between CloudFront and your custom origin</strong></p>\n\n<p><strong>Require HTTPS for communication between CloudFront and your S3 origin</strong></p>\n\n<p>Requiring HTTPS for communication between CloudFront and your custom origin (or S3 origin) only enables secure access to the underlying content. You cannot use HTTPS to restrict access to your private content. So both these options are incorrect.</p>\n\n<p><strong>Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers</strong> - This option is just added as a distractor. You cannot use HTTPS to restrict access to your private content.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html</a></p>\n",
                "options": [
                    {
                        "id": 10253,
                        "content": "<p>Use CloudFront signed URLs</p>",
                        "isValid": true
                    },
                    {
                        "id": 10254,
                        "content": "<p>Require HTTPS for communication between CloudFront and your custom origin</p>",
                        "isValid": false
                    },
                    {
                        "id": 10255,
                        "content": "<p>Use CloudFront signed cookies</p>",
                        "isValid": true
                    },
                    {
                        "id": 10256,
                        "content": "<p>Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers</p>",
                        "isValid": false
                    },
                    {
                        "id": 10257,
                        "content": "<p>Require HTTPS for communication between CloudFront and your S3 origin</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2454,
            "attributes": {
                "createdAt": "2023-09-09T20:34:04.843Z",
                "updatedAt": "2023-09-09T20:34:04.843Z",
                "content": "<p>The engineering team at a retail company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon RDS as the database tier for its flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on RDS Multi-AZ capabilities.</p>\n\n<p>Which of the following would you identify as correct for RDS Multi-AZ? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary, and finally performing maintenance on the old primary, which becomes the new standby</strong></p>\n\n<p>Running a DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event because Amazon RDS applies operating system updates by following these steps:</p>\n\n<p>Perform maintenance on the standby.</p>\n\n<p>Promote the standby to primary.</p>\n\n<p>Perform maintenance on the old primary, which becomes the new standby.</p>\n\n<p>When you modify the database engine for your DB instance in a Multi-AZ deployment, then Amazon RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade.</p>\n\n<p><strong>Amazon RDS automatically initiates a failover to the standby, in case the primary database fails for any reason</strong> - You also benefit from enhanced database availability when running your DB instance as a Multi-AZ deployment. If an Availability Zone failure or DB instance failure occurs, your availability impact is limited to the time automatic failover takes to complete.</p>\n\n<p>Another implied benefit of running your DB instance as a Multi-AZ deployment is that DB instance failover is automatic and requires no administration. In an Amazon RDS context, this means you are not required to monitor DB instance events and initiate manual DB instance recovery in the event of an Availability Zone failure or DB instance failure.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB</strong> - The availability benefits of Multi-AZ also extend to planned maintenance. For example, with automated backups, I/O activity is no longer suspended on your primary during your preferred backup window, since backups are taken from the standby.</p>\n\n<p><strong>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</strong> - A Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. As such, the feature uses synchronous replication between primary and standby. AWS implementation makes sure the primary and the standby are constantly in sync, but precludes using the standby for read or write operations.</p>\n\n<p><strong>Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</strong> - When you create your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across the Availability Zone to the standby in order to keep both in sync and protect your latest database updates against DB instance failure.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 10258,
                        "content": "<p>For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB</p>",
                        "isValid": false
                    },
                    {
                        "id": 10259,
                        "content": "<p>RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby</p>",
                        "isValid": true
                    },
                    {
                        "id": 10260,
                        "content": "<p>Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason</p>",
                        "isValid": true
                    },
                    {
                        "id": 10261,
                        "content": "<p>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</p>",
                        "isValid": false
                    },
                    {
                        "id": 10262,
                        "content": "<p>Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2455,
            "attributes": {
                "createdAt": "2023-09-09T20:34:04.955Z",
                "updatedAt": "2023-09-09T20:34:04.955Z",
                "content": "<p>A Hollywood production studio is looking at transferring their existing digital media assets of around 20PB to AWS Cloud in the shortest possible timeframe.</p>\n\n<p>Which of the following is an optimal solution for this requirement, given that the studio's data centers are located at a remote location?</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>AWS Snowmobile</strong></p>\n\n<p>AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective.  AWS recommends using Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Snowball</strong> - The AWS Snowball service uses physical storage devices to transfer large amounts of data between Amazon Simple Storage Service (Amazon S3) and client's onsite data storage location at faster-than-internet speeds. Snowball provides powerful interfaces that you can use to create jobs, track data, and track the status of your jobs through to completion. AWS recommends snowball only if you want to transfer greater than 10 TB of data between your on-premises data centers and Amazon S3.</p>\n\n<p><strong>AWS Storage Gateway</strong> - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage.  Used for key hybrid storage solutions that include moving tape backups to the cloud, reducing on-premises storage with cloud-backed file shares, providing low latency access to data in AWS for on-premises applications, as well as various migration, archiving, processing, and disaster recovery use cases. This is not an optimal solution since the studio's data centers are in remote locations where internet speed may not optimal, thereby increasing both cost and time for migrating 20TB of data.</p>\n\n<p><strong>AWS Direct Connect</strong> - AWS Direct Connect is a network service that provides an alternative to using the Internet to connect a customer’s on-premises sites to AWS. Data is transmitted through a private network connection between AWS and a customer’s datacenter or corporate network. Direct Connect connection takes significant cost as well as time to provision. This is not the correct solution since the studio wants the data transfer to be done in the shortest possible time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/snowmobile/\">https://aws.amazon.com/snowmobile/</a></p>\n",
                "options": [
                    {
                        "id": 10263,
                        "content": "<p>AWS Snowmobile</p>",
                        "isValid": true
                    },
                    {
                        "id": 10264,
                        "content": "<p>AWS Snowball</p>",
                        "isValid": false
                    },
                    {
                        "id": 10265,
                        "content": "<p>AWS Storage Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 10266,
                        "content": "<p>AWS Direct Connect</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2456,
            "attributes": {
                "createdAt": "2023-09-09T20:34:05.083Z",
                "updatedAt": "2023-09-09T20:34:05.083Z",
                "content": "<p>To support critical production workloads that require maximum resiliency, a company wants to configure network connections between its Amazon VPC and the on-premises infrastructure. The company needs AWS Direct Connect connections with speeds greater than 1 Gbps.</p>\n\n<p>As a solutions architect, which of the following will you suggest as the best architecture for this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Opt for two separate Direct Connect connections terminating on separate devices in more than one Direct Connect location</strong> - Maximum resilience is achieved by separate connections terminating on separate devices in more than one location. This configuration offers customers maximum resilience to failure. As shown in the figure above, such a topology provides resilience to device failure, connectivity failure, and complete location failure. You can use Direct Connect Gateway to access any AWS Region (except AWS Regions in China) from any AWS Direct Connect locations.</p>\n\n<p>Maximum Resiliency for Critical Workloads:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q22-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Opt for one Direct Connect connection at each of the multiple Direct Connect locations</strong> - For critical production workloads that require high resiliency, it is recommended to have one connection at multiple locations. As shown in the figure below, such a topology ensures resilience to connectivity failure due to a fiber cut or a device failure as well as a complete location failure. You can use Direct Connect Gateway to access any AWS Region (except AWS Regions in China) from any AWS Direct Connect location.</p>\n\n<p>High Resiliency for Critical Workloads:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q22-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a></p>\n\n<p><strong>Opt for at least two Direct Connect connections terminating on different devices at a single Direct Connect location</strong> - For non-critical production workloads and development workloads that do not require high resiliency, it is recommended to have at least two connections terminating on different devices at a single location. As shown in the figure above, such a topology helps in the case of the device failure at a location but does not help in the event of a total location failure.</p>\n\n<p>Non Critical Production Workloads or Development Workloads:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q22-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a></p>\n\n<p><strong>Use AWS Managed VPN as a backup for AWS Direct Connect connections to ensure maximum resiliency</strong> - It is important to understand that AWS Managed VPN supports up to 1.25 Gbps throughput per VPN tunnel and does not support Equal Cost Multi-Path (ECMP) for egress data path in the case of multiple AWS Managed VPN tunnels terminating on the same VGW. Thus, AWS does not recommend customers use AWS Managed VPN as a backup for AWS Direct Connect connections with speeds greater than 1 Gbps.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a></p>\n",
                "options": [
                    {
                        "id": 10267,
                        "content": "<p>Opt for two separate Direct Connect connections terminating on separate devices in more than one Direct Connect location</p>",
                        "isValid": true
                    },
                    {
                        "id": 10268,
                        "content": "<p>Opt for at least two Direct Connect connections terminating on different devices at a single Direct Connect location</p>",
                        "isValid": false
                    },
                    {
                        "id": 10269,
                        "content": "<p>Opt for one Direct Connect connection at each of the multiple Direct Connect locations</p>",
                        "isValid": false
                    },
                    {
                        "id": 10270,
                        "content": "<p>Use AWS Managed VPN as a backup for AWS Direct Connect connections to ensure maximum resiliency</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2457,
            "attributes": {
                "createdAt": "2023-09-09T20:34:05.232Z",
                "updatedAt": "2023-09-09T20:34:05.232Z",
                "content": "<p>A security consultant is designing a solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the individual developers will have AWS account root user-level access to their own accounts, the consultant wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified.</p>\n\n<p>Which of the following actions meets the given requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up a service control policy (SCP) that prohibits changes to CloudTrail, and attach it to the developer accounts</strong> - Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization’s access control guidelines.</p>\n\n<p>An SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user. Any account has only those permissions permitted by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the AdministratorAccess IAM policy with <em>/</em> permissions to the user.</p>\n\n<p>SCPs don't affect users or roles in the management account. They affect only the member accounts in your organization.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a new trail in CloudTrail from within the developer accounts with the organization trails option enabled</strong> - Configuring each developer account individually is not a viable solution to start with. In addition, any configuration changes can be undone by the user once they are logged into their individual accounts as root users.</p>\n\n<p><strong>Set up an IAM policy that prohibits changes to CloudTrail and attach it to the root user</strong> - The root user can modify this IAM policy itself, so this option is not correct.</p>\n\n<p><strong>Set up a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account</strong> - A service-linked role is a unique type of IAM role that is linked directly to an AWS service. Service-linked roles are predefined by the service and include all the permissions that the service requires to call other AWS services on your behalf. The linked service also defines how you create, modify, and delete a service-linked role.</p>\n\n<p>The linked service defines the permissions of its service-linked roles, and unless defined otherwise, only that service can assume the roles. The defined permissions include the trust policy and the permissions policy, and that permissions policy cannot be attached to any other entity such as the ARN in the master account.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n",
                "options": [
                    {
                        "id": 10271,
                        "content": "<p>Set up a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account</p>",
                        "isValid": false
                    },
                    {
                        "id": 10272,
                        "content": "<p>Set up an IAM policy that prohibits changes to CloudTrail and attach it to the root user</p>",
                        "isValid": false
                    },
                    {
                        "id": 10273,
                        "content": "<p>Configure a new trail in CloudTrail from within the developer accounts with the organization trails option enabled</p>",
                        "isValid": false
                    },
                    {
                        "id": 10274,
                        "content": "<p>Set up a service control policy (SCP) that prohibits changes to CloudTrail, and attach it to the developer accounts</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2458,
            "attributes": {
                "createdAt": "2023-09-09T20:34:05.396Z",
                "updatedAt": "2023-09-09T20:34:05.396Z",
                "content": "<p>A financial services company stores confidential data on an Amazon Simple Storage Service (S3) bucket. The compliance guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage the encryption keys.</p>\n\n<p>Which of the following options represents the most cost-optimal solution for the given use case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>SSE-S3</strong></p>\n\n<p>Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. There are no additional fees for using server-side encryption with Amazon S3-managed keys (SSE-S3).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-C</strong> - You manage the encryption keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects.</p>\n\n<p><strong>Client-Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p><strong>SSE-KMS</strong> - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself. Although SSE-KMS provides an option where AWS manages the encryption key on your behalf, however, this entails a usage fee for the KMS key. So this option is not the best fit for the given use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p>\n",
                "options": [
                    {
                        "id": 10275,
                        "content": "<p>SSE-C</p>",
                        "isValid": false
                    },
                    {
                        "id": 10276,
                        "content": "<p>SSE-S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 10277,
                        "content": "<p>SSE-KMS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10278,
                        "content": "<p>Client Side Encryption</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2459,
            "attributes": {
                "createdAt": "2023-09-09T20:34:05.508Z",
                "updatedAt": "2023-09-09T20:34:05.508Z",
                "content": "<p>A systems administration team has a requirement to run certain custom scripts only once during the launch of the Amazon EC2 instances that host their application.</p>\n\n<p>Which of the following represents the best way of configuring a solution for this requirement with minimal effort?</p>",
                "answerExplanation": "<p>Correct option:\n<strong>Run the custom scripts as user data scripts on the Amazon EC2 instances</strong> - When you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives.</p>\n\n<p>By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. Hence, no extra configuration is needed, apart from including the custom scripts in user data scripts.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process</strong> - You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance. By default, the scripts are run, only once during the boot process while first launching the instance.</p>\n\n<p><strong>Run the custom scripts as instance metadata scripts on the Amazon EC2 instances</strong> - Instance metadata is data about your instance that you can use to configure or manage the running instance. Metadata cannot be used to run custom scripts.</p>\n\n<p><strong>Use AWS CLI to run the user data scripts only once while launching the instance</strong> - This statement is incorrect and used only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html</a></p>\n",
                "options": [
                    {
                        "id": 10279,
                        "content": "<p>Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process</p>",
                        "isValid": false
                    },
                    {
                        "id": 10280,
                        "content": "<p>Run the custom scripts as instance metadata scripts on the Amazon EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10281,
                        "content": "<p>Run the custom scripts as user data scripts on the Amazon EC2 instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 10282,
                        "content": "<p>Use AWS CLI to run the user data scripts only once while launching the instance</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2460,
            "attributes": {
                "createdAt": "2023-09-09T20:34:05.602Z",
                "updatedAt": "2023-09-09T20:34:05.602Z",
                "content": "<p>A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets.</p>\n\n<p>How can you provide these users access in the least possible time, with minimal changes?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a group, attach the policy to the group and place the users in the group</strong> - An IAM group is a collection of IAM users. You can use groups to specify permissions for a collection of users, which can make those permissions easier to manage for those users. For example, you could have a group called Admins and give that group the types of permissions that administrators typically need. Any user in that group automatically has the permissions that are assigned to the group. If a new user joins your organization and should have administrator privileges, you can assign the appropriate permissions by adding the user to that group.</p>\n\n<p>Here creating a group, assigning users to that group and attaching policies to that group is the best way.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the S3 bucket policy</strong> - Updating the S3 bucket policy could work but would not scale, as the size of the S3 bucket policy is limited (Bucket policies are limited to 20 KB in size).</p>\n\n<p><strong>Create a policy and assign it manually to the 50 users</strong> -</p>\n\n<p>An IAM user is an entity that you create in AWS. The IAM user represents the person or service who uses the IAM user to interact with AWS. Primary use for IAM users is to give people the ability to sign in to the AWS Management Console for interactive tasks and to make programmatic requests to AWS services using the API or CLI. A user in AWS consists of a name, a password to sign in to the AWS Management Console, and up to two access keys that can be used with the API or CLI.</p>\n\n<p>A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied.</p>\n\n<p>Identity-based policies – Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity.</p>\n\n<p>Resource-based policies – Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts.</p>\n\n<p>Creating a policy and assigning it manually to users would work but would be hard to scale and manage.</p>\n\n<p><strong>Create an MFA user with read / write access and link 50 IAM with MFA</strong> - MFA adds extra security because it requires users to provide unique authentication from an AWS supported MFA mechanism in addition to their regular sign-in credentials when they access AWS websites or services. MFA cannot help in terms of granting read/write access to only 50 of the IAM users.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a></p>\n",
                "options": [
                    {
                        "id": 10283,
                        "content": "<p>Create an MFA user with read / write access and link 50 IAM with MFA</p>",
                        "isValid": false
                    },
                    {
                        "id": 10284,
                        "content": "<p>Update the S3 bucket policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 10285,
                        "content": "<p>Create a group, attach the policy to the group and place the users in the group</p>",
                        "isValid": true
                    },
                    {
                        "id": 10286,
                        "content": "<p>Create a policy and assign it manually to the 50 users</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2461,
            "attributes": {
                "createdAt": "2023-09-09T20:34:05.712Z",
                "updatedAt": "2023-09-09T20:34:05.712Z",
                "content": "<p>An e-commerce website is migrating towards a microservices-based approach for their website and plans to expose their website from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, mycorp.com/products, and mycorp.com/orders. The website would like to use ECS on the backend to manage these microservices and possibly host the same container of the application multiple times on the same EC2 instance.</p>\n\n<p>Which feature can help you achieve this with minimal effort?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Application Load Balancer + dynamic port mapping</strong></p>\n\n<p>Application Load Balancer can automatically distribute incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.</p>\n\n<p>Dynamic port mapping with an Application Load Balancer makes it easier to run multiple tasks on the same Amazon ECS service on an Amazon ECS cluster.</p>\n\n<p>Incorrect option:</p>\n\n<p><strong>Application Load Balancer + Reverse Proxy running as a Docker daemon on each ECS host</strong> - Dynamic Port Mapping is available for the Application Load Balancer. A reverse proxy solution would work but would be too much work to manage. Here the ALB has a feature that provides a direct dynamic port mapping feature and integration with the ECS service so we will leverage that.</p>\n\n<p><strong>Classic Load Balancer + dynamic port mapping</strong> - Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network.</p>\n\n<p>With the Classic Load Balancer, you must statically map port numbers on a container instance. The Classic Load Balancer does not allow you to run multiple copies of a task on the same instance because of the ports conflict. An Application Load Balancer uses dynamic port mapping so that you can run multiple tasks from a single service on the same container instance.</p>\n\n<p><strong>Network Load Balancer + dynamic port mapping</strong> - Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a></p>\n",
                "options": [
                    {
                        "id": 10287,
                        "content": "<p>Application Load Balancer + Reverse Proxy running as a Docker daemon on each ECS host</p>",
                        "isValid": false
                    },
                    {
                        "id": 10288,
                        "content": "<p>Application Load Balancer + dynamic port mapping</p>",
                        "isValid": true
                    },
                    {
                        "id": 10289,
                        "content": "<p>Classic Load Balancer + dynamic port mapping</p>",
                        "isValid": false
                    },
                    {
                        "id": 10290,
                        "content": "<p>Network Load Balancer + dynamic port mapping</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2462,
            "attributes": {
                "createdAt": "2023-09-09T20:34:05.822Z",
                "updatedAt": "2023-09-09T20:34:05.822Z",
                "content": "<p>A Big Data company wants to optimize its daily Extract-Transform-Load (ETL) process that migrates and transforms data from its S3 based data lake to a Redshift cluster. The team wants to manage this daily job in a serverless environment.</p>\n\n<p>Which AWS service is the best fit to manage this process without the need to configure or manage the underlying compute resources?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Glue</strong> - AWS Glue provides a managed ETL service that runs on a serverless Apache Spark environment. This allows you to focus on your ETL job and not worry about configuring and managing the underlying compute resources. AWS Glue takes a data-first approach and allows you to focus on the data properties and data manipulation to transform the data to a form where you can derive business insights. It provides an integrated data catalog that makes metadata available for ETL as well as querying via Amazon Athena and Amazon Redshift Spectrum.</p>\n\n<p>Create a unified catalog to find data across multiple data stores using Glue:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q48-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p>\n\n<p>AWS Glue automates much of the effort required for data integration. AWS Glue crawls your data sources, identifies data formats, and suggests schemas to store your data. It automatically generates the code to run your data transformations and loading processes. You can use AWS Glue to easily run and manage thousands of ETL jobs or to combine and replicate data across multiple data stores using SQL.</p>\n\n<p>AWS Glue runs in a serverless environment. There is no infrastructure to manage, and AWS Glue provisions, configures, and scales the resources required to run your data integration jobs. You pay only for the resources your jobs use while running.</p>\n\n<p>AWS Glue is the right fit since the company is looking at a managed ETL service without having the overhead of configuring, maintaining, or managing any servers.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/images/pattern-img/1f854a3e-44d4-4d70-9cd2-d61f852e3231/images/f26e2ee3-74be-49f1-8290-cd81e4ef9465.png\">\nvia - <a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Data Pipeline</strong> - AWS Data Pipeline provides a managed orchestration service that gives you greater flexibility in terms of the execution environment, access and control over the compute resources that run your code, as well as the code itself that does data processing. AWS Data Pipeline launches compute resources in your account allowing you direct access to the Amazon EC2 instances or Amazon EMR clusters. As this option provides access to the underlying EC2 instances so it's not a serverless solution. Therefore this option is incorrect for the given use case.</p>\n\n<p><strong>Amazon EMR</strong> - EMR is a web service to easily and cost-effectively process vast amounts of data. EMR utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). As this option provides access to the underlying EC2 instances so it's not a serverless solution. Therefore this option is incorrect for the given use case.</p>\n\n<p><strong>AWS Database Migration Service (DMS)</strong> - AWS Database Migration Service (DMS) helps you migrate databases to AWS easily and securely. For use cases that require a database migration from on-premises to AWS or database replication between on-premises sources and sources on AWS, AWS recommends you use AWS DMS. Once your data is in AWS, you can use AWS Glue to move, combine, replicate, and transform data from your data source into another database or data warehouse, such as Amazon Redshift. As the use-case talks about data migration and transformation between AWS services, so AWS Glue is a better fit than DMS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/glue/faqs/\">https://aws.amazon.com/glue/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html</a></p>\n",
                "options": [
                    {
                        "id": 10291,
                        "content": "<p>AWS Glue</p>",
                        "isValid": true
                    },
                    {
                        "id": 10292,
                        "content": "<p>Amazon EMR</p>",
                        "isValid": false
                    },
                    {
                        "id": 10293,
                        "content": "<p>AWS Data Pipeline</p>",
                        "isValid": false
                    },
                    {
                        "id": 10294,
                        "content": "<p>AWS Database Migration Service (DMS)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2463,
            "attributes": {
                "createdAt": "2023-09-09T20:34:05.928Z",
                "updatedAt": "2023-09-09T20:34:05.928Z",
                "content": "<p>The CTO of an online home rental marketplace wants to re-engineer the caching layer of the current architecture for its relational database. The CTO wants the caching layer to have replication and archival support built into the architecture.</p>\n\n<p>Which of the following AWS service offers the capabilities required for the re-engineering of the caching layer?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>ElastiCache for Redis</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store. ElastiCache for Redis supports replication and archival snapshots right out of the box. Hence this is the correct option.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review this comparison sheet for Redis vs Memcached features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q10-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>ElastiCache for Memcached</strong> - Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database. Session stores are easy to create with Amazon ElastiCache for Memcached. ElastiCache for Memcached does not support replication and archival snapshots, so this option is ruled out.</p>\n\n<p><strong>DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. DAX cannot be used as a caching layer for a relational database.</p>\n\n<p><strong>DocumentDB</strong> - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. DocumentDB cannot be used as a caching layer for a relational database.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n",
                "options": [
                    {
                        "id": 10295,
                        "content": "<p>ElastiCache for Memcached</p>",
                        "isValid": false
                    },
                    {
                        "id": 10296,
                        "content": "<p>DocumentDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 10297,
                        "content": "<p>ElastiCache for Redis</p>",
                        "isValid": true
                    },
                    {
                        "id": 10298,
                        "content": "<p>DynamoDB Accelerator (DAX)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2464,
            "attributes": {
                "createdAt": "2023-09-09T20:34:06.019Z",
                "updatedAt": "2023-09-09T20:34:06.019Z",
                "content": "<p>You are looking to build an index of your files in S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50TB of data.</p>\n\n<p>How can you build this index efficiently?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in RDS</strong></p>\n\n<p>Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.</p>\n\n<p>Using the Range HTTP header in a GET Object request, you can fetch a byte-range from an object, transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. This helps you achieve higher aggregate throughput versus a single whole-object request. Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted.</p>\n\n<p>A byte-range request is a perfect way to get the beginning of a file and ensuring we remain efficient during our scan of our S3 bucket. So this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the RDS Import feature to load the data from S3 to PostgreSQL, and run a SQL query to build the index</strong> - You cannot import data from S3 into RDS, so this option is incorrect.</p>\n\n<p><strong>Create an application that will traverse the S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in RDS</strong> - If you build an application that loads all the files from S3, that would work, but you would read 50TB of data and that may be very expensive and slow. So this option is incorrect.</p>\n\n<p><strong>Create an application that will traverse the S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in RDS</strong> - S3 Select is a new Amazon S3 capability designed to pull out only the data you need from an object, which can dramatically improve the performance and reduce the cost of applications that need to access data in S3. You cannot use Byte Range Fetch parameter with S3 Select to traverse the S3 bucket and get the first bytes of a file. So this option is incorrect.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please note that with Amazon S3 Select, you can scan a subset of an object by specifying a range of bytes to query using the ScanRange parameter. This capability lets you parallelize scanning the whole object by splitting the work into separate Amazon S3 Select requests for a series of non-overlapping scan ranges. Use the Amazon S3 Select ScanRange parameter and Start at (Byte) and End at (Byte).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q20-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range\">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range</a></p>\n",
                "options": [
                    {
                        "id": 10299,
                        "content": "<p>Create an application that will traverse the S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in RDS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10300,
                        "content": "<p>Use the RDS Import feature to load the data from S3 to PostgreSQL, and run a SQL query to build the index</p>",
                        "isValid": false
                    },
                    {
                        "id": 10301,
                        "content": "<p>Create an application that will traverse the S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in RDS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10302,
                        "content": "<p>Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in RDS</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2465,
            "attributes": {
                "createdAt": "2023-09-09T20:34:06.114Z",
                "updatedAt": "2023-09-09T20:34:06.114Z",
                "content": "<p>As a Solutions Architect, you would like to completely secure the communications between your CloudFront distribution and your S3 bucket which contains the static files for your website. Users should only be able to access the S3 bucket through CloudFront and not directly.</p>\n\n<p>What do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an origin access identity (OAI) and update the S3 Bucket Policy</strong></p>\n\n<p>To restrict access to content that you serve from Amazon S3 buckets, you need to follow the following steps:</p>\n\n<ol>\n<li>Create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution.</li>\n<li>Configure your S3 bucket permissions so that CloudFront can use the OAI to access the files in your bucket and serve them to your users. Make sure that users can’t use a direct URL to the S3 bucket to access a file there.</li>\n</ol>\n\n<p>After you take these steps, users can only access your files through CloudFront, not directly from the S3 bucket.</p>\n\n<p>In general, if you’re using an Amazon S3 bucket as the origin for a CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. If you restrict access by using, for example, CloudFront signed URLs or signed cookies, you also won’t want people to be able to view files by simply using the direct Amazon S3 URL for the file. Instead, you want them to only access the files by using the CloudFront URL, so your content remains protected.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the S3 bucket security groups to only allow traffic from the CloudFront security group</strong> - S3 buckets don't have security groups, hence this is an incorrect option.</p>\n\n<p><strong>Make the S3 bucket public</strong> - If the S3 bucket is made public, it can be accessed by anyone directly. This is not the requirement.</p>\n\n<p><strong>Create a bucket policy to only authorize the IAM role attached to the CloudFront distribution</strong> -  You cannot attach IAM roles to the CloudFront distribution. Here you need to use an OAI.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>\n",
                "options": [
                    {
                        "id": 10303,
                        "content": "<p>Make the S3 bucket public</p>",
                        "isValid": false
                    },
                    {
                        "id": 10304,
                        "content": "<p>Create an origin access identity (OAI) and update the S3 Bucket Policy</p>",
                        "isValid": true
                    },
                    {
                        "id": 10305,
                        "content": "<p>Update the S3 bucket security groups to only allow traffic from the CloudFront security group</p>",
                        "isValid": false
                    },
                    {
                        "id": 10306,
                        "content": "<p>Create a bucket policy to only authorize the IAM role attached to the CloudFront distribution</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2466,
            "attributes": {
                "createdAt": "2023-09-09T20:34:06.193Z",
                "updatedAt": "2023-09-09T20:34:06.193Z",
                "content": "<p>An application is hosted on multiple Amazon EC2 instances in the same Availability Zone. The engineering team wants to set up shared data access for these EC2 instances using EBS Multi-Attach volumes.</p>\n\n<p>Which EBS volume type is the correct choice for these EC2 instances?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Provisioned IOPS SSD EBS volumes</strong> - Amazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD (io1 or io2) volume to multiple instances that are in the same Availability Zone. You can attach multiple Multi-Attach enabled volumes to an instance or set of instances. Each instance to which the volume is attached has full read and write permission to the shared volume. Multi-Attach makes it easier for you to achieve higher application availability in clustered Linux applications that manage concurrent write operations.</p>\n\n<p>Multi-Attach is supported exclusively on Provisioned IOPS SSD volumes.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>General-purpose SSD-based EBS volumes</strong> - These SSD-backed EBS volumes provide a balance of price and performance. AWS recommends these volumes for most workloads. These volume types are not supported for Multi-Attach functionality.</p>\n\n<p><strong>Throughput Optimized HDD EBS volumes</strong> - These HDD-backed volumes provide a low-cost HDD designed for frequently accessed, throughput-intensive workloads. These volume types are not supported for Multi-Attach functionality.</p>\n\n<p><strong>Cold HDD EBS volumes</strong> - These HDD-backed volumes provide a lowest-cost HDD design for less frequently accessed workloads. These volume types are not supported for Multi-Attach functionality.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html</a></p>\n",
                "options": [
                    {
                        "id": 10307,
                        "content": "<p>Provisioned IOPS SSD EBS volumes</p>",
                        "isValid": true
                    },
                    {
                        "id": 10308,
                        "content": "<p>Cold HDD EBS volumes</p>",
                        "isValid": false
                    },
                    {
                        "id": 10309,
                        "content": "<p>General-purpose SSD-based EBS volumes</p>",
                        "isValid": false
                    },
                    {
                        "id": 10310,
                        "content": "<p>Throughput Optimized HDD EBS volumes</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2467,
            "attributes": {
                "createdAt": "2023-09-09T20:34:06.295Z",
                "updatedAt": "2023-09-09T20:34:06.295Z",
                "content": "<p>An application running on an EC2 instance needs to access a DynamoDB table in the same AWS account.</p>\n\n<p>Which of the following solutions should a solutions architect configure for the necessary permissions?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up an IAM service role with the appropriate permissions to allow access to the DynamoDB table. Configure an instance profile to assign this IAM role to the EC2 instance</strong></p>\n\n<p>A service role is an IAM role that a service assumes to perform actions on your behalf. Service roles provide access only within your account and cannot be used to grant access to services in other accounts. An IAM administrator can create, modify, and delete a service role from within IAM. When you create the service role, you define the <code>trusted entity</code> in the definition.</p>\n\n<p>If you are going to use the role with Amazon EC2 or another AWS service that uses Amazon EC2, you must store the role in an instance profile. An instance profile is a container for a role that can be attached to an Amazon EC2 instance when launched. An instance profile can contain only one role, and that limit cannot be increased. If you create the role using the AWS Management Console, the instance profile is created for you with the same name as the role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an IAM user with the appropriate permissions to allow access to the DynamoDB table. Store the access credentials in an S3 bucket and read them from within the application code directly</strong></p>\n\n<p><strong>Set up an IAM user with the appropriate permissions to allow access to the DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly</strong></p>\n\n<p>You should never store the IAM access credentials for a user in S3 or local storage or a database. It's a security bad practice. It is always recommended to use IAM roles to configure access to other AWS resources from EC2 instances. Therefore both these options are incorrect.</p>\n\n<p><strong>Set up an IAM service role with the appropriate permissions to allow access to the DynamoDB table. Add the EC2 instance to the trust relationship policy document so that the instance can assume the role</strong> - There is no need for this option because when you create an IAM service role for EC2, the role automatically has EC2 identified as a trusted entity. Therefore this option is not correct.</p>\n\n<p>Configuring a Service Role:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q34-i1.jpg\"></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p>\n",
                "options": [
                    {
                        "id": 10311,
                        "content": "<p>Set up an IAM user with the appropriate permissions to allow access to the DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly</p>",
                        "isValid": false
                    },
                    {
                        "id": 10312,
                        "content": "<p>Set up an IAM service role with the appropriate permissions to allow access to the DynamoDB table. Add the EC2 instance to the trust relationship policy document so that the instance can assume the role</p>",
                        "isValid": false
                    },
                    {
                        "id": 10313,
                        "content": "<p>Set up an IAM user with the appropriate permissions to allow access to the DynamoDB table. Store the access credentials in an S3 bucket and read them from within the application code directly</p>",
                        "isValid": false
                    },
                    {
                        "id": 10314,
                        "content": "<p>Set up an IAM service role with the appropriate permissions to allow access to the DynamoDB table. Configure an instance profile to assign this IAM role to the EC2 instance</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2468,
            "attributes": {
                "createdAt": "2023-09-09T20:34:06.382Z",
                "updatedAt": "2023-09-09T20:34:06.382Z",
                "content": "<p>A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year.</p>\n\n<p>As a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>EFS Infrequent Access</strong></p>\n\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed, elastic, NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. The EFS IA storage class costs only $0.025/GB-month. To get started with EFS IA, simply enable EFS Lifecycle Management for your file system by selecting a lifecycle policy that matches your needs.</p>\n\n<p>How EFS Infrequent Access Works:\n<img src=\"https://d1.awsstatic.com/EFS/product-page-diagram-Amazon-EFS-Infrequent-Access-How-It-Works.83f88e30a40c27f38abae1ff157712a336dd1320.png\">\nvia - <a href=\"https://aws.amazon.com/efs/features/infrequent-access/\">https://aws.amazon.com/efs/features/infrequent-access/</a></p>\n\n<p>Incorrect options</p>\n\n<p><strong>EFS Standard</strong> - EFS Infrequent Access is more cost-effective than EFS Standard for the given use-case, therefore this option is incorrect.</p>\n\n<p><strong>S3 Standard</strong></p>\n\n<p><strong>S3 Standard-IA</strong></p>\n\n<p>Both these options are object-based storage, whereas the given use-case requires a POSIX compliant file storage solution. Hence these two options are incorrect.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/efs/features/infrequent-access/</p>\n",
                "options": [
                    {
                        "id": 10315,
                        "content": "<p>EFS Standard</p>",
                        "isValid": false
                    },
                    {
                        "id": 10316,
                        "content": "<p>EFS Infrequent Access</p>",
                        "isValid": true
                    },
                    {
                        "id": 10317,
                        "content": "<p>S3 Standard-IA</p>",
                        "isValid": false
                    },
                    {
                        "id": 10318,
                        "content": "<p>S3 Standard</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2469,
            "attributes": {
                "createdAt": "2023-09-09T20:34:06.490Z",
                "updatedAt": "2023-09-09T20:34:06.490Z",
                "content": "<p>A Big Data consulting company runs large distributed and replicated workloads on the on-premises data center. The company now wants to move these workloads to Amazon EC2 instances by using the placement groups feature and it wants to minimize correlated hardware failures.</p>\n\n<p>Which of the following represents the correct placement group configuration for the given requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Partition placement groups</strong> - Partition placement groups help reduce the likelihood of correlated hardware failures for your application. When using partition placement groups, Amazon EC2 divides each group into logical segments called partitions. Amazon EC2 ensures that each partition within a placement group has its own set of racks. Each rack has its own network and power source. No two partitions within a placement group share the same racks, allowing you to isolate the impact of a hardware failure within your application.</p>\n\n<p>The following image is a simple visual representation of a partition placement group in a single Availability Zone. It shows instances that are placed into a partition placement group with three partitions—Partition 1, Partition 2, and Partition 3. Each partition comprises multiple instances. The instances in a partition do not share racks with the instances in the other partitions, allowing you to contain the impact of a single hardware failure to only the associated partition.</p>\n\n<p>Partition placement groups can be used to deploy large distributed and replicated workloads, such as HDFS, HBase, and Cassandra, across distinct racks. When you launch instances into a partition placement group, Amazon EC2 tries to distribute the instances evenly across the number of partitions that you specify. You can also launch instances into a specific partition to have more control over where the instances are placed.</p>\n\n<p>A partition placement group can have partitions in multiple Availability Zones in the same Region. A partition placement group can have a maximum of seven partitions per Availability Zone. The number of instances that can be launched into a partition placement group is limited only by the limits of your account.</p>\n\n<p>Partition placement groups:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q40-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Cluster placement groups</strong> - A cluster placement group is a logical grouping of instances within a single Availability Zone. A cluster placement group can span peered VPCs in the same Region. Instances in the same cluster placement group enjoy a higher per-flow throughput limit for TCP/IP traffic and are placed in the same high-bisection bandwidth segment of the network. Cluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. They are also recommended when the majority of the network traffic is between the instances in the group. As the instances are packed close together inside an Availability Zone, this option is not correct for the given use case.</p>\n\n<p>Cluster placement groups:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q40-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition</a></p>\n\n<p><strong>Spread placement groups</strong> - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Launching instances in a spread placement group reduces the risk of simultaneous failures that might occur when instances share the same racks. Spread placement groups provide access to distinct racks, and are therefore suitable for mixing instance types or launching instances over time. As the use-case talks about running large distributed and replicated workloads, so it needs more instances, therefore this option is not the right fit for the given use-case.</p>\n\n<p>A spread placement group can span multiple Availability Zones in the same Region. You can have a maximum of seven running instances per Availability Zone per group.</p>\n\n<p>The following image shows seven instances in a single Availability Zone that are placed into a spread placement group. The seven instances are placed on seven different racks.</p>\n\n<p>Spread placement groups:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q40-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition</a></p>\n\n<p><strong>Multi-AZ placement groups</strong> - This is a made-up option, given as a distractor. You should note that the Partition and Spread placement groups can span across multiple Availability Zones in the same Region.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 10319,
                        "content": "<p>Cluster placement groups</p>",
                        "isValid": false
                    },
                    {
                        "id": 10320,
                        "content": "<p>Spread placement groups</p>",
                        "isValid": false
                    },
                    {
                        "id": 10321,
                        "content": "<p>Multi-AZ placement groups</p>",
                        "isValid": false
                    },
                    {
                        "id": 10322,
                        "content": "<p>Partition placement groups</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2470,
            "attributes": {
                "createdAt": "2023-09-09T20:34:06.617Z",
                "updatedAt": "2023-09-09T20:34:06.617Z",
                "content": "<p>The engineering team at an IT company is deploying an Online Transactional Processing (OLTP) application that needs to support relational queries. The application will have unpredictable spikes of usage that the team does not know in advance.</p>\n\n<p>Which database would you recommend using?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Aurora Serverless</strong></p>\n\n<p>Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database instances. It's a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads. The database design for an OLTP application fits the relational model, therefore you can infer an OLTP system as a Relational Database.</p>\n\n<p>Aurora Serverless is the perfect way to create a database that can scale down to 0 servers, and scale up to many servers, as an OLTP database. So this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DynamoDB with Provisioned Capacity and Auto Scaling</strong></p>\n\n<p><strong>DynamoDB with On-Demand Capacity</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>DynamoDB is a NoSQL database and doesn't do relational queries, therefore it's a choice we have to eliminate, even though the two modes proposed here help us cope with an unpredictable amount of usage. So both these options are incorrect.</p>\n\n<p><strong>Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Elasticache is used as a caching layer in front of relational databases. ElastiCache is a NoSQL database and doesn't facilitate relational queries, so this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/aurora/serverless/\">https://aws.amazon.com/rds/aurora/serverless/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/\">https://aws.amazon.com/rds/</a></p>\n",
                "options": [
                    {
                        "id": 10323,
                        "content": "<p>DynamoDB with On-Demand Capacity</p>",
                        "isValid": false
                    },
                    {
                        "id": 10324,
                        "content": "<p>Amazon ElastiCache</p>",
                        "isValid": false
                    },
                    {
                        "id": 10325,
                        "content": "<p>DynamoDB with Provisioned Capacity and Auto Scaling</p>",
                        "isValid": false
                    },
                    {
                        "id": 10326,
                        "content": "<p>Aurora Serverless</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2471,
            "attributes": {
                "createdAt": "2023-09-09T20:34:06.714Z",
                "updatedAt": "2023-09-09T20:34:06.714Z",
                "content": "<p>The DevOps team at an e-commerce company has deployed a fleet of EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within the us-east-1 region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the EC2 instances under the ASG. As part of a test run, two instances (instance 1 and 2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones to become unbalanced. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the Application Load Balancer's health check.</p>\n\n<p>Can you identify the correct outcomes for these events? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>As the Availability Zones got unbalanced, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application</strong></p>\n\n<p>Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size.\nActions such as changing the Availability Zones for your group or explicitly terminating or detaching instances can lead to the Auto Scaling group becoming unbalanced between Availability Zones. Amazon EC2 Auto Scaling compensates by rebalancing the Availability Zones.</p>\n\n<p>When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application. Therefore, this option is correct.</p>\n\n<p>Availability Zone Rebalancing Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q6-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p>\n\n<p><strong>Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance</strong></p>\n\n<p>However, the scaling activity of Auto Scaling works in a different sequence compared to the rebalancing activity. Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it</strong> - This option contradicts the correct sequence of events outlined earlier for scaling activity created by EC2 Auto Scaling. Actually, Auto Scaling first terminates the unhealthy instance and then launches a new instance. Hence this is incorrect.</p>\n\n<p><strong>As the Availability Zones got unbalanced, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched</strong> - This option contradicts the correct sequence of events outlined earlier for rebalancing activity. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones. Hence this is incorrect.</p>\n\n<p><strong>Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously</strong> - This is a made-up option as both the terminate and launch activities can't happen simultaneously. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html</a></p>\n",
                "options": [
                    {
                        "id": 10327,
                        "content": "<p>As the Availability Zones got unbalanced, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched</p>",
                        "isValid": false
                    },
                    {
                        "id": 10328,
                        "content": "<p>As the Availability Zones got unbalanced, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application</p>",
                        "isValid": true
                    },
                    {
                        "id": 10329,
                        "content": "<p>Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance</p>",
                        "isValid": true
                    },
                    {
                        "id": 10330,
                        "content": "<p>Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously</p>",
                        "isValid": false
                    },
                    {
                        "id": 10331,
                        "content": "<p>Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2472,
            "attributes": {
                "createdAt": "2023-09-09T20:34:06.796Z",
                "updatedAt": "2023-09-09T20:34:06.796Z",
                "content": "<p>An engineering team wants to orchestrate multiple Amazon ECS task types running on EC2 instances that are part of the ECS cluster. The output and state data for all tasks need to be stored. The amount of data output by each task is approximately 20 MB and there could be hundreds of tasks running at a time. As old outputs are archived, the storage size is not expected to exceed 1 TB.</p>\n\n<p>As a solutions architect, which of the following would you recommend as an optimized solution for high-frequency reading and writing?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Amazon EFS file systems are distributed across an unconstrained number of storage servers. This distributed data storage design enables file systems to grow elastically to petabyte scale. It also enables massively parallel access from compute instances, including Amazon EC2, Amazon ECS, and AWS Lambda, to your data.</p>\n\n<p><strong>Use Amazon EFS with Provisioned Throughput mode</strong> - Provisioned Throughput mode is available for applications with high throughput to storage (MiB/s per TiB) ratios, or with requirements greater than those allowed by the Bursting Throughput mode. For example, say you're using Amazon EFS for development tools, web serving, or content management applications where the amount of data in your file system is low relative to throughput demands. Your file system can now get the high levels of throughput your applications require without having to pad your file system.</p>\n\n<p>If your file system is in the Provisioned Throughput mode, you can increase the Provisioned Throughput of your file system as often as you want. You can decrease your file system throughput in Provisioned Throughput mode as long as it's been more than 24 hours since the last decrease. Additionally, you can change between Provisioned Throughput mode and the default Bursting Throughput mode as long as it’s been more than 24 hours since the last throughput mode change.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q33-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon EFS with Bursting Throughput mode</strong> - With Bursting Throughput mode, a file system's throughput scales as the amount of data stored in the standard storage class grows. File-based workloads are typically spiky, driving high levels of throughput for short periods of time, and low levels of throughput the rest of the time. To accommodate this, Amazon EFS is designed to burst to high throughput levels for periods of time. By default, AWS recommends that you run your application in the Bursting Throughput mode. But, if you're planning to migrate large amounts of data into your file system, consider switching to Provisioned Throughput mode.</p>\n\n<p>The use-case mentions that the solution should be optimized for high-frequency reading and writing even when the old outputs are archived, therefore Provisioned Throughput mode is a better fit as it guarantees high levels of throughput your applications require without having to pad your file system.</p>\n\n<p><strong>Use an Amazon EBS volume mounted to the ECS cluster instances</strong> - EFS has a higher throughput than EBS. In addition, EBS can be attached to multiple EC2 instances when the underlying EBS type is io1/io2 and the instance is of Nitro type. The use-case does not provide any such details, so this option is ruled out.</p>\n\n<p><strong>Use a DynamoDB table that is accessible by all ECS cluster instances</strong> - DynamoDB is not a fit for this scenario as each task output is 20 MB but the storage limit for each item in a DynamoDB table is 400 KB. You could write custom code to split the task output data into multiple items but it is not an optimal solution compared to using EFS in Provisioned Throughput mode.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items</a></p>\n",
                "options": [
                    {
                        "id": 10332,
                        "content": "<p>Use a DynamoDB table that is accessible by all ECS cluster instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10333,
                        "content": "<p>Use Amazon EFS with Bursting Throughput mode</p>",
                        "isValid": false
                    },
                    {
                        "id": 10334,
                        "content": "<p>Use an Amazon EBS volume mounted to the ECS cluster instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10335,
                        "content": "<p>Use Amazon EFS with Provisioned Throughput mode</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2473,
            "attributes": {
                "createdAt": "2023-09-09T20:34:06.880Z",
                "updatedAt": "2023-09-09T20:34:06.880Z",
                "content": "<p>A development team is looking for a solution that saves development time and deployment costs for an application that uses a high-throughput request-response message pattern.</p>\n\n<p>Which of the following SQS queue types is the best fit to meet this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon SQS temporary queues</strong> - Temporary queues help you save development time and deployment costs when using common message patterns such as request-response. You can use the Temporary Queue Client to create high-throughput, cost-effective, application-managed temporary queues.</p>\n\n<p>The client maps multiple temporary queues—application-managed queues created on demand for a particular process—onto a single Amazon SQS queue automatically. This allows your application to make fewer API calls and have a higher throughput when the traffic to each temporary queue is low. When a temporary queue is no longer in use, the client cleans up the temporary queue automatically, even if some processes that use the client aren't shut down cleanly.</p>\n\n<p>The following are the benefits of temporary queues:</p>\n\n<ol>\n<li><p>They serve as lightweight communication channels for specific threads or processes.</p></li>\n<li><p>They can be created and deleted without incurring additional costs.</p></li>\n<li><p>They are API-compatible with static (normal) Amazon SQS queues. This means that existing code that sends and receives messages can send messages to and receive messages from virtual queues.</p></li>\n</ol>\n\n<p>To better support short-lived, lightweight messaging destinations, AWS recommends Amazon SQS Temporary Queue Client. This client makes it easy to create and delete many temporary messaging destinations without inflating your AWS bill. The key concept behind the client is the virtual queue. Virtual queues let you multiplex many low-traffic queues onto a single SQS queue. Creating a virtual queue only instantiates a local buffer to hold messages for consumers as they arrive; there is no API call to SQS and no costs associated with creating a virtual queue.</p>\n\n<p>End-to-end process for sending messages through virtual queues:</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2019/07/26/Selection_015.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/\">https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon SQS dead-letter queues</strong> - Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. Amazon SQS does not create the dead-letter queue automatically. You must first create the queue before using it as a dead-letter queue.</p>\n\n<p><strong>Amazon SQS FIFO queues</strong> - FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated. FIFO queues also provide exactly-once processing but have a limited number of transactions per second (TPS).</p>\n\n<p><strong>Amazon SQS delay queues</strong> - Delay queues let you postpone the delivery of new messages to a queue for a number of seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-temporary-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-temporary-queues.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/\">https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/</a></p>\n",
                "options": [
                    {
                        "id": 10336,
                        "content": "<p>Amazon SQS temporary queues</p>",
                        "isValid": true
                    },
                    {
                        "id": 10337,
                        "content": "<p>Amazon SQS FIFO queues</p>",
                        "isValid": false
                    },
                    {
                        "id": 10338,
                        "content": "<p>Amazon SQS delay queues</p>",
                        "isValid": false
                    },
                    {
                        "id": 10339,
                        "content": "<p>Amazon SQS dead-letter queues</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2474,
            "attributes": {
                "createdAt": "2023-09-09T20:34:06.979Z",
                "updatedAt": "2023-09-09T20:34:06.979Z",
                "content": "<p>A company has noticed several provisioned throughput exceptions on its DynamoDB database due to major spikes in the writes to the database. The development team wants to decouple the application layer from the database layer and dedicate a worker process to writing the data to DynamoDB.</p>\n\n<p>Which middleware do you recommend on using that can scale infinitely and meet these requirements in the most cost effective way?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Simple Queue Service (SQS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Using SQS as a middleware will help us sustain the write throughput during write peaks and therefore this option is the best fit for the given use-case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DynamoDB DAX</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.</p>\n\n<p>DAX is used for caching reads, not to help with writes. So this option is ruled out.</p>\n\n<p><strong>Kinesis Data Streams</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service.  KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. Kinesis is used to process consistent real-time data and does not scale as cost effectively as SQS to handle spikes in traffic.</p>\n\n<p><strong>Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. SNS won't keep our data if it cannot be delivered, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 10340,
                        "content": "<p>Amazon Simple Queue Service (SQS)</p>",
                        "isValid": true
                    },
                    {
                        "id": 10341,
                        "content": "<p>DynamoDB DAX</p>",
                        "isValid": false
                    },
                    {
                        "id": 10342,
                        "content": "<p>Kinesis Data Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 10343,
                        "content": "<p>Amazon Simple Notification Service (SNS)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2475,
            "attributes": {
                "createdAt": "2023-09-09T20:34:07.113Z",
                "updatedAt": "2023-09-09T20:34:07.113Z",
                "content": "<p>Your e-commerce application is using an RDS PostgreSQL database and an analytics workload also runs on the same database. When the analytics workload is run, your e-commerce application slows down which further affects your sales.</p>\n\n<p>Which of the following is the MOST cost-optimal solution to fix this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a Read Replica in the same Region as the Master database and point the analytics workload there</strong></p>\n\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance. Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.</p>\n\n<p>Creating a Read Replica is the answer. As we want to minimize the costs, we need to launch the Read Replica in the same Region as you are not charged for the data transfer incurred in replicating data between your source DB instance and read replica within the same AWS Region.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review this comparison vis-a-vis Multi-AZ vs Read Replica for RDS:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q19-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Multi-AZ for the RDS database and run the analytics workload on the standby database</strong> - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones within a single region.</p>\n\n<p>Enabling Multi-AZ helps make our database highly-available, but the standby database is not accessible and cannot be used for reads or write. It's just a database that will become primary when the other database encounters a failure. So this option is not correct.</p>\n\n<p><strong>Migrate the analytics application to AWS Lambda</strong>- AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>Running the application on AWS Lambda will not help, as it will still run against the main database and slow down our e-commerce application.</p>\n\n<p><strong>Create a Read Replica in another Region as the Master database and point the analytics workload there</strong> - This is incorrect because we have to pay for inter-Region data replication charges for the Read Replica, whereas the replication of data within a single Region is free.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n",
                "options": [
                    {
                        "id": 10344,
                        "content": "<p>Create a Read Replica in another Region as the Master database and point the analytics workload there</p>",
                        "isValid": false
                    },
                    {
                        "id": 10345,
                        "content": "<p>Create a Read Replica in the same Region as the Master database and point the analytics workload there</p>",
                        "isValid": true
                    },
                    {
                        "id": 10346,
                        "content": "<p>Migrate the analytics application to AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 10347,
                        "content": "<p>Enable Multi-AZ for the RDS database and run the analytics workload on the standby database</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2476,
            "attributes": {
                "createdAt": "2023-09-09T20:34:07.312Z",
                "updatedAt": "2023-09-09T20:34:07.312Z",
                "content": "<p>A development team wants to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?</p>\n\n<p>Which of the following options represents the correct solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set</strong> - Amazon S3 encrypts your data at the object level as it writes to disks in AWS data centers, and decrypts it for you when you access it. You can encrypt objects by using client-side encryption or server-side encryption. Client-side encryption occurs when an object is encrypted before you upload it to S3, and the keys are not managed by AWS. With server-side encryption, Amazon manages the keys in one of three ways:</p>\n\n<ol>\n<li>Server-side encryption with customer-provided encryption keys (SSE-C).</li>\n<li>SSE-S3.</li>\n<li>SSE-KMS.</li>\n</ol>\n\n<p>Server-side encryption is about data encryption at rest—that is, S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects.</p>\n\n<p>To encrypt an object at the time of upload, you need to add a header called x-amz-server-side-encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS.</p>\n\n<p>In order to enforce object encryption, create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption header. There are two possible values for the x-amz-server-side-encryption header: AES256, which tells S3 to use S3-managed keys, and aws:kms, which tells S3 to use AWS KMS–managed keys.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private</strong> - The x-amz-acl header is used to specify an ACL in the PutObject request. Access permissions are defined using this header.</p>\n\n<p><strong>Configure the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true</strong> - By default, Amazon S3 allows both HTTP and HTTPS requests. aws:SecureTransport key is used to check if the request is sent through HTTP or HTTPS. When this key is true, it means that the request is sent through HTTPS.</p>\n\n<p><strong>Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set</strong> - As discussed above, the s3:x-amz-acl header is used to set permissions on the specified S3 bucket and has nothing to do with encryption.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/\">https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html</a></p>\n",
                "options": [
                    {
                        "id": 10348,
                        "content": "<p>Configure the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true</p>",
                        "isValid": false
                    },
                    {
                        "id": 10349,
                        "content": "<p>Configure the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set</p>",
                        "isValid": true
                    },
                    {
                        "id": 10350,
                        "content": "<p>Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set</p>",
                        "isValid": false
                    },
                    {
                        "id": 10351,
                        "content": "<p>Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2477,
            "attributes": {
                "createdAt": "2023-09-09T20:34:07.422Z",
                "updatedAt": "2023-09-09T20:34:07.422Z",
                "content": "<p>A company is developing a document management application on AWS. The application runs on EC2 instances in multiple Availability Zones. The company requires the document store to be highly available and the documents need to be returned immediately when requested. The engineering team has configured the application to use EBS to store the documents but the team is willing to consider other options to meet the availability requirement.</p>\n\n<p>As a solutions architect, which of the following will you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up Amazon EBS as the EC2 instance root volume and then configure the application to use S3 as the document store</strong></p>\n\n<p>Instances that use Amazon EBS for the root device automatically have an Amazon EBS volume attached. When you launch an Amazon EBS-backed instance, AWS creates an Amazon EBS volume for each Amazon EBS snapshot referenced by the AMI you use. An Amazon EBS-backed instance can be stopped and later restarted without affecting data stored in the attached volumes.</p>\n\n<p>Amazon S3 provides access to reliable, fast, and inexpensive data storage infrastructure. It is designed to make web-scale computing easier by enabling you to store and retrieve any amount of data, at any time, from within Amazon EC2 or anywhere on the web. S3 is highly available and can be configured to work as a document store for the given use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up Amazon EBS as the EC2 instance root volume and then configure the application to use S3 Glacier as the document store</strong> - As the documents need to be returned immediately when requested, S3 Glacier is not the right fit, since there is a lag of several minutes/hours when you want to read data from Glacier.</p>\n\n<p><strong>Create snapshots for the EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones</strong> - You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. Hence, using EBS volumes as a primary storage solution is ineffective, and creating recurring snapshots is a management nightmare for the current use case.</p>\n\n<p><strong>Provision at least three Provisioned IOPS EBS volumes for the EC2 instances and then mount these volumes to the EC2 instances in a RAID 5 configuration</strong> - RAID 5 and RAID 6 are not recommended for Amazon EBS because the parity write operations of these RAID modes consume some of the IOPS available to your volumes. Depending on the configuration of your RAID array, these RAID modes provide 20-30% fewer usable IOPS than a RAID 0 configuration. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p>\n",
                "options": [
                    {
                        "id": 10352,
                        "content": "<p>Provision at least three Provisioned IOPS EBS volumes for the EC2 instances and then mount these volumes to the EC2 instances in a RAID 5 configuration</p>",
                        "isValid": false
                    },
                    {
                        "id": 10353,
                        "content": "<p>Set up Amazon EBS as the EC2 instance root volume and then configure the application to use S3 as the document store</p>",
                        "isValid": true
                    },
                    {
                        "id": 10354,
                        "content": "<p>Create snapshots for the EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones</p>",
                        "isValid": false
                    },
                    {
                        "id": 10355,
                        "content": "<p>Set up Amazon EBS as the EC2 instance root volume and then configure the application to use S3 Glacier as the document store</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2478,
            "attributes": {
                "createdAt": "2023-09-09T20:34:07.773Z",
                "updatedAt": "2023-09-09T20:34:07.773Z",
                "content": "<p>An e-commerce company uses RDS MySQL DB to store the data. The analytics department at the company runs its reports on the same DB. The engineering team has noticed sluggish performance on the DB when the analytics reporting process is in progress.</p>\n\n<p>As an AWS Certified Solutions Architect Associate, which of the following would you suggest as the MOST cost-optimal solution to improve the performance?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica</strong></p>\n\n<p>Amazon RDS uses the MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL DB engines' built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica. You can reduce the load on your primary DB instance by routing read queries from your applications to the read replica.</p>\n\n<p>RDS Read Replicas:\n<img src=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/read-replica.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q46-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n\n<p>You can use read replicas to improve the performance of your RDS MySQL DB by handling business reporting or data warehousing scenarios where you might want business reporting queries to run against your read replica, rather than your production DB instance.</p>\n\n<p>You can create up to five read replicas from one DB instance. For replication to operate effectively, each read replica should have the same amount of compute and storage resources as the source DB instance. If you scale the source DB instance, also scale the read replicas.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q46-i2.jpg\">\nvia - <a href=\"https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html\">https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica</strong> - As mentioned in the explanation above, you should create a read-replica with the same compute capacity and the same storage capacity as the primary.</p>\n\n<p><strong>Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance</strong></p>\n\n<p><strong>Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance</strong></p>\n\n<p>Multi-AZ deployments are not a read scaling solution, so you cannot use a standby to serve read traffic. The standby is there just for failover. Hence both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html\">https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html</a></p>\n",
                "options": [
                    {
                        "id": 10356,
                        "content": "<p>Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica</p>",
                        "isValid": true
                    },
                    {
                        "id": 10357,
                        "content": "<p>Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica</p>",
                        "isValid": false
                    },
                    {
                        "id": 10358,
                        "content": "<p>Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10359,
                        "content": "<p>Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2479,
            "attributes": {
                "createdAt": "2023-09-09T20:34:08.799Z",
                "updatedAt": "2023-09-09T20:34:08.799Z",
                "content": "<p>A data analytics company is running a proprietary database on an EC2 instance using EBS volumes. The database is heavily I/O bound. As a solutions architect, which of the following RAID configurations would you recommend improving the I/O performance?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use RAID 0 when I/O performance is more important than fault tolerance</strong></p>\n\n<p>With Amazon EBS, you can use any of the standard RAID configurations that you can use with a traditional bare metal server, as long as that particular RAID configuration is supported by the operating system for your instance. This is because all RAID is accomplished at the software level.</p>\n\n<p>RAID configuration options for I/O performance v/s fault tolerance:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q37-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use RAID 1 when I/O performance is more important than fault tolerance</strong> - This is incorrect because you should use RAID 1 when fault tolerance is more important than I/O performance.</p>\n\n<p><strong>Both RAID 0 and RAID 1 provide equally good I/O performance</strong> -  This is incorrect because RAID 0 provides better I/O performance.</p>\n\n<p><strong>Amazon EBS does not support the standard RAID configurations</strong> - This is incorrect because EBS supports the standard RAID configurations.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p>\n",
                "options": [
                    {
                        "id": 10360,
                        "content": "<p>Use RAID 0 when I/O performance is more important than fault tolerance</p>",
                        "isValid": true
                    },
                    {
                        "id": 10361,
                        "content": "<p>Use RAID 1 when I/O performance is more important than fault tolerance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10362,
                        "content": "<p>Amazon EBS does not support the standard RAID configurations</p>",
                        "isValid": false
                    },
                    {
                        "id": 10363,
                        "content": "<p>Both RAID 0 and RAID 1 provide equally good I/O performance</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2480,
            "attributes": {
                "createdAt": "2023-09-09T20:34:08.891Z",
                "updatedAt": "2023-09-09T20:34:08.891Z",
                "content": "<p>A software engineering intern at a company is documenting the features offered by EC2 Spot instances, Spot blocks, and Spot Fleets.</p>\n\n<p>Can you help the intern by selecting the correct options that identify the key characteristics of these three types of Spot entities? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Spot instances are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification</strong></p>\n\n<p>Spot instances are spare EC2 capacity that can save you up 90% off of On-Demand prices that Amazon Web Services can interrupt with a 2-minute notification. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted.</p>\n\n<p><strong>Spot blocks allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted</strong></p>\n\n<p>Spot blocks are designed not to be interrupted and will run continuously for the duration you select (1 to 6 hours), independent of the Spot market price. In rare situations, Spot blocks may be interrupted due to Amazon Web Services' capacity needs. In these cases, AWS will provide a two-minute warning before it terminates your instance and you will not be charged for the affected instance(s).</p>\n\n<p><strong>A Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity</strong></p>\n\n<p>A Spot Fleet is a collection, or fleet, of Spot Instances, and optionally On-Demand Instances. The Spot Fleet attempts to launch the number of Spot Instances and On-Demand Instances to meet the target capacity that you specified in the Spot Fleet request. A Spot Fleet allows you to automatically request and manage multiple Spot instances that provide the lowest price per unit of capacity for your cluster or application, like a batch processing job, a Hadoop workflow, or an HPC grid computing job.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q2-i1.jpg\">\nvia - <a href=\"https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html\">https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Spot Fleet allows you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted</strong></p>\n\n<p><strong>Spot blocks are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot blocks are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification</strong></p>\n\n<p><strong>A Spot block is a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://www.amazonaws.cn/en/ec2/spot-instances/faqs/\">https://www.amazonaws.cn/en/ec2/spot-instances/faqs/</a></p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html\">https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html</a></p>\n",
                "options": [
                    {
                        "id": 10364,
                        "content": "<p>A Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity</p>",
                        "isValid": true
                    },
                    {
                        "id": 10365,
                        "content": "<p>A Spot block is a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity</p>",
                        "isValid": false
                    },
                    {
                        "id": 10366,
                        "content": "<p>Spot Fleet allows you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted</p>",
                        "isValid": false
                    },
                    {
                        "id": 10367,
                        "content": "<p>Spot instances are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification</p>",
                        "isValid": true
                    },
                    {
                        "id": 10368,
                        "content": "<p>Spot blocks allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted</p>",
                        "isValid": true
                    },
                    {
                        "id": 10369,
                        "content": "<p>Spot blocks are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot blocks are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2481,
            "attributes": {
                "createdAt": "2023-09-09T20:34:09.022Z",
                "updatedAt": "2023-09-09T20:34:09.022Z",
                "content": "<p>You have deployed a database technology that has a synchronous replication mode to survive disasters in data centers. The database is therefore deployed on two EC2 instances in two Availability Zones. The database must be publicly available so you have deployed the EC2 instances in public subnets. The replication protocol currently uses the EC2 public IP addresses.</p>\n\n<p>What can you do to decrease the replication cost?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use the EC2 instances private IP for the replication</strong></p>\n\n<p>The source of the cost is that traffic between two EC2 instances is going over the public internet, thus incurring high costs. Here, the correct answer is to use Private IP, so that the network remains private, for a minimal cost.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Assign Elastic IPs to the EC2 instances and use them for the replication</strong> - Using Elastic IPs will not solve the problem as the traffic will still be going over the public internet.</p>\n\n<p><strong>Create a Private Link between the two EC2 instances</strong> - AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network.</p>\n\n<p>Private Link is a distractor in this question. Private Link is leveraged to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering and allowing the connections between the two to remain within the AWS network.</p>\n\n<p><strong>Use an Elastic Fabric Adapter</strong> - The Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run HPC applications requiring high levels of inter-instance communications, like computational fluid dynamics, weather modeling, and reservoir simulation, at scale on AWS. This option is not relevant to the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/privatelink/\">https://aws.amazon.com/privatelink/</a></p>\n\n<p><a href=\"https://aws.amazon.com/hpc/efa/\">https://aws.amazon.com/hpc/efa/</a></p>\n",
                "options": [
                    {
                        "id": 10370,
                        "content": "<p>Assign Elastic IPs to the EC2 instances and use them for the replication</p>",
                        "isValid": false
                    },
                    {
                        "id": 10371,
                        "content": "<p>Use the EC2 instances private IP for the replication</p>",
                        "isValid": true
                    },
                    {
                        "id": 10372,
                        "content": "<p>Use an Elastic Fabric Adapter</p>",
                        "isValid": false
                    },
                    {
                        "id": 10373,
                        "content": "<p>Create a Private Link between the two EC2 instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2482,
            "attributes": {
                "createdAt": "2023-09-09T20:34:09.143Z",
                "updatedAt": "2023-09-09T20:34:09.143Z",
                "content": "<p>A company is deploying a web application and it wants to ensure that only the web tier of the application is publicly accessible. To accomplish this, the engineering team has designed the VPC with a public subnet and a private subnet. The application will be hosted on several EC2 instances in an Auto Scaling group. The team also wants TLS termination to be offloaded from the EC2 instances.</p>\n\n<p>Which solution should a solutions architect implement to address these requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer</strong></p>\n\n<p>A load balancer serves as the single point of contact for clients. The load balancer distributes incoming traffic across multiple targets, such as Amazon EC2 instances. This increases the availability of your application. You add one or more listeners to your load balancer.</p>\n\n<p>With Network Load Balancer (NLB), you can offload the decryption/encryption of TLS traffic from your application servers to the Network Load Balancer, which helps you optimize the performance of your backend application servers while keeping your workloads secure. Additionally, Network Load Balancers preserve the source IP of the clients to the back-end applications, while terminating TLS on the load balancer.</p>\n\n<p>An Auto Scaling Group (ASG) contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service.</p>\n\n<p>The NLB has to be accessible over the internet and hence has to be in a public subnet and will act as a single point-of-contact for all incoming traffic. NLB will forward the incoming traffic to the EC2 instances managed by the ASG in the private subnet.</p>\n\n<p>Exam Alert:</p>\n\n<p>You should note that the Application Load Balancer also supports TLS offloading. The Classic Load Balancer supports SSL offloading.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer</strong> - ASG with its target EC2 instances should be in the private subnet to avoid access to EC2 instances over the public internet. Hence, this option is incorrect.</p>\n\n<p><strong>Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer</strong></p>\n\n<p><strong>Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer</strong></p>\n\n<p>NLB should be in the public subnet as it represents the internet-facing component of the web tier. Therefore, both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/\">https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/</a></p>\n",
                "options": [
                    {
                        "id": 10374,
                        "content": "<p>Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 10375,
                        "content": "<p>Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 10376,
                        "content": "<p>Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 10377,
                        "content": "<p>Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2483,
            "attributes": {
                "createdAt": "2023-09-09T20:34:09.232Z",
                "updatedAt": "2023-09-09T20:34:09.232Z",
                "content": "<p>A company has moved its business critical data to Amazon EFS file system which will be accessed by multiple EC2 instances.</p>\n\n<p>As an AWS Certified Solutions Architect Associate, which of the following would you recommend to exercise access control such that only the permitted EC2 instances can read from the EFS file system? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use VPC security groups to control the network traffic to and from your file system</strong></p>\n\n<p><strong>Use an IAM policy to control access for clients who can mount your file system with the required permissions</strong></p>\n\n<p>You control which EC2 instances can access your EFS file system by using VPC security group rules and AWS Identity and Access Management (IAM) policies. Use VPC security groups to control the network traffic to and from your file system. Attach an IAM policy to your file system to control which clients can mount your file system and with what permissions, and you may use EFS Access Points to manage application access. Control access to files and directories with POSIX-compliant user and group-level permissions.</p>\n\n<p>Files and directories in an Amazon EFS file system support standard Unix-style read, write, and execute permissions based on the user ID and group IDs. When an NFS client mounts an EFS file system without using an access point, the user ID and group ID provided by the client is trusted. You can also use EFS access points to override user ID and group IDs used by the NFS client. When users attempt to access files and directories, Amazon EFS checks their user IDs and group IDs to verify that each user has permission to access the objects</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Network ACLs to control the network traffic to and from your Amazon EC2 instance</strong> - Network ACLs operate at the subnet level and not at the instance level.</p>\n\n<p><strong>Set up the IAM policy root credentials to control and configure the clients accessing the EFS file system</strong> - There is no such thing as an IAM policy root credentials and this statement has been added as a distractor.</p>\n\n<p><strong>Use Amazon GuardDuty to curb unwanted access to EFS file system</strong> - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. It cannot be used for access control to the EFS file system.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-permissions.html\">https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-permissions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html\">https://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html</a></p>\n",
                "options": [
                    {
                        "id": 10378,
                        "content": "<p>Use an IAM policy to control access for clients who can mount your file system with the required permissions</p>",
                        "isValid": true
                    },
                    {
                        "id": 10379,
                        "content": "<p>Use Network ACLs to control the network traffic to and from your Amazon EC2 instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10380,
                        "content": "<p>Use VPC security groups to control the network traffic to and from your file system</p>",
                        "isValid": true
                    },
                    {
                        "id": 10381,
                        "content": "<p>Set up the IAM policy root credentials to control and configure the clients accessing the EFS file system</p>",
                        "isValid": false
                    },
                    {
                        "id": 10382,
                        "content": "<p>Use Amazon GuardDuty to curb unwanted access to EFS file system</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2484,
            "attributes": {
                "createdAt": "2023-09-09T20:34:09.351Z",
                "updatedAt": "2023-09-09T20:34:09.351Z",
                "content": "<p>A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance.</p>\n\n<p>Which combination of steps should the solutions architect take? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Set up Amazon Kinesis Data Streams to ingest the data</strong></p>\n\n<p><strong>Set up AWS Fargate with Amazon ECS to process the data</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.</p>\n\n<p>For the given use case, we can use Kinesis Data Streams as the ingestion layer and the containerized ECS application on AWS Fargate as the processing layer. Both these components are serverless and can scale to offer the desired performance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Database Migration Service (AWS DMS) to ingest the data</strong> - AWS Database Migration Service helps you migrate databases to AWS quickly and securely. DMS cannot be used for real-time data ingestion. Hence, this option is incorrect.</p>\n\n<p><strong>Set up AWS Lambda with AWS Step Functions to process the data</strong> - The maximum timeout value for any AWS Lambda function is 15 minutes. When the specified timeout is reached, AWS Lambda terminates the execution of your Lambda function. Since the use case talks about a job that runs for 30 minutes, Lambda is not an option here.</p>\n\n<p><strong>Provision EC2 instances in an Auto Scaling group to process the data</strong> - The given requirement is for a serverless solution to process the data. Hence, provisioning an EC2 instance is clearly not the right solution.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/building-a-scalable-streaming-data-processor-with-amazon-kinesis-data-streams-on-aws-fargate/\">https://aws.amazon.com/blogs/big-data/building-a-scalable-streaming-data-processor-with-amazon-kinesis-data-streams-on-aws-fargate/</a></p>\n",
                "options": [
                    {
                        "id": 10383,
                        "content": "<p>Set up AWS Lambda with AWS Step Functions to process the data</p>",
                        "isValid": false
                    },
                    {
                        "id": 10384,
                        "content": "<p>Provision EC2 instances in an Auto Scaling group to process the data</p>",
                        "isValid": false
                    },
                    {
                        "id": 10385,
                        "content": "<p>Set up AWS Database Migration Service (AWS DMS) to ingest the data</p>",
                        "isValid": false
                    },
                    {
                        "id": 10386,
                        "content": "<p>Set up Amazon Kinesis Data Streams to ingest the data</p>",
                        "isValid": true
                    },
                    {
                        "id": 10387,
                        "content": "<p>Set up AWS Fargate with Amazon ECS to process the data</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2485,
            "attributes": {
                "createdAt": "2023-09-09T20:34:09.450Z",
                "updatedAt": "2023-09-09T20:34:09.450Z",
                "content": "<p>A development team has noticed that one of the EC2 instances has been incorrectly configured with the 'DeleteOnTermination' attribute set to True for its root EBS volume.</p>\n\n<p>As a Solution's Architect, can you suggest a way to disable this flag while the instance is still running?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>When an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume and is set to False for all other volume types.</p>\n\n<p><strong>Set the <code>DeleteOnTermination</code> attribute to False using the command line</strong> - If the instance is already running, you can set <code>DeleteOnTermination</code> to False using the command line.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume</strong> - You can set the <code>DeleteOnTermination</code> attribute to False when you launch a new instance. It is not possible to update this attribute of a running instance from the AWS console.</p>\n\n<p><strong>Set the <code>DisableApiTermination</code> attribute of the instance using the API</strong> - By default, you can terminate your instance using the Amazon EC2 console, command-line interface, or API. To prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination protection for the instance. The <code>DisableApiTermination</code> attribute controls whether the instance can be terminated using the console, CLI, or API. This option cannot be used to control the delete status for the EBS volume when the instance terminates.</p>\n\n<p><strong>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</strong> - This statement is wrong and given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/\">https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance</a></p>\n",
                "options": [
                    {
                        "id": 10388,
                        "content": "<p>Set the <code>DisableApiTermination</code> attribute of the instance using the API</p>",
                        "isValid": false
                    },
                    {
                        "id": 10389,
                        "content": "<p>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</p>",
                        "isValid": false
                    },
                    {
                        "id": 10390,
                        "content": "<p>Set the <code>DeleteOnTermination</code> attribute to False using the command line</p>",
                        "isValid": true
                    },
                    {
                        "id": 10391,
                        "content": "<p>Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2486,
            "attributes": {
                "createdAt": "2023-09-09T20:34:09.553Z",
                "updatedAt": "2023-09-09T20:34:09.553Z",
                "content": "<p>As a Solutions Architect, you have set up a database on a single EC2 instance that has an EBS volume of type gp2. You currently have 300GB of space on the gp2 device. The EC2 instance is of type m5.large. The database performance has recently been poor and upon looking at CloudWatch, you realize the IOPS on the EBS volume is maxing out. The disk size of the database must not change because of a licensing issue.</p>\n\n<p>How do you troubleshoot this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Amazon EBS provides the following volume types, which differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications. The volumes types fall into two categories:</p>\n\n<p>SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS</p>\n\n<p>HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS</p>\n\n<p><strong>Convert the gp2 volume to an io1</strong> - Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.</p>\n\n<p>The only solution is to convert the volume into an io1 volume. This will allow us to keep the same disk size while independently increasing the IOPS for that volume.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Stop the CloudWatch agent to improve performance</strong> - The CloudWatch agent does not have any impact on the performance of the instance.</p>\n\n<p><strong>Increase the IOPS on the gp2 volume</strong> - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver their provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB.</p>\n\n<p>IOPS cannot be directly increased on a gp2 volume without increasing its size, which is not possible due to the question's constraints.</p>\n\n<p><strong>Convert the EC2 instance to an i3.4xlarge</strong> -  Converting the EC2 instance to i3.4xlarge won't improve the EBS drive's performance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops</a></p>\n",
                "options": [
                    {
                        "id": 10392,
                        "content": "<p>Convert the gp2 volume to an io1</p>",
                        "isValid": true
                    },
                    {
                        "id": 10393,
                        "content": "<p>Increase the IOPS on the gp2 volume</p>",
                        "isValid": false
                    },
                    {
                        "id": 10394,
                        "content": "<p>Stop the CloudWatch agent to improve performance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10395,
                        "content": "<p>Convert the EC2 instance to an i3.4xlarge</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2487,
            "attributes": {
                "createdAt": "2023-09-09T20:34:09.674Z",
                "updatedAt": "2023-09-09T20:34:09.674Z",
                "content": "<p>A company has media files that need to be shared internally. Users are first authenticated using Active Directory and then they access files on a Microsoft Windows platform. The engineering manager wants to keep the same user permissions but wants the company to migrate the storage layer to AWS Cloud as the company is reaching its storage capacity limit on the on-premises infrastructure.</p>\n\n<p>What should a solutions architect recommend to meet this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up Amazon FSx for Windows File Server and move all the media files</strong></p>\n\n<p>Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. To support a wide spectrum of workloads, Amazon FSx provides high levels of throughput and IOPS and consistent sub-millisecond latencies.</p>\n\n<p>Amazon FSx file storage is accessible from Windows, Linux, and macOS compute instances and devices running on AWS or on-premises. Thousands of compute instances and devices can access a file system concurrently. Amazon FSx for Windows File Server supports Microsoft Active Directory (AD) integration so the same user permissions and access credentials can be used to access the files on FSx Windows File Server.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a corporate Amazon S3 bucket and move all media files</strong> - S3 is object-based storage and it does not support file storage. Hence S3 is not the correct option.</p>\n\n<p><strong>Set up EFS and move all media files</strong> - Amazon EFS provides scalable file storage for use with Amazon EC2. You can use an EFS file system as a common data source for workloads and applications running on multiple instances. EFS is not compatible with the Windows platform, so this option is ruled out.</p>\n\n<p><strong>Provision EC2 with Windows OS, attach multiple EBS volumes, and move all media files</strong> - Multi-attach EBS volumes are supported only for Nitro EC2 instances which are Linux-based. So this option is ruled out.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p>\n",
                "options": [
                    {
                        "id": 10396,
                        "content": "<p>Create a corporate Amazon S3 bucket and move all media files</p>",
                        "isValid": false
                    },
                    {
                        "id": 10397,
                        "content": "<p>Set up Amazon FSx for Windows File Server and move all the media files</p>",
                        "isValid": true
                    },
                    {
                        "id": 10398,
                        "content": "<p>Set up EFS and move all media files</p>",
                        "isValid": false
                    },
                    {
                        "id": 10399,
                        "content": "<p>Provision EC2 with Windows OS, attach multiple EBS volumes, and move all media files</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2488,
            "attributes": {
                "createdAt": "2023-09-09T20:34:09.787Z",
                "updatedAt": "2023-09-09T20:34:09.787Z",
                "content": "<p>A company is experiencing stability issues with their cluster of self-managed RabbitMQ message brokers and the company now wants to explore an alternate solution on AWS.</p>\n\n<p>As a solutions architect, which of the following AWS services would you recommend that can provide support for quick and easy migration from RabbitMQ?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon MQ</strong></p>\n\n<p>Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Message brokers allow different software systems–often using different programming languages, and on different platforms–to communicate and exchange information. If an organization is using messaging with existing applications and wants to move the messaging service to the cloud quickly and easily, AWS recommends Amazon MQ for such a use case. So this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Notification Service (Amazon SNS)</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS does not provide support for migration from RabbitMQ as its a fully managed pub/sub messaging service. Hence this option is incorrect.</p>\n\n<p><strong>Amazon SQS Standard</strong> - Amazon SQS Standard offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. SQS Standard does not provide support for migration from RabbitMQ. Hence this option is incorrect.</p>\n\n<p><strong>Amazon SQS FIFO (First-In-First-Out)</strong> - Amazon SQS FIFO (First-In-First-Out) has all the capabilities of the standard queue. They are used when the order of operations and events is critical, or where duplicates can't be tolerated. SQS FIFO does not provide support for migration from RabbitMQ. Hence this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/amazon-mq/\">https://aws.amazon.com/amazon-mq/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/migrating-from-rabbitmq-to-amazon-mq/\">https://aws.amazon.com/blogs/compute/migrating-from-rabbitmq-to-amazon-mq/</a></p>\n",
                "options": [
                    {
                        "id": 10400,
                        "content": "<p>Amazon MQ</p>",
                        "isValid": true
                    },
                    {
                        "id": 10401,
                        "content": "<p>Amazon Simple Notification Service (Amazon SNS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10402,
                        "content": "<p>Amazon SQS FIFO (First-In-First-Out)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10403,
                        "content": "<p>Amazon SQS Standard</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2489,
            "attributes": {
                "createdAt": "2023-09-09T20:34:09.938Z",
                "updatedAt": "2023-09-09T20:34:09.938Z",
                "content": "<p>A company maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the company has transitioned from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all the data to an Amazon S3 bucket, the company is now looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data.</p>\n\n<p>How will you implement this requirement without adding the overhead of splitting the data into logical groups?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</strong></p>\n\n<p>Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. When you use server-side encryption with Amazon S3 managed keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates.</p>\n\n<p>Note: Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 will be automatically encrypted at no additional cost and with no impact on performance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</strong> - Server-side encryption with Amazon S3 managed keys (SSE-S3) is the easiest way to implement the given requirement, as there is no additional overhead of splitting data. Multiple S3 buckets are redundant for this requirement.</p>\n\n<p><strong>Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data</strong> - Server-side encryption is the encryption of data at its destination by the application or service that receives it. The requirement is about server-side encryption and not about client-side encryption, hence this choice is incorrect.</p>\n\n<p><strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket</strong> - An encryption context is a set of key-value pairs that contain additional contextual information about the data. When an encryption context is specified for an encryption operation, Amazon S3 must specify the same encryption context for the decryption operation. The encryption context offers another level of security for the encryption key. However, it is not useful for generating unique keys.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p>\n",
                "options": [
                    {
                        "id": 10404,
                        "content": "<p>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 10405,
                        "content": "<p>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</p>",
                        "isValid": true
                    },
                    {
                        "id": 10406,
                        "content": "<p>Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data</p>",
                        "isValid": false
                    },
                    {
                        "id": 10407,
                        "content": "<p>Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2490,
            "attributes": {
                "createdAt": "2023-09-09T20:34:10.085Z",
                "updatedAt": "2023-09-09T20:34:10.085Z",
                "content": "<p>An e-commerce application uses a relational database that runs several queries that perform joins on multiple tables. The development team has found that these queries are slow and expensive, therefore these are a good candidate for caching. The application needs to use a caching service that supports multi-threading.</p>\n\n<p>As a solutions architect, which of the following services would you recommend for the given use case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon ElastiCache for Memcached</strong> - Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store and cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.</p>\n\n<p>Memcached is an open-source, distributed, in-memory key-value store that can retrieve data in milliseconds. Caching site information with Memcached can help you improve the performance and scalability of your site while controlling cost.</p>\n\n<p>Choose Memcached if the following apply to you:</p>\n\n<p>You need the simplest model possible.</p>\n\n<p>You need to run large nodes with multiple cores or threads (support for multi-threading).</p>\n\n<p>You need the ability to scale out and in, adding and removing nodes as demand on your system increases and decreases.</p>\n\n<p>You need to cache objects.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q47-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon ElastiCache for Redis</strong> - Redis, which stands for Remote Dictionary Server, is a fast, open-source, in-memory key-value data store for use as a database, cache, message broker, and queue. Redis now delivers sub-millisecond response times enabling millions of requests per second for real-time applications in Gaming, Ad-Tech, Financial Services, Healthcare, and IoT. Redis is a popular choice for caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing, chat/messaging, media streaming, and pub/sub apps.</p>\n\n<p>Redis does not support multi-threading, so this option is not the right fit for the given use case.</p>\n\n<p><strong>Amazon DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB. DAX does not support relational databases.</p>\n\n<p><strong>AWS Global Accelerator</strong> - AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. This option has been added as a distractor, it has nothing to do with database caching.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/caching/aws-caching/\">https://aws.amazon.com/caching/aws-caching/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n",
                "options": [
                    {
                        "id": 10408,
                        "content": "<p>Amazon ElastiCache for Memcached</p>",
                        "isValid": true
                    },
                    {
                        "id": 10409,
                        "content": "<p>AWS Global Accelerator</p>",
                        "isValid": false
                    },
                    {
                        "id": 10410,
                        "content": "<p>Amazon ElastiCache for Redis</p>",
                        "isValid": false
                    },
                    {
                        "id": 10411,
                        "content": "<p>Amazon DynamoDB Accelerator (DAX)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2491,
            "attributes": {
                "createdAt": "2023-09-09T20:34:10.211Z",
                "updatedAt": "2023-09-09T20:34:10.211Z",
                "content": "<p>You are using AWS Lambda to implement a batch job for a big data analytics workflow. Based on historical trends, a similar job runs for 30 minutes on average. The Lambda function pulls data from Amazon S3, processes it, and then writes the results back to S3. When you deployed your AWS Lambda function, you noticed an issue where the Lambda function abruptly failed after 15 minutes of execution.</p>\n\n<p>As a solutions architect, which of the following would you identify as the root cause of the issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>With Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app. AWS Lambda functions can be configured to run up to 15 minutes per execution. You can set the timeout to any value between 1 second and 15 minutes.</p>\n\n<p><strong>The AWS Lambda function is timing out</strong> - AWS Lambda functions time out after 15 minutes, and are not usually meant for long-running jobs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The AWS Lambda function is running out of memory</strong> - Memory errors will not result in the abrupt termination of the function with no error message.</p>\n\n<p><strong>The AWS Lambda function chosen runtime is wrong</strong> - Lambda function execution will fail if there is an issue with runtime. So, this is not the issue in the current case.</p>\n\n<p><strong>The AWS Lambda function is missing IAM permissions</strong> - Without enough permissions, Lambda would not have been able to start its execution at all. So, permissions are not an issue here.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/lambda/faqs/\">https://aws.amazon.com/lambda/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 10412,
                        "content": "<p>The AWS Lambda function is timing out</p>",
                        "isValid": true
                    },
                    {
                        "id": 10413,
                        "content": "<p>The AWS Lambda function chosen runtime is wrong</p>",
                        "isValid": false
                    },
                    {
                        "id": 10414,
                        "content": "<p>The AWS Lambda function is running out of memory</p>",
                        "isValid": false
                    },
                    {
                        "id": 10415,
                        "content": "<p>The AWS Lambda function is missing IAM permissions</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2492,
            "attributes": {
                "createdAt": "2023-09-09T20:34:10.359Z",
                "updatedAt": "2023-09-09T20:34:10.359Z",
                "content": "<p>A photo-sharing company is storing user profile pictures in an S3 bucket and an image analysis application is deployed on four EC2 instances. A solutions architect would like to trigger an image analysis procedure only on one of the four EC2 instances for each photo uploaded.</p>\n\n<p>What do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an S3 Event Notification that sends a message to an SQS queue. Make the EC2 instances read from the SQS queue</strong></p>\n\n<p>The Amazon S3 event notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications.</p>\n\n<p>Amazon S3 supports the following destinations where it can publish events:</p>\n\n<p>Amazon Simple Notification Service (Amazon SNS) topic</p>\n\n<p>Amazon Simple Queue Service (Amazon SQS) queue</p>\n\n<p>AWS Lambda</p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Here we have to use S3 Event Notifications (which can send a message to either Lambda, SNS, or SQS) to send a message to the SQS queue. By using SQS, we know only one EC2 instance among the four will pick up a message and process it.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Subscribe the EC2 instances to the S3 Inventory stream</strong> - S3 Inventory is a distractor. If you're curious - Amazon S3 inventory helps you manage your storage by creating lists of the objects in an S3 bucket on a defined schedule.</p>\n\n<p><strong>Create a EventBridge event that reacts to objects uploads in S3 and invokes one of the EC2 instances</strong> - EventBridge events cannot invoke applications on EC2 instances, so we have to rule out that answer.</p>\n\n<p><strong>Create an S3 Event Notification that sends a message to an SNS topic. Subscribe the EC2 instances to the SNS topic</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.</p>\n\n<p>Using SNS would send a message to each EC2 instance via the SNS topic, therefore making all of them work for each upload. This is not the intended behavior.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n",
                "options": [
                    {
                        "id": 10416,
                        "content": "<p>Create a EventBridge event that reacts to objects uploads in S3 and invokes one of the EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10417,
                        "content": "<p>Subscribe the EC2 instances to the S3 Inventory stream</p>",
                        "isValid": false
                    },
                    {
                        "id": 10418,
                        "content": "<p>Create an S3 Event Notification that sends a message to an SNS topic. Subscribe the EC2 instances to the SNS topic</p>",
                        "isValid": false
                    },
                    {
                        "id": 10419,
                        "content": "<p>Create an S3 Event Notification that sends a message to an SQS queue. Make the EC2 instances read from the SQS queue</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2493,
            "attributes": {
                "createdAt": "2023-09-09T20:34:10.469Z",
                "updatedAt": "2023-09-09T20:34:10.469Z",
                "content": "<p>A startup uses a fleet of EC2 servers to manage its CRM application. These EC2 servers are behind an Elastic Load Balancer (ELB). Which of the following configurations are NOT allowed for the Elastic Load Balancer?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use the ELB to distribute traffic for four EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region</strong></p>\n\n<p>Elastic Load Balancer automatically distributes incoming traffic across multiple targets – Amazon EC2 instances, containers, IP addresses, and Lambda functions – in multiple Availability Zones and ensures only healthy targets receive traffic.\nELB cannot distribute incoming traffic for targets deployed in different regions. This configuration is NOT allowed for the Elastic Load Balancer and therefore this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the ELB to distribute traffic for four EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region</strong></p>\n\n<p><strong>Use the ELB to distribute traffic for four EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region</strong></p>\n\n<p><strong>Use the ELB to distribute traffic for four EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region</strong></p>\n\n<p>These three options are valid configurations for the ELB to distribute traffic (either within an Availability Zone or between two Availability Zones).</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticloadbalancing/\">https://aws.amazon.com/elasticloadbalancing/</a></p>\n",
                "options": [
                    {
                        "id": 10420,
                        "content": "<p>Use the ELB to distribute traffic for four EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region</p>",
                        "isValid": false
                    },
                    {
                        "id": 10421,
                        "content": "<p>Use the ELB to distribute traffic for four EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region</p>",
                        "isValid": false
                    },
                    {
                        "id": 10422,
                        "content": "<p>Use the ELB to distribute traffic for four EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region</p>",
                        "isValid": false
                    },
                    {
                        "id": 10423,
                        "content": "<p>Use the ELB to distribute traffic for four EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2494,
            "attributes": {
                "createdAt": "2023-09-09T20:34:10.567Z",
                "updatedAt": "2023-09-09T20:34:10.567Z",
                "content": "<p>A social media application lets users upload photos and perform image editing operations. The application offers two classes of service: pro and lite. The product team wants the photos submitted by pro users to be processed before those submitted by lite users. Photos are uploaded to S3 and the job information is sent to Amazon SQS.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create two SQS standard queues: one for pro and one for lite. Set up EC2 instances to prioritize polling for the pro queue over the lite queue</strong></p>\n\n<p>AWS recommends using separate queues to provide prioritization of work. Therefore, for the given use case, you need to create an SQS standard queue for processing pro users' photos and another SQS standard queue for processing lite users' photos. Then you can configure EC2 instances to prioritize polling for the pro queue over the lite queue.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q32-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create two SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling</strong></p>\n\n<p><strong>Create two SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling</strong></p>\n\n<p>Amazon SQS long polling is a way to retrieve messages from your Amazon SQS queues. While the regular short polling returns immediately, even if the message queue being polled is empty, long-polling doesn’t return a response until a message arrives in the message queue, or the long poll times out. Since long polling or short polling cannot impact the priority of processing for the two queues, so both these options are incorrect.</p>\n\n<p><strong>Create one SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up EC2 instances to prioritize visibility settings so pro photos are processed first</strong> - To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. Setting visibility timeout to zero can result in the same pro photo being processed by more than one consumer. This does not help in prioritizing the processing of pro photos over the lite photos.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/images/sqs-visibility-timeout-diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n",
                "options": [
                    {
                        "id": 10424,
                        "content": "<p>Create two SQS standard queues: one for pro and one for lite. Set up EC2 instances to prioritize polling for the pro queue over the lite queue</p>",
                        "isValid": true
                    },
                    {
                        "id": 10425,
                        "content": "<p>Create two SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling</p>",
                        "isValid": false
                    },
                    {
                        "id": 10426,
                        "content": "<p>Create two SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling</p>",
                        "isValid": false
                    },
                    {
                        "id": 10427,
                        "content": "<p>Create one SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up EC2 instances to prioritize visibility settings so pro photos are processed first</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2495,
            "attributes": {
                "createdAt": "2023-09-09T20:34:10.645Z",
                "updatedAt": "2023-09-09T20:34:10.645Z",
                "content": "<p>The engineering team at a startup is evaluating the most optimal block storage volume type for the EC2 instances hosting its flagship application. The storage volume should support very low latency but it does not need to persist the data when the instance terminates. As a solutions architect, you have proposed using Instance Store volumes to meet these requirements.</p>\n\n<p>Which of the following would you identify as the key characteristics of the Instance Store volumes? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>You can't detach an instance store volume from one instance and attach it to a different instance</strong> - You can specify instance store volumes for an instance only when you launch it. You can't detach an instance store volume from one instance and attach it to a different instance. The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists.</p>\n\n<p><strong>If you create an AMI from an instance, the data on its instance store volumes isn't preserved</strong> - If you create an AMI from an instance, the data on its instance store volumes isn't preserved and isn't present on the instance store volumes of the instances that you launch from the AMI.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation</strong> - When you stop, hibernate, or terminate an instance, every block of storage in the instance store is reset. Therefore, this option is incorrect.</p>\n\n<p><strong>You can specify instance store volumes for an instance when you launch or restart it</strong> - You can specify instance store volumes for an instance only when you launch it.</p>\n\n<p><strong>An instance store is a network storage type</strong> - An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p>\n",
                "options": [
                    {
                        "id": 10428,
                        "content": "<p>If you create an AMI from an instance, the data on its instance store volumes isn't preserved</p>",
                        "isValid": true
                    },
                    {
                        "id": 10429,
                        "content": "<p>You can specify instance store volumes for an instance when you launch or restart it</p>",
                        "isValid": false
                    },
                    {
                        "id": 10430,
                        "content": "<p>An instance store is a network storage type</p>",
                        "isValid": false
                    },
                    {
                        "id": 10431,
                        "content": "<p>Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation</p>",
                        "isValid": false
                    },
                    {
                        "id": 10432,
                        "content": "<p>You can't detach an instance store volume from one instance and attach it to a different instance</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2496,
            "attributes": {
                "createdAt": "2023-09-09T20:34:10.727Z",
                "updatedAt": "2023-09-09T20:34:10.727Z",
                "content": "<p>A healthcare company runs a fleet of EC2 instances in two private subnets (named PR1 and PR2) across two Availability Zones (named A1 and A2). The EC2 instances need access to the internet for OS patch management and third-party software maintenance. To facilitate this, the engineering team at the company wants to set up two NAT gateways in a highly available configuration.</p>\n\n<p>Which of the following options would you suggest?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2</strong></p>\n\n<p>A NAT gateway is a Network Address Translation (NAT) service. You can use a NAT gateway so that instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances.</p>\n\n<p>For the given use case, the EC2 instances in the private subnets can connect to the internet through public NAT gateways in their respective Availability Zones (AZ). You should create public NAT gateway in the public subnet of each AZ and must associate an elastic IP address with the NAT gateway at creation. Then, you can route traffic from the NAT gateway to the internet gateway for the VPC.</p>\n\n<p>If you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateway’s Availability Zone is down, resources in the other Availability Zones lose internet access. To create a highly available or an Availability Zone independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2</strong> - For the EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create NAT gateways in the private subnet for the given use case.</p>\n\n<p><strong>Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2</strong> - For the EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create both NAT gateways in a single public subnet, as this configuration would not be highly available.</p>\n\n<p><strong>Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2</strong> - For the EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create a single NAT gateway, as this configuration would not be highly available.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n",
                "options": [
                    {
                        "id": 10433,
                        "content": "<p>Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2</p>",
                        "isValid": true
                    },
                    {
                        "id": 10434,
                        "content": "<p>Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2</p>",
                        "isValid": false
                    },
                    {
                        "id": 10435,
                        "content": "<p>Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2</p>",
                        "isValid": false
                    },
                    {
                        "id": 10436,
                        "content": "<p>Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2497,
            "attributes": {
                "createdAt": "2023-09-09T20:34:10.809Z",
                "updatedAt": "2023-09-09T20:34:10.809Z",
                "content": "<p>A company manages a High Performance Computing (HPC) application that needs to be deployed on EC2 instances. The application requires high levels of inter-node communications and high network traffic between the instances.</p>\n\n<p>As a solutions architect, which of the following options would you recommend to the engineering team at the company? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Deploy EC2 instances with Elastic Fabric Adapter</strong></p>\n\n<p>Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communications at scale on AWS. Its custom-built operating system (OS) bypass hardware interface enhances the performance of inter-instance communications, which is critical to scaling these applications. Therefore this option is correct.</p>\n\n<p><strong>Deploy EC2 instances in a cluster placement group</strong></p>\n\n<p>Cluster placement groups pack instances close together inside an Availability Zone. They are recommended when the majority of the network traffic is between the instances in the group. These are also recommended for applications that benefit from low network latency, high network throughput, or both. Therefore this option is one of the correct answers.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy EC2 instances in a spread placement group</strong></p>\n\n<p>A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The instances are placed across distinct underlying hardware to reduce correlated failures. You can have a maximum of seven running instances per Availability Zone per group. Since the spread placement group can span across multiple Availability Zones in the same Region, it cannot support high levels of inter-node communications and high network traffic. So this option is incorrect.</p>\n\n<p><strong>Deploy EC2 instances in a partition placement group</strong></p>\n\n<p>A partition placement group spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. A partition placement group can have a maximum of seven partitions per Availability Zone. Since the partition placement group can have partitions in multiple Availability Zones in the same Region, it cannot support high levels of inter-node communications and high network traffic. So this option is incorrect.</p>\n\n<p><strong>Deploy EC2 instances behind a Network Load Balancer</strong></p>\n\n<p>A load balancer serves as the single point of contact for clients. The load balancer distributes incoming traffic across multiple targets, such as Amazon EC2 instances. A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. Network Load Balancer cannot facilitate high network traffic between instances. Network Load Balancer cannot support high levels of inter-node communication between EC2 instances. This option just serves as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/hpc/efa/\">https://aws.amazon.com/hpc/efa/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p>\n",
                "options": [
                    {
                        "id": 10437,
                        "content": "<p>Deploy EC2 instances behind a Network Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 10438,
                        "content": "<p>Deploy EC2 instances with Elastic Fabric Adapter</p>",
                        "isValid": true
                    },
                    {
                        "id": 10439,
                        "content": "<p>Deploy EC2 instances in a spread placement group</p>",
                        "isValid": false
                    },
                    {
                        "id": 10440,
                        "content": "<p>Deploy EC2 instances in a partition placement group</p>",
                        "isValid": false
                    },
                    {
                        "id": 10441,
                        "content": "<p>Deploy EC2 instances in a cluster placement group</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2498,
            "attributes": {
                "createdAt": "2023-09-09T20:34:10.912Z",
                "updatedAt": "2023-09-09T20:34:10.912Z",
                "content": "<p>A retail company's procurement application becomes slow when traffic spikes. The application has a three-tier architecture (web, application and database tier) that uses synchronous transactions. The engineering team at the company has identified certain bottlenecks in the application tier but it does not want to change the underlying application architecture.</p>\n\n<p>As a solutions architect, which of the following solutions would you suggest to meet the required application response times while accounting for any traffic spikes?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer</strong> - A horizontally scalable system is one that can increase capacity by adding more computers to the system. This is in contrast to a vertically scalable system, which is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory or storage.</p>\n\n<p>Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different computers.</p>\n\n<p>Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed.</p>\n\n<p>To use Elastic Load Balancing with your Auto Scaling group, you attach the load balancer to your Auto Scaling group to register the group with the load balancer. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group.</p>\n\n<p>When you use Elastic Load Balancing with your Auto Scaling group, it's not necessary to register individual EC2 instances with the load balancer. Instances that are launched by your Auto Scaling group are automatically registered with the load balancer. Likewise, instances that are terminated by your Auto Scaling group are automatically deregistered from the load balancer.</p>\n\n<p>This option will require fewer design changes, it's mostly configuration changes and the ability for the web/application tier to be able to communicate across instances. Hence, this is the right solution for the current use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage SQS with asynchronous AWS Lambda calls to decouple the application and data tiers</strong> - This is incorrect as it uses asynchronous AWS Lambda calls and the application uses synchronous transactions. The question says there should be no change in the application architecture.</p>\n\n<p><strong>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS</strong> - The issue is not with the persistence layer at all. This option has only been used as a distractor.</p>\n\n<p>You can deploy scalable Oracle Real Application Clusters (RAC) on Amazon EC2 using Amazon Machine Images (AMI) on AWS Marketplace. Oracle RAC is a shared-everything database cluster technology from Oracle that allows a single database (a set of data files) to be concurrently accessed and served by one or many database server instances.</p>\n\n<p><strong>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size</strong> - Vertical scaling is just a band-aid solution and will not work long term.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/\">https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/</a></p>\n",
                "options": [
                    {
                        "id": 10442,
                        "content": "<p>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10443,
                        "content": "<p>Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 10444,
                        "content": "<p>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size</p>",
                        "isValid": false
                    },
                    {
                        "id": 10445,
                        "content": "<p>Leverage SQS with asynchronous AWS Lambda calls to decouple the application and data tiers</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2499,
            "attributes": {
                "createdAt": "2023-09-09T20:34:11.001Z",
                "updatedAt": "2023-09-09T20:34:11.001Z",
                "content": "<p>A company needs an Active Directory service to run directory-aware workloads in the AWS Cloud and it should also support configuring a trust relationship with any existing on-premises Microsoft Active Directory.</p>\n\n<p>Which AWS Directory Service is the best fit for this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Managed Microsoft AD</strong> - AWS Directory Service lets you run Microsoft Active Directory (AD) as a managed service. AWS Directory Service for Microsoft Active Directory, also referred to as AWS Managed Microsoft AD, is powered by Windows Server 2012 R2. When you select and launch this directory type, it is created as a highly available pair of domain controllers connected to your virtual private cloud (VPC).</p>\n\n<p>With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud, including Microsoft SharePoint and custom .NET and SQL Server-based applications. You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO).</p>\n\n<p>AWS Managed Microsoft AD is your best choice if you need actual Active Directory features to support AWS applications or Windows workloads, including Amazon Relational Database Service for Microsoft SQL Server. It's also best if you want a standalone AD in the AWS Cloud that supports Office 365 or you need an LDAP directory to support your Linux applications.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AD Connector</strong> - AD Connector is a directory gateway with which you can redirect directory requests to your on-premises Microsoft Active Directory without caching any information in the cloud. AD Connector is your best choice when you want to use your existing on-premises directory with compatible AWS services.</p>\n\n<p><strong>Simple AD</strong> - Simple AD is a standalone directory in the cloud, where you create and manage user identities and manage access to applications. Simple AD provides a subset of the features offered by AWS Managed Microsoft AD. However, note that Simple AD does not support features such as multi-factor authentication (MFA), trust relationships with other domains, Active Directory Administrative Center, PowerShell support, Active Directory recycle bin, group managed service accounts, and schema extensions for POSIX and Microsoft applications.</p>\n\n<p><strong>AWS Transit Gateway</strong> - AWS Transit Gateway connects VPCs and on-premises networks through a central hub. Transit Gateway is not an Active Directory service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html</a></p>\n",
                "options": [
                    {
                        "id": 10446,
                        "content": "<p>AWS Transit Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 10447,
                        "content": "<p>AD Connector</p>",
                        "isValid": false
                    },
                    {
                        "id": 10448,
                        "content": "<p>Simple AD</p>",
                        "isValid": false
                    },
                    {
                        "id": 10449,
                        "content": "<p>AWS Managed Microsoft AD</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2500,
            "attributes": {
                "createdAt": "2023-09-09T20:34:11.112Z",
                "updatedAt": "2023-09-09T20:34:11.112Z",
                "content": "<p>The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Route 53 to accomplish this.</p>\n\n<p>Which of the following settings of the VPC need to be enabled? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>enableDnsHostnames</strong></p>\n\n<p><strong>enableDnsSupport</strong></p>\n\n<p>A private hosted zone is a container for records for a domain that you host in one or more Amazon virtual private clouds (VPCs). You create a hosted zone for a domain (such as example.com), and then you create records to tell Amazon Route 53 how you want traffic to be routed for that domain within and among your VPCs.</p>\n\n<p>For each VPC that you want to associate with the Route 53 hosted zone, change the following VPC settings to true:</p>\n\n<p>enableDnsHostnames</p>\n\n<p>enableDnsSupport</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>enableVpcSupport</strong></p>\n\n<p><strong>enableVpcHostnames</strong></p>\n\n<p><strong>enableDnsDomain</strong></p>\n\n<p>The options enableVpcSupport, enableVpcHostnames and enableDnsDomain have been added as distractors.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-creating.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-creating.html</a></p>\n",
                "options": [
                    {
                        "id": 10450,
                        "content": "<p>enableDnsDomain</p>",
                        "isValid": false
                    },
                    {
                        "id": 10451,
                        "content": "<p>enableDnsSupport</p>",
                        "isValid": true
                    },
                    {
                        "id": 10452,
                        "content": "<p>enableDnsHostnames</p>",
                        "isValid": true
                    },
                    {
                        "id": 10453,
                        "content": "<p>enableVpcSupport</p>",
                        "isValid": false
                    },
                    {
                        "id": 10454,
                        "content": "<p>enableVpcHostnames</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2501,
            "attributes": {
                "createdAt": "2023-09-09T20:34:11.205Z",
                "updatedAt": "2023-09-09T20:34:11.205Z",
                "content": "<p>A startup wants to create a highly available architecture for its multi-tier application. Currently, the startup manages a single EC2 instance along with a single RDS MySQL DB instance. The startup has hired you as an AWS Certified Solutions Architect Associate to build a solution that meets these requirements while minimizing the underlying infrastructure maintenance effort.</p>\n\n<p>What will you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an Auto-Scaling group with a desired capacity of a total of two EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these EC2 instances. Set up RDS MySQL DB in a multi-AZ configuration</strong></p>\n\n<p>Amazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q53-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n\n<p>Application Load Balancer automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q53-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p>\n\n<p>In a multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across Availability Zones to the standby to keep both in sync and protect your latest database updates against DB instance failure.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q53-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p>To create a highly available architecture for the given use case, you need to set up an Auto-Scaling group with a desired capacity of a total of two EC2 instances across two Availability Zones and then point the Application Load Balancer to the target group having the EC2 instances.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Auto-Scaling group with a desired capacity of a total of two EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these EC2 instances. Set up a read replica of the RDS MySQL DB in another Availability Zone</strong> - A read replica cannot be used to enhance the availability of an RDS MySQL DB. You must use the multi-AZ configuration of RDS MySQL for this use case.</p>\n\n<p><strong>Create an Auto-Scaling group with a desired capacity of a total of two EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these EC2 instances. Set up RDS MySQL DB in a multi-AZ configuration</strong> - Having the EC2 instances in a single Availability Zone will not create a highly available solution. In the case of an outage for the entire Availability Zone, the EC2 instances would be unreachable. Hence this option is incorrect.</p>\n\n<p><strong>Provision a second EC2 instance in another Availability Zone. Provision a second RDS MySQL DB in another Availabililty Zone. Leverage Route 53 for equal distribution of incoming traffic to the EC2 instances. Use a custom script to sync data across the two MySQL DBs</strong> - This option has been added as a distractor. It requires significant monitoring and development effort to keep the EC2 instances highly available as well as keep the MySQL DBs in sync.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n",
                "options": [
                    {
                        "id": 10455,
                        "content": "<p>Create an Auto-Scaling group with a desired capacity of a total of two EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these EC2 instances. Set up RDS MySQL DB in a multi-AZ configuration</p>",
                        "isValid": true
                    },
                    {
                        "id": 10456,
                        "content": "<p>Provision a second EC2 instance in another Availability Zone. Provision a second RDS MySQL DB in another Availabililty Zone. Leverage Route 53 for equal distribution of incoming traffic to the EC2 instances. Use a custom script to sync data across the two MySQL DBs</p>",
                        "isValid": false
                    },
                    {
                        "id": 10457,
                        "content": "<p>Create an Auto-Scaling group with a desired capacity of a total of two EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these EC2 instances. Set up a read replica of the RDS MySQL DB in another Availability Zone</p>",
                        "isValid": false
                    },
                    {
                        "id": 10458,
                        "content": "<p>Create an Auto-Scaling group with a desired capacity of a total of two EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these EC2 instances. Set up RDS MySQL DB in a multi-AZ configuration</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2502,
            "attributes": {
                "createdAt": "2023-09-09T20:34:11.291Z",
                "updatedAt": "2023-09-09T20:34:11.291Z",
                "content": "<p>The DevOps team at a major financial services company uses Multi-Availability Zone (Multi-AZ) deployment for its MySQL RDS database in order to automate its database replication and augment data durability. The DevOps team has scheduled a maintenance window for a database engine level upgrade for the coming weekend.</p>\n\n<p>Which of the following is the correct outcome during the maintenance window?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. This causes downtime until the upgrade is complete</strong></p>\n\n<p>Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups.</p>\n\n<p>Upgrades to the database engine level require downtime. Even if your RDS DB instance uses a Multi-AZ deployment, both the primary and standby DB instances are upgraded at the same time. This causes downtime until the upgrade is complete, and the duration of the downtime varies based on the size of your DB instance.</p>\n\n<p>RDS DB Engine Maintenance:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q8-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete</strong> - For RDS database engine level upgrade, primary and standby DB instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect.</p>\n\n<p><strong>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers the standby DB instance to be upgraded which is then followed by the upgrade of the primary DB instance. This does not cause any downtime for the duration of the upgrade</strong> - For RDS database engine level upgrade, primary and standby DB instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect.</p>\n\n<p><strong>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers the primary DB instance to be upgraded which is then followed by the upgrade of the standby DB instance. This does not cause any downtime for the duration of the upgrade</strong> - For RDS database engine level upgrade, primary and standby DB instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/</a></p>\n",
                "options": [
                    {
                        "id": 10459,
                        "content": "<p>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers the primary DB instance to be upgraded which is then followed by the upgrade of the standby DB instance. This does not cause any downtime for the duration of the upgrade</p>",
                        "isValid": false
                    },
                    {
                        "id": 10460,
                        "content": "<p>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. This causes downtime until the upgrade is complete</p>",
                        "isValid": true
                    },
                    {
                        "id": 10461,
                        "content": "<p>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers the standby DB instance to be upgraded which is then followed by the upgrade of the primary DB instance. This does not cause any downtime for the duration of the upgrade</p>",
                        "isValid": false
                    },
                    {
                        "id": 10462,
                        "content": "<p>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2503,
            "attributes": {
                "createdAt": "2023-09-09T20:34:11.376Z",
                "updatedAt": "2023-09-09T20:34:11.376Z",
                "content": "<p>The engineering team at a company wants to create a daily big data analysis job leveraging Spark for analyzing online/offline sales and customer loyalty data to create customized reports on a client-by-client basis. The big data analysis job needs to read the data from Amazon S3 and output it back to S3.</p>\n\n<p>Which technology do you recommend to run the Big Data analysis job? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EMR</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. EMR is used for launching Hadoop / Spark clusters. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.</p>\n\n<p><strong>AWS Glue</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. AWS Glue ETL jobs can use Amazon S3, data stores in a VPC, or on-premises JDBC data stores as a source. AWS Glue jobs extract data, transform it, and load the resulting data back to S3, data stores in a VPC, or on-premises JDBC data stores as a target.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications.</p>\n\n<p><strong>Amazon Athena</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p><strong>AWS Batch</strong> AWS Batch can be used to plan, schedule, and execute your batch computing workloads on Amazon EC2 Instances. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements of the batch jobs submitted.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/emr/\">https://aws.amazon.com/emr/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/\">https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/</a></p>\n",
                "options": [
                    {
                        "id": 10463,
                        "content": "<p>AWS Glue</p>",
                        "isValid": true
                    },
                    {
                        "id": 10464,
                        "content": "<p>Amazon Athena</p>",
                        "isValid": false
                    },
                    {
                        "id": 10465,
                        "content": "<p>Amazon EMR</p>",
                        "isValid": true
                    },
                    {
                        "id": 10466,
                        "content": "<p>Amazon Redshift</p>",
                        "isValid": false
                    },
                    {
                        "id": 10467,
                        "content": "<p>AWS Batch</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2504,
            "attributes": {
                "createdAt": "2023-09-09T20:34:11.467Z",
                "updatedAt": "2023-09-09T20:34:11.467Z",
                "content": "<p>Your application is deployed on EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second.</p>\n\n<p>How can you efficiently prevent attackers from overwhelming your application?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use a Web Application Firewall and setup a rate-based rule</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.</p>\n\n<p>The correct answer is to use WAF (which has integration on top of your ALB) and define a rate-based rule.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Sticky Sessions on the Application Load Balancer</strong> - Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.</p>\n\n<p>Sticky Sessions on your ALB is a distractor here. Sticky sessions are a mechanism to route requests from the same client to the same target. Application Load Balancer supports sticky sessions using load balancer generated cookies. If you enable sticky sessions, the same target receives the request and can use the cookie to recover the session context.</p>\n\n<p><strong>Define a Network ACL (NACL) on your Application Load Balancer</strong> - An NACL does not work, as this only helps to block specific IPs. On top of things, NACLs are defined at the subnet level, not Application Load Balancers.</p>\n\n<p><strong>Use AWS Shield Advanced and setup a rate-based rule</strong> - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. There are two tiers of AWS Shield - Standard and Advanced.</p>\n\n<p>AWS Shield Advanced provides enhanced resource-specific detection and employs advanced mitigation and routing techniques for sophisticated or larger attacks.</p>\n\n<p>AWS Shield Advanced will give you DDoS protection overall, and you cannot set up rate-based rules in Shield.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/shield/\">https://aws.amazon.com/shield/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions</a></p>\n",
                "options": [
                    {
                        "id": 10468,
                        "content": "<p>Use a Web Application Firewall and setup a rate-based rule</p>",
                        "isValid": true
                    },
                    {
                        "id": 10469,
                        "content": "<p>Define a Network ACL (NACL) on your Application Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 10470,
                        "content": "<p>Use AWS Shield Advanced and setup a rate-based rule</p>",
                        "isValid": false
                    },
                    {
                        "id": 10471,
                        "content": "<p>Configure Sticky Sessions on the Application Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2505,
            "attributes": {
                "createdAt": "2023-09-09T20:34:11.569Z",
                "updatedAt": "2023-09-09T20:34:11.569Z",
                "content": "<p>A retail company needs a secure connection between its on-premises data center and AWS Cloud. This connection does not need high bandwidth and will handle a small amount of traffic. The company wants a quick turnaround time to set up the connection.</p>\n\n<p>What is the MOST cost-effective way to establish such a connection?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up an AWS Site-to-Site VPN connection</strong> - By default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network. You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection. A VPN connection refers to the connection between your VPC and your own on-premises network.</p>\n\n<p>A Site-to-Site VPN connection offers two VPN tunnels between a virtual private gateway or a transit gateway on the AWS side, and a customer gateway (which represents a VPN device) on the remote (on-premises) side.</p>\n\n<p>A virtual private gateway is the VPN concentrator on the Amazon side of the Site-to-Site VPN connection. You create a virtual private gateway and attach it to the VPC from which you want to create the Site-to-Site VPN connection.</p>\n\n<p>How virtual private gateway works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q25-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html</a></p>\n\n<p>A transit gateway is a transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. For more information, see Amazon VPC Transit Gateways. You can create a Site-to-Site VPN connection as an attachment on a transit gateway.</p>\n\n<p>How transit gateway works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q25-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a bastion host on Amazon EC2</strong> - A bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet. The bastion host runs on an Amazon EC2 instance that is typically in a public subnet of your Amazon VPC. Other EC2 instances can be in a subnet that is not publicly accessible, and they are set up with a security group that allows SSH access from the security group attached to the underlying EC2 instance running the bastion host. A bastion host cannot be used to set up a connection between its on-premises data center and AWS Cloud.</p>\n\n<p><strong>Set up AWS Direct Connect</strong> - AWS Direct Connect is a network service that provides an alternative to using the Internet to utilize AWS cloud services. AWS Direct Connect enables customers to have low latency, secure and private connections to AWS for workloads that require higher speed or lower latency than the internet. A Dedicated Connection is made through a 1 Gbps, 10 Gbps, or 100 Gbps Ethernet port dedicated to a single customer. AWS Direct Connect takes about a month to provision the connection, so this option is ruled out for the given use case.</p>\n\n<p><strong>Set up an Internet Gateway between the on-premises data center and AWS cloud</strong> - An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An Internet Gateway cannot be used to set up a connection between its on-premises data center and AWS Cloud.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html</a></p>\n",
                "options": [
                    {
                        "id": 10472,
                        "content": "<p>Set up an Internet Gateway between the on-premises data center and AWS cloud</p>",
                        "isValid": false
                    },
                    {
                        "id": 10473,
                        "content": "<p>Set up an AWS Site-to-Site VPN connection</p>",
                        "isValid": true
                    },
                    {
                        "id": 10474,
                        "content": "<p>Set up AWS Direct Connect</p>",
                        "isValid": false
                    },
                    {
                        "id": 10475,
                        "content": "<p>Set up a bastion host on Amazon EC2</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2506,
            "attributes": {
                "createdAt": "2023-09-09T20:34:11.665Z",
                "updatedAt": "2023-09-09T20:34:11.665Z",
                "content": "<p>The systems administrator at a company wants to set up a highly available architecture for a bastion host solution.</p>\n\n<p>As a solutions architect, which of the following options would you recommend as the solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a public Network Load Balancer that links to EC2 instances that are bastion hosts managed by an ASG</strong></p>\n\n<p>Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.</p>\n\n<p>Including bastion hosts in your VPC environment enables you to securely connect to your Linux instances without exposing your environment to the Internet. After you set up your bastion hosts, you can access the other instances in your VPC through Secure Shell (SSH) connections on Linux. Bastion hosts are also configured with security groups to provide fine-grained ingress control.</p>\n\n<p>You need to remember that Bastion Hosts are using the SSH protocol, which is a TCP based protocol on port 22. They must be publicly accessible.</p>\n\n<p>Here, the correct answer is to use a Network Load Balancer, which supports TCP traffic, and will automatically allow you to connect to the EC2 instance in the backend.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Elastic IP and assign it to all EC2 instances that are bastion hosts managed by an ASG</strong> - An Elastic IP can only be attached to one EC2 instance at a time, so it won't provide you a highly available setup on its own. Note that if we had two Elastic IPs and two Bastion Hosts, this would work.</p>\n\n<p><strong>Create a VPC Endpoint for a fleet of EC2 instances that are bastion hosts managed by an ASG</strong> - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p>\n\n<p>VPC Endpoints are not used on top of EC2 instances. They're a way to access AWS services privately within your VPC (without using the public internet). This is a distractor.</p>\n\n<p><strong>Create a public Application Load Balancer that links to EC2 instances that are bastion hosts managed by an ASG</strong> - Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.</p>\n\n<p>An ALB only supports HTTP traffic, which is layer 7, while the SSH protocol is based on TCP and is layer 4. So, the Application Load Balancer doesn't work.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html\">https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a></p>\n",
                "options": [
                    {
                        "id": 10476,
                        "content": "<p>Create a VPC Endpoint for a fleet of EC2 instances that are bastion hosts managed by an ASG</p>",
                        "isValid": false
                    },
                    {
                        "id": 10477,
                        "content": "<p>Create an Elastic IP and assign it to all EC2 instances that are bastion hosts managed by an ASG</p>",
                        "isValid": false
                    },
                    {
                        "id": 10478,
                        "content": "<p>Create a public Application Load Balancer that links to EC2 instances that are bastion hosts managed by an ASG</p>",
                        "isValid": false
                    },
                    {
                        "id": 10479,
                        "content": "<p>Create a public Network Load Balancer that links to EC2 instances that are bastion hosts managed by an ASG</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2507,
            "attributes": {
                "createdAt": "2023-09-09T20:34:11.772Z",
                "updatedAt": "2023-09-09T20:34:11.772Z",
                "content": "<p>A company uses a legacy on-premises reporting application that operates on gigabytes of .json files and represents years of data. The legacy application cannot handle the growing size of .json files. New .json files are added daily from various data sources to a central on-premises storage location. The company wants to continue to support the legacy application. The company has hired you as a solutions architect to build a solution that can manage ongoing data updates from your on-premises application to Amazon S3.</p>\n\n<p>Which of the following solutions would you suggest to address the given requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up an on-premises file gateway. Configure data sources to write the .json files to the file gateway. Point the legacy analytics application to the file gateway. The file gateway should replicate the .json files to Amazon S3</strong></p>\n\n<p>A file gateway provides a simple solution for presenting one or more Amazon S3 buckets and their objects as a mountable NFS or SMB file share to one or more clients on-premises.</p>\n\n<p>The file gateway is deployed as a virtual machine in VMware ESXi or Microsoft Hyper-V environments on-premises, or in an Amazon Elastic Compute Cloud (Amazon EC2) instance in AWS. File gateway can also be deployed in data center and remote office locations on a Storage Gateway hardware appliance. When deployed, file gateway provides a seamless connection between on-premises NFS (v3.0 or v4.1) or SMB (v1 or v2) clients—typically applications—and Amazon S3 buckets hosted in a given AWS Region. The file gateway employs a local read/write cache to provide low-latency access to data for file share clients in the same local area network (LAN) as the file gateway.</p>\n\n<p>A bucket share consists of a file share hosted from a file gateway across a single Amazon S3 bucket. The file gateway virtual machine appliance currently supports up to 10 bucket shares.</p>\n\n<p>File Gateway Architecture:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html\">https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an on-premises volume gateway. Configure data sources to write the .json files to the volume gateway. Point the legacy analytics application to the volume gateway. The volume gateway should replicate data to Amazon S3</strong> - The Volume Gateway provides block storage to your on-premises applications using iSCSI connectivity. Data on the volumes is stored in Amazon S3 and you can take point in time copies of volumes that are stored in AWS as Amazon EBS snapshots. Volume Gateway is for block storage and not for file storage, so it is not the right option.</p>\n\n<p><strong>Set up AWS DataSync on-premises. Configure DataSync to continuously replicate the .json files between the company's on-premises storage and the company's S3 bucket</strong></p>\n\n<p><strong>Set up AWS DataSync on-premises. Configure DataSync to continuously replicate the .json files between on-premises and Amazon Elastic File System (Amazon EFS). Enable replication from Amazon EFS to the company's S3 bucket</strong></p>\n\n<p>AWS recommends that you should use AWS DataSync to migrate existing data to Amazon S3, and subsequently use the File Gateway configuration of AWS Storage Gateway to retain access to the migrated data and for ongoing updates from your on-premises file-based applications. Therefore, both these options are incorrect, as they use DataSync for ongoing replication.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html\">https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html</a></p>\n",
                "options": [
                    {
                        "id": 10480,
                        "content": "<p>Set up AWS DataSync on-premises. Configure DataSync to continuously replicate the .json files between the company's on-premises storage and the company's S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 10481,
                        "content": "<p>Set up AWS DataSync on-premises. Configure DataSync to continuously replicate the .json files between on-premises and Amazon Elastic File System (Amazon EFS). Enable replication from Amazon EFS to the company's S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 10482,
                        "content": "<p>Set up an on-premises file gateway. Configure data sources to write the .json files to the file gateway. Point the legacy analytics application to the file gateway. The file gateway should replicate the .json files to Amazon S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 10483,
                        "content": "<p>Set up an on-premises volume gateway. Configure data sources to write the .json files to the volume gateway. Point the legacy analytics application to the volume gateway. The volume gateway should replicate data to Amazon S3</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2508,
            "attributes": {
                "createdAt": "2023-09-09T20:34:11.868Z",
                "updatedAt": "2023-09-09T20:34:11.868Z",
                "content": "<p>The engineering team at a social media company has noticed that while some of the images stored in S3 are frequently accessed, others sit idle for a considerable span of time.</p>\n\n<p>As a solutions architect, what is your recommendation to build the MOST cost-effective solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Store the images using the S3 Intelligent-Tiering storage class</strong></p>\n\n<p>The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access.</p>\n\n<p>For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. Therefore using the S3 Intelligent-Tiering storage class is the correct solution for the given problem statement.</p>\n\n<p>S3 Storage Classes Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q9-i1.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the images using the S3 Standard-IA storage class</strong></p>\n\n<p>S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days. As some of the objects are frequently accessed, the per GB retrieval fee for S3 Standard-IA can cause the costs to shoot up, hence this option is incorrect.</p>\n\n<p><strong>Create a data monitoring application on an EC2 instance in the same region as the bucket storing the images. The application is triggered daily via CloudWatch and it changes the storage class of infrequently accessed objects to S3 One Zone-IA and the frequently accessed objects are migrated to S3 Standard class</strong></p>\n\n<p><strong>Create a data monitoring application on an EC2 instance in the same region as the bucket storing the images. The application is triggered daily via CloudWatch and it changes the storage class of infrequently accessed objects to S3 Standard-IA and the frequently accessed objects are migrated to S3 Standard class</strong></p>\n\n<p>Creating a data monitoring application on an EC2 instance for managing the desired S3 storage class entails significant development cost as well as infrastructure maintenance effort. The S3 Intelligent-Tiering storage class does the job in a cost-effective way. Therefore both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n",
                "options": [
                    {
                        "id": 10484,
                        "content": "<p>Create a data monitoring application on an EC2 instance in the same region as the bucket storing the images. The application is triggered daily via CloudWatch and it changes the storage class of infrequently accessed objects to S3 One Zone-IA and the frequently accessed objects are migrated to S3 Standard class</p>",
                        "isValid": false
                    },
                    {
                        "id": 10485,
                        "content": "<p>Create a data monitoring application on an EC2 instance in the same region as the bucket storing the images. The application is triggered daily via CloudWatch and it changes the storage class of infrequently accessed objects to S3 Standard-IA and the frequently accessed objects are migrated to S3 Standard class</p>",
                        "isValid": false
                    },
                    {
                        "id": 10486,
                        "content": "<p>Store the images using the S3 Intelligent-Tiering storage class</p>",
                        "isValid": true
                    },
                    {
                        "id": 10487,
                        "content": "<p>Store the images using the S3 Standard-IA storage class</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2509,
            "attributes": {
                "createdAt": "2023-09-09T20:34:11.970Z",
                "updatedAt": "2023-09-09T20:34:11.970Z",
                "content": "<p>Your company has created a data warehouse using Redshift that is used to analyze data from Amazon S3. From the usage pattern, you have detected that after 30 days, the data is rarely queried in Redshift and it's not \"hot data\" anymore. You would like to preserve the SQL querying capability on your data and get the queries started immediately. Also, you want to adopt a pricing model that allows you to save the maximum amount of cost on Redshift.</p>\n\n<p>What do you recommend? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Move the data to S3 Standard IA after 30 days</strong> - S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.</p>\n\n<p><strong>Analyze the cold data with Athena</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>Moving the data to S3 glacier will prevent us from being able to query it.  Therefore, we should migrate the data to S3 Standard IA and use Athena to analyze the cold data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the Redshift underlying storage to S3 IA</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications.</p>\n\n<p>Redshift's internal storage does not have \"tiers\" of storage classes like Amazon S3, so this option is also ruled out.</p>\n\n<p><strong>Create a smaller Redshift Cluster with the cold data</strong> - Creating a smaller cluster with the cold data would not decrease the storage cost of Redshift, which will only increase with time as we keep on creating data. Therefore this option is ruled out.</p>\n\n<p><strong>Move the data to S3 Glacier after 30 days</strong> - Amazon S3 Glacier and S3 Glacier Deep Archive are secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html</a></p>\n",
                "options": [
                    {
                        "id": 10488,
                        "content": "<p>Analyze the cold data with Athena</p>",
                        "isValid": true
                    },
                    {
                        "id": 10489,
                        "content": "<p>Move the data to S3 Glacier after 30 days</p>",
                        "isValid": false
                    },
                    {
                        "id": 10490,
                        "content": "<p>Move the data to S3 Standard IA after 30 days</p>",
                        "isValid": true
                    },
                    {
                        "id": 10491,
                        "content": "<p>Create a smaller Redshift Cluster with the cold data</p>",
                        "isValid": false
                    },
                    {
                        "id": 10492,
                        "content": "<p>Migrate the Redshift underlying storage to S3 IA</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2510,
            "attributes": {
                "createdAt": "2023-09-09T20:34:12.115Z",
                "updatedAt": "2023-09-09T20:34:12.115Z",
                "content": "<p>A company helps its customers legally sign highly confidential contracts. To meet the strong industry requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary algorithm. The company is now migrating to AWS Cloud using AWS S3 and would like you, the solution architect, to advise them on the encryption scheme to adopt.</p>\n\n<p>What do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Client Side Encryption</strong></p>\n\n<p>Client-side encryption is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options:</p>\n\n<ol>\n<li><p>Use a customer master key (CMK) stored in AWS Key Management Service (AWS KMS).</p></li>\n<li><p>Use a master key you store within your application.</p></li>\n</ol>\n\n<p>Because the company has its proprietary encryption algorithm, you have to leverage client-side encryption.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-KMS</strong> - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom.</p>\n\n<p><strong>SSE-S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key.</p>\n\n<p><strong>SSE-C</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n",
                "options": [
                    {
                        "id": 10493,
                        "content": "<p>SSE-S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 10494,
                        "content": "<p>SSE-C</p>",
                        "isValid": false
                    },
                    {
                        "id": 10495,
                        "content": "<p>SSE-KMS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10496,
                        "content": "<p>Client Side Encryption</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2511,
            "attributes": {
                "createdAt": "2023-09-09T20:34:12.236Z",
                "updatedAt": "2023-09-09T20:34:12.236Z",
                "content": "<p>You are deploying a critical monolith application that must be deployed on a single web server, as it hasn't been created to work in distributed mode. Still, you want to make sure your setup can automatically recover from the failure of an AZ.</p>\n\n<p>Which of the following options should be combined to form the MOST cost-efficient solution? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Create an auto-scaling group that spans across 2 AZ, which min=1, max=1, desired=1</strong></p>\n\n<p>Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size.</p>\n\n<p>So we have an ASG with desired=1, across two AZ, so that if an instance goes down, it is automatically recreated in another AZ. So this option is correct.</p>\n\n<p><strong>Create an Elastic IP and use the EC2 user-data script to attach it</strong></p>\n\n<p>Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.</p>\n\n<p>An Elastic IP address is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.</p>\n\n<p>Now, between the ALB and the Elastic IP. If we use an ALB, things will still work, but we will have to pay for the provisioned ALB which sends traffic to only one EC2 instance. Instead, to minimize costs, we must use an Elastic IP.</p>\n\n<p><strong>Assign an EC2 Instance Role to perform the necessary API calls</strong></p>\n\n<p>For that Elastic IP to be attached to our EC2 instance, we must use an EC2 user data script, and our EC2 instance must have the correct IAM permissions to perform the API call, so we need an EC2 instance role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Spot Fleet request</strong> - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price.</p>\n\n<p>The Spot Fleet selects the Spot Instance pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated.</p>\n\n<p>Spot Fleets requests would not fit our purpose as we are looking at a critical application. Spot instances can be terminated. So this option is incorrect.</p>\n\n<p><strong>Create an auto-scaling group that spans across 2 AZ, which min=1, max=2, desired=2</strong> - An ASG with desired=2 would create two instances, and this won't work for us as our monolith application is not made to work with two instances as per the given use-case.</p>\n\n<p><strong>Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group</strong> - If we use an ALB, things will still work, but we will have to pay for the provisioned ALB which sends traffic to only one EC2 instance. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html</a></p>\n",
                "options": [
                    {
                        "id": 10497,
                        "content": "<p>Create an Elastic IP and use the EC2 user-data script to attach it</p>",
                        "isValid": true
                    },
                    {
                        "id": 10498,
                        "content": "<p>Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group</p>",
                        "isValid": false
                    },
                    {
                        "id": 10499,
                        "content": "<p>Create an auto-scaling group that spans across 2 AZ, which min=1, max=2, desired=2</p>",
                        "isValid": false
                    },
                    {
                        "id": 10500,
                        "content": "<p>Assign an EC2 Instance Role to perform the necessary API calls</p>",
                        "isValid": true
                    },
                    {
                        "id": 10501,
                        "content": "<p>Create a Spot Fleet request</p>",
                        "isValid": false
                    },
                    {
                        "id": 10502,
                        "content": "<p>Create an auto-scaling group that spans across 2 AZ, which min=1, max=1, desired=1</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2512,
            "attributes": {
                "createdAt": "2023-09-09T20:34:12.325Z",
                "updatedAt": "2023-09-09T20:34:12.325Z",
                "content": "<p>The development team at a company manages a flexible nightly process which runs for 1 hour using Python. The process currently runs on the on-premises infrastructure and it needs to be migrated to AWS.</p>\n\n<p>Which of the following options do you recommend as the MOST cost-effective solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Run on a Spot Instance with Spot Block</strong></p>\n\n<p>A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price.</p>\n\n<p>Spot Instances with a defined duration (also known as Spot blocks) are designed not to be interrupted and will run continuously for the duration you select. This makes them ideal for jobs that take a finite time to complete, such as batch processing, encoding and rendering, modeling and analysis, and continuous integration.</p>\n\n<p>Running our load on a Spot Instance with Spot Block sounds like the perfect use case, as we can block the spot instance for 1 hour, run the script there, and then the instance will be terminated.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Run on an Application Load Balancer</strong> - Application Load Balancer operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.</p>\n\n<p>Application Load Balancer helps distribute load for HTTP(S) requests. This option has been added as a distractor.</p>\n\n<p><strong>Run on EMR</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.</p>\n\n<p>EMR is to run Big Data load that is meant to be run on Hadoop, this is also a distractor.</p>\n\n<p><strong>Run on Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>Lambda would be the perfect fit if our script could run in less than 15 minutes, as this is the maximum timeout for Lambda.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-ec2-spot-blocks-for-defined-duration-workloads/\">https://aws.amazon.com/blogs/aws/new-ec2-spot-blocks-for-defined-duration-workloads/</a></p>\n",
                "options": [
                    {
                        "id": 10503,
                        "content": "<p>Run on Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 10504,
                        "content": "<p>Run on EMR</p>",
                        "isValid": false
                    },
                    {
                        "id": 10505,
                        "content": "<p>Run on an Application Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 10506,
                        "content": "<p>Run on a Spot Instance with Spot Block</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2513,
            "attributes": {
                "createdAt": "2023-09-09T20:34:12.460Z",
                "updatedAt": "2023-09-09T20:34:12.460Z",
                "content": "<p>The engineering team at a multi-national company uses AWS Firewall Manager to centrally configure and manage firewall rules across its accounts and applications using AWS Organizations.</p>\n\n<p>Which of the following AWS resources can the AWS Firewall Manager configure rules on? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>AWS WAF</strong></p>\n\n<p><strong>AWS Shield Advanced</strong></p>\n\n<p><strong>VPC Security Groups</strong></p>\n\n<p>AWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure.</p>\n\n<p>Using AWS Firewall Manager, you can centrally configure AWS WAF rules, AWS Shield Advanced protection, Amazon Virtual Private Cloud (VPC) security groups, AWS Network Firewalls, and Amazon Route 53 Resolver DNS Firewall rules across accounts and resources in your organization. It does not support Network ACLs as of today.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q4-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/firewall-manager/faqs/\">https://aws.amazon.com/firewall-manager/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon GuardDuty</strong> - Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs.</p>\n\n<p>How GaurdDuty Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png\"></p>\n\n<p><strong>Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances.</p>\n\n<p><strong>Network Access Control Lists (NACLs)</strong> - A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.</p>\n\n<p>These three options are not in the list of AWS resources supported by AWS Firewall Manager, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/firewall-manager/faqs/\">https://aws.amazon.com/firewall-manager/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n\n<p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p>\n",
                "options": [
                    {
                        "id": 10507,
                        "content": "<p>AWS Shield Advanced</p>",
                        "isValid": true
                    },
                    {
                        "id": 10508,
                        "content": "<p>Amazon Inspector</p>",
                        "isValid": false
                    },
                    {
                        "id": 10509,
                        "content": "<p>VPC Security Groups</p>",
                        "isValid": true
                    },
                    {
                        "id": 10510,
                        "content": "<p>Network Access Control Lists (NACLs)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10511,
                        "content": "<p>Amazon GuardDuty</p>",
                        "isValid": false
                    },
                    {
                        "id": 10512,
                        "content": "<p>AWS WAF</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2514,
            "attributes": {
                "createdAt": "2023-09-09T20:34:12.593Z",
                "updatedAt": "2023-09-09T20:34:12.593Z",
                "content": "<p>A Big Data analytics company is using a fleet of Amazon EC2 instances to ingest Internet-of-Things (IoT) data from various data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is restarted, the in-flight data is lost. The analytics team at the company wants to store as well as query the ingested data in near-real-time.</p>\n\n<p>Which of the following solutions provides near-real-time data querying that is scalable with minimal data loss?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Capture data in Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data</strong> - Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Datadog, New Relic, MongoDB, and Splunk.</p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to capture, transform, and load streaming data into Redshift for near real-time analytics. It is also an auto-scaling solution as there is no need to provision any shards like Kinesis Data Streams.</p>\n\n<p>Redshift allows you to run complex analytic queries against petabytes of structured data, using sophisticated query optimization, columnar storage on high-performance local disks, and massively parallel query execution. Most results come back in seconds.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Capture data in an EC2 instance store and then publish this data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data</strong> - Instance store is a temporary storage available on Amazon EC2 instances. The in-flight data (that is, data arriving from the source) being processed by a specific EC2 instance will be lost in case that instance is restarted. Hence, this cannot be the option for the given use case.</p>\n\n<p><strong>Capture data in an EBS volume and then publish this data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data</strong> - EBS volumes cannot be used to store high volume data. EBS can be used to store cache data if a database is hosted on an EC2 instance. However, EBS cannot be used in place of a database. ElastiCache is a caching service. It is not relevant to the given use case.</p>\n\n<p><strong>Capture data in Amazon Kinesis Data Streams. Use Kinesis Data Analytics to query and analyze this streaming data in real-time</strong> - For Kinesis Data Streams, you have to manually allocate the shards for scaling the data ingestion process. Kinesis Data Streams (KDS) and Kinesis Data Analytics are for real-time processing of data and cannot provide long-term storage of data unlike a database or a data warehouse. So, this option is not right for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/redshift/features/\">https://aws.amazon.com/redshift/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 10513,
                        "content": "<p>Capture data in an EBS volume and then publish this data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data</p>",
                        "isValid": false
                    },
                    {
                        "id": 10514,
                        "content": "<p>Capture data in Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data</p>",
                        "isValid": true
                    },
                    {
                        "id": 10515,
                        "content": "<p>Capture data in Amazon Kinesis Data Streams. Use Kinesis Data Analytics to query and analyze this streaming data in real-time</p>",
                        "isValid": false
                    },
                    {
                        "id": 10516,
                        "content": "<p>Capture data in an EC2 instance store and then publish this data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2515,
            "attributes": {
                "createdAt": "2023-09-09T20:39:44.188Z",
                "updatedAt": "2023-09-09T20:39:44.188Z",
                "content": "<p>A company's application is running on Amazon EC2 instances in a single Region. In the event of a disaster, a solutions architect needs to ensure that the resources can also be deployed to a second Region.</p><p>Which combination of actions should the solutions architect take to accomplish this? (Select TWO.)</p>",
                "answerExplanation": "<p>You can copy an Amazon Machine Image (AMI) within or across AWS Regions using the AWS Management Console, the AWS Command Line Interface or SDKs, or the Amazon EC2 API, all of which support the CopyImage action.</p><p>Using the copied AMI the solutions architect would then be able to launch an instance from the same EBS volume in the second Region.</p><p><strong>Note: </strong>the AMIs are stored on Amazon S3, however you cannot view them in the S3 management console or work with them programmatically using the S3 API.</p><p><strong>CORRECT: </strong>\"Copy an Amazon Machine Image (AMI) of an EC2 instance and specify the second Region for the destination\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Launch a new EC2 instance from an Amazon Machine Image (AMI) in the second Region\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Detach a volume on an EC2 instance and copy it to an Amazon S3 bucket in the second Region\" is incorrect. You cannot copy EBS volumes directly from EBS to Amazon S3.</p><p><strong>INCORRECT:</strong> \"Launch a new EC2 instance in the second Region and copy a volume from Amazon S3 to the new instance\" is incorrect. You cannot create an EBS volume directly from Amazon S3.</p><p><strong>INCORRECT:</strong> \"Copy an Amazon Elastic Block Store (Amazon EBS) volume from Amazon S3 and launch an EC2 instance in the second Region using that EBS volume\" is incorrect. You cannot create an EBS volume directly from Amazon S3.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 10517,
                        "content": "<p>Launch a new EC2 instance in the second Region and copy a volume from Amazon S3 to the new instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10518,
                        "content": "<p>Copy an Amazon Elastic Block Store (Amazon EBS) volume from Amazon S3 and launch an EC2 instance in the second Region using that EBS volume</p>",
                        "isValid": false
                    },
                    {
                        "id": 10519,
                        "content": "<p>Copy an Amazon Machine Image (AMI) of an EC2 instance and specify the second Region for the destination</p>",
                        "isValid": true
                    },
                    {
                        "id": 10520,
                        "content": "<p>Launch a new EC2 instance from an Amazon Machine Image (AMI) in the second Region</p>",
                        "isValid": true
                    },
                    {
                        "id": 10521,
                        "content": "<p>Detach a volume on an EC2 instance and copy it to an Amazon S3 bucket in the second Region</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2516,
            "attributes": {
                "createdAt": "2023-09-09T20:39:44.340Z",
                "updatedAt": "2023-09-09T20:39:44.340Z",
                "content": "<p>A retail company with many stores and warehouses is implementing IoT sensors to gather monitoring data from devices in each location. The data will be sent to AWS in real time. A solutions architect must provide a solution for ensuring events are received in order for each device and ensure that data is saved for future processing.</p><p>Which solution would be MOST efficient?</p>",
                "answerExplanation": "<p>Amazon Kinesis Data Streams collect and process data in real time. A <em>Kinesis data stream</em> is a set of <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html#shard\">shards</a>. Each shard has a sequence of data records. Each data record has a <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html#sequence-number\">sequence number</a> that is assigned by Kinesis Data Streams. A <em>shard</em> is a uniquely identified sequence of data records in a stream.</p><p>A <em>partition key</em> is used to group data by shard within a stream. Kinesis Data Streams segregates the data records belonging to a stream into multiple shards. It uses the partition key that is associated with each data record to determine which shard a given data record belongs to.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-04-57-65202de89627ab9ac70ef6b89817c981.jpg\"></p><p>For this scenario, the solutions architect can use a partition key for each device. This will ensure the records for that device are grouped by shard and the shard will ensure ordering. Amazon S3 is a valid destination for saving the data records.</p><p><strong>CORRECT: </strong>\"Use Amazon Kinesis Data Streams for real-time events with a partition key for each device. Use Amazon Kinesis Data Firehose to save data to Amazon S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams for real-time events with a shard for each device. Use Amazon Kinesis Data Firehose to save data to Amazon EBS\" is incorrect as you cannot save data to EBS from Kinesis.</p><p><strong>INCORRECT:</strong> \"Use an Amazon SQS FIFO queue for real-time events with one queue for each device. Trigger an AWS Lambda function for the SQS queue to save data to Amazon EFS\" is incorrect as SQS is not the most efficient service for streaming, real time data.</p><p><strong>INCORRECT:</strong> \"Use an Amazon SQS standard queue for real-time events with one queue for each device. Trigger an AWS Lambda function from the SQS queue to save data to Amazon S3\" is incorrect as SQS is not the most efficient service for streaming, real time data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 10522,
                        "content": "<p>Use an Amazon SQS FIFO queue for real-time events with one queue for each device. Trigger an AWS Lambda function for the SQS queue to save data to Amazon EFS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10523,
                        "content": "<p>Use an Amazon SQS standard queue for real-time events with one queue for each device. Trigger an AWS Lambda function from the SQS queue to save data to Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 10524,
                        "content": "<p>Use Amazon Kinesis Data Streams for real-time events with a shard for each device. Use Amazon Kinesis Data Firehose to save data to Amazon EBS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10525,
                        "content": "<p>Use Amazon Kinesis Data Streams for real-time events with a partition key for each device. Use Amazon Kinesis Data Firehose to save data to Amazon S3</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2517,
            "attributes": {
                "createdAt": "2023-09-09T20:39:44.471Z",
                "updatedAt": "2023-09-09T20:39:44.471Z",
                "content": "<p>A company runs an application in a factory that has a small rack of physical compute resources. The application stores data on a network attached storage (NAS) device using the NFS protocol. The company requires a daily offsite backup of the application data.</p><p>Which solution can a Solutions Architect recommend to meet this requirement?</p>",
                "answerExplanation": "<p>The AWS Storage Gateway Hardware Appliance is a physical, standalone, validated server configuration for on-premises deployments. It comes pre-loaded with Storage Gateway software, and provides all the required CPU, memory, network, and SSD cache resources for creating and configuring File Gateway, Volume Gateway, or Tape Gateway.</p><p>A file gateway is the correct type of appliance to use for this use case as it is suitable for mounting via the NFS and SMB protocols.</p><p><strong>CORRECT: </strong>\"Use an AWS Storage Gateway file gateway hardware appliance on premises to replicate the data to Amazon S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an AWS Storage Gateway volume gateway with stored volumes on premises to replicate the data to Amazon S3\" is incorrect. Volume gateways are used for block-based storage and this solution requires NFS (file-based storage).</p><p><strong>INCORRECT:</strong> \"Use an AWS Storage Gateway volume gateway with cached volumes on premises to replicate the data to Amazon S3\" is incorrect. Volume gateways are used for block-based storage and this solution requires NFS (file-based storage).</p><p><strong>INCORRECT:</strong> \"Create an IPSec VPN to AWS and configure the application to mount the Amazon EFS file system. Run a copy job to backup the data to EFS\" is incorrect. It would be better to use a Storage Gateway which will automatically take care of synchronizing a copy of the data to AWS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/storagegateway/hardware-appliance/\">https://aws.amazon.com/storagegateway/hardware-appliance/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>",
                "options": [
                    {
                        "id": 10526,
                        "content": "<p>Create an IPSec VPN to AWS and configure the application to mount the Amazon EFS file system. Run a copy job to backup the data to EFS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10527,
                        "content": "<p>Use an AWS Storage Gateway volume gateway with stored volumes on premises to replicate the data to Amazon S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10528,
                        "content": "<p>Use an AWS Storage Gateway volume gateway with cached volumes on premises to replicate the data to Amazon S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10529,
                        "content": "<p>Use an AWS Storage Gateway file gateway hardware appliance on premises to replicate the data to Amazon S3.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2518,
            "attributes": {
                "createdAt": "2023-09-09T20:39:44.571Z",
                "updatedAt": "2023-09-09T20:39:44.571Z",
                "content": "<p>Amazon EC2 instances in a development environment run between 9am and 5pm Monday-Friday. Production instances run 24/7. Which pricing models should be used to optimize cost and ensure capacity is available? (Select TWO.)</p>",
                "answerExplanation": "<p>Capacity reservations have no commitment and can be created and canceled as needed. This is ideal for the development environment as it will ensure the capacity is available. There is no price advantage but none of the other options provide a price advantage whilst also ensuring capacity is available</p><p>Reserved instances are a good choice for workloads that run continuously. This is a good option for the production environment<strong>.</strong></p><p><strong>CORRECT: </strong>\"On-demand capacity reservations for the development environment\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Use Reserved instances for the production environment\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Use Spot instances for the development environment\" is incorrect. Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted. Spot instances are not suitable for the development environment as important work may be interrupted.</p><p><strong>INCORRECT:</strong> \"Use Reserved instances for the development environment\" is incorrect as they require a long-term commitment which is not ideal for a development environment.</p><p><strong>INCORRECT:</strong> \"Use On-Demand instances for the production environment\" is incorrect. There is no long-term commitment required when you purchase On-Demand Instances. However, you do not get any discount and therefore this is the most expensive option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/instance-purchasing-options.html\">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/instance-purchasing-options.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 10530,
                        "content": "<p>On-demand capacity reservations for the development environment</p>",
                        "isValid": true
                    },
                    {
                        "id": 10531,
                        "content": "<p>Use Reserved instances for the production environment</p>",
                        "isValid": true
                    },
                    {
                        "id": 10532,
                        "content": "<p>Use Reserved instances for the development environment</p>",
                        "isValid": false
                    },
                    {
                        "id": 10533,
                        "content": "<p>Use On-Demand instances for the production environment</p>",
                        "isValid": false
                    },
                    {
                        "id": 10534,
                        "content": "<p>Use Spot instances for the development environment</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2519,
            "attributes": {
                "createdAt": "2023-09-09T20:39:44.684Z",
                "updatedAt": "2023-09-09T20:39:44.684Z",
                "content": "<p>A company delivers content to subscribers distributed globally from an application running on AWS. The application uses a fleet of Amazon EC2 instance in a private subnet behind an Application Load Balancer (ALB). Due to an update in copyright restrictions, it is necessary to block access for specific countries.</p><p>What is the EASIEST method to meet this requirement?</p>",
                "answerExplanation": "<p>When a user requests your content, CloudFront typically serves the requested content regardless of where the user is located. If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following:</p><p>Allow your users to access your content only if they're in one of the countries on a whitelist of approved countries.</p><p>Prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries.</p><p>For example, if a request comes from a country where, for copyright reasons, you are not authorized to distribute your content, you can use CloudFront geo restriction to block the request.</p><p>This is the easiest and most effective way to implement a geographic restriction for the delivery of content.</p><p><strong>CORRECT: </strong>\"Use Amazon CloudFront to serve the application and deny access to blocked countries\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a Network ACL to block the IP address ranges associated with the specific countries\" is incorrect as this would be extremely difficult to manage.</p><p><strong>INCORRECT:</strong> \"Modify the ALB security group to deny incoming traffic from blocked countries\" is incorrect as security groups cannot block traffic by country.</p><p><strong>INCORRECT:</strong> \"Modify the security group for EC2 instances to deny incoming traffic from blocked countries\" is incorrect as security groups cannot block traffic by country.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 10535,
                        "content": "<p>Use a network ACL to block the IP address ranges associated with the specific countries</p>",
                        "isValid": false
                    },
                    {
                        "id": 10536,
                        "content": "<p>Modify the ALB security group to deny incoming traffic from blocked countries</p>",
                        "isValid": false
                    },
                    {
                        "id": 10537,
                        "content": "<p>Use Amazon CloudFront to serve the application and deny access to blocked countries</p>",
                        "isValid": true
                    },
                    {
                        "id": 10538,
                        "content": "<p>Modify the security group for EC2 instances to deny incoming traffic from blocked countries</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2520,
            "attributes": {
                "createdAt": "2023-09-09T20:39:44.801Z",
                "updatedAt": "2023-09-09T20:39:44.801Z",
                "content": "<p>A new application will run across multiple Amazon ECS tasks. Front-end application logic will process data and then pass that data to a back-end ECS task to perform further processing and write the data to a datastore. The Architect would like to reduce-interdependencies so failures do no impact other components.</p><p>Which solution should the Architect use?</p>",
                "answerExplanation": "<p>This is a good use case for Amazon SQS. SQS is a service that is used for decoupling applications, thus reducing interdependencies, through a message bus. The front-end application can place messages on the queue and the back-end can then poll the queue for new messages. Please remember that Amazon SQS is pull-based (polling) not push-based (use SNS for push-based).</p><p><strong>CORRECT: </strong>\"Create an Amazon SQS queue and configure the front-end to add messages to the queue and the back-end to poll the queue for messages\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Kinesis Firehose delivery stream and configure the front-end to add data to the stream and the back-end to read data from the stream\" is incorrect. Amazon Kinesis Firehose is used for streaming data. With Firehose the data is immediately loaded into a destination that can be Amazon S3, RedShift, Elasticsearch, or Splunk. This is not an ideal use case for Firehose as this is not streaming data and there is no need to load data into an additional AWS service.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Kinesis Firehose delivery stream that delivers data to an Amazon S3 bucket, configure the front-end to write data to the stream and the back-end to read data from Amazon S3\" is incorrect as per the previous explanation.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SQS queue that pushes messages to the back-end. Configure the front-end to add messages to the queue \" is incorrect as SQS is pull-based, not push-based. EC2 instances must poll the queue to find jobs to process.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/common_use_cases.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/common_use_cases.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 10539,
                        "content": "<p>Create an Amazon Kinesis Firehose delivery stream that delivers data to an Amazon S3 bucket, configure the front-end to write data to the stream and the back-end to read data from Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 10540,
                        "content": "<p>Create an Amazon SQS queue that pushes messages to the back-end. Configure the front-end to add messages to the queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 10541,
                        "content": "<p>Create an Amazon Kinesis Firehose delivery stream and configure the front-end to add data to the stream and the back-end to read data from the stream</p>",
                        "isValid": false
                    },
                    {
                        "id": 10542,
                        "content": "<p>Create an Amazon SQS queue and configure the front-end to add messages to the queue and the back-end to poll the queue for messages</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2521,
            "attributes": {
                "createdAt": "2023-09-09T20:39:44.906Z",
                "updatedAt": "2023-09-09T20:39:44.906Z",
                "content": "<p>A legacy tightly-coupled High Performance Computing (HPC) application will be migrated to AWS. Which network adapter type should be used?</p>",
                "answerExplanation": "<p>An Elastic Fabric Adapter is an AWS Elastic Network Adapter (ENA) with added capabilities. The EFA lets you apply the scale, flexibility, and elasticity of the AWS Cloud to tightly-coupled HPC apps. It is ideal for tightly coupled app as it uses the Message Passing Interface (MPI).</p><p><strong>CORRECT: </strong>\"Elastic Fabric Adapter (EFA)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Elastic Network Interface (ENI)\" is incorrect. The ENI is a basic type of adapter and is not the best choice for this use case.</p><p><strong>INCORRECT:</strong> \"Elastic Network Adapter (ENA)\" is incorrect. The ENA, which provides Enhanced Networking, does provide high bandwidth and low inter-instance latency but it does not support the features for a tightly-coupled app that the EFA does.</p><p><strong>INCORRECT:</strong> \"Elastic IP Address\" is incorrect. An Elastic IP address is just a static public IP address, it is not a type of network adapter.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/now-available-elastic-fabric-adapter-efa-for-tightly-coupled-hpc-workloads/\">https://aws.amazon.com/blogs/aws/now-available-elastic-fabric-adapter-efa-for-tightly-coupled-hpc-workloads/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 10543,
                        "content": "<p>Elastic IP Address</p>",
                        "isValid": false
                    },
                    {
                        "id": 10544,
                        "content": "<p>Elastic Network Interface (ENI)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10545,
                        "content": "<p>Elastic Fabric Adapter (EFA)</p>",
                        "isValid": true
                    },
                    {
                        "id": 10546,
                        "content": "<p>Elastic Network Adapter (ENA)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2522,
            "attributes": {
                "createdAt": "2023-09-09T20:39:45.036Z",
                "updatedAt": "2023-09-09T20:39:45.036Z",
                "content": "<p>An application is being created that will use Amazon EC2 instances to generate and store data. Another set of EC2 instances will then analyze and modify the data. Storage requirements will be significant and will continue to grow over time. The application architects require a storage solution.</p><p>Which actions would meet these needs?</p>",
                "answerExplanation": "<p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_00-51-10-5bfba0da41e8553b6b332f2566f0f56e.jpg\"></p><p>Amazon EFS supports the Network File System version 4 (NFSv4.1 and NFSv4.0) protocol. Multiple Amazon EC2 instances can access an Amazon EFS file system at the same time, providing a common data source for workloads and applications running on more than one instance or server.</p><p>For this scenario, EFS is a great choice as it will provide a scalable file system that can be mounted by multiple EC2 instances and accessed simultaneously.</p><p><strong>CORRECT: </strong>\"Store the data in an Amazon EFS filesystem. Mount the file system on the application instances\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Store the data in an Amazon EBS volume. Mount the EBS volume on the application instances\" is incorrect. Though there is a new feature that allows (EBS multi-attach) that allows attaching multiple Nitro instances to a volume, this is not on the exam yet, and has some specific constraints.</p><p><strong>INCORRECT:</strong> \"Store the data in Amazon S3 Glacier. Update the vault policy to allow access to the application instances\" is incorrect as S3 Glacier is not a suitable storage location for live access to data, it is used for archival.</p><p><strong>INCORRECT:</strong> \"Store the data in AWS Storage Gateway. Setup AWS Direct Connect between the Gateway appliance and the EC2 instances\" is incorrect. There is no reason to store the data on-premises in a Storage Gateway, using EFS is a much better solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html\">https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
                "options": [
                    {
                        "id": 10547,
                        "content": "<p>Store the data in an Amazon EBS volume. Mount the EBS volume on the application instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10548,
                        "content": "<p>Store the data in an Amazon EFS filesystem. Mount the file system on the application instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 10549,
                        "content": "<p>Store the data in AWS Storage Gateway. Setup AWS Direct Connect between the Gateway appliance and the EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10550,
                        "content": "<p>Store the data in Amazon S3 Glacier. Update the vault policy to allow access to the application instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2523,
            "attributes": {
                "createdAt": "2023-09-09T20:39:45.157Z",
                "updatedAt": "2023-09-09T20:39:45.157Z",
                "content": "<p>A surveying team is using a fleet of drones to collect images of construction sites. The surveying team's laptops lack the inbuilt storage and compute capacity to transfer the images and process the data. While the team has Amazon EC2 instances for processing and Amazon S3 buckets for storage, network connectivity is intermittent and unreliable. The images need to be processed to evaluate the progress of each construction site.</p><p>What should a solutions architect recommend?</p>",
                "answerExplanation": "<p>AWS physical Snowball Edge device will provide much more inbuilt compute and storage compared to the current team’s laptops. This negates the need to rely on a stable connection to process any images and solves the team's problems easily and efficiently.</p><p><strong>CORRECT: </strong>\"Process and store the images using AWS Snowball Edge devices” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"During intermittent connectivity to EC2 instances, upload images to Amazon SQS” is incorrect as you would still need a reliable internet connection to upload any images to Amazon SQS.</p><p><strong>INCORRECT:</strong> \"Configure Amazon Kinesis Data Firehose to create multiple delivery streams aimed separately at the S3 buckets for storage and the EC2 instances for processing the images\" is incorrect as you would still need a reliable internet connection to upload any images to the Amazon Kinesis Service.</p><p><strong>INCORRECT:</strong> \"Cache the images locally on a hardware appliance pre-installed with AWS Storage Gateway to process the images when connectivity is restored” is incorrect as you would still need reliable internet connection to upload any images to the Amazon Storage Gateway service.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/snowball/latest/developer-guide/whatisedge.html\">https://docs.aws.amazon.com/snowball/latest/developer-guide/whatisedge.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 10551,
                        "content": "<p>Cache the images locally on a hardware appliance pre-installed with AWS Storage Gateway to process the images when connectivity is restored.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10552,
                        "content": "<p>Configure Amazon Kinesis Data Firehose to create multiple delivery streams aimed separately at the S3 buckets for storage and the EC2 instances for processing the images.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10553,
                        "content": "<p>Process and store the images using AWS Snowball Edge devices.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10554,
                        "content": "<p>During intermittent connectivity to EC2 instances, upload images to Amazon SQS.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2524,
            "attributes": {
                "createdAt": "2023-09-09T20:39:45.283Z",
                "updatedAt": "2023-09-09T20:39:45.283Z",
                "content": "<p>A web application allows users to upload photos and add graphical elements to them. The application offers two tiers of service: free and paid. Photos uploaded by paid users should be processed before those submitted using the free tier. The photos are uploaded to an Amazon S3 bucket which uses an event notification to send the job information to Amazon SQS.</p><p>How should a Solutions Architect configure the Amazon SQS deployment to meet these requirements?</p>",
                "answerExplanation": "<p>AWS recommend using separate queues when you need to provide prioritization of work. The logic can then be implemented at the application layer to prioritize the queue for the paid photos over the queue for the free photos.</p><p><strong>CORRECT: </strong>\"Use a separate SQS Standard queue for each tier. Configure Amazon EC2 instances to prioritize polling for the paid queue over the free queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use one SQS FIFO queue. Assign a higher priority to the paid photos so they are processed first\" is incorrect. FIFO queues preserve the order of messages but they do not prioritize messages within the queue. The orders would need to be placed into the queue in a priority order and there’s no way of doing this as the messages are sent automatically through event notifications as they are received by Amazon S3.</p><p><strong>INCORRECT:</strong> \"Use one SQS standard queue. Use batching for the paid photos and short polling for the free photos\" is incorrect. Batching adds efficiency but it has nothing to do with ordering or priority.</p><p><strong>INCORRECT:</strong> \"Use a separate SQS FIFO queue for each tier. Set the free queue to use short polling and the paid queue to use long polling\" is incorrect. Short polling and long polling are used to control the amount of time the consumer process waits before closing the API call and trying again. Polling should be configured for efficiency of API calls and processing of messages but does not help with message prioritization.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-how-it-works.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-how-it-works.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 10555,
                        "content": "<p>Use a separate SQS Standard queue for each tier. Configure Amazon EC2 instances to prioritize polling for the paid queue over the free queue.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10556,
                        "content": "<p>Use one SQS standard queue. Use batching for the paid photos and short polling for the free photos.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10557,
                        "content": "<p>Use a separate SQS FIFO queue for each tier. Set the free queue to use short polling and the paid queue to use long polling.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10558,
                        "content": "<p>Use one SQS FIFO queue. Assign a higher priority to the paid photos so they are processed first.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2525,
            "attributes": {
                "createdAt": "2023-09-09T20:39:45.392Z",
                "updatedAt": "2023-09-09T20:39:45.392Z",
                "content": "<p>A web application runs in public and private subnets. The application architecture consists of a web tier and database tier running on Amazon EC2 instances. Both tiers run in a single Availability Zone (AZ).</p><p>Which combination of steps should a solutions architect take to provide high availability for this architecture? (Select TWO.)</p>",
                "answerExplanation": "<p>To add high availability to this architecture both the web tier and database tier require changes. For the web tier an Auto Scaling group across multiple AZs with an ALB will ensure there are always instances running and traffic is being distributed to them.</p><p>The database tier should be migrated from the EC2 instances to Amazon RDS to take advantage of a managed database with Multi-AZ functionality. This will ensure that if there is an issue preventing access to the primary database a secondary database can take over.</p><p><strong>CORRECT: </strong>\"Create an Amazon EC2 Auto Scaling group and Application Load Balancer (ALB) spanning multiple AZs\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Create new public and private subnets in the same VPC, each in a new AZ. Migrate the database to an Amazon RDS multi-AZ deployment\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create new public and private subnets in the same AZ for high availability\" is incorrect as this would not add high availability.</p><p><strong>INCORRECT:</strong> \"Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer (ALB)\" is incorrect because the existing servers are in a single subnet. For HA we need to instances in multiple subnets.</p><p><strong>INCORRECT:</strong> \"Create new public and private subnets in a new AZ. Create a database using Amazon EC2 in one AZ\" is incorrect because we also need HA for the database layer.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html</a></p><p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 10559,
                        "content": "<p>Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer (ALB)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10560,
                        "content": "<p>Create new public and private subnets in a new AZ. Create a database using Amazon EC2 in one AZ</p>",
                        "isValid": false
                    },
                    {
                        "id": 10561,
                        "content": "<p>Create new public and private subnets in the same AZ for high availability</p>",
                        "isValid": false
                    },
                    {
                        "id": 10562,
                        "content": "<p>Create new public and private subnets in the same VPC, each in a new AZ. Migrate the database to an Amazon RDS multi-AZ deployment</p>",
                        "isValid": true
                    },
                    {
                        "id": 10563,
                        "content": "<p>Create an Amazon EC2 Auto Scaling group and Application Load Balancer (ALB) spanning multiple AZs</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2526,
            "attributes": {
                "createdAt": "2023-09-09T20:39:45.497Z",
                "updatedAt": "2023-09-09T20:39:45.497Z",
                "content": "<p>A company requires that all AWS IAM user accounts have specific complexity requirements and minimum password length.</p><p>How should a Solutions Architect accomplish this?</p>",
                "answerExplanation": "<p>The easiest way to enforce this requirement is to update the password policy that applies to the entire AWS account. When you create or change a password policy, most of the password policy settings are enforced the next time your users change their passwords. However, some of the settings are enforced immediately such as the password expiration period.</p><p><strong>CORRECT: </strong>\"Set a password policy for the entire AWS account\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Set a password policy for each IAM user in the AWS account\" is incorrect. There’s no need to set an individual password policy for each user, it will be easier to set the policy for everyone.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy that enforces the requirements and apply it to all users\" is incorrect. As there is no specific targeting required it is easier to update the account password policy.</p><p><strong>INCORRECT:</strong> \"Use an AWS Config rule to enforce the requirements when creating user accounts\" is incorrect. You cannot use AWS Config to enforce the password requirements at the time of creating a user account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_account-policy.html?icmpid=docs_iam_console\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_account-policy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 10564,
                        "content": "<p>Set a password policy for each IAM user in the AWS account.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10565,
                        "content": "<p>Set a password policy for the entire AWS account.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10566,
                        "content": "<p>Use an AWS Config rule to enforce the requirements when creating user accounts.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10567,
                        "content": "<p>Create an IAM policy that enforces the requirements and apply it to all users.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2527,
            "attributes": {
                "createdAt": "2023-09-09T20:39:45.624Z",
                "updatedAt": "2023-09-09T20:39:45.624Z",
                "content": "<p>There are two applications in a company: a sender application that sends messages containing payloads, and a processing application that receives messages containing payloads. The company wants to implement an AWS service to handle messages between these two different applications. The sender application sends on average 1,000 messages each hour and the messages depending on the type sometimes take up to 2 days to be processed. If the messages fail to process, they must be retained so that they do not impact the processing of any remaining messages.</p><p>Which solution meets these requirements and is the MOST operationally efficient?</p>",
                "answerExplanation": "<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work.</p><p><strong>CORRECT: </strong>\"Provide an Amazon Simple Queue Service (Amazon SQS) queue for the sender and processor applications. Set up a dead-letter queue to collect failed messages” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Set up a Redis database on Amazon EC2. Configure the instance to be used by both applications. The messages should be stored, processed, and deleted, respectively” is incorrect, as the most operationally efficient way is to use the managed service Amazon SQS.</p><p><strong>INCORRECT:</strong> \"Receive the messages from the sender application using an Amazon Kinesis data stream. Utilize the Kinesis Client Library (KCL) to integrate the processing application” is incorrect, as the most operationally efficient way is to use the managed service Amazon SQS</p><p><strong>INCORRECT:</strong> \"Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications. Write to the SNS topic using the sender application” is incorrect as Amazon SNS is not a queuing service, but a pub-sub one to many notification service and cannot be used as a queue.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 10568,
                        "content": "<p>Set up a Redis database on Amazon EC2. Configure the instance to be used by both applications. The messages should be stored, processed, and deleted, respectively.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10569,
                        "content": "<p>Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications. Write to the SNS topic using the sender application.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10570,
                        "content": "<p>Receive the messages from the sender application using an Amazon Kinesis data stream. Utilize the Kinesis Client Library (KCL) to integrate the processing application.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10571,
                        "content": "<p>Provide an Amazon Simple Queue Service (Amazon SQS) queue for the sender and processor applications. Set up a dead-letter queue to collect failed messages.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2528,
            "attributes": {
                "createdAt": "2023-09-09T20:39:45.739Z",
                "updatedAt": "2023-09-09T20:39:45.739Z",
                "content": "<p>The database tier of a web application is running on a Windows server on-premises. The database is a Microsoft SQL Server database. The application owner would like to migrate the database to an Amazon RDS instance.</p><p>How can the migration be executed with minimal administrative effort and downtime?</p>",
                "answerExplanation": "<p>You can directly migrate Microsoft SQL Server from an on-premises server into Amazon RDS using the Microsoft SQL Server database engine. This can be achieved using the native Microsoft SQL Server tools, or using AWS DMS as depicted below:</p><p><br></p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-06-30_17-10-28-5ac42be5b303d82ec1ea4dceda72ef02.jpg\"><p><br></p><p><strong>CORRECT: </strong>\"Use the AWS Database Migration Service (DMS) to directly migrate the database to RDS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use the AWS Server Migration Service (SMS) to migrate the server to Amazon EC2. Use AWS Database Migration Service (DMS) to migrate the database to RDS\" is incorrect. You do not need to use the AWS SMS service to migrate the server into EC2 first. You can directly migrate the database online with minimal downtime.</p><p><strong>INCORRECT:</strong> \"Use AWS DataSync to migrate the data from the database to Amazon S3. Use AWS Database Migration Service (DMS) to migrate the database to RDS\" is incorrect. AWS DataSync is used for migrating data, not databases.</p><p><strong>INCORRECT:</strong> \"Use the AWS Database Migration Service (DMS) to directly migrate the database to RDS. Use the Schema Conversion Tool (SCT) to enable conversion from Microsoft SQL Server to Amazon RDS\" is incorrect. You do not need to use the SCT as you are migrating into the same destination database engine (RDS is just the platform).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-microsoft-sql-server-database-to-amazon-rds-for-sql-server.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-microsoft-sql-server-database-to-amazon-rds-for-sql-server.html</a></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.html</a></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.html</a></p><p><a href=\"https://aws.amazon.com/dms/schema-conversion-tool/\">https://aws.amazon.com/dms/schema-conversion-tool/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 10572,
                        "content": "<p>Use the AWS Server Migration Service (SMS) to migrate the server to Amazon EC2.Use AWS Database Migration Service (DMS) to migrate the database to RDS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10573,
                        "content": "<p>Use the AWS Database Migration Service (DMS) to directly migrate the database to RDS</p>",
                        "isValid": true
                    },
                    {
                        "id": 10574,
                        "content": "<p>Use the AWS Database Migration Service (DMS) to directly migrate the database to RDS. Use the Schema Conversion Tool (SCT) to enable conversion from Microsoft SQL Server to Amazon RDS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10575,
                        "content": "<p>Use AWS DataSync to migrate the data from the database to Amazon S3. Use AWS Database Migration Service (DMS) to migrate the database to RDS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2529,
            "attributes": {
                "createdAt": "2023-09-09T20:39:45.825Z",
                "updatedAt": "2023-09-09T20:39:45.825Z",
                "content": "<p>A solutions architect is creating a system that will run analytics on financial data for several hours a night, 5 days a week. The analysis is expected to run for the same duration and cannot be interrupted once it is started. The system will be required for a minimum of 1 year.</p><p>What should the solutions architect configure to ensure the EC2 instances are available when they are needed?</p>",
                "answerExplanation": "<p>On-Demand Capacity Reservations enable you to reserve compute capacity for your Amazon EC2 instances in a specific Availability Zone for any duration. This gives you the ability to create and manage Capacity Reservations independently from the billing discounts offered by Savings Plans or Regional Reserved Instances.</p><p>By creating Capacity Reservations, you ensure that you always have access to EC2 capacity when you need it, for as long as you need it. You can create Capacity Reservations at any time, without entering a one-year or three-year term commitment, and the capacity is available immediately.</p><p>The table below shows the difference between capacity reservations and other options:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-01-10_20-24-52-810be510ec31f342fd622453a1ebec94.png\"></p><p><strong>CORRECT: </strong>\"On-Demand Capacity Reservations\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Regional Reserved Instances\" is incorrect. This type of reservation does not reserve capacity.</p><p><strong>INCORRECT:</strong> \"On-Demand Instances\" is incorrect. This does not provide any kind of capacity reservation.</p><p><strong>INCORRECT:</strong> \"Savings Plans\" is incorrect. This pricing option does not provide a capacity reservation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 10576,
                        "content": "<p>Regional Reserved Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10577,
                        "content": "<p>Savings Plans</p>",
                        "isValid": false
                    },
                    {
                        "id": 10578,
                        "content": "<p>On-Demand Capacity Reservations</p>",
                        "isValid": true
                    },
                    {
                        "id": 10579,
                        "content": "<p>On-Demand Instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2530,
            "attributes": {
                "createdAt": "2023-09-09T20:39:45.912Z",
                "updatedAt": "2023-09-09T20:39:45.912Z",
                "content": "<p>A company needs to connect its on-premises data center network to a new virtual private cloud (VPC). There is a symmetrical internet connection of 100 Mbps in the data center network. The data transfer rate for an on-premises application is multiple gigabytes per day. Processing will be done using an Amazon Kinesis Data Firehose stream.</p><p>What should a solutions architect recommend for maximum performance?</p>",
                "answerExplanation": "<p><strong>Explanation:</strong></p><p>Using AWS PrivateLink to create an interface endpoint will allow your traffic to traverse the AWS Global Backbone to allow maximum performance and security. Also by using an AWS Direct Connect cable you can ensure you have a dedicated cable to provide maximum performance and low latency to and from AWS.</p><p><strong>CORRECT: </strong>\"Kinesis Data Firehose can be connected to the VPC using AWS PrivateLink. Install a 1 Gbps AWS Direct Connect connection between the on-premises network and AWS. To send data from on-premises to Kinesis Data Firehose, use the PrivateLink endpoint” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Establish a peering connection between the on-premises network and the VPC. Configure routing for the on-premises network to use the VPC peering connection” is incorrect also because VPC peering connections can only exist between two VPCs within the AWS Cloud.</p><p><strong>INCORRECT:</strong> \"Get an AWS Snowball Edge Storage Optimized device. Data must be copied to the device after several days and shipped to AWS for expedited transfer to Kinesis Data Firehose. Repeat as necessary” is incorrect. AWS Snowball Edge is designed to be more of a one-time migration service which you physically receive from AWS, and then ship it into an AWS Region of your choice.</p><p><strong>INCORRECT:</strong> \"Establish an AWS Site-to-Site VPN connection between the on-premises network and the VPC. Set up BGP routing between the customer gateway and the virtual private gateway. Send data to Kinesis Data Firehose using a VPN connection” is incorrect. This is a functional solution; however a physical connection would provide a much more reliable and performant solution.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/privatelink/\">https://aws.amazon.com/privatelink/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 10580,
                        "content": "<p>Get an AWS Snowball Edge Storage Optimized device. Data must be copied to the device after several days and shipped to AWS for expedited transfer to Kinesis Data Firehose. Repeat as necessary.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10581,
                        "content": "<p>Establish a peering connection between the on-premises network and the VPC. Configure routing for the on-premises network to use the VPC peering connection.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10582,
                        "content": "<p>Kinesis Data Firehose can be connected to the VPC using AWS PrivateLink. Install a 1 Gbps AWS Direct Connect connection between the on-premises network and AWS. To send data from on-premises to Kinesis Data Firehose, use the PrivateLink endpoint.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10583,
                        "content": "<p>Establish an AWS Site-to-Site VPN connection between the on-premises network and the VPC. Set up BGP routing between the customer gateway and the virtual private gateway. Send data to Kinesis Data Firehose using a VPN connection.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2531,
            "attributes": {
                "createdAt": "2023-09-09T20:39:46.042Z",
                "updatedAt": "2023-09-09T20:39:46.042Z",
                "content": "<p>A solutions architect is creating a document submission application for a school. The application will use an Amazon S3 bucket for storage. The solution must prevent accidental deletion of the documents and ensure that all versions of the documents are available. Users must be able to upload and modify the documents.</p><p>Which combination of actions should be taken to meet these requirements? (Select TWO.)</p>",
                "answerExplanation": "<p>None of the options present a good solution for specifying permissions required to write and modify objects so that requirement needs to be taken care of separately. The other requirements are to prevent accidental deletion and the ensure that all versions of the document are available.</p><p>The two solutions for these requirements are versioning and MFA delete. Versioning will retain a copy of each version of the document and multi-factor authentication delete (MFA delete) will prevent any accidental deletion as you need to supply a second factor when attempting a delete.</p><p><strong>CORRECT: </strong>\"Enable versioning on the bucket\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Enable MFA Delete on the bucket\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Set read-only permissions on the bucket\" is incorrect as this will also prevent any writing to the bucket which is not desired.</p><p><strong>INCORRECT:</strong> \"Attach an IAM policy to the bucket\" is incorrect as users need to modify documents which will also allow delete. Therefore, a method must be implemented to just control deletes.</p><p><strong>INCORRECT:</strong> \"Encrypt the bucket using AWS SSE-S3\" is incorrect as encryption doesn’t stop you from deleting an object.<strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 10584,
                        "content": "<p>Set read-only permissions on the bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 10585,
                        "content": "<p>Enable versioning on the bucket</p>",
                        "isValid": true
                    },
                    {
                        "id": 10586,
                        "content": "<p>Attach an IAM policy to the bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 10587,
                        "content": "<p>Encrypt the bucket using AWS SSE-S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 10588,
                        "content": "<p>Enable MFA Delete on the bucket</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2532,
            "attributes": {
                "createdAt": "2023-09-09T20:39:46.142Z",
                "updatedAt": "2023-09-09T20:39:46.142Z",
                "content": "<p>A team are planning to run analytics jobs on log files each day and require a storage solution. The size and number of logs is unknown and data will persist for 24 hours only.</p><p>What is the MOST cost-effective solution?</p>",
                "answerExplanation": "<p>S3 standard is the best choice in this scenario for a short term storage solution. In this case the size and number of logs is unknown and it would be difficult to fully assess the access patterns at this stage. Therefore, using S3 standard is best as it is cost-effective, provides immediate access, and there are no retrieval fees or minimum capacity charge per object.</p><p><strong>CORRECT: </strong>\"Amazon S3 Standard\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon S3 Intelligent-Tiering\" is incorrect as there is an additional fee for using this service and for a short-term requirement it may not be beneficial.</p><p><strong>INCORRECT:</strong> \"Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)\" is incorrect as this storage class has a minimum capacity charge per object (128 KB) and a per GB retrieval fee.</p><p><strong>INCORRECT:</strong> \"Amazon S3 Glacier Deep Archive\" is incorrect as this storage class is used for archiving data. There are retrieval fees and it take hours to retrieve data from an archive.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 10589,
                        "content": "<p>Amazon S3 Glacier Deep Archive</p>",
                        "isValid": false
                    },
                    {
                        "id": 10590,
                        "content": "<p>Amazon S3 Intelligent-Tiering</p>",
                        "isValid": false
                    },
                    {
                        "id": 10591,
                        "content": "<p>Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10592,
                        "content": "<p>Amazon S3 Standard</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2533,
            "attributes": {
                "createdAt": "2023-09-09T20:39:46.232Z",
                "updatedAt": "2023-09-09T20:39:46.232Z",
                "content": "<p>A company plans to make an Amazon EC2 Linux instance unavailable outside of business hours to save costs. The instance is backed by an Amazon EBS volume. There is a requirement that the contents of the instance’s memory must be preserved when it is made unavailable.</p><p>How can a solutions architect meet these requirements?</p>",
                "answerExplanation": "<p>When you hibernate an instance, Amazon EC2 signals the operating system to perform hibernation (suspend-to-disk). Hibernation saves the contents from the instance memory (RAM) to your Amazon Elastic Block Store (Amazon EBS) root volume. Amazon EC2 persists the instance's EBS root volume and any attached EBS data volumes. When you start your instance:</p><p>- The EBS root volume is restored to its previous state</p><p>- The RAM contents are reloaded</p><p>- The processes that were previously running on the instance are resumed</p><p>- Previously attached data volumes are reattached and the instance retains its instance ID</p><p><strong>CORRECT: </strong>\"Hibernate the instance outside business hours. Start the instance again when required\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Stop the instance outside business hours. Start the instance again when required\" is incorrect. When an instance is stopped the operating system is shut down and the contents of memory will be lost.</p><p><strong>INCORRECT:</strong> \"Use Auto Scaling to scale down the instance outside of business hours. Scale out the instance when required\" is incorrect. Auto Scaling scales does not scale up and down, it scales in by terminating instances and out by launching instances. When scaling out new instances are launched and no state will be available from terminated instances.</p><p><strong>INCORRECT:</strong> \"Terminate the instance outside business hours. Recover the instance again when required\" is incorrect. You cannot recover terminated instances, you can recover instances that have become impaired in some circumstances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 10593,
                        "content": "<p>Use Auto Scaling to scale down the instance outside of business hours. Scale up the instance when required.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10594,
                        "content": "<p>Terminate the instance outside business hours. Recover the instance again when required.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10595,
                        "content": "<p>Hibernate the instance outside business hours. Start the instance again when required.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10596,
                        "content": "<p>Stop the instance outside business hours. Start the instance again when required.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2534,
            "attributes": {
                "createdAt": "2023-09-09T20:39:46.327Z",
                "updatedAt": "2023-09-09T20:39:46.327Z",
                "content": "<p>A company runs a dynamic website that is hosted on an on-premises server in the United States. The company is expanding to Europe and is investigating how they can optimize the performance of the website for European users. The website’s backed must remain in the United States. The company requires a solution that can be implemented within a few days.</p><p>What should a Solutions Architect recommend?</p>",
                "answerExplanation": "<p>A custom origin can point to an on-premises server and CloudFront is able to cache content for dynamic websites. CloudFront can provide performance optimizations for custom origins even if they are running on on-premises servers. These include persistent TCP connections to the origin, SSL enhancements such as Session tickets and OCSP stapling.</p><p>Additionally, connections are routed from the nearest Edge Location to the user across the AWS global network. If the on-premises server is connected via a Direct Connect (DX) link this can further improve performance.</p><p><strong>CORRECT: </strong>\"Use Amazon CloudFront with a custom origin pointing to the on-premises servers\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudFront with Lambda@Edge to direct traffic to an on-premises origin\" is incorrect. Lambda@Edge is not used to direct traffic to on-premises origins.</p><p><strong>INCORRECT:</strong> \"Launch an Amazon EC2 instance in an AWS Region in the United States and migrate the website to it\" is incorrect. This would not necessarily improve performance for European users.</p><p><strong>INCORRECT:</strong> \"Migrate the website to Amazon S3. Use cross-Region replication between Regions and a latency-based Route 53 policy\" is incorrect. You cannot host dynamic websites on Amazon S3 (static only).</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/dynamic-content/\">https://aws.amazon.com/cloudfront/dynamic-content/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 10597,
                        "content": "<p>Migrate the website to Amazon S3. Use cross-Region replication between Regions and a latency-based Route 53 policy.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10598,
                        "content": "<p>Use Amazon CloudFront with Lambda@Edge to direct traffic to an on-premises origin.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10599,
                        "content": "<p>Launch an Amazon EC2 instance in an AWS Region in the United States and migrate the website to it.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10600,
                        "content": "<p>Use Amazon CloudFront with a custom origin pointing to the on-premises servers.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2535,
            "attributes": {
                "createdAt": "2023-09-09T20:39:46.454Z",
                "updatedAt": "2023-09-09T20:39:46.454Z",
                "content": "<p>A website runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) which serves as an origin for an Amazon CloudFront distribution. An AWS WAF is being used to protect against SQL injection attacks. A review of security logs revealed an external malicious IP that needs to be blocked from accessing the website.</p><p>What should a solutions architect do to protect the application?</p>",
                "answerExplanation": "<p>A new version of the AWS Web Application Firewall was released in November 2019. With AWS WAF classic you create “IP match conditions”, whereas with AWS WAF (new version) you create “IP set match statements”. Look out for wording on the exam.</p><p>The IP match condition / IP set match statement inspects the IP address of a web request's origin against a set of IP addresses and address ranges. Use this to allow or block web requests based on the IP addresses that the requests originate from.</p><p>AWS WAF supports all IPv4 and IPv6 address ranges. An IP set can hold up to 10,000 IP addresses or IP address ranges to check.</p><p><strong>CORRECT: </strong>\"Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP address\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Modify the network ACL on the CloudFront distribution to add a deny rule for the malicious IP address\" is incorrect as CloudFront does not sit within a subnet so network ACLs do not apply to it.</p><p><strong>INCORRECT:</strong> \"Modify the network ACL for the EC2 instances in the target groups behind the ALB to deny the malicious IP address\" is incorrect as the source IP addresses of the data in the EC2 instances’ subnets will be the ELB IP addresses.</p><p><strong>INCORRECT:</strong> \"Modify the security groups for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.\" is incorrect as you cannot create deny rules with security groups.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>",
                "options": [
                    {
                        "id": 10601,
                        "content": "<p>Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP address</p>",
                        "isValid": true
                    },
                    {
                        "id": 10602,
                        "content": "<p>Modify the network ACL on the CloudFront distribution to add a deny rule for the malicious IP address</p>",
                        "isValid": false
                    },
                    {
                        "id": 10603,
                        "content": "<p>Modify the network ACL for the EC2 instances in the target groups behind the ALB to deny the malicious IP address</p>",
                        "isValid": false
                    },
                    {
                        "id": 10604,
                        "content": "<p>Modify the security groups for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2536,
            "attributes": {
                "createdAt": "2023-09-09T20:39:46.551Z",
                "updatedAt": "2023-09-09T20:39:46.551Z",
                "content": "<p>A persistent database must be migrated from an on-premises server to an Amazon EC2 instances. The database requires 64,000 IOPS and, if possible, should be stored on a single Amazon EBS volume.</p><p>Which solution should a Solutions Architect recommend?</p>",
                "answerExplanation": "<p>Amazon EC2 Nitro-based systems are not required for this solution but do offer advantages in performance that will help to maximize the usage of the EBS volume. For the data storage volume an i01 volume can support up to 64,000 IOPS so a single volume with sufficient capacity (50 IOPS per GiB) can be deliver the requirements.</p><p>The current list of EBS volume types is in the table below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_10-17-09-93913bab7527ec0862c9e21624ba7869.jpg\"></p><p><strong>CORRECT: </strong>\"Create a Nitro-based Amazon EC2 instance with an Amazon EBS Provisioned IOPS SSD (i01) volume attached. Provision 64,000 IOPS for the volume\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an instance from the I3 I/O optimized family and leverage instance store storage to achieve the IOPS requirement\" is incorrect.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EC2 instance with four Amazon EBS General Purpose SSD (gp2) volumes attached. Max out the IOPS on each volume and use a RAID 0 stripe set\" is incorrect. This is not a good use case for gp2 volumes. It is much better to use io1 which also meets the requirement of having a single volume with 64,000 IOPS.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EC2 instance with two Amazon EBS Provisioned IOPS SSD (i01) volumes attached. Provision 32,000 IOPS per volume and create a logical volume using the OS that aggregates the capacity\" is incorrect. There is no need to create two volumes and aggregate capacity through the OS, the Solutions Architect can simply create a single volume with 64,000 IOPS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 10605,
                        "content": "<p>Create an Amazon EC2 instance with two Amazon EBS Provisioned IOPS SSD (i01) volumes attached. Provision 32,000 IOPS per volume and create a logical volume using the OS that aggregates the capacity.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10606,
                        "content": "<p>Create an Amazon EC2 instance with four Amazon EBS General Purpose SSD (gp2) volumes attached. Max out the IOPS on each volume and use a RAID 0 stripe set.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10607,
                        "content": "<p>Use an instance from the I3 I/O optimized family and leverage instance store storage to achieve the IOPS requirement.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10608,
                        "content": "<p>Create a Nitro-based Amazon EC2 instance with an Amazon EBS Provisioned IOPS SSD (i01) volume attached. Provision 64,000 IOPS for the volume.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2537,
            "attributes": {
                "createdAt": "2023-09-09T20:39:46.660Z",
                "updatedAt": "2023-09-09T20:39:46.660Z",
                "content": "<p>An application running on Amazon EC2 needs to asynchronously invoke an AWS Lambda function to perform data processing. The services should be decoupled.</p><p>Which service can be used to decouple the compute services?</p>",
                "answerExplanation": "<p>You can use a Lambda function to process Amazon Simple Notification Service notifications. Amazon SNS supports Lambda functions as a target for messages sent to a topic. This solution decouples the Amazon EC2 application from Lambda and ensures the Lambda function is invoked.</p><p><strong>CORRECT: </strong>\"Amazon SNS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Config\" is incorrect. AWS Config is a service that is used for continuous compliance, not application decoupling.</p><p><strong>INCORRECT:</strong> \"Amazon MQ\" is incorrect. Amazon MQ is similar to SQS but is used for existing applications that are being migrated into AWS. SQS should be used for new applications being created in the cloud.</p><p><strong>INCORRECT:</strong> \"AWS Step Functions\" is incorrect. AWS Step Functions is a workflow service. It is not the best solution for this scenario.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html</a></p><p><a href=\"https://aws.amazon.com/sns/features/\">https://aws.amazon.com/sns/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 10609,
                        "content": "<p>AWS Step Functions</p>",
                        "isValid": false
                    },
                    {
                        "id": 10610,
                        "content": "<p>Amazon MQ</p>",
                        "isValid": false
                    },
                    {
                        "id": 10611,
                        "content": "<p>AWS Config</p>",
                        "isValid": false
                    },
                    {
                        "id": 10612,
                        "content": "<p>Amazon SNS</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2538,
            "attributes": {
                "createdAt": "2023-09-09T20:39:46.772Z",
                "updatedAt": "2023-09-09T20:39:46.772Z",
                "content": "<p>A new application is to be published in multiple regions around the world. The Architect needs to ensure only 2 IP addresses need to be whitelisted. The solution should intelligently route traffic for lowest latency and provide fast regional failover.</p><p>How can this be achieved?</p>",
                "answerExplanation": "<p>AWS Global Accelerator uses the vast, congestion-free AWS global network to route TCP and UDP traffic to a healthy application endpoint in the closest AWS Region to the user.</p><p>This means it will intelligently route traffic to the closest point of presence (reducing latency). Seamless failover is ensured as AWS Global Accelerator uses anycast IP address which means the IP does not change when failing over between regions so there are no issues with client caches having incorrect entries that need to expire.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_00-46-55-314c6149f4e1a3552921ab6fa213d8d8.jpg\"></p><p>This is the only solution that provides deterministic failover.</p><p><strong>CORRECT: </strong>\"Launch EC2 instances into multiple regions behind an NLB and use AWS Global Accelerator\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Launch EC2 instances into multiple regions behind an NLB with a static IP address\" is incorrect. An NLB with a static IP is a workable solution as you could configure a primary and secondary address in applications. However, this solution does not intelligently route traffic for lowest latency.</p><p><strong>INCORRECT:</strong> \"Launch EC2 instances into multiple regions behind an ALB and use a Route 53 failover routing policy\" is incorrect. A Route 53 failover routing policy uses a primary and standby configuration. Therefore, it sends all traffic to the primary until it fails a health check at which time it sends traffic to the secondary. This solution does not intelligently route traffic for lowest latency.</p><p><strong>INCORRECT:</strong> \"Launch EC2 instances into multiple regions behind an ALB and use Amazon CloudFront with a pair of static IP addresses\" is incorrect. Amazon CloudFront cannot be configured with “a pair of static IP addresses”.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p><p><a href=\"https://aws.amazon.com/global-accelerator/faqs/\">https://aws.amazon.com/global-accelerator/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html\">https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-global-accelerator/\">https://digitalcloud.training/aws-global-accelerator/</a></p>",
                "options": [
                    {
                        "id": 10613,
                        "content": "<p>Launch EC2 instances into multiple regions behind an ALB and use a Route 53 failover routing policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 10614,
                        "content": "<p>Launch EC2 instances into multiple regions behind an NLB and use AWS Global Accelerator</p>",
                        "isValid": true
                    },
                    {
                        "id": 10615,
                        "content": "<p>Launch EC2 instances into multiple regions behind an ALB and use Amazon CloudFront with a pair of static IP addresses</p>",
                        "isValid": false
                    },
                    {
                        "id": 10616,
                        "content": "<p>Launch EC2 instances into multiple regions behind an NLB with a static IP address</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2539,
            "attributes": {
                "createdAt": "2023-09-09T20:39:46.887Z",
                "updatedAt": "2023-09-09T20:39:46.887Z",
                "content": "<p>A company runs a large batch processing job at the end of every quarter. The processing job runs for 5 days and uses 15 Amazon EC2 instances. The processing must run uninterrupted for 5 hours per day. The company is investigating ways to reduce the cost of the batch processing job.</p><p>Which pricing model should the company choose?</p>",
                "answerExplanation": "<p>Each EC2 instance runs for 5 hours a day for 5 days per quarter or 20 days per year. This is time duration is insufficient to warrant reserved instances as these require a commitment of a minimum of 1 year and the discounts would not outweigh the costs of having the reservations unused for a large percentage of time. In this case, there are no options presented that can reduce the cost and therefore on-demand instances should be used.</p><p><strong>CORRECT: </strong>\"On-Demand Instances\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Reserved Instances\" is incorrect. Reserved instances are good for continuously running workloads that run for a period of 1 or 3 years.</p><p><strong>INCORRECT:</strong> \"Spot Instances\" is incorrect. Spot instances may be interrupted and this is not acceptable. Note that Spot Block is deprecated and unavailable to new customers.</p><p><strong>INCORRECT:</strong> \"Dedicated Instances\" is incorrect. These do not provide any cost advantages and will be more expensive.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 10617,
                        "content": "<p>Dedicated Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10618,
                        "content": "<p>On-Demand Instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 10619,
                        "content": "<p>Reserved Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10620,
                        "content": "<p>Spot Instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2540,
            "attributes": {
                "createdAt": "2023-09-09T20:39:46.995Z",
                "updatedAt": "2023-09-09T20:39:46.995Z",
                "content": "<p>A company is migrating from an on-premises infrastructure to the AWS Cloud. One of the company's applications stores files on a Windows file server farm that uses Distributed File System Replication (DFSR) to keep data in sync. A solutions architect needs to replace the file server farm.</p><p>Which service should the solutions architect use?</p>",
                "answerExplanation": "<p>Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol.</p><p>Amazon FSx is built on Windows Server and provides a rich set of administrative features that include end-user file restore, user quotas, and Access Control Lists (ACLs).</p><p>Additionally, Amazon FSX for Windows File Server supports Distributed File System Replication (DFSR) in Single-AZ deployments as can be seen in the feature comparison table below.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-17_14-56-37-c7a7eff7f9da52f8dadafb160c3ac4c0.JPG\"></p><p><strong>CORRECT: </strong>\"Amazon FSx\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EFS\" is incorrect as EFS only supports Linux systems.</p><p><strong>INCORRECT:</strong> \"Amazon S3\" is incorrect as this is not a suitable replacement for a Microsoft filesystem.</p><p><strong>INCORRECT:</strong> \"AWS Storage Gateway\" is incorrect as this service is primarily used for connecting on-premises storage to cloud storage. It consists of a software device installed on-premises and can be used with SMB shares but it actually stores the data on S3. It is also used for migration. However, in this case the company need to replace the file server farm and Amazon FSx is the best choice for this job.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multiAZ.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multiAZ.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 10621,
                        "content": "<p>Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 10622,
                        "content": "<p>AWS Storage Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 10623,
                        "content": "<p>Amazon EFS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10624,
                        "content": "<p>Amazon FSx</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2541,
            "attributes": {
                "createdAt": "2023-09-09T20:39:47.088Z",
                "updatedAt": "2023-09-09T20:39:47.088Z",
                "content": "<p>An Amazon VPC contains several Amazon EC2 instances. The instances need to make API calls to Amazon DynamoDB. A solutions architect needs to ensure that the API calls do not traverse the internet.</p><p>How can this be accomplished? (Select TWO.)</p>",
                "answerExplanation": "<p>Amazon DynamoDB and Amazon S3 support gateway endpoints, not interface endpoints. With a gateway endpoint you create the endpoint in the VPC, attach a policy allowing access to the service, and then specify the route table to create a route table entry in.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-00-45-ac665c89acb1641afb831f1eb795210e.jpg\"></p><p><strong>CORRECT: </strong>\"Create a route table entry for the endpoint\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Create a gateway endpoint for DynamoDB\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create a new DynamoDB table that uses the endpoint\" is incorrect as it is not necessary to create a new DynamoDB table.</p><p><strong>INCORRECT:</strong> \"Create an ENI for the endpoint in each of the subnets of the VPC\" is incorrect as an ENI is used by an interface endpoint, not a gateway endpoint.</p><p><strong>INCORRECT:</strong> \"Create a VPC peering connection between the VPC and DynamoDB\" is incorrect as you cannot create a VPC peering connection between a VPC and a public AWS service as public services are outside of VPCs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 10625,
                        "content": "<p>Create a route table entry for the endpoint</p>",
                        "isValid": true
                    },
                    {
                        "id": 10626,
                        "content": "<p>Create an ENI for the endpoint in each of the subnets of the VPC</p>",
                        "isValid": false
                    },
                    {
                        "id": 10627,
                        "content": "<p>Create a VPC peering connection between the VPC and DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 10628,
                        "content": "<p>Create a gateway endpoint for DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 10629,
                        "content": "<p>Create a new DynamoDB table that uses the endpoint</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2542,
            "attributes": {
                "createdAt": "2023-09-09T20:39:47.183Z",
                "updatedAt": "2023-09-09T20:39:47.183Z",
                "content": "<p>An eCommerce company runs an application on Amazon EC2 instances in public and private subnets. The web application runs in a public subnet and the database runs in a private subnet. Both the public and private subnets are in a single Availability Zone.</p><p>Which combination of steps should a solutions architect take to provide high availability for this architecture? (Select TWO.)</p>",
                "answerExplanation": "<p>High availability can be achieved by using multiple Availability Zones within the same VPC. An EC2 Auto Scaling group can then be used to launch web application instances in multiple public subnets across multiple AZs and an ALB can be used to distribute incoming load.</p><p>The database solution can be made highly available by migrating from EC2 to Amazon RDS and using a Multi-AZ deployment model. This will provide the ability to failover to another AZ in the event of a failure of the primary database or the AZ in which it runs.</p><p><strong>CORRECT: </strong>\"Create an EC2 Auto Scaling group and Application Load Balancer that spans across multiple AZs\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Create new public and private subnets in a different AZ. Migrate the database to an Amazon RDS multi-AZ deployment\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create new public and private subnets in the same AZ but in a different Amazon VPC\" is incorrect. You cannot use multiple VPCs for this solution as it would be difficult to manage and direct traffic (you can’t load balance across VPCs).</p><p><strong>INCORRECT:</strong> \"Create an EC2 Auto Scaling group in the public subnet and use an Application Load Balancer\" is incorrect. This does not achieve HA as you need multiple public subnets across multiple AZs.</p><p><strong>INCORRECT:</strong> \"Create new public and private subnets in a different AZ. Create a database using Amazon EC2 in one AZ\" is incorrect. The database solution is not HA in this answer option.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/ec2/autoscaling/\">https://aws.amazon.com/ec2/autoscaling/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 10630,
                        "content": "<p>Create an EC2 Auto Scaling group and Application Load Balancer that spans across multiple AZs.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10631,
                        "content": "<p>Create new public and private subnets in a different AZ. Migrate the database to an Amazon RDS multi-AZ deployment.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10632,
                        "content": "<p>Create new public and private subnets in a different AZ. Create a database using Amazon EC2 in one AZ.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10633,
                        "content": "<p>Create new public and private subnets in the same AZ but in a different Amazon VPC.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10634,
                        "content": "<p>Create an EC2 Auto Scaling group in the public subnet and use an Application Load Balancer.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2543,
            "attributes": {
                "createdAt": "2023-09-09T20:39:47.271Z",
                "updatedAt": "2023-09-09T20:39:47.271Z",
                "content": "<p>A solutions architect is designing the infrastructure to run an application on Amazon EC2 instances. The application requires high availability and must dynamically scale based on demand to be cost efficient.</p><p>What should the solutions architect do to meet these requirements?</p>",
                "answerExplanation": "<p>The Amazon EC2-based application must be highly available and elastically scalable. Auto Scaling can provide the elasticity by dynamically launching and terminating instances based on demand. This can take place across availability zones for high availability.</p><p>Incoming connections can be distributed to the instances by using an Application Load Balancer (ALB).</p><p><strong>CORRECT: </strong>\"Configure an Application Load Balancer in front of an Auto Scaling group to deploy instances to multiple Availability Zones\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon API Gateway API in front of an Auto Scaling group to deploy instances to multiple Availability Zones\" is incorrect as API gateway is not used for load balancing connections to Amazon EC2 instances.</p><p><strong>INCORRECT:</strong> \"Configure an Application Load Balancer in front of an Auto Scaling group to deploy instances to multiple Regions\" is incorrect as you cannot launch instances in multiple Regions from a single Auto Scaling group.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon CloudFront distribution in front of an Auto Scaling group to deploy instances to multiple Regions\" is incorrect as you cannot launch instances in multiple Regions from a single Auto Scaling group.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p><p><a href=\"https://aws.amazon.com/elasticloadbalancing/\">https://aws.amazon.com/elasticloadbalancing/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 10635,
                        "content": "<p>Configure an Amazon CloudFront distribution in front of an Auto Scaling group to deploy instances to multiple Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 10636,
                        "content": "<p>Configure an Amazon API Gateway API in front of an Auto Scaling group to deploy instances to multiple Availability Zones</p>",
                        "isValid": false
                    },
                    {
                        "id": 10637,
                        "content": "<p>Configure an Application Load Balancer in front of an Auto Scaling group to deploy instances to multiple Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 10638,
                        "content": "<p>Configure an Application Load Balancer in front of an Auto Scaling group to deploy instances to multiple Availability Zones</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2544,
            "attributes": {
                "createdAt": "2023-09-09T20:39:47.368Z",
                "updatedAt": "2023-09-09T20:39:47.368Z",
                "content": "<p>A company's web application is using multiple Amazon EC2 Linux instances and storing data on Amazon EBS volumes. The company is looking for a solution to increase the resiliency of the application in case of a failure.</p><p>What should a solutions architect do to meet these requirements?</p>",
                "answerExplanation": "<p>To increase the resiliency of the application the solutions architect can use Auto Scaling groups to launch and terminate instances across multiple availability zones based on demand. An application load balancer (ALB) can be used to direct traffic to the web application running on the EC2 instances.</p><p>Lastly, the Amazon Elastic File System (EFS) can assist with increasing the resilience of the application by providing a shared file system that can be mounted by multiple EC2 instances from multiple availability zones.</p><p><strong>CORRECT: </strong>\"Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Store data on Amazon EFS and mount a target on each instance\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Launch the application on EC2 instances in each Availability Zone. Attach EBS volumes to each EC2 instance\" is incorrect as the EBS volumes are single points of failure which are not shared with other instances.</p><p><strong>INCORRECT:</strong> \"Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Mount an instance store on each EC2 instance\" is incorrect as instance stores are ephemeral data stores which means data is lost when powered down. Also, instance stores cannot be shared between instances.</p><p><strong>INCORRECT:</strong> \"Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Store data using Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)\" is incorrect as there are data retrieval charges associated with this S3 tier. It is not a suitable storage tier for application files.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/\">https://docs.aws.amazon.com/efs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
                "options": [
                    {
                        "id": 10639,
                        "content": "<p>Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Store data on Amazon EFS and mount a target on each instance</p>",
                        "isValid": true
                    },
                    {
                        "id": 10640,
                        "content": "<p>Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Store data using Amazon S3 One Zone-Infrequent Access (S3 One Zone-A)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10641,
                        "content": "<p>Launch the application on EC2 instances in each Availability Zone. Attach EBS volumes to each EC2 instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10642,
                        "content": "<p>Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Mount an instance store on each EC2 instance</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2545,
            "attributes": {
                "createdAt": "2023-09-09T20:39:47.467Z",
                "updatedAt": "2023-09-09T20:39:47.467Z",
                "content": "<p>An organization want to share regular updates about their charitable work using static webpages. The pages are expected to generate a large amount of views from around the world. The files are stored in an Amazon S3 bucket. A solutions architect has been asked to design an efficient and effective solution.</p><p>Which action should the solutions architect take to accomplish this?</p>",
                "answerExplanation": "<p>Amazon CloudFront can be used to cache the files in edge locations around the world and this will improve the performance of the webpages.</p><p>To serve a static website hosted on Amazon S3, you can deploy a CloudFront distribution using one of these configurations:</p><p>Using a REST API endpoint as the origin with access restricted by an <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">origin access identity (OAI)</a></p><p>Using a website endpoint as the origin with anonymous (public) access allowed</p><p>Using a website endpoint as the origin with access restricted by a Referer header</p><p><strong>CORRECT: </strong>\"Use Amazon CloudFront with the S3 bucket as its origin\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Generate presigned URLs for the files\" is incorrect as this is used to restrict access which is not a requirement.</p><p><strong>INCORRECT:</strong> \"Use cross-Region replication to all Regions\" is incorrect as this does not provide a mechanism for directing users to the closest copy of the static webpages.</p><p><strong>INCORRECT:</strong> \"Use the geoproximity feature of Amazon Route 53\" is incorrect as this does not include a solution for having multiple copies of the data in different geographic lcoations.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 10643,
                        "content": "<p>Generate presigned URLs for the files</p>",
                        "isValid": false
                    },
                    {
                        "id": 10644,
                        "content": "<p>Use Amazon CloudFront with the S3 bucket as its origin</p>",
                        "isValid": true
                    },
                    {
                        "id": 10645,
                        "content": "<p>Use the geoproximity feature of Amazon Route 53</p>",
                        "isValid": false
                    },
                    {
                        "id": 10646,
                        "content": "<p>Use cross-Region replication to all Regions</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2546,
            "attributes": {
                "createdAt": "2023-09-09T20:39:47.554Z",
                "updatedAt": "2023-09-09T20:39:47.554Z",
                "content": "<p>A company runs an application that uses an Amazon RDS PostgreSQL database. The database is currently not encrypted. A Solutions Architect has been instructed that due to new compliance requirements all existing and new data in the database must be encrypted. The database experiences high volumes of changes and no data can be lost.</p><p>How can the Solutions Architect enable encryption for the database without incurring any data loss?</p>",
                "answerExplanation": "<p>You cannot change the encryption status of an existing RDS DB instance. Encryption must be specified when creating the RDS DB instance. The best way to encrypt an existing database is to take a snapshot, encrypt a copy of the snapshot and restore the snapshot to a new RDS DB instance. This results in an encrypted database that is a new instance. Applications must be updated to use the new RDS DB endpoint.</p><p>In this scenario as there is a high rate of change, the databases will be out of sync by the time the new copy is created and is functional. The best way to capture the changes between the source (unencrypted) and destination (encrypted) DB is to use AWS Database Migration Service (DMS) to synchronize the data.</p><p>The slide below depicts the process for encrypting an unencrypted RDS DB instance:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_10-43-00-2b10c6ed31c5f439b4d82b7c3dbf77e5.jpg\"></p><p><strong>CORRECT: </strong>\"Create a snapshot of the existing RDS DB instance. Create an encrypted copy of the snapshot. Create a new RDS DB instance from the encrypted snapshot and update the application. Use AWS DMS to synchronize data between the source and destination RDS DBs\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a snapshot of the existing RDS DB instance. Create an encrypted copy of the snapshot. Create a new RDS DB instance from the encrypted snapshot. Configure the application to use the new DB endpoint\" is incorrect. This answer creates an encrypted DB instance but does not synchronize the data.</p><p><strong>INCORRECT:</strong> \"Create an RDS read replica and specify an encryption key. Promote the encrypted read replica to primary. Update the application to point to the new RDS DB endpoint\" is incorrect. You cannot create an encrypted read replica of an unencrypted RDS DB. The read replica will always have the same encryption status as the RDS DB it is created from.</p><p><strong>INCORRECT:</strong> \"Update the RDS DB to Multi-AZ mode and enable encryption for the standby replica. Perform a failover to the standby instance and then delete the unencrypted RDS DB instance\" is incorrect. You also cannot have an encrypted Multi-AZ standby instance of an unencrypted RDS DB.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 10647,
                        "content": "<p>Create a snapshot of the existing RDS DB instance. Create an encrypted copy of the snapshot. Create a new RDS DB instance from the encrypted snapshot. Configure the application to use the new DB endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10648,
                        "content": "<p>Create an RDS read replica and specify an encryption key. Promote the encrypted read replica to primary. Update the application to point to the new RDS DB endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10649,
                        "content": "<p>Create a snapshot of the existing RDS DB instance. Create an encrypted copy of the snapshot. Create a new RDS DB instance from the encrypted snapshot and update the application. Use AWS DMS to synchronize data between the source and destination RDS DBs.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10650,
                        "content": "<p>Update the RDS DB to Multi-AZ mode and enable encryption for the standby replica. Perform a failover to the standby instance and then delete the unencrypted RDS DB instance.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2547,
            "attributes": {
                "createdAt": "2023-09-09T20:39:47.658Z",
                "updatedAt": "2023-09-09T20:39:47.658Z",
                "content": "<p>A company offers an online product brochure that is delivered from a static website running on Amazon S3. The company’s customers are mainly in the United States, Canada, and Europe. The company is looking to cost-effectively reduce the latency for users in these regions.</p><p>What is the most cost-effective solution to these requirements?</p>",
                "answerExplanation": "<p>With Amazon CloudFront you can set the price class to determine where in the world the content will be cached. One of the price classes is “U.S, Canada and Europe” and this is where the company’s users are located. Choosing this price class will result in lower costs and better performance for the company’s users.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudFront distribution and set the price class to use only U.S, Canada and Europe.\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution and set the price class to use all Edge Locations for best performance\" is incorrect. This will be more expensive as it will cache content in Edge Locations all over the world.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution that uses origins in U.S, Canada and Europe\" is incorrect. The origin can be in one place, there’s no need to add origins in different Regions. The price class should be used to limit the caching of the content to reduce cost.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution and use Lambda@Edge to run the website's data processing closer to the users\" is incorrect. Lambda@Edge will not assist in this situation as there is no data processing required, the content from the static website must simply be cached at an edge location.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 10651,
                        "content": "<p>Create an Amazon CloudFront distribution and set the price class to use all Edge Locations for best performance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10652,
                        "content": "<p>Create an Amazon CloudFront distribution and use Lambda@Edge to run the website's data processing closer to the users.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10653,
                        "content": "<p>Create an Amazon CloudFront distribution and set the price class to use only U.S, Canada and Europe.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10654,
                        "content": "<p>Create an Amazon CloudFront distribution that uses origins in U.S, Canada and Europe.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2548,
            "attributes": {
                "createdAt": "2023-09-09T20:39:47.751Z",
                "updatedAt": "2023-09-09T20:39:47.751Z",
                "content": "<p>An Amazon S3 bucket in the us-east-1 Region hosts the static website content of a company. The content is made available through an Amazon CloudFront origin pointing to that bucket. A second copy of the bucket is created in the ap-southeast-1 Region using cross-region replication. The chief solutions architect wants a solution that provides greater availability for the website.</p><p>Which combination of actions should a solutions architect take to increase availability? (Select TWO.)</p>",
                "answerExplanation": "<p>You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an <em>origin group</em> with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin.</p><p><strong>CORRECT: </strong>\"Add an origin for ap-southeast-1 to CloudFront” is the correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Using us-east-1 bucket as the primary bucket and ap-southeast-1 bucket as the secondary bucket, create a CloudFront origin group” is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an origin for CloudFront for both buckets” is incorrect. This would not increase the availability of the solution on its own.</p><p><strong>INCORRECT:</strong> \"Set up failover routing in Amazon Route 53” is incorrect as we are trying to enable failover in CloudFront and using Route 53 is for routing domain names.</p><p><strong>INCORRECT:</strong> \"Create a record in Amazon Route 53 pointing to the replica bucket\" is incorrect as we are trying to enable failover in CloudFront and using Route 53 is for routing domain names.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 10655,
                        "content": "<p>Create an origin for CloudFront for both buckets.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10656,
                        "content": "<p>Add an origin for ap-southeast-1 to CloudFront.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10657,
                        "content": "<p>Set up failover routing in Amazon Route 53.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10658,
                        "content": "<p>Point Amazon Route 53 to the replica bucket by creating a record.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10659,
                        "content": "<p>Using us-east-1 bucket as the primary bucket and ap-southeast-1 bucket as the secondary bucket, create a CloudFront origin group.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2549,
            "attributes": {
                "createdAt": "2023-09-09T20:39:47.841Z",
                "updatedAt": "2023-09-09T20:39:47.841Z",
                "content": "<p>A company has uploaded some highly critical data to an Amazon S3 bucket. Management are concerned about data availability and require that steps are taken to protect the data from accidental deletion. The data should still be accessible, and a user should be able to delete the data intentionally.</p><p>Which combination of steps should a solutions architect take to accomplish this? (Select TWO.)</p>",
                "answerExplanation": "<p>Multi-factor authentication (MFA) delete adds an additional step before an object can be deleted from a versioning-enabled bucket.</p><p>With MFA delete the bucket owner must include the x-amz-mfa request header in requests to permanently delete an object version or change the versioning state of the bucket.</p><p><strong>CORRECT: </strong>\"Enable versioning on the S3 bucket\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Enable MFA Delete on the S3 bucket\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create a bucket policy on the S3 bucket\" is incorrect. A bucket policy is not required to enable MFA delete.</p><p><strong>INCORRECT:</strong> \"Enable default encryption on the S3 bucket\" is incorrect. Encryption does not protect against deletion.</p><p><strong>INCORRECT:</strong> \"Create a lifecycle policy for the objects in the S3 bucket\" is incorrect. A lifecycle policy will move data to another storage class but does not protect against deletion.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 10660,
                        "content": "<p>Create a bucket policy on the S3 bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10661,
                        "content": "<p>Enable MFA Delete on the S3 bucket.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10662,
                        "content": "<p>Enable versioning on the S3 bucket.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10663,
                        "content": "<p>Create a lifecycle policy for the objects in the S3 bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10664,
                        "content": "<p>Enable default encryption on the S3 bucket.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2550,
            "attributes": {
                "createdAt": "2023-09-09T20:39:47.951Z",
                "updatedAt": "2023-09-09T20:39:47.951Z",
                "content": "<p>A company hosts an application on Amazon EC2 instances behind Application Load Balancers in several AWS Regions. Distribution rights for the content require that users in different geographies must be served content from specific regions.</p><p>Which configuration meets these requirements?</p>",
                "answerExplanation": "<p>To protect the distribution rights of the content and ensure that users are directed to the appropriate AWS Region based on the location of the user, the geolocation routing policy can be used with Amazon Route 53.</p><p>Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from.</p><p>When you use geolocation routing, you can localize your content and present some or all of your website in the language of your users. You can also use geolocation routing to restrict distribution of content to only the locations in which you have distribution rights.</p><p><strong>CORRECT: </strong>\"Create Amazon Route 53 records with a geolocation routing policy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create Amazon Route 53 records with a geoproximity routing policy\" is incorrect. Use this routing policy when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.</p><p><strong>INCORRECT:</strong> \"Configure Amazon CloudFront with multiple origins and AWS WAF\" is incorrect. AWS WAF protects against web exploits but will not assist with directing users to different content (from different origins).</p><p><strong>INCORRECT:</strong> \"Configure Application Load Balancers with multi-Region routing\" is incorrect. There is no such thing as multi-Region routing for ALBs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
                "options": [
                    {
                        "id": 10665,
                        "content": "<p>Create Amazon Route 53 records with a geolocation routing policy.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10666,
                        "content": "<p>Configure Application Load Balancers with multi-Region routing.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10667,
                        "content": "<p>Create Amazon Route 53 records with a geoproximity routing policy.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10668,
                        "content": "<p>Configure Amazon CloudFront with multiple origins and AWS WAF.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2551,
            "attributes": {
                "createdAt": "2023-09-09T20:39:48.038Z",
                "updatedAt": "2023-09-09T20:39:48.038Z",
                "content": "<p>A Microsoft Windows file server farm uses Distributed File System Replication (DFSR) to synchronize data in an on-premises environment. The infrastructure is being migrated to the AWS Cloud.</p><p>Which service should the solutions architect use to replace the file server farm?</p>",
                "answerExplanation": "<p>Amazon FSx for Windows file server supports DFS namespaces and DFS replication. This is the best solution for replacing the on-premises infrastructure. Note the limitations for deployment:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-11-27_09-52-13-238b160c6f54f8650622a66a33fc34a4.jpg\"></p><p><strong>CORRECT: </strong>\"Amazon FSx\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EFS\" is incorrect. You cannot replace a Windows file server farm with EFS as it uses a completely different protocol.</p><p><strong>INCORRECT:</strong> \"Amazon EBS\" is incorrect. Amazon EBS provides block-based volumes that are attached to EC2 instances. It cannot be used for replacing a shared Windows file server farm using DFSR.</p><p><strong>INCORRECT:</strong> \"AWS Storage Gateway\" is incorrect. This service is used for providing cloud storage solutions for on-premises servers. In this case the infrastructure is being migrated into the AWS Cloud.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multiAZ.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multiAZ.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 10669,
                        "content": "<p>AWS Storage Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 10670,
                        "content": "<p>Amazon EBS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10671,
                        "content": "<p>Amazon FSx</p>",
                        "isValid": true
                    },
                    {
                        "id": 10672,
                        "content": "<p>Amazon EFS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2552,
            "attributes": {
                "createdAt": "2023-09-09T20:39:48.120Z",
                "updatedAt": "2023-09-09T20:39:48.120Z",
                "content": "<p>A financial services company has a web application with an application tier running in the U.S and Europe. The database tier consists of a MySQL database running on Amazon EC2 in us-west-1. Users are directed to the closest application tier using Route 53 latency-based routing. The users in Europe have reported poor performance when running queries.</p><p>Which changes should a Solutions Architect make to the database tier to improve performance?</p>",
                "answerExplanation": "<p>Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.</p><p>A global database can be configured in the European region and then the application tier in Europe will need to be configured to use the local database for reads/queries. The diagram below depicts an Aurora Global Database deployment.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_10-40-32-e096373484158f919b4e661ab7c1aa8d.jpg\"></p><p><strong>CORRECT: </strong>\"Migrate the database to an Amazon Aurora global database in MySQL compatibility mode. Configure the application tier in Europe to use the local reader endpoint\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Migrate the database to Amazon RDS for MySQL. Configure Multi-AZ in one of the European Regions\" is incorrect. You cannot configure a multi-AZ DB instance to run in another Region, it must be in the same Region but in a different Availability Zone.</p><p><strong>INCORRECT:</strong> \"Migrate the database to Amazon RedShift. Use AWS DMS to synchronize data. Configure applications to use the RedShift data warehouse for queries\" is incorrect. RedShift is a data warehouse and used for running analytics queries on data that is exported from transactional database systems. It should not be used to reduce latency for users of a database, and is not a live copy of the data.</p><p><strong>INCORRECT:</strong> \"Create an Amazon RDS Read Replica in one of the European regions. Configure the application tier in Europe to use the read replica for queries\" is incorrect. You cannot create an RDS Read Replica of a database that is running on Amazon EC2. You can only create read replicas of databases running on Amazon RDS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 10673,
                        "content": "<p>Create an Amazon RDS Read Replica in one of the European regions. Configure the application tier in Europe to use the read replica for queries.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10674,
                        "content": "<p>Migrate the database to Amazon RedShift. Use AWS DMS to synchronize data. Configure applications to use the RedShift data warehouse for queries.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10675,
                        "content": "<p>Migrate the database to Amazon RDS for MySQL. Configure Multi-AZ in one of the European Regions.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10676,
                        "content": "<p>Migrate the database to an Amazon Aurora global database in MySQL compatibility mode. Configure the application tier in Europe to use the local reader endpoint.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2553,
            "attributes": {
                "createdAt": "2023-09-09T20:39:48.216Z",
                "updatedAt": "2023-09-09T20:39:48.216Z",
                "content": "<p>An application running on an Amazon ECS container instance using the EC2 launch type needs permissions to write data to Amazon DynamoDB.</p><p>How can you assign these permissions only to the specific ECS task that is running the application?</p>",
                "answerExplanation": "<p>To specify permissions for a specific task on Amazon ECS you should use IAM Roles for Tasks. The permissions policy can be applied to tasks when creating the task definition, or by using an IAM task role override using the AWS CLI or SDKs. The <em>taskRoleAr</em>n parameter is used to specify the policy.</p><p><strong>CORRECT: </strong>\"Create an IAM policy with permissions to DynamoDB and assign It to a task using the <em>taskRoleArn</em> parameter\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy with permissions to DynamoDB and attach it to the container instance\" is incorrect. You should not apply the permissions to the container instance as they will then apply to all tasks running on the instance as well as the instance itself.</p><p><strong>INCORRECT:</strong> \"Use a security group to allow outbound connections to DynamoDB and assign it to the container instance\" is incorrect. Though you will need a security group to allow outbound connections to DynamoDB, the question is asking how to assign permissions to write data to DynamoDB and a security group cannot provide those permissions.</p><p><strong>INCORRECT:</strong> \"Modify the <em>AmazonECSTaskExecutionRolePolicy</em> policy to add permissions for DynamoDB\" is incorrect. The <em>AmazonECSTaskExecutionRolePolicy</em> policy is the Task Execution IAM Role. This is used by the container agent to be able to pull container images, write log file etc.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 10677,
                        "content": "<p>Use a security group to allow outbound connections to DynamoDB and assign it to the container instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10678,
                        "content": "<p>Create an IAM policy with permissions to DynamoDB and attach it to the container instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10679,
                        "content": "<p>Create an IAM policy with permissions to DynamoDB and assign It to a task using the <em>taskRoleArn</em> parameter</p>",
                        "isValid": true
                    },
                    {
                        "id": 10680,
                        "content": "<p>Modify the <em>AmazonECSTaskExecutionRolePolicy</em> policy to add permissions for DynamoDB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2554,
            "attributes": {
                "createdAt": "2023-09-09T20:39:48.359Z",
                "updatedAt": "2023-09-09T20:39:48.359Z",
                "content": "<p>A company uses an Amazon RDS MySQL database instance to store customer order data. The security team have requested that SSL/TLS encryption in transit must be used for encrypting connections to the database from application servers. The data in the database is currently encrypted at rest using an AWS KMS key.</p><p>How can a Solutions Architect enable encryption in transit?</p>",
                "answerExplanation": "<p>Amazon RDS creates an SSL certificate and installs the certificate on the DB instance when Amazon RDS provisions the instance. These certificates are signed by a certificate authority. The SSL certificate includes the DB instance endpoint as the Common Name (CN) for the SSL certificate to guard against spoofing attacks.</p><p>You can download a root certificate from AWS that works for all Regions or you can download Region-specific intermediate certificates.</p><p><strong>CORRECT: </strong>\"Download the AWS-provided root certificates. Use the certificates when connecting to the RDS DB instance\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of the RDS instance. Restore the snapshot to a new instance with encryption in transit enabled\" is incorrect. There is no need to do this as a certificate is created when the DB instances is launched.</p><p><strong>INCORRECT:</strong> \"Enable encryption in transit using the RDS Management console and obtain a key using AWS KMS\" is incorrect. You cannot enable/disable encryption in transit using the RDS management console or use a KMS key.</p><p><strong>INCORRECT:</strong> \"Add a self-signed certificate to the RDS DB instance. Use the certificates in all connections to the RDS DB instance\" is incorrect. You cannot use self-signed certificates with RDS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 10681,
                        "content": "<p>Download the AWS-provided root certificates. Use the certificates when connecting to the RDS DB instance.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10682,
                        "content": "<p>Add a self-signed certificate to the RDS DB instance. Use the certificates in all connections to the RDS DB instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10683,
                        "content": "<p>Take a snapshot of the RDS instance. Restore the snapshot to a new instance with encryption in transit enabled.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10684,
                        "content": "<p>Enable encryption in transit using the RDS Management console and obtain a key using AWS KMS.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2555,
            "attributes": {
                "createdAt": "2023-09-09T20:39:48.457Z",
                "updatedAt": "2023-09-09T20:39:48.457Z",
                "content": "<p>A company has two accounts for perform testing and each account has a single VPC: VPC-TEST1 and VPC-TEST2. The operations team require a method of securely copying files between Amazon EC2 instances in these VPCs. The connectivity should not have any single points of failure or bandwidth constraints.</p><p>Which solution should a Solutions Architect recommend?</p>",
                "answerExplanation": "<p>A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network.</p><p>You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection).</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_09-50-39-d2cdb66079d7dfcc2c83b098f21eafa2.jpg\"></p><p><strong>CORRECT: </strong>\"Create a VPC peering connection between VPC-TEST1 and VPC-TEST2\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a VPC gateway endpoint for each EC2 instance and update route tables\" is incorrect. You cannot create VPC gateway endpoints for Amazon EC2 instances. These are used with DynamoDB and S3 only.</p><p><strong>INCORRECT:</strong> \"Attach a virtual private gateway to VPC-TEST1 and VPC-TEST2 and enable routing\" is incorrect. You cannot create an AWS Managed VPN connection between two VPCs.</p><p><strong>INCORRECT:</strong> \"Attach a Direct Connect gateway to VPC-TEST1 and VPC-TEST2 and enable routing\" is incorrect. Direct Connect gateway is used to connect a Direct Connect connection to multiple VPCs, it is not useful in this scenario as there is no Direct Connect connection.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\">https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 10685,
                        "content": "<p>Create a VPC peering connection between VPC-TEST1 and VPC-TEST2.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10686,
                        "content": "<p>Attach a Direct Connect gateway to VPC-TEST1 and VPC-TEST2 and enable routing.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10687,
                        "content": "<p>Create a VPC gateway endpoint for each EC2 instance and update route tables.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10688,
                        "content": "<p>Attach a virtual private gateway to VPC-TEST1 and VPC-TEST2 and enable routing.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2556,
            "attributes": {
                "createdAt": "2023-09-09T20:39:48.555Z",
                "updatedAt": "2023-09-09T20:39:48.555Z",
                "content": "<p>Storage capacity has become an issue for a company that runs application servers on-premises. The servers are connected to a combination of block storage and NFS storage solutions. The company requires a solution that supports local caching without re-architecting its existing applications.</p><p>Which combination of changes can the company make to meet these requirements? (Select TWO.)</p>",
                "answerExplanation": "<p>In this scenario the company should use cloud storage to replace the existing storage solutions that are running out of capacity. The on-premises servers mount the existing storage using block protocols (iSCSI) and file protocols (NFS). As there is a requirement to avoid re-architecting existing applications these protocols must be used in the revised solution.</p><p>The AWS Storage Gateway volume gateway should be used to replace the block-based storage systems as it is mounted over iSCSI and the file gateway should be used to replace the NFS file systems as it uses NFS.</p><p><strong>CORRECT: </strong>\"Use an AWS Storage Gateway file gateway to replace the NFS storage\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Use an AWS Storage Gateway volume gateway to replace the block storage\" is a correct answer.</p><p><strong>INCORRECT:</strong> \"Use the mount command on servers to mount Amazon S3 buckets using NFS\" is incorrect. You cannot mount S3 buckets using NFS as it is an object-based storage system (not file-based) and uses an HTTP REST API.</p><p><strong>INCORRECT:</strong> \"Use AWS Direct Connect and mount an Amazon FSx for Windows File Server using iSCSI\" is incorrect. You cannot mount FSx for Windows File Server file systems using iSCSI, you must use SMB.</p><p><strong>INCORRECT:</strong> \"Use Amazon Elastic File System (EFS) volumes to replace the block storage\" is incorrect. You cannot use EFS to replace block storage as it uses NFS rather than iSCSI.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/\">https://docs.aws.amazon.com/storagegateway/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>",
                "options": [
                    {
                        "id": 10689,
                        "content": "<p>Use an AWS Storage Gateway file gateway to replace the NFS storage.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10690,
                        "content": "<p>Use the mount command on servers to mount Amazon S3 buckets using NFS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10691,
                        "content": "<p>Use AWS Direct Connect and mount an Amazon FSx for Windows File Server using iSCSI.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10692,
                        "content": "<p>Use an AWS Storage Gateway volume gateway to replace the block storage.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10693,
                        "content": "<p>Use Amazon Elastic File System (EFS) volumes to replace the block storage.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2557,
            "attributes": {
                "createdAt": "2023-09-09T20:39:48.646Z",
                "updatedAt": "2023-09-09T20:39:48.646Z",
                "content": "<p>A company runs containerized applications for many application workloads in an on-premise data center. The company is planning to deploy containers to AWS and the chief architect has mandated that the same configuration and administrative tools must be used across all containerized environments. The company also wishes to remain cloud agnostic to safeguard against the impact of future changes in cloud strategy.</p><p>How can a Solutions Architect design a managed solution that will align with open-source software?</p>",
                "answerExplanation": "<p>Amazon EKS is a managed service that can be used to run Kubernetes on AWS. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. Applications running on Amazon EKS are fully compatible with applications running on any standard Kubernetes environment, whether running in on-premises data centers or public clouds. This means that you can easily migrate any standard Kubernetes application to Amazon EKS without any code modification.</p><p>This solution ensures that the same open-source software is used for automating the deployment, scaling, and management of containerized applications both on-premises and in the AWS Cloud.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-11-27_09-55-33-2abdd31ee10dd8c28e19ec75e2a29309.jpg\"></p><p><strong>CORRECT: </strong>\"Launch the containers on Amazon Elastic Kubernetes Service (EKS) and EKS worker nodes\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Launch the containers on a fleet of Amazon EC2 instances in a cluster placement group\" is incorrect</p><p><strong>INCORRECT:</strong> \"Launch the containers on Amazon Elastic Container Service (ECS) with AWS Fargate instances\" is incorrect</p><p><strong>INCORRECT:</strong> \"Launch the containers on Amazon Elastic Container Service (ECS) with Amazon EC2 instance worker nodes\" is incorrect</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\">https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 10694,
                        "content": "<p>Launch the containers on Amazon Elastic Container Service (ECS) with Amazon EC2 instance worker nodes.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10695,
                        "content": "<p>Launch the containers on Amazon Elastic Kubernetes Service (EKS) and EKS worker nodes.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10696,
                        "content": "<p>Launch the containers on Amazon Elastic Container Service (ECS) with AWS Fargate instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10697,
                        "content": "<p>Launch the containers on a fleet of Amazon EC2 instances in a cluster placement group.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2558,
            "attributes": {
                "createdAt": "2023-09-09T20:39:48.741Z",
                "updatedAt": "2023-09-09T20:39:48.741Z",
                "content": "<p>A company runs an application on six web application servers in an Amazon EC2 Auto Scaling group in a single Availability Zone. The application is fronted by an Application Load Balancer (ALB). A Solutions Architect needs to modify the infrastructure to be highly available without making any modifications to the application.</p><p>Which architecture should the Solutions Architect choose to enable high availability?</p>",
                "answerExplanation": "<p>The only thing that needs to be changed in this scenario to enable HA is to split the instances across multiple Availability Zones. The architecture already uses Auto Scaling and Elastic Load Balancing so there is plenty of resilience to failure. Once the instances are running across multiple AZs there will be AZ-level fault tolerance as well.</p><p><strong>CORRECT: </strong>\"Modify the Auto Scaling group to use two instances across each of three Availability Zones\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution with a custom origin across multiple Regions\" is incorrect. CloudFront is not used to create HA for your application, it is used to accelerate access to media content.</p><p><strong>INCORRECT:</strong> \"Create a launch template that can be used to quickly create more instances in another Region\" is incorrect. Multi-AZ should be enabled rather than multi-Region.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group to launch three instances across each of two Regions\" is incorrect. HA can be achieved within a Region by simply enabling more AZs in the ASG. An ASG cannot launch instances in multiple Regions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 10698,
                        "content": "<p>Create a launch template that can be used to quickly create more instances in another Region.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10699,
                        "content": "<p>Modify the Auto Scaling group to use two instances across each of three Availability Zones.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10700,
                        "content": "<p>Create an Auto Scaling group to launch three instances across each of two Regions.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10701,
                        "content": "<p>Create an Amazon CloudFront distribution with a custom origin across multiple Regions.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2559,
            "attributes": {
                "createdAt": "2023-09-09T20:39:48.856Z",
                "updatedAt": "2023-09-09T20:39:48.856Z",
                "content": "<p>A company is investigating methods to reduce the expenses associated with on-premises backup infrastructure. The Solutions Architect wants to reduce costs by eliminating the use of physical backup tapes. It is a requirement that existing backup applications and workflows should continue to function.</p><p>What should the Solutions Architect recommend?</p>",
                "answerExplanation": "<p>The AWS Storage Gateway Tape Gateway enables you to replace using physical tapes on premises with virtual tapes in AWS without changing existing backup workflows. Tape Gateway emulates physical tape libraries, removes the cost and complexity of managing physical tape infrastructure, and provides more durability than physical tapes.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-11-27_09-45-15-153ae046c9838c5d880c4f0fce858218.jpg\"></p><p><strong>CORRECT: </strong>\"Connect the backup applications to an AWS Storage Gateway using an iSCSI-virtual tape library (VTL)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EFS file system and connect the backup applications using the NFS protocol\" is incorrect. The NFS protocol is used by AWS Storage Gateway File Gateways but these do not provide virtual tape functionality that is suitable for replacing the existing backup infrastructure.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EFS file system and connect the backup applications using the iSCSI protocol\" is incorrect. The NFS protocol is used by AWS Storage Gateway File Gateways but these do not provide virtual tape functionality that is suitable for replacing the existing backup infrastructure.</p><p><strong>INCORRECT:</strong> \"Connect the backup applications to an AWS Storage Gateway using the NFS protocol\" is incorrect. The iSCSI protocol is used by AWS Storage Gateway Volume Gateways but these do not provide virtual tape functionality that is suitable for replacing the existing backup infrastructure.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/storagegateway/vtl/\">https://aws.amazon.com/storagegateway/vtl/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>",
                "options": [
                    {
                        "id": 10702,
                        "content": "<p>Create an Amazon EFS file system and connect the backup applications using the NFS protocol.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10703,
                        "content": "<p>Connect the backup applications to an AWS Storage Gateway using the iSCSI protocol.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10704,
                        "content": "<p>Connect the backup applications to an AWS Storage Gateway using an iSCSI-virtual tape library (VTL).</p>",
                        "isValid": true
                    },
                    {
                        "id": 10705,
                        "content": "<p>Create an Amazon EFS file system and connect the backup applications using the iSCSI protocol.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2560,
            "attributes": {
                "createdAt": "2023-09-09T20:39:48.956Z",
                "updatedAt": "2023-09-09T20:39:48.956Z",
                "content": "<p>A company is deploying a fleet of Amazon EC2 instances running Linux across multiple Availability Zones within an AWS Region. The application requires a data storage solution that can be accessed by all of the EC2 instances simultaneously. The solution must be highly scalable and easy to implement. The storage must be mounted using the NFS protocol.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>Amazon EFS provides scalable file storage for use with Amazon EC2. You can use an EFS file system as a common data source for workloads and applications running on multiple instances. The EC2 instances can run in multiple AZs within a Region and the NFS protocol is used to mount the file system.</p><p>With EFS you can create mount targets in each AZ for lower latency. The application instances in each AZ will mount the file system using the local mount target.</p><p><strong>CORRECT: </strong>\"Create an Amazon EFS file system with mount targets in each Availability Zone. Configure the application instances to mount the file system\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon S3 bucket and create an S3 gateway endpoint to allow access to the file system using the NFS protocol\" is incorrect. You cannot use NFS with S3 or with gateway endpoints.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EBS volume and use EBS Multi-Attach to mount the volume to all EC2 instances across each Availability Zone\" is incorrect. You cannot use Amazon EBS Multi-Attach across multiple AZs.</p><p><strong>INCORRECT:</strong> \"Create an Amazon RDS database and store the data in a BLOB format. Point the application instances to the RDS endpoint\" is incorrect. This is not a suitable storage solution for a file system that is mounted over NFS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEFS.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEFS.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
                "options": [
                    {
                        "id": 10706,
                        "content": "<p>Create an Amazon EFS file system with mount targets in each Availability Zone. Configure the application instances to mount the file system.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10707,
                        "content": "<p>Create an Amazon EBS volume and use EBS Multi-Attach to mount the volume to all EC2 instances across each Availability Zone.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10708,
                        "content": "<p>Create an Amazon S3 bucket and create an S3 gateway endpoint to allow access to the file system using the NFS protocol.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10709,
                        "content": "<p>Create an Amazon RDS database and store the data in a BLOB format. Point the application instances to the RDS endpoint.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2561,
            "attributes": {
                "createdAt": "2023-09-09T20:39:49.046Z",
                "updatedAt": "2023-09-09T20:39:49.046Z",
                "content": "<p>A company is working with a strategic partner that has an application that must be able to send messages to one of the company’s Amazon SQS queues. The partner company has its own AWS account.</p><p>How can a Solutions Architect provide least privilege access to the partner?</p>",
                "answerExplanation": "<p>Amazon SQS supports resource-based policies. The best way to grant the permissions using the principle of least privilege is to use a resource-based policy attached to the SQS queue that grants the partner company’s AWS account the sqs:SendMessage privilege.</p><p>The following policy is an example of how this could be configured:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_10-46-16-3d75509bc08efb886557569410905bfe.jpg\"></p><p><strong>CORRECT: </strong>\"Update the permission policy on the SQS queue to grant the sqs:SendMessage permission to the partner’s AWS account\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a user account that and grant the sqs:SendMessage permission for Amazon SQS. Share the credentials with the partner company\" is incorrect. This would provide the permissions for all SQS queues, not just the queue the partner company should be able to access.</p><p><strong>INCORRECT:</strong> \"Create a cross-account role with access to all SQS queues and use the partner's AWS account in the trust document for the role\" is incorrect. This would provide access to all SQS queues and the partner company should only be able to access one SQS queue.</p><p><strong>INCORRECT:</strong> \"Update the permission policy on the SQS queue to grant all permissions to the partner’s AWS account\" is incorrect. This provides too many permissions; the partner company only needs to send messages to the queue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-examples-of-sqs-policies.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-examples-of-sqs-policies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 10710,
                        "content": "<p>Update the permission policy on the SQS queue to grant the sqs:SendMessage permission to the partner’s AWS account.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10711,
                        "content": "<p>Create a user account that and grant the sqs:SendMessage permission for Amazon SQS. Share the credentials with the partner company.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10712,
                        "content": "<p>Create a cross-account role with access to all SQS queues and use the partner's AWS account in the trust document for the role.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10713,
                        "content": "<p>Update the permission policy on the SQS queue to grant all permissions to the partner’s AWS account.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2562,
            "attributes": {
                "createdAt": "2023-09-09T20:39:49.132Z",
                "updatedAt": "2023-09-09T20:39:49.132Z",
                "content": "<p>A company runs an application in an on-premises data center that collects environmental data from production machinery. The data consists of JSON files stored on network attached storage (NAS) and around 5 TB of data is collected each day. The company must upload this data to Amazon S3 where it can be processed by an analytics application. The data must be transferred securely.</p><p>Which solution offers the MOST reliable and time-efficient data transfer?</p>",
                "answerExplanation": "<p>The most reliable and time-efficient solution that keeps the data secure is to use AWS DataSync and synchronize the data from the NAS device directly to Amazon S3. This should take place over an AWS Direct Connect connection to ensure reliability, speed, and security.</p><p>AWS DataSync can copy data between Network File System (NFS) shares, Server Message Block (SMB) shares, self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, and Amazon FSx for Windows File Server file systems.</p><p><strong>CORRECT: </strong>\"AWS DataSync over AWS Direct Connect\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Database Migration Service over the Internet\" is incorrect. DMS is for migrating databases, not files.</p><p><strong>INCORRECT:</strong> \"Amazon S3 Transfer Acceleration over the Internet\" is incorrect. The Internet does not offer the reliability, speed or performance that this company requires.</p><p><strong>INCORRECT:</strong> \"Multiple AWS Snowcone devices\" is incorrect. This is not a time-efficient approach as it can take time to ship these devices in both directions.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/datasync/\">https://aws.amazon.com/datasync/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 10714,
                        "content": "<p>Multiple AWS Snowcone devices.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10715,
                        "content": "<p>AWS DataSync over AWS Direct Connect.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10716,
                        "content": "<p>AWS Database Migration Service over the Internet.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10717,
                        "content": "<p>Amazon S3 Transfer Acceleration over the Internet.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2563,
            "attributes": {
                "createdAt": "2023-09-09T20:39:49.239Z",
                "updatedAt": "2023-09-09T20:39:49.239Z",
                "content": "<p>A company has deployed a new website on Amazon EC2 instances behind an Application Load Balancer (ALB). Amazon Route 53 is used for the DNS service. The company has asked a Solutions Architect to create a backup website with support contact details that users will be directed to automatically if the primary website is down.</p><p>How should the Solutions Architect deploy this solution cost-effectively?</p>",
                "answerExplanation": "<p>The most cost-effective solution is to create a static website using an Amazon S3 bucket and then use a failover routing policy in Amazon Route 53. With a failover routing policy users will be directed to the main website as long as it is responding to health checks successfully.</p><p>If the main website fails to respond to health checks (its down), Route 53 will begin to direct users to the backup website running on the Amazon S3 bucket. It’s important to set the TTL on the Route 53 records appropriately to ensure that users resolve the failover address within a short time.</p><p><strong>CORRECT: </strong>\"Configure a static website using Amazon S3 and create a Route 53 failover routing policy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure a static website using Amazon S3 and create a Route 53 weighted routing policy\" is incorrect. Weighted routing is used when you want to send a percentage of traffic between multiple endpoints. In this case all traffic should go to the primary until if fails, then all should go to the backup.</p><p><strong>INCORRECT:</strong> \"Deploy the backup website on EC2 and ALB in another Region and use Route 53 health checks for failover routing\" is incorrect. This is not a cost-effective solution for the backup website. It can be implemented using Route 53 failover routing which uses health checks but would be an expensive option.</p><p><strong>INCORRECT:</strong> \"Create the backup website on EC2 and ALB in another Region and create an AWS Global Accelerator endpoint\" is incorrect. Global Accelerator is used for performance as it directs traffic to the nearest healthy endpoint. It is not useful for failover in this scenario and is also a very expensive solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
                "options": [
                    {
                        "id": 10718,
                        "content": "<p>Configure a static website using Amazon S3 and create a Route 53 weighted routing policy.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10719,
                        "content": "<p>Create the backup website on EC2 and ALB in another Region and create an AWS Global Accelerator endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10720,
                        "content": "<p>Deploy the backup website on EC2 and ALB in another Region and use Route 53 health checks for failover routing.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10721,
                        "content": "<p>Configure a static website using Amazon S3 and create a Route 53 failover routing policy.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2564,
            "attributes": {
                "createdAt": "2023-09-09T20:39:49.347Z",
                "updatedAt": "2023-09-09T20:39:49.347Z",
                "content": "<p>A company wishes to restrict access to their Amazon DynamoDB table to specific, private source IP addresses from their VPC. What should be done to secure access to the table?</p>",
                "answerExplanation": "<p>There are two different types of VPC endpoint: interface endpoint, and gateway endpoint. With an interface endpoint you use an ENI in the VPC. With a gateway endpoint you configure your route table to point to the endpoint. Amazon S3 and DynamoDB use gateway endpoints. This solution means that all traffic will go through the VPC endpoint straight to DynamoDB using private IP addresses.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_00-48-44-654011ba439713ad1e049b259d7d5611.jpg\"></p><p><strong>CORRECT: </strong>\"Create a gateway VPC endpoint and add an entry to the route table\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an interface VPC endpoint in the VPC with an Elastic Network Interface (ENI)\" is incorrect. As mentioned above, an interface endpoint is not used for DynamoDB, you must use a gateway endpoint.</p><p><strong>INCORRECT:</strong> \"Create the Amazon DynamoDB table in the VPC\" is incorrect. You cannot create a DynamoDB table in a VPC, to connect securely using private addresses you should use a gateway endpoint instead.</p><p><strong>INCORRECT:</strong> \"Create an AWS VPN connection to the Amazon DynamoDB endpoint\" is incorrect. You cannot create an AWS VPN connection to the Amazon DynamoDB endpoint.</p><p><strong>References:</strong></p><p><a href=\"https://docs.amazonaws.cn/en_us/vpc/latest/userguide/vpc-endpoints-ddb.html\">https://docs.amazonaws.cn/en_us/vpc/latest/userguide/vpc-endpoints-ddb.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-restrict-calls-ip-addresses/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-restrict-calls-ip-addresses/</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-vpc-endpoints-for-dynamodb/\">https://aws.amazon.com/blogs/aws/new-vpc-endpoints-for-dynamodb/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 10722,
                        "content": "<p>Create an AWS VPN connection to the Amazon DynamoDB endpoint</p>",
                        "isValid": false
                    },
                    {
                        "id": 10723,
                        "content": "<p>Create a gateway VPC endpoint and add an entry to the route table</p>",
                        "isValid": true
                    },
                    {
                        "id": 10724,
                        "content": "<p>Create an interface VPC endpoint in the VPC with an Elastic Network Interface (ENI)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10725,
                        "content": "<p>Create the Amazon DynamoDB table in the VPC</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2565,
            "attributes": {
                "createdAt": "2023-09-09T20:39:49.456Z",
                "updatedAt": "2023-09-09T20:39:49.456Z",
                "content": "<p>A company hosts a multiplayer game on AWS. The application uses Amazon EC2 instances in a single Availability Zone and users connect over Layer 4. Solutions Architect has been tasked with making the architecture highly available and also more cost-effective.</p><p>How can the solutions architect best meet these requirements? (Select TWO.)</p>",
                "answerExplanation": "<p>The solutions architect must enable high availability for the architecture and ensure it is cost-effective. To enable high availability an Amazon EC2 Auto Scaling group should be created to add and remove instances across multiple availability zones.</p><p>In order to distribute the traffic to the instances the architecture should use a Network Load Balancer which operates at Layer 4. This architecture will also be cost-effective as the Auto Scaling group will ensure the right number of instances are running based on demand.</p><p><strong>CORRECT: </strong>\"Configure a Network Load Balancer in front of the EC2 instances\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Configure an Auto Scaling group to add or remove instances in multiple Availability Zones automatically\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Increase the number of instances and use smaller EC2 instance types\" is incorrect as this is not the most cost-effective option. Auto Scaling should be used to maintain the right number of active instances.</p><p><strong>INCORRECT:</strong> \"Configure an Auto Scaling group to add or remove instances in the Availability Zone automatically\" is incorrect as this is not highly available as it’s a single AZ.</p><p><strong>INCORRECT:</strong> \"Configure an Application Load Balancer in front of the EC2 instances\" is incorrect as an ALB operates at Layer 7 rather than Layer 4.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\">https://docsaws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 10726,
                        "content": "<p>Increase the number of instances and use smaller EC2 instance types</p>",
                        "isValid": false
                    },
                    {
                        "id": 10727,
                        "content": "<p>Configure a Network Load Balancer in front of the EC2 instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 10728,
                        "content": "<p>Configure an Auto Scaling group to add or remove instances in multiple Availability Zones automatically</p>",
                        "isValid": true
                    },
                    {
                        "id": 10729,
                        "content": "<p>Configure an Auto Scaling group to add or remove instances in the Availability Zone automatically</p>",
                        "isValid": false
                    },
                    {
                        "id": 10730,
                        "content": "<p>Configure an Application Load Balancer in front of the EC2 instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2566,
            "attributes": {
                "createdAt": "2023-09-09T20:39:49.560Z",
                "updatedAt": "2023-09-09T20:39:49.560Z",
                "content": "<p>A Solutions Architect has deployed an application on several Amazon EC2 instances across three private subnets. The application must be made accessible to internet-based clients with the least amount of administrative effort.</p><p>How can the Solutions Architect make the application available on the internet?</p>",
                "answerExplanation": "<p>To make the application instances accessible on the internet the Solutions Architect needs to place them behind an internet-facing Elastic Load Balancer. The way you add instances in private subnets to a public facing ELB is to add public subnets in the same AZs as the private subnets to the ELB. You can then add the instances and to the ELB and they will become targets for load balancing.</p><p>An example of this architecture is shown below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_10-09-08-4d552faa7e6a02f1057665490b64d36b.jpg\"></p><p><strong>CORRECT: </strong>\"Create an Application Load Balancer and associate three public subnets from the same Availability Zones as the private instances. Add the private instances to the ALB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Application Load Balancer and associate three private subnets from the same Availability Zones as the private instances. Add the private instances to the ALB\" is incorrect. Public subnets in the same AZs as the private subnets must be added to make this configuration work.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Machine Image (AMI) of the instances in the private subnet and launch new instances from the AMI in public subnets. Create an Application Load Balancer and add the public instances to the ALB\" is incorrect. There is no need to use an AMI to create new instances in a public subnet. You can add instances in private subnets to a public-facing ELB.</p><p><strong>INCORRECT:</strong> \"Create a NAT gateway in a public subnet. Add a route to the NAT gateway to the route tables of the three private subnets\" is incorrect. A NAT gateway is used for outbound traffic not inbound traffic and cannot make the application available to internet-based clients.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/\">https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 10731,
                        "content": "<p>Create a NAT gateway in a public subnet. Add a route to the NAT gateway to the route tables of the three private subnets.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10732,
                        "content": "<p>Create an Application Load Balancer and associate three private subnets from the same Availability Zones as the private instances. Add the private instances to the ALB.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10733,
                        "content": "<p>Create an Amazon Machine Image (AMI) of the instances in the private subnet and launch new instances from the AMI in public subnets. Create an Application Load Balancer and add the public instances to the ALB.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10734,
                        "content": "<p>Create an Application Load Balancer and associate three public subnets from the same Availability Zones as the private instances. Add the private instances to the ALB.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2567,
            "attributes": {
                "createdAt": "2023-09-09T20:39:49.659Z",
                "updatedAt": "2023-09-09T20:39:49.659Z",
                "content": "<p>A Solutions Architect has been tasked with re-deploying an application running on AWS to enable high availability. The application processes messages that are received in an ActiveMQ queue running on a single Amazon EC2 instance. Messages are then processed by a consumer application running on Amazon EC2. After processing the messages the consumer application writes results to a MySQL database running on Amazon EC2.</p><p>Which architecture offers the highest availability and low operational complexity?</p>",
                "answerExplanation": "<p>The correct answer offers the highest availability as it includes Amazon MQ active/standby brokers across two AZs, an Auto Scaling group across two AZ,s and a Multi-AZ Amazon RDS MySQL database deployment.</p><p>This architecture not only offers the highest availability it is also operationally simple as it maximizes the usage of managed services.</p><p><strong>CORRECT: </strong>\"Deploy Amazon MQ with active/standby brokers configured across two Availability Zones. Create an Auto Scaling group for the consumer EC2 instances across two Availability Zones. Use an Amazon RDS MySQL database with Multi-AZ enabled\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Deploy a second Active MQ server to another Availability Zone. Launch an additional consumer EC2 instance in another Availability Zone. Use MySQL database replication to another Availability Zone\" is incorrect. This architecture does not offer the highest availability as it does not use Auto Scaling. It is also not the most operationally efficient architecture as it does not use AWS managed services.</p><p><strong>INCORRECT:</strong> \"Deploy Amazon MQ with active/standby brokers configured across two Availability Zones. Launch an additional consumer EC2 instance in another Availability Zone. Use MySQL database replication to another Availability Zone\" is incorrect. This architecture does not use Auto Scaling for best HA or the RDS managed service.</p><p><strong>INCORRECT:</strong> \"Deploy Amazon MQ with active/standby brokers configured across two Availability Zones. Launch an additional consumer EC2 instance in another Availability Zone. Use Amazon RDS for MySQL with Multi-AZ enabled\" is incorrect. This solution does not use Auto Scaling.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/architecture/well-architected/\">https://aws.amazon.com/architecture/well-architected/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 10735,
                        "content": "<p>Deploy a second Active MQ server to another Availability Zone. Launch an additional consumer EC2 instance in another Availability Zone. Use MySQL database replication to another Availability Zone.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10736,
                        "content": "<p>Deploy Amazon MQ with active/standby brokers configured across two Availability Zones. Launch an additional consumer EC2 instance in another Availability Zone. Use Amazon RDS for MySQL with Multi-AZ enabled.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10737,
                        "content": "<p>Deploy Amazon MQ with active/standby brokers configured across two Availability Zones. Launch an additional consumer EC2 instance in another Availability Zone. Use MySQL database replication to another Availability Zone.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10738,
                        "content": "<p>Deploy Amazon MQ with active/standby brokers configured across two Availability Zones. Create an Auto Scaling group for the consumer EC2 instances across two Availability Zones. Use an Amazon RDS MySQL database with Multi-AZ enabled.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2568,
            "attributes": {
                "createdAt": "2023-09-09T20:39:49.746Z",
                "updatedAt": "2023-09-09T20:39:49.746Z",
                "content": "<p>An organization has a large amount of data on Windows (SMB) file shares in their on-premises data center. The organization would like to move data into Amazon S3. They would like to automate the migration of data over their AWS Direct Connect link.</p><p>Which AWS service can assist them?</p>",
                "answerExplanation": "<p>AWS DataSync can be used to move large amounts of data online between on-premises storage and Amazon S3 or Amazon Elastic File System (Amazon EFS). DataSync eliminates or automatically handles many of these tasks, including scripting copy jobs, scheduling and monitoring transfers, validating data, and optimizing network utilization. The source datastore can be Server Message Block (SMB) file servers.</p><p><strong>CORRECT: </strong>\"AWS DataSync\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Database Migration Service (DMS)\" is incorrect. AWS Database Migration Service (DMS) is used for migrating databases, not data on file shares.</p><p><strong>INCORRECT:</strong> \"AWS CloudFormation\" is incorrect. AWS CloudFormation can be used for automating infrastructure provisioning. This is not the best use case for CloudFormation as DataSync is designed specifically for this scenario.</p><p><strong>INCORRECT:</strong> \"AWS Snowball\" is incorrect. AWS Snowball is a hardware device that is used for migrating data into AWS. The organization plan to use their Direct Connect link for migrating data rather than sending it in via a physical device. Also, Snowball will not automate the migration.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/datasync/faqs/\">https://aws.amazon.com/datasync/faqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 10739,
                        "content": "<p>AWS Snowball</p>",
                        "isValid": false
                    },
                    {
                        "id": 10740,
                        "content": "<p>AWS DataSync</p>",
                        "isValid": true
                    },
                    {
                        "id": 10741,
                        "content": "<p>AWS Database Migration Service (DMS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10742,
                        "content": "<p>AWS CloudFormation</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2569,
            "attributes": {
                "createdAt": "2023-09-09T20:39:49.832Z",
                "updatedAt": "2023-09-09T20:39:49.832Z",
                "content": "<p>An AWS Organization has an OU with multiple member accounts in it. The company needs to restrict the ability to launch only specific Amazon EC2 instance types. How can this policy be applied across the accounts with the least effort?</p>",
                "answerExplanation": "<p>To apply the restrictions across multiple member accounts you must use a Service Control Policy (SCP) in the AWS Organization. The way you would do this is to create a deny rule that applies to anything that does not equal the specific instance type you want to allow.</p><p>The following architecture could be used to achieve this goal:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_00-48-06-e73551726ea3baf15e8e687947948cba.jpg\"></p><p><strong>CORRECT: </strong>\"Create an SCP with a deny rule that denies all but the specific instance types\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an SCP with an allow rule that allows launching the specific instance types\" is incorrect as a deny rule is required.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy to deny launching all but the specific instance types\" is incorrect. With IAM you need to apply the policy within each account rather than centrally so this would require much more effort.</p><p><strong>INCORRECT:</strong> \"Use AWS Resource Access Manager to control which launch types can be used\" is incorrect. AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. It is not used for restricting access or permissions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_example-scps.html#example-ec2-instances\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_example-scps.html#example-ec2-instances</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
                "options": [
                    {
                        "id": 10743,
                        "content": "<p>Create an IAM policy to deny launching all but the specific instance types</p>",
                        "isValid": false
                    },
                    {
                        "id": 10744,
                        "content": "<p>Create an SCP with a deny rule that denies all but the specific instance types</p>",
                        "isValid": true
                    },
                    {
                        "id": 10745,
                        "content": "<p>Use AWS Resource Access Manager to control which launch types can be used</p>",
                        "isValid": false
                    },
                    {
                        "id": 10746,
                        "content": "<p>Create an SCP with an allow rule that allows launching the specific instance types</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2570,
            "attributes": {
                "createdAt": "2023-09-09T20:39:49.925Z",
                "updatedAt": "2023-09-09T20:39:49.925Z",
                "content": "<p>An insurance company has a web application that serves users in the United Kingdom and Australia. The application includes a database tier using a MySQL database hosted in eu-west-2. The web tier runs from eu-west-2 and ap-southeast-2. Amazon Route 53 geoproximity routing is used to direct users to the closest web tier. It has been noted that Australian users receive slow response times to queries.</p><p>Which changes should be made to the database tier to improve performance?</p>",
                "answerExplanation": "<p>The issue here is latency with read queries being directed from Australia to UK which is great physical distance. A solution is required for improving read performance in Australia.</p><p>An Aurora global database consists of one primary AWS Region where your data is mastered, and up to five read-only, secondary AWS Regions. Aurora replicates data to the secondary AWS Regions with typical latency of under a second. You issue write operations directly to the primary DB instance in the primary AWS Region.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-05-41-0e623951d5ed62017a22f93bb14c6c67.jpg\"></p><p>This solution will provide better performance for users in the Australia Region for queries. Writes must still take place in the UK Region but read performance will be greatly improved.</p><p><strong>CORRECT: </strong>\"Migrate the database to an Amazon Aurora global database in MySQL compatibility mode. Configure read replicas in ap-southeast-2\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Migrate the database to Amazon RDS for MySQL. Configure Multi-AZ in the Australian Region\" is incorrect. The database is located in UK. If the database is migrated to Australia then the reverse problem will occur. Multi-AZ does not assist with improving query performance across Regions.</p><p><strong>INCORRECT:</strong> \"Migrate the database to Amazon DynamoDB. Use DynamoDB global tables to enable replication to additional Regions\" is incorrect as a relational database running on MySQL is unlikely to be compatible with DynamoDB.</p><p><strong>INCORRECT:</strong> \"Deploy MySQL instances in each Region. Deploy an Application Load Balancer in front of MySQL to reduce the load on the primary instance\" is incorrect as you can only put ALBs in front of the web tier, not the DB tier.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
                "options": [
                    {
                        "id": 10747,
                        "content": "<p>Migrate the database to Amazon DynamoDB. Use DynamoDB global tables to enable replication to additional Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 10748,
                        "content": "<p>Migrate the database to an Amazon Aurora global database in MySQL compatibility mode. Configure read replicas in ap-southeast-2</p>",
                        "isValid": true
                    },
                    {
                        "id": 10749,
                        "content": "<p>Deploy MySQL instances in each Region. Deploy an Application Load Balancer in front of MySQL to reduce the load on the primary instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10750,
                        "content": "<p>Migrate the database to Amazon RDS for MySQL. Configure Multi-AZ in the Australian Region</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2571,
            "attributes": {
                "createdAt": "2023-09-09T20:39:50.069Z",
                "updatedAt": "2023-09-09T20:39:50.069Z",
                "content": "<p>A video production company is planning to move some of its workloads to the AWS Cloud. The company will require around 5 TB of storage for video processing with the maximum possible I/O performance. They also require over 400 TB of extremely durable storage for storing video files and 800 TB of storage for long-term archival.</p><p>Which combinations of services should a Solutions Architect use to meet these requirements?</p>",
                "answerExplanation": "<p>The best I/O performance can be achieved by using instance store volumes for the video processing. This is safe to use for use cases where the data can be recreated from the source files so this is a good use case.</p><p>For storing data durably Amazon S3 is a good fit as it provides 99.999999999% of durability. For archival the video files can then be moved to Amazon S3 Glacier which is a low cost storage option that is ideal for long-term archival.</p><p><strong>CORRECT: </strong>\"Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage\" is incorrect. EBS is not going to provide as much I/O performance as an instance store volume so is not the best choice for this use case.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 instance store for maximum performance, Amazon EFS for durable data storage, and Amazon S3 for archival storage\" is incorrect. EFS does not provide as much durability as Amazon S3 and will not be as cost-effective.</p><p><strong>INCORRECT:</strong> \"Amazon EBS for maximum performance, Amazon EFS for durable data storage, and Amazon S3 Glacier for archival storage\" is incorrect. EBS and EFS are not the best choices here as described above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 10751,
                        "content": "<p>Amazon EBS for maximum performance, Amazon EFS for durable data storage, and Amazon S3 Glacier for archival storage.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10752,
                        "content": "<p>Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10753,
                        "content": "<p>Amazon EC2 instance store for maximum performance, Amazon EFS for durable data storage, and Amazon S3 for archival storage.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10754,
                        "content": "<p>Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2572,
            "attributes": {
                "createdAt": "2023-09-09T20:39:50.160Z",
                "updatedAt": "2023-09-09T20:39:50.160Z",
                "content": "<p>An eCommerce application consists of three tiers. The web tier includes EC2 instances behind an Application Load balancer, the middle tier uses EC2 instances and an Amazon SQS queue to process orders, and the database tier consists of an Auto Scaling DynamoDB table. During busy periods customers have complained about delays in the processing of orders. A Solutions Architect has been tasked with reducing processing times.</p><p>Which action will be MOST effective in accomplishing this requirement?</p>",
                "answerExplanation": "<p>The most likely cause of the processing delays is insufficient instances in the middle tier where the order processing takes place. The most effective solution to reduce processing times in this case is to scale based on the backlog per instance (number of messages in the SQS queue) as this reflects the amount of work that needs to be done.</p><p><strong>CORRECT: </strong>\"Use Amazon EC2 Auto Scaling to scale out the middle tier instances based on the SQS queue depth\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Replace the Amazon SQS queue with Amazon Kinesis Data Firehose\" is incorrect. The issue is not the efficiency of queuing messages but the processing of the messages. In this case scaling the EC2 instances to reflect the workload is a better solution.</p><p><strong>INCORRECT:</strong> \"Use Amazon DynamoDB Accelerator (DAX) in front of the DynamoDB backend tier\" is incorrect. The DynamoDB table is configured with Auto Scaling so this is not likely to be the bottleneck in order processing.</p><p><strong>INCORRECT:</strong> \"Add an Amazon CloudFront distribution with a custom origin to cache the responses for the web tier\" is incorrect. This will cache media files to speed up web response times but not order processing times as they take place in the middle tier.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 10755,
                        "content": "<p>Use Amazon EC2 Auto Scaling to scale out the middle tier instances based on the SQS queue depth.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10756,
                        "content": "<p>Replace the Amazon SQS queue with Amazon Kinesis Data Firehose.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10757,
                        "content": "<p>Use Amazon DynamoDB Accelerator (DAX) in front of the DynamoDB backend tier.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10758,
                        "content": "<p>Add an Amazon CloudFront distribution with a custom origin to cache the responses for the web tier.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2573,
            "attributes": {
                "createdAt": "2023-09-09T20:39:50.269Z",
                "updatedAt": "2023-09-09T20:39:50.269Z",
                "content": "<p>A solutions architect is designing a new service that will use an Amazon API Gateway API on the frontend. The service will need to persist data in a backend database using key-value requests. Initially, the data requirements will be around 1 GB and future growth is unknown. Requests can range from 0 to over 800 requests per second.</p><p>Which combination of AWS services would meet these requirements? (Select TWO.)</p>",
                "answerExplanation": "<p>In this case AWS Lambda can perform the computation and store the data in an Amazon DynamoDB table. Lambda can scale concurrent executions to meet demand easily and DynamoDB is built for key-value data storage requirements and is also serverless and easily scalable. This is therefore a cost effective solution for unpredictable workloads.</p><p><strong>CORRECT: </strong>\"AWS Lambda\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Amazon DynamoDB\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Fargate\" is incorrect as containers run constantly and therefore incur costs even when no requests are being made.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 Auto Scaling\" is incorrect as this uses EC2 instances which will incur costs even when no requests are being made.</p><p><strong>INCORRECT:</strong> \"Amazon RDS\" is incorrect as this is a relational database not a No-SQL database. It is therefore not suitable for key-value data storage requirements.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/lambda/features/\">https://aws.amazon.com/lambda/features/</a></p><p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 10759,
                        "content": "<p>Amazon RDS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10760,
                        "content": "<p>Amazon EC2 Auto Scaling</p>",
                        "isValid": false
                    },
                    {
                        "id": 10761,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 10762,
                        "content": "<p>AWS Lambda</p>",
                        "isValid": true
                    },
                    {
                        "id": 10763,
                        "content": "<p>AWS Fargate</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2574,
            "attributes": {
                "createdAt": "2023-09-09T20:39:50.362Z",
                "updatedAt": "2023-09-09T20:39:50.362Z",
                "content": "<p>A company provides a REST-based interface to an application that allows a partner company to send data in near-real time. The application then processes the data that is received and stores it for later analysis. The application runs on Amazon EC2 instances.</p><p>The partner company has received many 503 Service Unavailable Errors when sending data to the application and the compute capacity reaches its limits and is unable to process requests when spikes in data volume occur.</p><p>Which design should a Solutions Architect implement to improve scalability?</p>",
                "answerExplanation": "<p>Amazon Kinesis enables you to ingest, buffer, and process streaming data in real-time. Kinesis can handle any amount of streaming data and process data from hundreds of thousands of sources with very low latencies. This is an ideal solution for data ingestion.</p><p>To ensure the compute layer can scale to process increasing workloads, the EC2 instances should be replaced by AWS Lambda functions. Lambda can scale seamlessly by running multiple executions in parallel.</p><p><strong>CORRECT: </strong>\"Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon API Gateway in front of the existing application. Create a usage plan with a quota limit for the partner company\" is incorrect. A usage plan will limit the amount of data that is received and cause more errors to be received by the partner company.</p><p><strong>INCORRECT:</strong> \"Use Amazon SQS to ingest the data. Configure the EC2 instances to process messages from the SQS queue\" is incorrect. Amazon Kinesis Data Streams should be used for near-real time or real-time use cases instead of Amazon SQS.</p><p><strong>INCORRECT:</strong> \"Use Amazon SNS to ingest the data and trigger AWS Lambda functions to process the data in near-real time\" is incorrect. SNS is not a near-real time solution for data ingestion. SNS is used for sending notifications.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/\">https://aws.amazon.com/kinesis/</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 10764,
                        "content": "<p>Use Amazon API Gateway in front of the existing application. Create a usage plan with a quota limit for the partner company.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10765,
                        "content": "<p>Use Amazon SNS to ingest the data and trigger AWS Lambda functions to process the data in near-real time.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10766,
                        "content": "<p>Use Amazon SQS to ingest the data. Configure the EC2 instances to process messages from the SQS queue.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10767,
                        "content": "<p>Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2575,
            "attributes": {
                "createdAt": "2023-09-09T20:39:50.505Z",
                "updatedAt": "2023-09-09T20:39:50.505Z",
                "content": "<p>An Amazon RDS Read Replica is being deployed in a separate region. The master database is not encrypted but all data in the new region must be encrypted. How can this be achieved?</p>",
                "answerExplanation": "<p>You cannot create an encrypted Read Replica from an unencrypted master DB instance. You also cannot enable encryption after launch time for the master DB instance. Therefore, you must create a new master DB by taking a snapshot of the existing DB, encrypting it, and then creating the new DB from the snapshot. You can then create the encrypted cross-region Read Replica of the master DB.</p><p><strong>CORRECT: </strong>\"Encrypt a snapshot from the master DB instance, create a new encrypted master DB instance, and then create an encrypted cross-region Read Replica\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable encryption using Key Management Service (KMS) when creating the cross-region Read Replica\" is incorrect. All other options will not work due to the limitations explained above.</p><p><strong>INCORRECT:</strong> \"Encrypt a snapshot from the master DB instance, create an encrypted cross-region Read Replica from the snapshot\" is incorrect. All other options will not work due to the limitations explained above.</p><p><strong>INCORRECT:</strong> \"Enabled encryption on the master DB instance, then create an encrypted cross-region Read Replica\" is incorrect. All other options will not work due to the limitations explained above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 10768,
                        "content": "<p>Encrypt a snapshot from the master DB instance, create an encrypted cross-region Read Replica from the snapshot</p>",
                        "isValid": false
                    },
                    {
                        "id": 10769,
                        "content": "<p>Enable encryption using Key Management Service (KMS) when creating the cross-region Read Replica</p>",
                        "isValid": false
                    },
                    {
                        "id": 10770,
                        "content": "<p>Enable encryption on the master DB instance, then create an encrypted cross-region Read Replica</p>",
                        "isValid": false
                    },
                    {
                        "id": 10771,
                        "content": "<p>Encrypt a snapshot from the master DB instance, create a new encrypted master DB instance, and then create an encrypted cross-region Read Replica</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2576,
            "attributes": {
                "createdAt": "2023-09-09T20:39:50.610Z",
                "updatedAt": "2023-09-09T20:39:50.610Z",
                "content": "<p>A developer created an application that uses Amazon EC2 and an Amazon RDS MySQL database instance. The developer stored the database user name and password in a configuration file on the root EBS volume of the EC2 application instance. A Solutions Architect has been asked to design a more secure solution.</p><p>What should the Solutions Architect do to achieve this requirement?</p>",
                "answerExplanation": "<p>The key problem here is having plain text credentials stored in a file. Even if you encrypt the volume there is still as security risk as the credentials are loaded by the application and passed to RDS.</p><p>The best way to secure this solution is to get rid of the credentials completely by using an IAM role instead. The IAM role can be assigned permissions to the database instance and can be attached to the EC2 instance. The instance will then obtain temporary security credentials from AWS STS which is much more secure.</p><p><strong>CORRECT: </strong>\"Create an IAM role with permission to access the database. Attach this IAM role to the EC2 instance\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Move the configuration file to an Amazon S3 bucket. Create an IAM role with permission to the bucket and attach it to the EC2 instance\" is incorrect. This just relocates the file; the contents are still unsecured and must be loaded by the application and passed to RDS. This is an insecure process.</p><p><strong>INCORRECT:</strong> \"Attach an additional volume to the EC2 instance with encryption enabled. Move the configuration file to the encrypted volume\" is incorrect. This will only encrypt the file at rest, it still must be read, and the contents passed to RDS which is insecure.</p><p><strong>INCORRECT:</strong> \"Install an Amazon-trusted root certificate on the application instance and use SSL/TLS encrypted connections to the database\" is incorrect. The file is still unsecured on the EBS volume so encrypting the credentials in an encrypted channel between the EC2 instance and RDS does not solve all security issues.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 10772,
                        "content": "<p>Attach an additional volume to the EC2 instance with encryption enabled. Move the configuration file to the encrypted volume.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10773,
                        "content": "<p>Install an Amazon-trusted root certificate on the application instance and use SSL/TLS encrypted connections to the database.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10774,
                        "content": "<p>Create an IAM role with permission to access the database. Attach this IAM role to the EC2 instance.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10775,
                        "content": "<p>Move the configuration file to an Amazon S3 bucket. Create an IAM role with permission to the bucket and attach it to the EC2 instance.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2577,
            "attributes": {
                "createdAt": "2023-09-09T20:39:50.725Z",
                "updatedAt": "2023-09-09T20:39:50.725Z",
                "content": "<p>A company runs an application on an Amazon EC2 instance the requires 250 GB of storage space. The application is not used often and has small spikes in usage on weekday mornings and afternoons. The disk I/O can vary with peaks hitting a maximum of 3,000 IOPS. A Solutions Architect must recommend the most cost-effective storage solution that delivers the performance required.</p><p>Which configuration should the Solutions Architect recommend?</p><p>Which solution should the solutions architect recommend?</p>",
                "answerExplanation": "<p>General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods of time.</p><p>Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver their provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB.</p><p>In this configuration the volume will provide a baseline performance of 750 IOPS but will always be able to burst to the required 3,000 IOPS during periods of increased traffic.</p><p><strong>CORRECT: </strong>\"Amazon EBS General Purpose SSD (gp2)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EBS Provisioned IOPS SSD (i01)\" is incorrect. The i01 volume type will be more expensive and is not necessary for the performance levels required.</p><p><strong>INCORRECT:</strong> \"Amazon EBS Cold HDD (sc1)\" is incorrect. The sc1 volume type is not going to deliver the performance requirements as it cannot burst to 3,000 IOPS.</p><p><strong>INCORRECT:</strong> \"Amazon EBS Throughput Optimized HDD (st1)\" is incorrect. The st1 volume type is not going to deliver the performance requirements as it cannot burst to 3,000 IOPS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 10776,
                        "content": "<p>Amazon EBS General Purpose SSD (gp2)</p>",
                        "isValid": true
                    },
                    {
                        "id": 10777,
                        "content": "<p>Amazon EBS Throughput Optimized HDD (st1)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10778,
                        "content": "<p>Amazon EBS Cold HDD (sc1)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10779,
                        "content": "<p>Amazon EBS Provisioned IOPS SSD (i01)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2578,
            "attributes": {
                "createdAt": "2023-09-09T20:39:50.841Z",
                "updatedAt": "2023-09-09T20:39:50.841Z",
                "content": "<p>A company runs a web application that serves weather updates. The application runs on a fleet of Amazon EC2 instances in a Multi-AZ Auto scaling group behind an Application Load Balancer (ALB). The instances store data in an Amazon Aurora database. A solutions architect needs to make the application more resilient to sporadic increases in request rates.</p><p>Which architecture should the solutions architect implement? (Select TWO.)</p>",
                "answerExplanation": "<p>The architecture is already highly resilient but the may be subject to performance degradation if there are sudden increases in request rates. To resolve this situation Amazon Aurora Read Replicas can be used to serve read traffic which offloads requests from the main database. On the frontend an Amazon CloudFront distribution can be placed in front of the ALB and this will cache content for better performance and also offloads requests from the backend.</p><p><strong>CORRECT: </strong>\"Add Amazon Aurora Replicas\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Add an Amazon CloudFront distribution in front of the ALB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Add and AWS WAF in front of the ALB\" is incorrect. A web application firewall protects applications from malicious attacks. It does not improve performance.</p><p><strong>INCORRECT:</strong> \"Add an AWS Transit Gateway to the Availability Zones\" is incorrect as this is used to connect on-premises networks to VPCs.</p><p><strong>INCORRECT:</strong> \"Add an AWS Global Accelerator endpoint\" is incorrect as this service is used for directing users to different instances of the application in different regions based on latency.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 10780,
                        "content": "<p>Add an AWS Transit Gateway to the Availability Zones</p>",
                        "isValid": false
                    },
                    {
                        "id": 10781,
                        "content": "<p>Add Amazon Aurora Replicas</p>",
                        "isValid": true
                    },
                    {
                        "id": 10782,
                        "content": "<p>Add and AWS WAF in front of the ALB</p>",
                        "isValid": false
                    },
                    {
                        "id": 10783,
                        "content": "<p>Add an Amazon CloudFront distribution in front of the ALB</p>",
                        "isValid": true
                    },
                    {
                        "id": 10784,
                        "content": "<p>Add an AWS Global Accelerator endpoint</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2579,
            "attributes": {
                "createdAt": "2023-09-09T20:39:50.929Z",
                "updatedAt": "2023-09-09T20:39:50.929Z",
                "content": "<p>A solutions architect needs to backup some application log files from an online ecommerce store to Amazon S3. It is unknown how often the logs will be accessed or which logs will be accessed the most. The solutions architect must keep costs as low as possible by using the appropriate S3 storage class.</p><p>Which S3 storage class should be implemented to meet these requirements?</p>",
                "answerExplanation": "<p>The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead.</p><p>It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access. This is an ideal use case for intelligent-tiering as the access patterns for the log files are not known.</p><p><strong>CORRECT: </strong>\"S3 Intelligent-Tiering\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"S3 Standard-Infrequent Access (S3 Standard-IA)\" is incorrect as if the data is accessed often retrieval fees could become expensive.</p><p><strong>INCORRECT:</strong> \"S3 One Zone-Infrequent Access (S3 One Zone-IA)\" is incorrect as if the data is accessed often retrieval fees could become expensive.</p><p><strong>INCORRECT:</strong> \"S3 Glacier\" is incorrect as if the data is accessed often retrieval fees could become expensive. Glacier also requires more work in retrieving the data from the archive and quick access requirements can add further costs.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/#Unknown_or_changing_access\">https://aws.amazon.com/s3/storage-classes/#Unknown_or_changing_access</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 10785,
                        "content": "<p>S3 Intelligent-Tiering</p>",
                        "isValid": true
                    },
                    {
                        "id": 10786,
                        "content": "<p>S3 Standard-Infrequent Access (S3 Standard-IA)</p>",
                        "isValid": false
                    },
                    {
                        "id": 10787,
                        "content": "<p>S3 Glacier</p>",
                        "isValid": false
                    },
                    {
                        "id": 10788,
                        "content": "<p>S3 One Zone-Infrequent Access (S3 One Zone-IA)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2580,
            "attributes": {
                "createdAt": "2023-09-09T20:39:51.014Z",
                "updatedAt": "2023-09-09T20:39:51.014Z",
                "content": "<p>An application stores transactional data in an Amazon S3 bucket. The data is analyzed for the first week and then must remain immediately available for occasional analysis.</p><p>What is the MOST cost-effective storage solution that meets the requirements?</p>",
                "answerExplanation": "<p>The transition should be to Standard-IA rather than One Zone-IA. Though One Zone-IA would be cheaper, it also offers lower availability and the question states the objects “must remain immediately available”. Therefore the availability is a consideration.</p><p>Though there is no minimum duration when storing data in S3 Standard, you cannot transition to Standard IA within 30 days. This can be seen when trying to create a lifecycle rule:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-12-38-75ade2c799e4d5cf4b201792e2ee34b6.jpg\"></p><p><strong>Therefore, the best solution is to transition after 30 days.</strong></p><p><strong>CORRECT: </strong>\"Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days\" is incorrect as explained above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 10789,
                        "content": "<p>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10790,
                        "content": "<p>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10791,
                        "content": "<p>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10792,
                        "content": "<p>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2581,
            "attributes": {
                "createdAt": "2023-09-09T20:39:51.100Z",
                "updatedAt": "2023-09-09T20:39:51.100Z",
                "content": "<p>A web application has recently been launched on AWS. The architecture includes two tier with a web layer and a database layer. It has been identified that the web server layer may be vulnerable to cross-site scripting (XSS) attacks.</p><p>What should a solutions architect do to remediate the vulnerability?</p>",
                "answerExplanation": "<p>The AWS Web Application Firewall (WAF) is available on the Application Load Balancer (ALB). You can use AWS WAF directly on Application Load Balancers (both internal and external) in a VPC, to protect your websites and web services.</p><p>Attackers sometimes insert scripts into web requests in an effort to exploit vulnerabilities in web applications. You can create one or more cross-site scripting match conditions to identify the parts of web requests, such as the URI or the query string, that you want AWS WAF to inspect for possible malicious scripts.</p><p><strong>CORRECT: </strong>\"Create an Application Load Balancer. Put the web layer behind the load balancer and enable AWS WAF\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a Classic Load Balancer. Put the web layer behind the load balancer and enable AWS WAF\" is incorrect as you cannot use AWS WAF with a classic load balancer.</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer. Put the web layer behind the load balancer and enable AWS WAF\" is incorrect as you cannot use AWS WAF with a network load balancer.</p><p><strong>INCORRECT:</strong> \"Create an Application Load Balancer. Put the web layer behind the load balancer and use AWS Shield Standard\" is incorrect as you cannot use AWS Shield to protect against XSS attacks. Shield is used to protect against DDoS attacks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-xss-conditions.html\">https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-xss-conditions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>",
                "options": [
                    {
                        "id": 10793,
                        "content": "<p>Create an Application Load Balancer. Put the web layer behind the load balancer and use AWS Shield Standard</p>",
                        "isValid": false
                    },
                    {
                        "id": 10794,
                        "content": "<p>Create a Classic Load Balancer. Put the web layer behind the load balancer and enable AWS WAF</p>",
                        "isValid": false
                    },
                    {
                        "id": 10795,
                        "content": "<p>Create a Network Load Balancer. Put the web layer behind the load balancer and enable AWS WAF</p>",
                        "isValid": false
                    },
                    {
                        "id": 10796,
                        "content": "<p>Create an Application Load Balancer. Put the web layer behind the load balancer and enable AWS WAF</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2582,
            "attributes": {
                "createdAt": "2023-09-09T20:39:51.196Z",
                "updatedAt": "2023-09-09T20:39:51.196Z",
                "content": "<p>A web application that allows users to upload and share documents is running on a single Amazon EC2 instance with an Amazon EBS volume. To increase availability the architecture has been updated to use an Auto Scaling group of several instances across Availability Zones behind an Application Load Balancer. After the change users can only see a subset of the documents.</p><p>What is the BEST method for a solutions architect to modify the solution so users can see all documents?</p>",
                "answerExplanation": "<p>The problem that is being described is that the users are uploading the documents to an individual EC2 instance with a local EBS volume. Therefore, as EBS volumes cannot be shared across AZs, the data is stored separately and the ALB will be distributing incoming connections to different instances / data sets.</p><p>The simple resolution is to implement a shared storage layer for the documents so that they can be stored in one place and seen by any user who connects no matter which instance they connect to.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-30-06-e730f4c928621d8e81593b31b92020f6.jpg\"></p><p><strong>CORRECT: </strong>\"Copy the data from all EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Run a script to synchronize the data between Amazon EBS volumes\" is incorrect. This is a complex and messy approach. A better solution is to use a shared storage layer.</p><p><strong>INCORRECT:</strong> \"Use Sticky Sessions with the ALB to ensure users are directed to the same EC2 instance in a session\" is incorrect as this will just “stick” a user to the same instance. They won’t see documents uploaded to other instances / EBS volumes.</p><p><strong>INCORRECT:</strong> \"Configure the Application Load Balancer to send the request to all servers. Return each document from the correct server\" is incorrect as there is no mechanism here for selecting a specific document. The requirement also requests that all documents are visible.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html\">https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
                "options": [
                    {
                        "id": 10797,
                        "content": "<p>Configure the Application Load Balancer to send the request to all servers. Return each document from the correct server</p>",
                        "isValid": false
                    },
                    {
                        "id": 10798,
                        "content": "<p>Run a script to synchronize the data between Amazon EBS volumes</p>",
                        "isValid": false
                    },
                    {
                        "id": 10799,
                        "content": "<p>Use Sticky Sessions with the ALB to ensure users are directed to the same EC2 instance in a session</p>",
                        "isValid": false
                    },
                    {
                        "id": 10800,
                        "content": "<p>Copy the data from all EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2583,
            "attributes": {
                "createdAt": "2023-09-09T20:39:51.312Z",
                "updatedAt": "2023-09-09T20:39:51.312Z",
                "content": "<p>A company wants to migrate a legacy web application from an on-premises data center to AWS. The web application consists of a web tier, an application tier, and a MySQL database. The company does not want to manage instances or clusters.</p><p>Which combination of services should a solutions architect include in the overall architecture? (Select TWO.)</p>",
                "answerExplanation": "<p>Amazon RDS is a managed service and you do not need to manage the instances. This is an ideal backend for the application and you can run a MySQL database on RDS without any refactoring. For the application components these can run on Docker containers with AWS Fargate. Fargate is a serverless service for running containers on AWS.</p><p><strong>CORRECT: </strong>\"AWS Fargate\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Amazon RDS for MySQL\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon DynamoDB\" is incorrect. This is a NoSQL database and would be incompatible with the relational MySQL DB.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 Spot Instances\" is incorrect. This would require managing instances.</p><p><strong>INCORRECT:</strong> \"Amazon Kinesis Data Streams\" is incorrect. This is a service for streaming data.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/\">https://aws.amazon.com/rds/</a></p><p><a href=\"https://aws.amazon.com/fargate/\">https://aws.amazon.com/fargate/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 10801,
                        "content": "<p>AWS Fargate</p>",
                        "isValid": true
                    },
                    {
                        "id": 10802,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 10803,
                        "content": "<p>Amazon RDS for MySQL</p>",
                        "isValid": true
                    },
                    {
                        "id": 10804,
                        "content": "<p>Amazon EC2 Spot Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10805,
                        "content": "<p>Amazon Kinesis Data Streams</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2584,
            "attributes": {
                "createdAt": "2023-09-09T20:39:51.456Z",
                "updatedAt": "2023-09-09T20:39:51.456Z",
                "content": "<p>A company runs a number of core enterprise applications in an on-premises data center. The data center is connected to an Amazon VPC using AWS Direct Connect. The company will be creating additional AWS accounts and these accounts will also need to be quickly, and cost-effectively connected to the on-premises data center in order to access the core applications.</p><p>What deployment changes should a Solutions Architect implement to meet these requirements with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>AWS Transit Gateway connects VPCs and on-premises networks through a central hub. With AWS Transit Gateway, you can quickly add Amazon VPCs, AWS accounts, VPN capacity, or AWS Direct Connect gateways to meet unexpected demand, without having to wrestle with complex connections or massive routing tables. This is the operationally least complex solution and is also cost-effective.</p><p>The image below depicts how transit gateway can assist with simplifying network deployments:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-11-27_10-12-08-05293542d6a09dfb24fcf7e9a59f123e.jpg\"></p><p><strong>CORRECT: </strong>\"Configure AWS Transit Gateway between the accounts. Assign Direct Connect to the transit gateway and route network traffic to the on-premises servers\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a VPN connection between each new account and the Direct Connect VPC. Route the network traffic to the on-premises servers\" is incorrect. You cannot connect VPCs using AWS managed VPNs and would need to configure a software VPN and then complex routing configurations. This is not the best solution.</p><p><strong>INCORRECT:</strong> \"Create a Direct Connect connection in each new account. Route the network traffic to the on-premises servers\" is incorrect. This is an expensive solution as you would need to have multiple Direct Connect links.</p><p><strong>INCORRECT:</strong> \"Configure VPC endpoints in the Direct Connect VPC for all required services. Route the network traffic to the on-premises servers\" is incorrect. You cannot create VPC endpoints for all services and this would be a complex solution for those you can.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-direct-connect/\">https://digitalcloud.training/aws-direct-connect/</a></p>",
                "options": [
                    {
                        "id": 10806,
                        "content": "<p>Configure AWS Transit Gateway between the accounts. Assign Direct Connect to the transit gateway and route network traffic to the on-premises servers.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10807,
                        "content": "<p>Create a VPN connection between each new account and the Direct Connect VPC. Route the network traffic to the on-premises servers.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10808,
                        "content": "<p>Create a Direct Connect connection in each new account. Route the network traffic to the on-premises servers.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10809,
                        "content": "<p>Configure VPC endpoints in the Direct Connect VPC for all required services. Route the network traffic to the on-premises servers.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2585,
            "attributes": {
                "createdAt": "2023-09-09T20:39:51.556Z",
                "updatedAt": "2023-09-09T20:39:51.556Z",
                "content": "<p>A highly sensitive application runs on Amazon EC2 instances using EBS volumes. The application stores data temporarily on Amazon EBS volumes during processing before saving results to an Amazon RDS database. The company’s security team mandate that the sensitive data must be encrypted at rest.</p><p>Which solution should a Solutions Srchitect recommend to meet this requirement?</p>",
                "answerExplanation": "<p>As the data is stored both in the EBS volumes (temporarily) and the RDS database, both the EBS and RDS volumes must be encrypted at rest. This can be achieved by enabling encryption at creation time of the volume and AWS KMS keys can be used to encrypt the data. This solution meets all requirements.</p><p><strong>CORRECT: </strong>\"Configure encryption for the Amazon EBS volumes and Amazon RDS database with AWS KMS keys\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS Certificate Manager to generate certificates that can be used to encrypt the connections between the EC2 instances and RDS\" is incorrect. This would encrypt the data in-transit but not at-rest.</p><p><strong>INCORRECT:</strong> \"Use Amazon Data Lifecycle Manager to encrypt all data as it is stored to the EBS volumes and RDS database\" is incorrect. DLM is used for automating the process of taking and managing snapshots for EBS volumes.</p><p><strong>INCORRECT:</strong> \"Configure SSL/TLS encryption using AWS KMS customer master keys (CMKs) to encrypt database volumes\" is incorrect. You cannot configure SSL/TLS encryption using KMS CMKs or use SSL/TLS to encrypt data at rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 10810,
                        "content": "<p>Configure SSL/TLS encryption using AWS KMS customer master keys (CMKs) to encrypt database volumes.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10811,
                        "content": "<p>Use AWS Certificate Manager to generate certificates that can be used to encrypt the connections between the EC2 instances and RDS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10812,
                        "content": "<p>Use Amazon Data Lifecycle Manager to encrypt all data as it is stored to the EBS volumes and RDS database.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10813,
                        "content": "<p>Configure encryption for the Amazon EBS volumes and Amazon RDS database with AWS KMS keys.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2586,
            "attributes": {
                "createdAt": "2023-09-09T20:39:51.648Z",
                "updatedAt": "2023-09-09T20:39:51.648Z",
                "content": "<p>A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by midmorning</p><p>How should the scaling be changed to address the staff complaints and keep costs to a minimum?</p>",
                "answerExplanation": "<p>Though this sounds like a good use case for scheduled actions, both answers using scheduled actions will have 20 instances running regardless of actual demand. A better option to be more cost effective is to use a target tracking action that triggers at a lower CPU threshold.</p><p>With this solution the scaling will occur before the CPU utilization gets to a point where performance is affected. This will result in resolving the performance issues whilst minimizing costs. Using a reduced cooldown period will also more quickly terminate unneeded instances, further reducing costs.</p><p><strong>CORRECT: </strong>\"Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens\" is incorrect as this is not the most cost-effective option. Note you can choose min, max, or desired for a scheduled action.</p><p><strong>INCORRECT:</strong> \"Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens\" is incorrect as this is not the most cost-effective option. Note you can choose min, max, or desired for a scheduled action.</p><p><strong>INCORRECT:</strong> \"Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period\" is incorrect as AWS recommend you use target tracking in place of step scaling for most use cases.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 10814,
                        "content": "<p>Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period</p>",
                        "isValid": true
                    },
                    {
                        "id": 10815,
                        "content": "<p>Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens</p>",
                        "isValid": false
                    },
                    {
                        "id": 10816,
                        "content": "<p>Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period</p>",
                        "isValid": false
                    },
                    {
                        "id": 10817,
                        "content": "<p>Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2587,
            "attributes": {
                "createdAt": "2023-09-09T20:39:51.740Z",
                "updatedAt": "2023-09-09T20:39:51.740Z",
                "content": "<p>A multi-tier application runs with eight front-end web servers in an Amazon EC2 Auto Scaling group in a single Availability Zone behind an Application Load Balancer. A solutions architect needs to modify the infrastructure to be highly available without modifying the application.</p><p>Which architecture should the solutions architect choose that provides high availability?</p>",
                "answerExplanation": "<p>High availability can be enabled for this architecture quite simply by modifying the existing Auto Scaling group to use multiple availability zones. The ASG will automatically balance the load so you don’t actually need to specify the instances per AZ.</p><p>The architecture for the web tier will look like the one below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-27-38-903c5a1a8a69212049571597cbc31b0f.jpg\"></p><p><strong>CORRECT: </strong>\"Modify the Auto Scaling group to use four instances across each of two Availability Zones\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group that uses four instances across each of two Regions\" is incorrect as EC2 Auto Scaling does not support multiple regions.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling template that can be used to quickly create more instances in another Region\" is incorrect as EC2 Auto Scaling does not support multiple regions.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group that uses four instances across each of two subnets\" is incorrect as the subnets could be in the same AZ.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/ec2/autoscaling/\">https://aws.amazon.com/ec2/autoscaling/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 10818,
                        "content": "<p>Create an Auto Scaling group that uses four instances across each of two subnets</p>",
                        "isValid": false
                    },
                    {
                        "id": 10819,
                        "content": "<p>Create an Auto Scaling template that can be used to quickly create more instances in another Region</p>",
                        "isValid": false
                    },
                    {
                        "id": 10820,
                        "content": "<p>Create an Auto Scaling group that uses four instances across each of two Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 10821,
                        "content": "<p>Modify the Auto Scaling group to use four instances across each of two Availability Zones</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2588,
            "attributes": {
                "createdAt": "2023-09-09T20:39:51.835Z",
                "updatedAt": "2023-09-09T20:39:51.835Z",
                "content": "<p>A company is planning a migration for a high performance computing (HPC) application and associated data from an on-premises data center to the AWS Cloud. The company uses tiered storage on premises with hot high-performance parallel storage to support the application during periodic runs of the application, and more economical cold storage to hold the data when the application is not actively running.</p><p>Which combination of solutions should a solutions architect recommend to support the storage needs of the application? (Select TWO.)</p>",
                "answerExplanation": "<p>Amazon FSx for Lustre provides a high-performance file system optimized for fast processing of workloads such as machine learning, high-performance computing (HPC), video processing, financial modeling, and electronic design automation (EDA).</p><p>These workloads commonly require data to be presented via a fast and scalable file system interface, and typically have data sets stored on long-term data stores like Amazon S3.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-29-14-9ca4fa2525b286cbb51131ad2546dea3.jpg\"></p><p>Amazon FSx works natively with Amazon S3, making it easy to access your S3 data to run data processing workloads. Your S3 objects are presented as files in your file system, and you can write your results back to S3. This lets you run data processing workloads on FSx for Lustre and store your long-term data on S3 or on-premises data stores.</p><p>Therefore, the best combination for this scenario is to use S3 for cold data and FSx for Lustre for the parallel HPC job.</p><p><strong>CORRECT: </strong>\"Amazon S3 for cold data storage\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Amazon FSx for Lustre for high-performance parallel storage\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EFS for cold data storage\" is incorrect as FSx works natively with S3 which is also more economical.</p><p><strong>INCORRECT:</strong> \"Amazon S3 for high-performance parallel storage\" is incorrect as S3 is not suitable for running high-performance computing jobs.</p><p><strong>INCORRECT:</strong> \"Amazon FSx for Windows for high-performance parallel storage\" is incorrect as FSx for Lustre should be used for HPC use cases and use cases that require storing data on S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/lustre/\">https://aws.amazon.com/fsx/lustre/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 10822,
                        "content": "<p>Amazon FSx for Windows for high-performance parallel storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 10823,
                        "content": "<p>Amazon S3 for cold data storage</p>",
                        "isValid": true
                    },
                    {
                        "id": 10824,
                        "content": "<p>Amazon EFS for cold data storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 10825,
                        "content": "<p>Amazon S3 for high-performance parallel storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 10826,
                        "content": "<p>Amazon FSx for Lustre for high-performance parallel storage</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2589,
            "attributes": {
                "createdAt": "2023-09-09T20:39:51.933Z",
                "updatedAt": "2023-09-09T20:39:51.933Z",
                "content": "<p>An IoT sensor is being rolled out to thousands of a company’s existing customers. The sensors will stream high volumes of data each second to a central location. A solution must be designed to ingest and store the data for analytics. The solution must provide near-real time performance and millisecond responsiveness.</p><p>Which solution should a Solutions Architect recommend?</p>",
                "answerExplanation": "<p>A Kinesis data stream is a set of shards. Each shard contains a sequence of data records. A <strong>consumer</strong> is an application that processes the data from a Kinesis data stream. You can map a Lambda function to a shared-throughput consumer (standard iterator), or to a dedicated-throughput consumer with enhanced fan-out.</p><p>Amazon DynamoDB is the best database for this use case as it supports near-real time performance and millisecond responsiveness.</p><p><strong>CORRECT: </strong>\"Ingest the data into an Amazon Kinesis Data Stream. Process the data with an AWS Lambda function and then store the data in Amazon DynamoDB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Ingest the data into an Amazon Kinesis Data Stream. Process the data with an AWS Lambda function and then store the data in Amazon RedShift\" is incorrect. Amazon RedShift cannot provide millisecond responsiveness.</p><p><strong>INCORRECT:</strong> \"Ingest the data into an Amazon SQS queue. Process the data using an AWS Lambda function and then store the data in Amazon RedShift\" is incorrect. Amazon SQS does not provide near real-time performance and RedShift does not provide millisecond responsiveness.</p><p><strong>INCORRECT:</strong> \"Ingest the data into an Amazon SQS queue. Process the data using an AWS Lambda function and then store the data in Amazon DynamoDB\" is incorrect. Amazon SQS does not provide near real-time performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 10827,
                        "content": "<p>Ingest the data into an Amazon Kinesis Data Stream. Process the data with an AWS Lambda function and then store the data in Amazon DynamoDB.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10828,
                        "content": "<p>Ingest the data into an Amazon SQS queue. Process the data using an AWS Lambda function and then store the data in Amazon RedShift.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10829,
                        "content": "<p>Ingest the data into an Amazon Kinesis Data Stream. Process the data with an AWS Lambda function and then store the data in Amazon RedShift.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10830,
                        "content": "<p>Ingest the data into an Amazon SQS queue. Process the data using an AWS Lambda function and then store the data in Amazon DynamoDB.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2590,
            "attributes": {
                "createdAt": "2023-09-09T20:39:52.049Z",
                "updatedAt": "2023-09-09T20:39:52.049Z",
                "content": "<p>An application running on Amazon ECS processes data and then writes objects to an Amazon S3 bucket. The application requires permissions to make the S3 API calls.</p><p>How can a Solutions Architect ensure the application has the required permissions?</p>",
                "answerExplanation": "<p>With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task. Applications must sign their AWS API requests with AWS credentials, and this feature provides a strategy for managing credentials for your applications to use, similar to the way that Amazon EC2 instance profiles provide credentials to EC2 instances.</p><p>You define the IAM role to use in your task definitions, or you can use a taskRoleArn override when running a task manually with the RunTask API operation.</p><p>Note that there are instances roles and task roles that you can assign in ECS when using the EC2 launch type. The task role is better when you need to assign permissions for just that specific task:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-02-13-cca043ef28407cf8376847f12e461e32.jpg\"></p><p><strong>CORRECT: </strong>\"Create an IAM role that has read/write permissions to the bucket and update the task definition to specify the role as the taskRoleArn\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Update the S3 policy in IAM to allow read/write access from Amazon ECS, and then relaunch the container\" is incorrect. Policies must be assigned to tasks using IAM Roles and this is not mentioned here.</p><p><strong>INCORRECT:</strong> \"Create a set of Access Keys with read/write permissions to the bucket and update the task credential ID\" is incorrect. You cannot update the task credential ID with access keys and roles should be used instead.</p><p><strong>INCORRECT:</strong> \"Attach an IAM policy with read/write permissions to the bucket to an IAM group and add the container instances to the group\" is incorrect. You cannot add container instances to an IAM group.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 10831,
                        "content": "<p>Attach an IAM policy with read/write permissions to the bucket to an IAM group and add the container instances to the group.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10832,
                        "content": "<p>Update the S3 policy in IAM to allow read/write access from Amazon ECS, and then relaunch the container.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10833,
                        "content": "<p>Create an IAM role that has read/write permissions to the bucket and update the task definition to specify the role as the taskRoleArn.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10834,
                        "content": "<p>Create a set of Access Keys with read/write permissions to the bucket and update the task credential ID.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2591,
            "attributes": {
                "createdAt": "2023-09-09T20:39:52.137Z",
                "updatedAt": "2023-09-09T20:39:52.137Z",
                "content": "<p>A solutions architect is optimizing a website for real-time streaming and on-demand videos. The website’s users are located around the world and the solutions architect needs to optimize the performance for both the real-time and on-demand streaming.</p><p>Which service should the solutions architect choose?</p>",
                "answerExplanation": "<p>Amazon CloudFront can be used to stream video to users across the globe using a wide variety of protocols that are layered on top of HTTP. This can include both on-demand video as well as real time streaming video.</p><p><strong>CORRECT: </strong>\"Amazon CloudFront\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Global Accelerator\" is incorrect as this would be an expensive way of getting the content closer to users compared to using CloudFront. As this is a use case for CloudFront and there are so many edge locations it is the better option.</p><p><strong>INCORRECT:</strong> \"Amazon Route 53\" is incorrect as you still need a solution for getting the content closer to users.</p><p><strong>INCORRECT:</strong> \"Amazon S3 Transfer Acceleration\" is incorrect as this is used to accelerate uploads of data to Amazon S3 buckets.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/streaming/\">https://aws.amazon.com/cloudfront/streaming/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-streaming-video.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-streaming-video.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 10835,
                        "content": "<p>Amazon Route 53</p>",
                        "isValid": false
                    },
                    {
                        "id": 10836,
                        "content": "<p>Amazon S3 Transfer Acceleration</p>",
                        "isValid": false
                    },
                    {
                        "id": 10837,
                        "content": "<p>AWS Global Accelerator</p>",
                        "isValid": false
                    },
                    {
                        "id": 10838,
                        "content": "<p>Amazon CloudFront</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2592,
            "attributes": {
                "createdAt": "2023-09-09T20:39:52.236Z",
                "updatedAt": "2023-09-09T20:39:52.236Z",
                "content": "<p>A company runs an eCommerce application that uses an Amazon Aurora database. The database performs well except for short periods when monthly sales reports are run. A Solutions Architect has reviewed metrics in Amazon CloudWatch and found that the Read Ops and CPUUtilization metrics are spiking during the periods when the sales reports are run.</p><p>What is the MOST cost-effective solution to solve this performance issue?</p>",
                "answerExplanation": "<p>The simplest and most cost-effective option is to use an Aurora Replica. The replica can serve read operations which will mean the reporting application can run reports on the replica endpoint without causing any performance impact on the production database.</p><p><strong>CORRECT: </strong>\"Create an Aurora Replica and use the replica endpoint for reporting\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable storage Auto Scaling for the Amazon Aurora database\" is incorrect. Aurora storage automatically scales based on volumes, there is no storage auto scaling feature for Aurora.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Redshift data warehouse and run the reporting there\" is incorrect. This would be less cost-effective and require more work in copying the data to the data warehouse.</p><p><strong>INCORRECT:</strong> \"Modify the Aurora database to use an instance class with more CPU\" is incorrect. This may not resolve the storage performance issues and could be more expensive depending on instances sizes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
                "options": [
                    {
                        "id": 10839,
                        "content": "<p>Create an Amazon Redshift data warehouse and run the reporting there.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10840,
                        "content": "<p>Create an Aurora Replica and use the replica endpoint for reporting.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10841,
                        "content": "<p>Modify the Aurora database to use an instance class with more CPU.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10842,
                        "content": "<p>Enable storage Auto Scaling for the Amazon Aurora database.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2593,
            "attributes": {
                "createdAt": "2023-09-09T20:39:52.349Z",
                "updatedAt": "2023-09-09T20:39:52.349Z",
                "content": "<p>A website runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The website has a mix of dynamic and static content. Customers around the world are reporting performance issues with the website.</p><p>Which set of actions will improve website performance for users worldwide?</p>",
                "answerExplanation": "<p>Amazon CloudFront is a content delivery network (CDN) that improves website performance by caching content at edge locations around the world. It can serve both dynamic and static content. This is the best solution for improving the performance of the website.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudFront distribution and configure the ALB as an origin. Then update the Amazon Route 53 record to point to the CloudFront distribution\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a latency-based Amazon Route 53 record for the ALB. Then launch new EC2 instances with larger instance sizes and register the instances with the ALB\" is incorrect. Latency routing routes based on the latency between the client and AWS. There is no mention in the answer about creating the new instances in another region therefore the only advantage is in using larger instance sizes. For a dynamic site this adds complexity in keeping the instances in sync.</p><p><strong>INCORRECT:</strong> \"Launch new EC2 instances hosting the same web application in different Regions closer to the users. Use an AWS Transit Gateway to connect customers to the closest region\" is incorrect as Transit Gateway is a service for connecting on-premises networks and VPCs to a single gateway.</p><p><strong>INCORRECT:</strong> \"Migrate the website to an Amazon S3 bucket in the Regions closest to the users. Then create an Amazon Route 53 geolocation record to point to the S3 buckets\" is incorrect as with S3 you can only host static websites, not dynamic websites.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/dynamic-content/\">https://aws.amazon.com/cloudfront/dynamic-content/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 10843,
                        "content": "<p>Create a latency-based Amazon Route 53 record for the ALB. Then launch new EC2 instances with larger instance sizes and register the instances with the ALB</p>",
                        "isValid": false
                    },
                    {
                        "id": 10844,
                        "content": "<p>Migrate the website to an Amazon S3 bucket in the Regions closest to the users. Then create an Amazon Route 53 geolocation record to point to the S3 buckets</p>",
                        "isValid": false
                    },
                    {
                        "id": 10845,
                        "content": "<p>Create an Amazon CloudFront distribution and configure the ALB as an origin. Then update the Amazon Route 53 record to point to the CloudFront distribution</p>",
                        "isValid": true
                    },
                    {
                        "id": 10846,
                        "content": "<p>Launch new EC2 instances hosting the same web application in different Regions closer to the users. Use an AWS Transit Gateway to connect customers to the closest region</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2594,
            "attributes": {
                "createdAt": "2023-09-09T20:39:52.445Z",
                "updatedAt": "2023-09-09T20:39:52.445Z",
                "content": "<p>An Amazon RDS PostgreSQL database is configured as Multi-AZ. A solutions architect needs to scale read performance and the solution must be configured for high availability. What is the most cost-effective solution?</p>",
                "answerExplanation": "<p>You can create a read replica as a Multi-AZ DB instance. Amazon RDS creates a standby of your replica in another Availability Zone for failover support for the replica. Creating your read replica as a Multi-AZ DB instance is independent of whether the source database is a Multi-AZ DB instance.</p><p><strong>CORRECT: </strong>\"Create a read replica as a Multi-AZ DB instance\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Deploy a read replica in a different AZ to the master DB instance\" is incorrect as this does not provide high availability for the read replica</p><p><strong>INCORRECT:</strong> \"Deploy a read replica using Amazon ElastiCache\" is incorrect as ElastiCache is not used to create read replicas of RDS database.</p><p><strong>INCORRECT:</strong> \"Deploy a read replica in the same AZ as the master DB instance\" is incorrect as this solution does not include HA for the read replica.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/\">https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 10847,
                        "content": "<p>Deploy a read replica in a different AZ to the master DB instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10848,
                        "content": "<p>Deploy a read replica in the same AZ as the master DB instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 10849,
                        "content": "<p>Create a read replica as a Multi-AZ DB instance</p>",
                        "isValid": true
                    },
                    {
                        "id": 10850,
                        "content": "<p>Deploy a read replica using Amazon ElastiCache</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2595,
            "attributes": {
                "createdAt": "2023-09-09T20:39:52.553Z",
                "updatedAt": "2023-09-09T20:39:52.553Z",
                "content": "<p>A company needs to store data from an application. Data in the application changes frequently. All levels of stored data must be audited under a new regulation which the company adheres to.</p><p>Application storage capacity is running out on the company's on-premises infrastructure. To comply with the new regulation, a solutions architect must offload some data securely to AWS to relieve the on-premises capacity issues.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>AWS Storage Gateway is a set of hybrid cloud storage services that provide on-premises access to virtually unlimited cloud storage. Secondly AWS CloudTrail monitors and records account activity across your AWS infrastructure, giving you control over storage, analysis, and remediation actions.</p><p><strong>CORRECT: </strong>\"Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Move existing data to Amazon S3 using AWS DataSync. Log data events using AWS CloudTrail” is incorrect. AWS DataSync is a secure, online service that automates and accelerates moving data between on-premises and AWS storage service and is not designed as a hybrid storage service.</p><p><strong>INCORRECT:</strong> \"The existing data can be transferred to Amazon S3 with the help of Amazon S3 Transfer Acceleration. Log data events using AWS CloudTrail” is incorrect. Amazon S3 Transfer Acceleration is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration is designed to optimize transfer speeds from across the world into S3 buckets and is not a migration service.</p><p><strong>INCORRECT:</strong> \"Move the existing data to Amazon S3 with AWS Snowcone. Using AWS CloudTrail, you can log management events\" is incorrect. AWS Snowcone is not suitable as a hybrid cloud service.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/storagegateway/\">https://aws.amazon.com/storagegateway/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>",
                "options": [
                    {
                        "id": 10851,
                        "content": "<p>Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10852,
                        "content": "<p>Move the existing data to Amazon S3 with AWS Snowcone. Using AWS CloudTrail, you can log management events.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10853,
                        "content": "<p>Move existing data to Amazon S3 using AWS DataSync. Log data events using AWS CloudTrail.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10854,
                        "content": "<p>The existing data can be transferred to Amazon S3 with the help of Amazon S3 Transfer Acceleration. Log data events using AWS CloudTrail.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2596,
            "attributes": {
                "createdAt": "2023-09-09T20:39:52.646Z",
                "updatedAt": "2023-09-09T20:39:52.646Z",
                "content": "<p>A Solutions Architect must select the most appropriate database service for two use cases. A team of data scientists perform complex queries on a data warehouse that take several hours to complete. Another team of scientists need to run fast, repeat queries and update dashboards for customer support staff.</p><p>Which solution delivers these requirements MOST cost-effectively?</p>",
                "answerExplanation": "<p>RedShift is a columnar data warehouse DB that is ideal for running long complex queries. RedShift can also improve performance for repeat queries by caching the result and returning the cached result when queries are re-run. Dashboard, visualization, and business intelligence (BI) tools that execute repeat queries see a significant boost in performance due to result caching.</p><p><strong>CORRECT: </strong>\"RedShift for both use cases\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"RDS for both use cases\" is incorrect. RDS may be a good fit for the fast queries (not for the complex queries) but you now have multiple DBs to manage and multiple sets of data which is not going to be cost-effective.</p><p><strong>INCORRECT:</strong> \"RedShift for the analytics use case and ElastiCache in front of RedShift for the customer support dashboard\" is incorrect. You could put ElastiCache in front of the RedShift DB and this would provide good performance for the fast, repeat queries. However, it is not essential and would add cost to the solution so is not the most cost-effective option available.</p><p><strong>INCORRECT:</strong> \"RedShift for the analytics use case and RDS for the customer support dashboard\" is incorrect as RedShift is a better fit for both use cases.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-redshift-introduces-result-caching-for-sub-second-response-for-repeat-queries/\">https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-redshift-introduces-result-caching-for-sub-second-response-for-repeat-queries/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-redshift/\">https://digitalcloud.training/amazon-redshift/</a></p>",
                "options": [
                    {
                        "id": 10855,
                        "content": "<p>RedShift for the analytics use case and ElastiCache in front of RedShift for the customer support dashboard</p>",
                        "isValid": false
                    },
                    {
                        "id": 10856,
                        "content": "<p>RedShift for the analytics use case and RDS for the customer support dashboard</p>",
                        "isValid": false
                    },
                    {
                        "id": 10857,
                        "content": "<p>RedShift for both use cases</p>",
                        "isValid": true
                    },
                    {
                        "id": 10858,
                        "content": "<p>RDS for both use cases</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2597,
            "attributes": {
                "createdAt": "2023-09-09T20:39:52.824Z",
                "updatedAt": "2023-09-09T20:39:52.824Z",
                "content": "<p>An organization manages its own MySQL databases, which are hosted on Amazon EC2 instances. In response to changes in demand, replication and scaling are manually managed by the company. It is essential for the company to have a way to add and remove compute capacity as needed from the database tier. The solution also must offer improved performance, scaling, and durability with minimal effort from operations.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>Amazon Aurora provides automatic scaling for MySQL databases. Amazon Aurora provides built-in security, continuous backups, serverless compute, up to 15 read replicas, automated multi-Region replication, and integrations with other AWS services. Aurora Serverless reduces any effort from operations also to provision any servers to manage the database cluster.</p><p><strong>CORRECT: </strong>\"Migrate the databases to Amazon Aurora Serverless (Aurora MySQL)” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the databases to Amazon Aurora Serverless (Aurora PostgreSQL)” is incorrect. Although PostgreSQL is an option for Aurora, the database schema would have to be changed to allow PostgreSQL compatibility.</p><p><strong>INCORRECT:</strong> \"Consolidate the databases into a single MySQL database. Use larger EC2 instances for the larger database” is incorrect. Databases run on a larger EC2 instance would not provide improved performance, scaling, and durability.</p><p><strong>INCORRECT:</strong> \"For the database tier, create an EC2 Auto Scaling group. Create a new database environment and migrate the existing databases” is incorrect. This would improve the scalability of the solution but would still have to be heavily managed by the organization, something that would not be needed with Aurora Serverless.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/serverless/\">https://aws.amazon.com/rds/aurora/serverless/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 10859,
                        "content": "<p>Migrate the databases to Amazon Aurora Serverless (Aurora PostgreSQL).</p>",
                        "isValid": false
                    },
                    {
                        "id": 10860,
                        "content": "<p>For the database tier, create an EC2 Auto Scaling group. Create a new database environment and migrate the existing databases.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10861,
                        "content": "<p>Consolidate the databases into a single MySQL database. Use larger EC2 instances for the larger database.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10862,
                        "content": "<p>Migrate the databases to Amazon Aurora Serverless (Aurora MySQL).</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2598,
            "attributes": {
                "createdAt": "2023-09-09T20:39:52.950Z",
                "updatedAt": "2023-09-09T20:39:52.950Z",
                "content": "<p>A solutions architect is required to move 750 TB of data from a branch office's network-attached file system to Amazon S3 Glacier. The branch office’s internet connection is poor, and the solution must not saturate the connection. Normal business traffic loads must not be affected by the migration.</p><p>What is the MOST cost-effective solution?</p>",
                "answerExplanation": "<p>The AWS Snow Family consists of several physical devices which can be used for edge computing, and to migrate large amounts of data into Amazon S3.</p><p>The process for using AWS Snowball is as follows:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-27_10-10-17-cd2ebeb567b249ba6ff7c90a61e6d34c.jpg\"><p>The solutions architect will need 10 Snowball devices to fulfill the capacity required in this instance as each Snowball contains up to 80TB of usable storage.</p><p><strong>CORRECT: </strong>\"Order 10 AWS Snowball appliances and select an Amazon S3 bucket as the destination. Create a lifecycle policy to transition the S3 objects to Amazon S3 Glacier” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a site-to-site VPN connection directly to an Amazon S3 bucket, Enforce the connection with an VPC Endpoint\" is incorrect, as although this would achieve the solution, it would be using the branch’s internet connection and would saturate it - preventing normal business activities from taking place.</p><p><strong>INCORRECT:</strong> \"Order 10 AWS Snowball appliances and point these appliances to an S3 Glacier vault and put in place a bucket policy which will only allow access via a VPC endpoint\" is incorrect as S3 Glacier is not a viable destination for Snowball, and you need to place it into S3 Standard first then transition the data in Glacier after the fact.</p><p><strong>INCORRECT:</strong> \"Copy the files directly from the network-attached file system to Amazon S3. Build a lifecycle policy to move the S3 objects across storage classes into Amazon S3 Glacier\" is incorrect. It is not possible to mount third-party NAS appliance to an S3 bucket. You could use a service like AWS DataSync to move data from the network attached file system into S3, however this would still be traveling over the branch’s internet line.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 10863,
                        "content": "<p>Order 10 AWS Snowball appliances and select an Amazon S3 bucket as the destination. Create a lifecycle policy to transition the S3 objects to Amazon S3 Glacier.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10864,
                        "content": "<p>Order 10 AWS Snowball appliances and point these appliances to an S3 Glacier vault and put in place a bucket policy which will only allow access via a VPC endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10865,
                        "content": "<p>Create a site-to-site VPN connection directly to an Amazon S3 bucket, Enforce the connection with an VPC Endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10866,
                        "content": "<p>Copy the files directly from the network-attached file system to Amazon S3. Build a lifecycle policy to move the S3 objects across storage classes into Amazon S3 Glacier.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2599,
            "attributes": {
                "createdAt": "2023-09-09T20:39:53.039Z",
                "updatedAt": "2023-09-09T20:39:53.039Z",
                "content": "<p>A company runs several NFS file servers in an on-premises data center. The NFS servers must run periodic backups to Amazon S3 using automatic synchronization for small volumes of data.</p><p>Which solution meets these requirements and is MOST cost-effective?</p>",
                "answerExplanation": "<p>AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data between on-premises systems and AWS Storage services, as well as between AWS Storage services. DataSync can copy data between Network File System (NFS) shares, or Server Message Block (SMB) shares, self-managed object storage, <a href=\"https://aws.amazon.com/snowcone/\">AWS Snowcone</a>, <a href=\"https://aws.amazon.com/s3/\">Amazon Simple Storage Service (Amazon S3)</a> buckets, <a href=\"https://aws.amazon.com/efs/\">Amazon Elastic File System (Amazon EFS)</a> file systems, and <a href=\"https://aws.amazon.com/fsx/windows/\">Amazon FSx for Windows File Server</a> file systems.</p><p>This is the most cost-effective solution from the answer options available.</p><p><strong>CORRECT: </strong>\"Set up an AWS DataSync agent on the on-premises servers and sync the data to Amazon S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Set up an SFTP sync using AWS Transfer for SFTP to sync data from on premises to Amazon S3\" is incorrect. This solution does not provide the scheduled synchronization features of AWS DataSync and is more expensive.</p><p><strong>INCORRECT:</strong> \"Set up AWS Glue to extract the data from the NFS shares and load it into Amazon S3\" is incorrect. AWS Glue is an ETL service and cannot be used for copying data to Amazon S3 from NFS shares.</p><p><strong>INCORRECT:</strong> \"Set up an AWS Direct Connect connection between the on-premises data center and AWS and copy the data to Amazon S3\" is incorrect. An AWS Direct Connect connection is an expensive option and no solution is provided for automatic synchronization.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/datasync/features/\">https://aws.amazon.com/datasync/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 10867,
                        "content": "<p>Set up an SFTP sync using AWS Transfer for SFTP to sync data from on premises to Amazon S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10868,
                        "content": "<p>Set up an AWS Direct Connect connection between the on-premises data center and AWS and copy the data to Amazon S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10869,
                        "content": "<p>Set up AWS Glue to extract the data from the NFS shares and load it into Amazon S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10870,
                        "content": "<p>Set up an AWS DataSync agent on the on-premises servers and sync the data to Amazon S3.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2600,
            "attributes": {
                "createdAt": "2023-09-09T20:39:53.134Z",
                "updatedAt": "2023-09-09T20:39:53.134Z",
                "content": "<p>A company is deploying a new web application that will run on Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. The application requires a shared storage solution that offers strong consistency as the content will be regularly updated.</p><p>Which solution requires the LEAST amount of effort?</p>",
                "answerExplanation": "<p>Amazon EFS is a fully-managed service that makes it easy to set up, scale, and cost-optimize file storage in the Amazon Cloud. EFS file systems are accessible to Amazon EC2 instances via a file system interface (using standard operating system file I/O APIs) and support full file system access semantics (such as strong consistency and file locking).</p><p>EFS is a good solution for when you need to attach a shared filesystem to multiple EC2 instances across multiple Availability Zones.</p><p><strong>CORRECT: </strong>\"Create an Amazon Elastic File System (Amazon EFS) file system and mount it on the individual Amazon EC2 instances\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon S3 bucket to store the web content and use Amazon CloudFront to deliver the content\" is incorrect as this may require more effort in terms of reprogramming the application to use the S3 API.</p><p><strong>INCORRECT:</strong> \"Create a shared Amazon Block Store (Amazon EBS) volume and mount it on the individual Amazon EC2 instances\" is incorrect. Please note that you can multi-attach an EBS volume to multiple EC2 instances but the instances must be in the same AZ.</p><p><strong>INCORRECT:</strong> \"Create a volume gateway using AWS Storage Gateway to host the data and mount it to the Auto Scaling group\" is incorrect as a storage gateway is used on-premises.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/efs/faq/\">https://aws.amazon.com/efs/faq/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
                "options": [
                    {
                        "id": 10871,
                        "content": "<p>Create a volume gateway using AWS Storage Gateway to host the data and mount it to the Auto Scaling group</p>",
                        "isValid": false
                    },
                    {
                        "id": 10872,
                        "content": "<p>Create an Amazon S3 bucket to store the web content and use Amazon CloudFront to deliver the content</p>",
                        "isValid": false
                    },
                    {
                        "id": 10873,
                        "content": "<p>Create an Amazon Elastic File System (Amazon EFS) file system and mount it on the individual Amazon EC2 instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 10874,
                        "content": "<p>Create a shared Amazon Block Store (Amazon EBS) volume and mount it on the individual Amazon EC2 instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2601,
            "attributes": {
                "createdAt": "2023-09-09T20:39:53.236Z",
                "updatedAt": "2023-09-09T20:39:53.236Z",
                "content": "<p>A High Performance Computing (HPC) application will be migrated to AWS. The application requires low network latency and high throughput between nodes and will be deployed in a single AZ.</p><p>How should the application be deployed for best inter-node performance?</p>",
                "answerExplanation": "<p>A cluster placement group provides low latency and high throughput for instances deployed in a single AZ. It is the best way to provide the performance required for this application.</p><p><strong>CORRECT: </strong>\"In a cluster placement group\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"In a partition placement group\" is incorrect. A partition placement group is used for grouping instances into logical segments. It provides control and visibility into instance placement but is not the best option for performance.</p><p><strong>INCORRECT:</strong> \"In a spread placement group\" is incorrect. A spread placement group is used to spread instances across underlying hardware. It is not the best option for performance.</p><p><strong>INCORRECT:</strong> \"Behind a Network Load Balancer (NLB)\" is incorrect. A network load balancer is used for distributing incoming connections, this does assist with inter-node performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 10875,
                        "content": "<p>In a spread placement group</p>",
                        "isValid": false
                    },
                    {
                        "id": 10876,
                        "content": "<p>In a partition placement group</p>",
                        "isValid": false
                    },
                    {
                        "id": 10877,
                        "content": "<p>In a cluster placement group</p>",
                        "isValid": true
                    },
                    {
                        "id": 10878,
                        "content": "<p>Behind a Network Load Balancer (NLB)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2602,
            "attributes": {
                "createdAt": "2023-09-09T20:39:53.349Z",
                "updatedAt": "2023-09-09T20:39:53.349Z",
                "content": "<p>A company is planning to upload a large quantity of sensitive data to Amazon S3. The company’s security department require that the data is encrypted before it is uploaded.</p><p>Which option meets these requirements?</p>",
                "answerExplanation": "<p>The requirement is that the objects must be encrypted before they are uploaded. The only option presented that meets this requirement is to use client-side encryption. You then have two options for the keys you use to perform the encryption:</p><p> • Use a customer master key (CMK) stored in AWS Key Management Service (AWS KMS).</p><p> • Use a master key that you store within your application.</p><p>In this case the correct answer is to use an AWS KMS key. Note that you cannot use client-side encryption with keys managed by Amazon S3.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-00-48-9dad6f62c8e7c9414b51a788e346c78c.jpg\"></p><p><strong>CORRECT: </strong>\"Use client-side encryption with a master key stored in AWS KMS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use client-side encryption with Amazon S3 managed encryption keys\" is incorrect. You cannot use S3 managed keys with client-side encryption.</p><p><strong>INCORRECT:</strong> \"Use server-side encryption with customer-provided encryption keys\" is incorrect. With this option the encryption takes place after uploading to S3.</p><p><strong>INCORRECT:</strong> \"Use server-side encryption with keys stored in KMS\" is incorrect. With this option the encryption takes place after uploading to S3.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSideEncryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 10879,
                        "content": "<p>Use client-side encryption with a master key stored in AWS KMS.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10880,
                        "content": "<p>Use server-side encryption with customer-provided encryption keys.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10881,
                        "content": "<p>Use server-side encryption with keys stored in KMS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10882,
                        "content": "<p>Use client-side encryption with Amazon S3 managed encryption keys.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2603,
            "attributes": {
                "createdAt": "2023-09-09T20:39:53.459Z",
                "updatedAt": "2023-09-09T20:39:53.459Z",
                "content": "<p>A company has refactored a legacy application to run as two microservices using Amazon ECS. The application processes data in two parts and the second part of the process takes longer than the first.</p><p>How can a solutions architect integrate the microservices and allow them to scale independently?</p>",
                "answerExplanation": "<p>This is a good use case for Amazon SQS. The microservices must be decoupled so they can scale independently. An Amazon SQS queue will enable microservice 1 to add messages to the queue. Microservice 2 can then pick up the messages and process them. This ensures that if there’s a spike in traffic on the frontend, messages do not get lost due to the backend process not being ready to process them.</p><p><strong>CORRECT: </strong>\"Implement code in microservice 1 to send data to an Amazon SQS queue. Implement code in microservice 2 to process messages from the queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement code in microservice 1 to send data to an Amazon S3 bucket. Use S3 event notifications to invoke microservice 2\" is incorrect as a message queue would be preferable to an S3 bucket.</p><p><strong>INCORRECT:</strong> \"Implement code in microservice 1 to publish data to an Amazon SNS topic. Implement code in microservice 2 to subscribe to this topic\" is incorrect as notifications to topics are pushed to subscribers. In this case we want the second microservice to pickup the messages when ready (pull them).</p><p><strong>INCORRECT:</strong> \"Implement code in microservice 1 to send data to Amazon Kinesis Data Firehose. Implement code in microservice 2 to read from Kinesis Data Firehose\" is incorrect as this is not how Firehose works. Firehose sends data directly to destinations, it is not a message queue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 10883,
                        "content": "<p>Implement code in microservice 1 to send data to an Amazon SQS queue. Implement code in microservice 2 to process messages from the queue</p>",
                        "isValid": true
                    },
                    {
                        "id": 10884,
                        "content": "<p>Implement code in microservice 1 to publish data to an Amazon SNS topic. Implement code in microservice 2 to subscribe to this topic</p>",
                        "isValid": false
                    },
                    {
                        "id": 10885,
                        "content": "<p>Implement code in microservice 1 to send data to Amazon Kinesis Data Firehose. Implement code in microservice 2 to read from Kinesis Data Firehose</p>",
                        "isValid": false
                    },
                    {
                        "id": 10886,
                        "content": "<p>Implement code in microservice 1 to send data to an Amazon S3 bucket. Use S3 event notifications to invoke microservice 2</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2604,
            "attributes": {
                "createdAt": "2023-09-09T20:39:53.548Z",
                "updatedAt": "2023-09-09T20:39:53.548Z",
                "content": "<p>A company has acquired another business and needs to migrate their 50TB of data into AWS within 1 month. They also require a secure, reliable and private connection to the AWS cloud.</p><p>How are these requirements best accomplished?</p>",
                "answerExplanation": "<p>AWS Direct Connect provides a secure, reliable and private connection. However, lead times are often longer than 1 month so it cannot be used to migrate data within the timeframes. Therefore, it is better to use AWS Snowball to move the data and order a Direct Connect connection to satisfy the other requirement later on. In the meantime the organization can use an AWS VPN for secure, private access to their VPC.</p><p><strong>CORRECT: </strong>\"Migrate data using AWS Snowball. Provision an AWS VPN initially and order a Direct Connect link\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Provision an AWS Direct Connect connection and migrate the data over the link\" is incorrect due to the lead time for installation.</p><p><strong>INCORRECT:</strong> \"Launch a Virtual Private Gateway (VPG) and migrate the data over the AWS VPN\" is incorrect. A VPG is the AWS-side of an AWS VPN. A VPN does not provide a private connection and is not reliable as you can never guarantee the latency over the Internet</p><p><strong>INCORRECT:</strong> \"Provision an AWS VPN CloudHub connection and migrate the data over redundant links\" is incorrect. AWS VPN CloudHub is a service for connecting multiple sites into your VPC over VPN connections. It is not used for aggregating links and the limitations of Internet bandwidth from the company where the data is stored will still be an issue. It also uses the public Internet so is not a private or reliable connection.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p><p><a href=\"https://aws.amazon.com/directconnect/\">https://aws.amazon.com/directconnect/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-direct-connect/\">https://digitalcloud.training/aws-direct-connect/</a></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 10887,
                        "content": "<p>Provision an AWS VPN CloudHub connection and migrate the data over redundant links</p>",
                        "isValid": false
                    },
                    {
                        "id": 10888,
                        "content": "<p>Launch a Virtual Private Gateway (VPG) and migrate the data over the AWS VPN</p>",
                        "isValid": false
                    },
                    {
                        "id": 10889,
                        "content": "<p>Migrate data using AWS Snowball. Provision an AWS VPN initially and order a Direct Connect link</p>",
                        "isValid": true
                    },
                    {
                        "id": 10890,
                        "content": "<p>Provision an AWS Direct Connect connection and migrate the data over the link</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2605,
            "attributes": {
                "createdAt": "2023-09-09T20:39:53.637Z",
                "updatedAt": "2023-09-09T20:39:53.637Z",
                "content": "<p>An application has been deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). A Solutions Architect must improve the security posture of the application and minimize the impact of a DDoS attack on resources.</p><p>Which of the following solutions is MOST effective?</p>",
                "answerExplanation": "<p>A rate-based rule tracks the rate of requests for each originating IP address, and triggers the rule action on IPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span.</p><p>You can use this type of rule to put a temporary block on requests from an IP address that's sending excessive requests. By default, AWS WAF aggregates requests based on the IP address from the web request origin, but you can configure the rule to use an IP address from an HTTP header, like X-Forwarded-For, instead.</p><p><strong>CORRECT: </strong>\"Configure an AWS WAF ACL with rate-based rules. Enable the WAF ACL on the Application Load Balancer\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a custom AWS Lambda function that monitors for suspicious traffic and modifies a network ACL when a potential DDoS attack is identified\" is incorrect. There’s not description here of how Lambda is going to monitor for traffic.</p><p><strong>INCORRECT:</strong> \"Enable VPC Flow Logs and store them in Amazon S3. Use Amazon Athena to parse the logs and identify and block potential DDoS attacks\" is incorrect. Amazon Athena is not able to block DDoS attacks, another service would be needed.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the Application Load Balancer and configure Amazon CloudWatch to monitor the access logs and trigger a Lambda function when potential attacks are identified. Configure the Lambda function to modify the ALBs security group and block the attack\" is incorrect. Access logs are exported to S3 but not to CloudWatch. Also, it would not be possible to block an attack from a specific IP using a security group (while still allowing any other source access) as they do not support deny rules.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>",
                "options": [
                    {
                        "id": 10891,
                        "content": "<p>Enable access logs on the Application Load Balancer and configure Amazon CloudWatch to monitor the access logs and trigger a Lambda function when potential attacks are identified. Configure the Lambda function to modify the ALBs security group and block the attack.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10892,
                        "content": "<p>Enable VPC Flow Logs and store them in Amazon S3. Use Amazon Athena to parse the logs and identify and block potential DDoS attacks.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10893,
                        "content": "<p>Configure an AWS WAF ACL with rate-based rules. Enable the WAF ACL on the Application Load Balancer.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10894,
                        "content": "<p>Create a custom AWS Lambda function that monitors for suspicious traffic and modifies a network ACL when a potential DDoS attack is identified.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2606,
            "attributes": {
                "createdAt": "2023-09-09T20:39:53.736Z",
                "updatedAt": "2023-09-09T20:39:53.736Z",
                "content": "<p>A company has a file share on a Microsoft Windows Server in an on-premises data center. The server uses a local network attached storage (NAS) device to store several terabytes of files. The management team require a reduction in the data center footprint and to minimize storage costs by moving on-premises storage to AWS.</p><p>What should a Solutions Architect do to meet these requirements?</p>",
                "answerExplanation": "<p>An AWS Storage Gateway File Gateway provides your applications a file interface to seamlessly store files as objects in Amazon S3, and access them using industry standard file protocols. This removes the files from the on-premises NAS device and provides a method of directly mounting the file share for on-premises servers and clients.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-04-55-d6065baf4b15649e450a4fb5cd195708.jpg\"></p><p><strong>CORRECT: </strong>\"Configure an AWS Storage Gateway file gateway\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure an AWS Storage Gateway as a volume gateway\" is incorrect. A volume gateway uses block-based protocols. In this case we are replacing a NAS device which uses file-level protocols so the best option is a file gateway.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EFS volume and use an IPSec VPN\" is incorrect. EFS can be mounted over a VPN but it would have more latency than using a storage gateway.</p><p><strong>INCORRECT:</strong> \"Create an Amazon S3 bucket and an S3 gateway endpoint\" is incorrect. S3 is an object-level storage system so is not suitable for this use case. A gateway endpoint is a method of accessing S3 using private addresses from your VPC, not from your data center.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/storagegateway/faqs/\">https://aws.amazon.com/storagegateway/faqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>",
                "options": [
                    {
                        "id": 10895,
                        "content": "<p>Create an Amazon S3 bucket and an S3 gateway endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10896,
                        "content": "<p>Configure an AWS Storage Gateway file gateway.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10897,
                        "content": "<p>Configure an AWS Storage Gateway as a volume gateway.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10898,
                        "content": "<p>Create an Amazon EFS volume and use an IPSec VPN.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2607,
            "attributes": {
                "createdAt": "2023-09-09T20:39:53.845Z",
                "updatedAt": "2023-09-09T20:39:53.845Z",
                "content": "<p>To accelerate experimentation and agility, a company allows developers to apply existing IAM policies to existing IAM roles. Nevertheless, the security operations team is concerned that the developers could attach the existing administrator policy, circumventing any other security policies.</p><p>How should a solutions architect address this issue?</p>",
                "answerExplanation": "<p>Setting a permissions boundary is the easiest and safest way to ensure that any IAM users cannot assume any elevated permissions. A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.</p><p><strong>CORRECT: </strong>\"Set a permissions boundary on the developer IAM role that denies attaching administrator access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Send an alert every time a developer creates a new policy using an Amazon SNS topic” is incorrect. This does not explicitly prevent any developers from attaching the policy, only sending a notification.</p><p><strong>INCORRECT:</strong> \"Disable IAM activity across all organizational accounts using service control policies\" is incorrect. If all IAM activity was disabled across all accounts within the Organizational unit, each IAM user would not be able to do anything within the account.</p><p><strong>INCORRECT:</strong> \"Assign all IAM duties to the security operations team and prevent developers from attaching policies\" is incorrect. The easiest way to do this is to use a permissions boundary, to make sure the permissions are being administered appropriately.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 10899,
                        "content": "<p>Set a permissions boundary on the developer IAM role that denies attaching administrator access.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10900,
                        "content": "<p>Disable IAM activity across all organizational accounts using service control policies.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10901,
                        "content": "<p>Send an alert every time a developer creates a new policy using an Amazon SNS topic.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10902,
                        "content": "<p>Assign all IAM duties to the security operations team and prevent developers from attaching policies.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2608,
            "attributes": {
                "createdAt": "2023-09-09T20:39:53.950Z",
                "updatedAt": "2023-09-09T20:39:53.950Z",
                "content": "<p>A company allows its developers to attach existing IAM policies to existing IAM roles to enable faster experimentation and agility. However, the security operations team is concerned that the developers could attach the existing administrator policy, which would allow the developers to circumvent any other security policies.</p><p>How should a solutions architect address this issue?</p>",
                "answerExplanation": "<p>The permissions boundary for an IAM entity (user or role) sets the maximum permissions that the entity can have. This can change the effective permissions for that user or role. The effective permissions for an entity are the permissions that are granted by all the policies that affect the user or role. Within an account, the permissions for an entity can be affected by identity-based policies, resource-based policies, permissions boundaries, Organizations SCPs, or session policies.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-18_13-16-47-d821741ed0b16e76b6ea87b337c0aeeb.JPG\"></p><p>Therefore, the solutions architect can set an IAM permissions boundary on the developer IAM role that explicitly denies attaching the administrator policy.</p><p><strong>CORRECT: </strong>\"Set an IAM permissions boundary on the developer IAM role that explicitly denies attaching the administrator policy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic to send an alert every time a developer creates a new policy\" is incorrect as this would mean investigating every incident which is not an efficient solution.</p><p><strong>INCORRECT:</strong> \"Use service control policies to disable IAM activity across all accounts in the organizational unit\" is incorrect as this would prevent the developers from being able to work with IAM completely.</p><p><strong>INCORRECT:</strong> \"Prevent the developers from attaching any policies and assign all IAM duties to the security operations team\" is incorrect as this is not necessary. The requirement is to allow developers to work with policies, the solution needs to find a secure way of achieving this.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 10903,
                        "content": "<p>Prevent the developers from attaching any policies and assign all IAM duties to the security operations team</p>",
                        "isValid": false
                    },
                    {
                        "id": 10904,
                        "content": "<p>Create an Amazon SNS topic to send an alert every time a developer creates a new policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 10905,
                        "content": "<p>Set an IAM permissions boundary on the developer IAM role that explicitly denies attaching the administrator policy</p>",
                        "isValid": true
                    },
                    {
                        "id": 10906,
                        "content": "<p>Use service control policies to disable IAM activity across all accounts in the organizational unit</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2609,
            "attributes": {
                "createdAt": "2023-09-09T20:39:54.070Z",
                "updatedAt": "2023-09-09T20:39:54.070Z",
                "content": "<p>A company uses a Microsoft Windows file share for storing documents and media files. Users access the share using Microsoft Windows clients and are authenticated using the company’s Active Directory. The chief information officer wants to move the data to AWS as they are approaching capacity limits. The existing user authentication and access management system should be used.</p><p>How can a Solutions Architect meet these requirements?</p>",
                "answerExplanation": "<p>Amazon FSx for Windows File Server makes it easy for you to launch and scale reliable, performant, and secure shared file storage for your applications and end users. With Amazon FSx, you can launch highly durable and available file systems that can span multiple availability zones (AZs) and can be accessed from up to thousands of compute instances using the industry-standard Server Message Block (SMB) protocol.</p><p>It provides a rich set of administrative and security features, and integrates with Microsoft Active Directory (AD). To serve a wide spectrum of workloads, Amazon FSx provides high levels of file system throughput and IOPS and consistent sub-millisecond latencies.</p><p>You can also mount FSx file systems from on-premises using a VPN or Direct Connect connection. This topology is depicted in the image below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-06-44-85272267dfd4d12b26b830545075dc49.jpg\"></p><p><strong>CORRECT: </strong>\"Move the documents and media files to an Amazon FSx for Windows File Server file system\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Move the documents and media files to an Amazon FSx for Lustre file system\" is incorrect. FSx for Lustre is not suitable for migrating a Microsoft Windows File Server implementation.</p><p><strong>INCORRECT:</strong> \"Move the documents and media files to an Amazon Elastic File System and use POSIX permissions\" is incorrect. EFS can be used from on-premises over a VPN or DX connection but POSIX permissions are very different to Microsoft permissions and mean a different authentication and access management solution is required.</p><p><strong>INCORRECT:</strong> \"Move the documents and media files to an Amazon Simple Storage Service bucket and apply bucket ACLs\" is incorrect. S3 with bucket ACLs would be changing to an object-based storage system and a completely different authentication and access management solution.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/windows/features/?nc=sn&amp;loc=2\">https://aws.amazon.com/fsx/windows/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 10907,
                        "content": "<p>Move the documents and media files to an Amazon Simple Storage Service bucket and apply bucket ACLs.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10908,
                        "content": "<p>Move the documents and media files to an Amazon Elastic File System and use POSIX permissions.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10909,
                        "content": "<p>Move the documents and media files to an Amazon FSx for Windows File Server file system.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10910,
                        "content": "<p>Move the documents and media files to an Amazon FSx for Lustre file system.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2610,
            "attributes": {
                "createdAt": "2023-09-09T20:39:54.203Z",
                "updatedAt": "2023-09-09T20:39:54.203Z",
                "content": "<p>A company have 500 TB of data in an on-premises file share that needs to be moved to Amazon S3 Glacier. The migration must not saturate the company’s low-bandwidth internet connection and the migration must be completed within a few weeks.</p><p>What is the MOST cost-effective solution?</p>",
                "answerExplanation": "<p>As the company’s internet link is low-bandwidth uploading directly to Amazon S3 (ready for transition to Glacier) would saturate the link. The best alternative is to use AWS Snowball appliances. The Snowball edge appliance can hold up to 80 TB of data so 7 devices would be required to migrate 500 TB of data.</p><p>Snowball moves data into AWS using a hardware device and the data is then copied into an Amazon S3 bucket of your choice. From there, lifecycle policies can transition the S3 objects to Amazon S3 Glacier.</p><p><strong>CORRECT: </strong>\"Order 7 AWS Snowball appliances and select an Amazon S3 bucket as the destination. Create a lifecycle policy to transition the S3 objects to Amazon S3 Glacier\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Order 7 AWS Snowball appliances and select an S3 Glacier vault as the destination. Create a bucket policy to enforce a VPC endpoint\" is incorrect as you cannot set a Glacier vault as the destination, it must be an S3 bucket. You also can’t enforce a VPC endpoint using a bucket policy.</p><p><strong>INCORRECT:</strong> \"Create an AWS Direct Connect connection and migrate the data straight into Amazon Glacier\" is incorrect as this is not the most cost-effective option and takes time to setup.</p><p><strong>INCORRECT:</strong> \"Use AWS Global Accelerator to accelerate upload and optimize usage of the available bandwidth\" is incorrect as this service is not used for accelerating or optimizing the upload of data from on-premises networks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/snowball/latest/developer-guide/specifications.html\">https://docs.aws.amazon.com/snowball/latest/developer-guide/specifications.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 10911,
                        "content": "<p>Use AWS Global Accelerator to accelerate upload and optimize usage of the available bandwidth</p>",
                        "isValid": false
                    },
                    {
                        "id": 10912,
                        "content": "<p>Order 7 AWS Snowball appliances and select an S3 Glacier vault as the destination. Create a bucket policy to enforce a VPC endpoint</p>",
                        "isValid": false
                    },
                    {
                        "id": 10913,
                        "content": "<p>Order 7 AWS Snowball appliances and select an Amazon S3 bucket as the destination. Create a lifecycle policy to transition the S3 objects to Amazon S3 Glacier</p>",
                        "isValid": true
                    },
                    {
                        "id": 10914,
                        "content": "<p>Create an AWS Direct Connect connection and migrate the data straight into Amazon Glacier</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2611,
            "attributes": {
                "createdAt": "2023-09-09T20:39:54.304Z",
                "updatedAt": "2023-09-09T20:39:54.304Z",
                "content": "<p>A shared services VPC is being setup for use by several AWS accounts. An application needs to be securely shared from the shared services VPC. The solution should not allow consumers to connect to other instances in the VPC.</p><p>How can this be setup with the least administrative effort? (choose 2)</p>",
                "answerExplanation": "<p>VPCs can be shared among multiple AWS accounts. Resources can then be shared amongst those accounts. However, to restrict access so that consumers cannot connect to other instances in the VPC the best solution is to use PrivateLink to create an endpoint for the application. The endpoint type will be an interface endpoint and it uses an NLB in the shared services VPC.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-25_01-23-21-8df694147b53b155434524cea44ac356.jpg\"></p><p><strong>CORRECT: </strong>\"Create a Network Load Balancer (NLB)\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Use AWS PrivateLink to expose the application as an endpoint service\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS ClassicLink to expose the application as an endpoint service\" is incorrect. ClassicLink allows you to link EC2-Classic instances to a VPC in your account, within the same region. This solution does not include EC2-Classic which is now deprecated (replaced by VPC).</p><p><strong>INCORRECT:</strong> \"Setup VPC peering between each AWS VPC\" is incorrect. VPC peering could be used along with security groups to restrict access to the application and other instances in the VPC. However, this would be administratively difficult as you would need to ensure that you maintain the security groups as resources and addresses change.</p><p><strong>INCORRECT:</strong> \"Configure security groups to restrict access\" is incorrect. This could be used in conjunction with VPC peering but better method is to use PrivateLink for this use case.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-virtual-private-clouds-can-now-be-shared-with-other-aws-accounts/\">https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-virtual-private-clouds-can-now-be-shared-with-other-aws-accounts/</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/\">https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/aws-privatelink.pdf\">https://d1.awsstatic.com/whitepapers/aws-privatelink.pdf</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 10915,
                        "content": "<p>Use AWS ClassicLink to expose the application as an endpoint service</p>",
                        "isValid": false
                    },
                    {
                        "id": 10916,
                        "content": "<p>Use AWS PrivateLink to expose the application as an endpoint service</p>",
                        "isValid": true
                    },
                    {
                        "id": 10917,
                        "content": "<p>Create a Network Load Balancer (NLB)</p>",
                        "isValid": true
                    },
                    {
                        "id": 10918,
                        "content": "<p>Setup VPC peering between each AWS VPC</p>",
                        "isValid": false
                    },
                    {
                        "id": 10919,
                        "content": "<p>Configure security groups to restrict access</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2612,
            "attributes": {
                "createdAt": "2023-09-09T20:39:54.396Z",
                "updatedAt": "2023-09-09T20:39:54.396Z",
                "content": "<p>A company requires a solution for replicating data to AWS for disaster recovery. Currently, the company uses scripts to copy data from various sources to a Microsoft Windows file server in the on-premises data center. The company also requires that a small amount of recent files are accessible to administrators with low latency.</p><p>What should a Solutions Architect recommend to meet these requirements?</p>",
                "answerExplanation": "<p>The best solution here is to use an AWS Storage Gateway File Gateway virtual appliance in the on-premises data center. This can be accessed the same protocols as the existing Microsoft Windows File Server (SMB/CIFS). Therefore, the script simply needs to be updated to point to the gateway.</p><p>The file gateway will then store data on Amazon S3 and has a local cache for data that can be accessed at low latency. The file gateway provides an excellent method of enabling file protocol access to low cost S3 object storage.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-10-11-1743a212516d41e3c51c5f97fc6d09c5.jpg\"></p><p><strong>CORRECT: </strong>\"Update the script to copy data to an AWS Storage Gateway for File Gateway virtual appliance instead of the on-premises file server\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Update the script to copy data to an Amazon EBS volume instead of the on-premises file server\" is incorrect. This would also need an attached EC2 instance running Windows to be able to mount using the same protocols and would not offer any local low-latency access.</p><p><strong>INCORRECT:</strong> \"Update the script to copy data to an Amazon EFS volume instead of the on-premises file server\" is incorrect. This solution would not provide a local cache.</p><p><strong>INCORRECT:</strong> \"Update the script to copy data to an Amazon S3 Glacier archive instead of the on-premises file server\" is incorrect. This would not provide any immediate access with low-latency.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
                "options": [
                    {
                        "id": 10920,
                        "content": "<p>Update the script to copy data to an AWS Storage Gateway for File Gateway virtual appliance instead of the on-premises file server.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10921,
                        "content": "<p>Update the script to copy data to an Amazon S3 Glacier archive instead of the on-premises file server.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10922,
                        "content": "<p>Update the script to copy data to an Amazon EBS volume instead of the on-premises file server.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10923,
                        "content": "<p>Update the script to copy data to an Amazon EFS volume instead of the on-premises file server.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2613,
            "attributes": {
                "createdAt": "2023-09-09T20:39:54.525Z",
                "updatedAt": "2023-09-09T20:39:54.525Z",
                "content": "<p>A solutions architect is finalizing the architecture for a distributed database that will run across multiple Amazon EC2 instances. Data will be replicated across all instances so the loss of an instance will not cause loss of data. The database requires block storage with low latency and throughput that supports up to several million transactions per second per server.</p><p>Which storage solution should the solutions architect use?</p>",
                "answerExplanation": "<p>An <em>instance store</em> provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-18_12-54-38-9844f13d68a10a257851c57f0f920791.JPG\"></p><p>Some instance types use NVMe or SATA-based solid state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures.</p><p>In this scenario the data is replicated and fault tolerant so the best option to provide the level of performance required is to use instance store volumes.</p><p><strong>CORRECT: </strong>\"Amazon EC2 instance store\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EBS \" is incorrect. The Elastic Block Store (EBS) is a block storage device but as the data is distributed and fault tolerant a better option for performance would be to use instance stores.</p><p><strong>INCORRECT:</strong> \"Amazon EFS \" is incorrect as EFS is not a block device, it is a filesystem that is accessed using the NFS protocol.</p><p><strong>INCORRECT:</strong> \"Amazon S3\" is incorrect as S3 is an object-based storage system, not a block-based storage system.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 10924,
                        "content": "<p>Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 10925,
                        "content": "<p>Amazon EFS</p>",
                        "isValid": false
                    },
                    {
                        "id": 10926,
                        "content": "<p>Amazon EC2 instance store</p>",
                        "isValid": true
                    },
                    {
                        "id": 10927,
                        "content": "<p>Amazon EBS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2614,
            "attributes": {
                "createdAt": "2023-09-09T20:39:54.834Z",
                "updatedAt": "2023-09-09T20:39:54.834Z",
                "content": "<p>IAM permissions-related Access Denied errors and Unauthorized errors need to be analyzed and troubleshooted by a company. AWS CloudTrail has been enabled at the company.</p><p>Which solution will meet these requirements with the LEAST effort?</p>",
                "answerExplanation": "<p>CloudTrail logs are stored natively within an S3 bucket , which can then be easily integrated with Amazon QuickSight. Amazon QuickSight is a data visualization tool which will show any IAM permissions-related Access Denied errors and Unauthorized errors. </p><p><strong>CORRECT: </strong>\"Search CloudTrail logs with Amazon QuickSight. Create a dashboard to identify the errors” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a custom script and execute it against CloudTrail logs to find errors using AWS Batch” is incorrect. Writing custom scripts is inevitably more effort than using the native connection between AWS CloudTrail and Amazon QuickSight.</p><p><strong>INCORRECT:</strong> \"Search CloudTrail logs with Amazon RedShift. Create a dashboard to identify the errors” is incorrect. Amazon RedShift would not be a simple way of achieving this outcome.</p><p><strong>INCORRECT:</strong> “Write custom scripts to query CloudTrail logs using AWS Glue” is incorrect. AWS Batch requires configuring several EC2 instances to run jobs for you. This, and writing custom scripts will significantly increase the effort involved.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/logging-using-cloudtrail.html\">https://docs.aws.amazon.com/quicksight/latest/user/logging-using-cloudtrail.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-analytics-services/\">https://digitalcloud.training/aws-analytics-services/</a></p>",
                "options": [
                    {
                        "id": 10928,
                        "content": "<p>Search CloudTrail logs with Amazon RedShift. Create a dashboard to identify the errors</p>",
                        "isValid": false
                    },
                    {
                        "id": 10929,
                        "content": "<p>Write custom scripts to query CloudTrail logs using AWS Glue</p>",
                        "isValid": false
                    },
                    {
                        "id": 10930,
                        "content": "<p>Create a custom script and execute it against CloudTrail logs to find errors using AWS Batch</p>",
                        "isValid": false
                    },
                    {
                        "id": 10931,
                        "content": "<p>Search CloudTrail logs with Amazon QuickSight. Create a dashboard to identify the errors.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2615,
            "attributes": {
                "createdAt": "2023-09-09T20:39:55.001Z",
                "updatedAt": "2023-09-09T20:39:55.001Z",
                "content": "<p>A website is running on Amazon EC2 instances and access is restricted to a limited set of IP ranges. A solutions architect is planning to migrate static content from the website to an Amazon S3 bucket configured as an origin for an Amazon CloudFront distribution. Access to the static content must be restricted to the same set of IP addresses.</p><p>Which combination of steps will meet these requirements? (Select TWO.)</p>",
                "answerExplanation": "<p>To prevent users from circumventing the controls implemented on CloudFront (using WAF or presigned URLs / signed cookies) you can use an origin access identity (OAI). An OAI is a special CloudFront user that you associate with a distribution.</p><p>The next step is to change the permissions either on your Amazon S3 bucket or on the files in your bucket so that only the origin access identity has read permission (or read and download permission). This can be implemented through a bucket policy.</p><p>To control access at the CloudFront layer the AWS Web Application Firewall (WAF) can be used. With WAF you must create an ACL that includes the IP restrictions required and then associate the web ACL with the CloudFront distribution.</p><p><strong>CORRECT: </strong>\"Create an origin access identity (OAI) and associate it with the distribution. Change the permissions in the bucket policy so that only the OAI can read the objects\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Create an AWS WAF web ACL that includes the same IP restrictions that exist in the EC2 security group. Associate this new web ACL with the CloudFront distribution\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create an origin access identity (OAI) and associate it with the distribution. Generate presigned URLs that limit access to the OAI\" is incorrect. Presigned URLs can be used to protect access to CloudFront but they cannot be used to limit access to an OAI.</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF web ACL that includes the same IP restrictions that exist in the EC2 security group. Associate this new web ACL with the Amazon S3 bucket\" is incorrect. The Web ACL should be associated with CloudFront, not S3.</p><p><strong>INCORRECT:</strong> \"Attach the existing security group that contains the IP restrictions to the Amazon CloudFront distribution\" is incorrect. You cannot attach a security group to a CloudFront distribution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>",
                "options": [
                    {
                        "id": 10932,
                        "content": "<p>Create an origin access identity (OAI) and associate it with the distribution. Generate presigned URLs that limit access to the OAI.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10933,
                        "content": "<p>Create an AWS WAF web ACL that includes the same IP restrictions that exist in the EC2 security group. Associate this new web ACL with the CloudFront distribution.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10934,
                        "content": "<p>Create an origin access identity (OAI) and associate it with the distribution. Change the permissions in the bucket policy so that only the OAI can read the objects.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10935,
                        "content": "<p>Create an AWS WAF web ACL that includes the same IP restrictions that exist in the EC2 security group. Associate this new web ACL with the Amazon S3 bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10936,
                        "content": "<p>Attach the existing security group that contains the IP restrictions to the Amazon CloudFront distribution.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2616,
            "attributes": {
                "createdAt": "2023-09-09T20:39:55.120Z",
                "updatedAt": "2023-09-09T20:39:55.120Z",
                "content": "<p>An application has been migrated to Amazon EC2 Linux instances. The EC2 instances run several 1-hour tasks on a schedule. There is no common programming language among these tasks, as they were written by different teams. Currently, these tasks run on a single instance, which raises concerns about performance and scalability. To resolve these concerns, a solutions architect must implement a solution.</p><p>Which solution will meet these requirements with the LEAST Operational overhead?</p>",
                "answerExplanation": "<p>The best solution is to create an AMI of the EC2 instance, and then use it as a template for which to launch additional instances using an Auto Scaling Group. This removes the issues of performance, scalability, and redundancy by allowing the EC2 instances to automatically scale and be launched across multiple Availability Zones.</p><p><strong>CORRECT: </strong>\"Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events)\" is incorrect. AWS Batch is designed to run jobs across multiple instances, there would be less operational overhead by creating an AMI instead.</p><p><strong>INCORRECT:</strong> \"Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs\" is incorrect. Converting your EC2 instances to containers is not the easiest way to achieve this task.</p><p><strong>INCORRECT:</strong> \"Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events)\" is incorrect. The maximum execution time for a Lambda function is 15 minutes, making it unsuitable for tasks running on a one-hour schedule.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 10937,
                        "content": "<p>Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10938,
                        "content": "<p>Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).</p>",
                        "isValid": false
                    },
                    {
                        "id": 10939,
                        "content": "<p>Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events).</p>",
                        "isValid": false
                    },
                    {
                        "id": 10940,
                        "content": "<p>Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2617,
            "attributes": {
                "createdAt": "2023-09-09T20:39:55.218Z",
                "updatedAt": "2023-09-09T20:39:55.218Z",
                "content": "<p>A solutions architect has been tasked with designing a highly resilient hybrid cloud architecture connecting an on-premises data center and AWS. The network should include AWS Direct Connect (DX).</p><p>Which DX configuration offers the HIGHEST resiliency?</p>",
                "answerExplanation": "<p>The most resilient solution is to configure DX connections at multiple DX locations. This ensures that any issues impacting a single DX location do not affect availability of the network connectivity to AWS.</p><p>Take note of the following AWS recommendations for resiliency:</p><p><em>AWS recommends connecting from multiple data centers for physical location redundancy. When designing remote connections, consider using redundant hardware and telecommunications providers. Additionally, it is a best practice to use dynamically routed, active/active connections for automatic load balancing and failover across redundant network connections. Provision sufficient network capacity to ensure that the failure of one network connection does not overwhelm and degrade redundant connections.</em></p><p>The diagram below is an example of an architecture that offers high resiliency:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-11-27_10-13-36-8f8a2d3dea9984331c8894ec400a4303.jpg\"></p><p><strong>CORRECT: </strong>\"Configure DX connections at multiple DX locations\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure a DX connection with an encrypted VPN on top of it\" is incorrect. A VPN that is separate to the DX connection can be a good backup. But a VPN on top of the DX connection does not help. Also, encryption provides security but not resilience.</p><p><strong>INCORRECT:</strong> \"Configure multiple public VIFs on top of a DX connection\" is incorrect. Virtual interfaces do not add resiliency as resiliency must be designed into the underlying connection.</p><p><strong>INCORRECT:</strong> \"Configure multiple private VIFs on top of a DX connection\" is incorrect. Virtual interfaces do not add resiliency as resiliency must be designed into the underlying connection.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-direct-connect/\">https://digitalcloud.training/aws-direct-connect/</a></p>",
                "options": [
                    {
                        "id": 10941,
                        "content": "<p>Configure multiple public VIFs on top of a DX connection.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10942,
                        "content": "<p>Configure a DX connection with an encrypted VPN on top of it.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10943,
                        "content": "<p>Configure DX connections at multiple DX locations.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10944,
                        "content": "<p>Configure multiple private VIFs on top of a DX connection.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2618,
            "attributes": {
                "createdAt": "2023-09-09T20:39:55.317Z",
                "updatedAt": "2023-09-09T20:39:55.317Z",
                "content": "<p>A gaming company uses a web application to display scores. An Application Load Balancer is used to distribute load across Amazon EC2 instances which run the application. The application stores data in an Amazon RDS for MySQL database. Users are experiencing long delays and interruptions due to poor database read performance. It is important for the company to improve the user experience while minimizing changes to the application's architecture.</p><p>What should a solutions architect do to meet these requirements?</p>",
                "answerExplanation": "<p>Amazon ElastiCache is a fully managed, in-memory caching service supporting flexible, real-time use cases. You can use ElastiCache for caching, which accelerates application and database performance, or as a primary data store for use cases that don't require durability like session stores, gaming leaderboards, streaming, and analytics. ElastiCache is compatible with Redis and Memcached.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-27_10-08-02-b5493317752977ece44ad2ece006f1c3.jpg\"><p>As the issues in this instance are caused by poor read performance, a caching solution would offload reads from the primary database instance and allow the application to perform better.<br><strong>CORRECT: </strong>\"Use Amazon ElastiCache to cache the database layer” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Connect the database and the application layer using RDS Proxy” is incorrect. RDS proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability. It does not however specifically improve read performance like a caching layer would.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda instead of Amazon EC2 for the compute layer\" is incorrect. AWS Lambda would not be a suitable use case for hosting leaderboards, as the maximum timeout is 15 minutes, and the issue lies with the database layer, not the compute later.</p><p><strong>INCORRECT:</strong> \"Use an Amazon DynamoDB table instead of RDS\" is incorrect. Migrating to DynamoDB would not help the load of reads on the database and changing the schema of the database would cause massive changes to the application’s architecture.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 10945,
                        "content": "<p>Use Amazon ElastiCache to cache the database layer.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10946,
                        "content": "<p>Use an Amazon DynamoDB table instead of RDS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10947,
                        "content": "<p>Use AWS Lambda instead of Amazon EC2 for the compute layer.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10948,
                        "content": "<p>Connect the database and the application layer using RDS Proxy.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2619,
            "attributes": {
                "createdAt": "2023-09-09T20:39:55.415Z",
                "updatedAt": "2023-09-09T20:39:55.415Z",
                "content": "<p>A group of business analysts perform read-only SQL queries on an Amazon RDS database. The queries have become quite numerous and the database has experienced some performance degradation. The queries must be run against the latest data. A Solutions Architect must solve the performance problems with minimal changes to the existing web application.</p><p>What should the Solutions Architect recommend?</p>",
                "answerExplanation": "<p>The performance issues can be easily resolved by offloading the SQL queries the business analysts are performing to a read replica. This ensures that data that is being queries is up to date and the existing web application does not require any modifications to take place.</p><p><strong>CORRECT: </strong>\"Create a read replica of the primary database and instruct the business analysts to direct queries to the replica\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Export the data to Amazon S3 and instruct the business analysts to run their queries using Amazon Athena\" is incorrect. The data must be the latest data and this method would therefore require constant exporting of the data.</p><p><strong>INCORRECT:</strong> \"Load the data into an Amazon Redshift cluster and instruct the business analysts to run their queries against the cluster\" is incorrect. This is another solution that requires exporting the loading the data which means over time it will become out of date.</p><p><strong>INCORRECT:</strong> \"Load the data into Amazon ElastiCache and instruct the business analysts to run their queries against the ElastiCache endpoint\" is incorrect. It will be much easier to create a read replica. ElastiCache requires updates to the application code so should be avoided in this example.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 10949,
                        "content": "<p>Export the data to Amazon S3 and instruct the business analysts to run their queries using Amazon Athena.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10950,
                        "content": "<p>Load the data into Amazon ElastiCache and instruct the business analysts to run their queries against the ElastiCache endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10951,
                        "content": "<p>Load the data into an Amazon Redshift cluster and instruct the business analysts to run their queries against the cluster.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10952,
                        "content": "<p>Create a read replica of the primary database and instruct the business analysts to direct queries to the replica.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2620,
            "attributes": {
                "createdAt": "2023-09-09T20:39:55.517Z",
                "updatedAt": "2023-09-09T20:39:55.517Z",
                "content": "<p>Some objects that are uploaded to Amazon S3 standard storage class are initially accessed frequently for a period of 30 days. Then, objects are infrequently accessed for up to 90 days. After that, the objects are no longer needed.</p><p>How should lifecycle management be configured?</p>",
                "answerExplanation": "<p>In this scenario we need to keep the objects in the STANDARD storage class for 30 days as the objects are being frequently accessed. We can configure a lifecycle action that then transitions the objects to INTELLIGENT_TIERING, STANDARD_IA, or ONEZONE_IA. After that we don’t need the objects so they can be expired.</p><p>All other options do not meet the stated requirements or are not supported lifecycle transitions. For example:</p><p>· You cannot transition to REDUCED_REDUNDANCY from any storage class.</p><p>· Transitioning from STANDARD_IA to ONEZONE_IA is possible but we do not want to keep the objects so it incurs unnecessary costs.</p><p>· Transitioning to GLACIER is possible but again incurs unnecessary costs.</p><p><strong>CORRECT: </strong>\"Transition to ONEZONE_IA after 30 days. After 90 days expire the objects \" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Transition to STANDARD_IA after 30 days. After 90 days transition to GLACIER\" is incorrect.</p><p><strong>INCORRECT:</strong> \"Transition to STANDARD_IA after 30 days. After 90 days transition to ONEZONE_IA\" is incorrect.</p><p><strong>INCORRECT:</strong> \"Transition to REDUCED_REDUNDANCY after 30 days. After 90 days expire the objects \" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 10953,
                        "content": "<p>Transition to STANDARD_IA after 30 days. After 90 days transition to ONEZONE_IA</p>",
                        "isValid": false
                    },
                    {
                        "id": 10954,
                        "content": "<p>Transition to ONEZONE_IA after 30 days. After 90 days expire the objects</p>",
                        "isValid": true
                    },
                    {
                        "id": 10955,
                        "content": "<p>Transition to STANDARD_IA after 30 days. After 90 days transition to GLACIER</p>",
                        "isValid": false
                    },
                    {
                        "id": 10956,
                        "content": "<p>Transition to REDUCED_REDUNDANCY after 30 days. After 90 days expire the objects</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2621,
            "attributes": {
                "createdAt": "2023-09-09T20:39:55.635Z",
                "updatedAt": "2023-09-09T20:39:55.635Z",
                "content": "<p>A company runs an application in an Amazon VPC that requires access to an Amazon Elastic Container Service (Amazon ECS) cluster that hosts an application in another VPC. The company’s security team requires that all traffic must not traverse the internet.</p><p>Which solution meets this requirement?</p>",
                "answerExplanation": "<p>The correct solution is to use AWS PrivateLink in a service provider model. In this configuration a network load balancer will be implemented in the service provider VPC (the one with the ECS cluster in this example), and a PrivateLink endpoint will be created in the consumer VPC (the one with the company’s application).</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-11-25-32f12f4c4d3d65de75af283af88992aa.jpg\"></p><p><strong>CORRECT: </strong>\"Create a Network Load Balancer in one VPC and an AWS PrivateLink endpoint for Amazon ECS in another VPC\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer and AWS PrivateLink endpoint for Amazon ECS in the VPC that hosts the ECS cluster\" is incorrect. The endpoint should be in the consumer VPC, not the service provider VPC (see the diagram above).</p><p><strong>INCORRECT:</strong> \"Configure a gateway endpoint for Amazon ECS. Update the route table to include an entry pointing to the ECS cluster\" is incorrect. You cannot use a gateway endpoint to connect to a private service. Gateway endpoints are only for S3 and DynamoDB.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon Route 53 private hosted zone for each VPC. Use private records to resolve internal IP addresses in each VPC\" is incorrect. This does not provide a mechanism for resolving each other’s addresses and there’s no method of internal communication using private IPs such as VPC peering.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/privatelink/\">https://aws.amazon.com/privatelink/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 10957,
                        "content": "<p>Configure an Amazon Route 53 private hosted zone for each VPC. Use private records to resolve internal IP addresses in each VPC.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10958,
                        "content": "<p>Configure a gateway endpoint for Amazon ECS. Update the route table to include an entry pointing to the ECS cluster.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10959,
                        "content": "<p>Create a Network Load Balancer in one VPC and an AWS PrivateLink endpoint for Amazon ECS in another VPC.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10960,
                        "content": "<p>Create a Network Load Balancer and AWS PrivateLink endpoint for Amazon ECS in the VPC that hosts the ECS cluster.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2622,
            "attributes": {
                "createdAt": "2023-09-09T20:39:55.733Z",
                "updatedAt": "2023-09-09T20:39:55.733Z",
                "content": "<p>A solutions architect is designing a two-tier web application. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the company.</p><p>How should security groups be configured in this situation? (Select TWO.)</p>",
                "answerExplanation": "<p>In this scenario an inbound rule is required to allow traffic from any internet client to the web front end on SSL/TLS port 443. The source should therefore be set to 0.0.0.0/0 to allow any inbound traffic.</p><p>To secure the connection from the web frontend to the database tier, an outbound rule should be created from the public EC2 security group with a destination of the private EC2 security group. The port should be set to 1433 for MySQL. The private EC2 security group will also need to allow inbound traffic on 1433 from the public EC2 security group.</p><p>This configuration can be seen in the diagram:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-31-06-6dcaf8d88c9ec27f837343a0ae2630f9.jpg\"></p><p><strong>CORRECT: </strong>\"Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0 and to allow outbound traffic on port 1433 to the RDS\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0\" is incorrect as this is configured backwards.</p><p><strong>INCORRECT:</strong> \"Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier\" is incorrect as the MySQL database instance does not need to send outbound traffic on either of these ports.</p><p><strong>INCORRECT:</strong> \"Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier\" is incorrect as the database tier does not need to allow inbound traffic on port 443.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 10961,
                        "content": "<p>Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0 and to allow outbound traffic on port 1433 to the RDS</p>",
                        "isValid": true
                    },
                    {
                        "id": 10962,
                        "content": "<p>Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0</p>",
                        "isValid": false
                    },
                    {
                        "id": 10963,
                        "content": "<p>Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier</p>",
                        "isValid": true
                    },
                    {
                        "id": 10964,
                        "content": "<p>Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier</p>",
                        "isValid": false
                    },
                    {
                        "id": 10965,
                        "content": "<p>Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2623,
            "attributes": {
                "createdAt": "2023-09-09T20:39:55.848Z",
                "updatedAt": "2023-09-09T20:39:55.848Z",
                "content": "<p>An Amazon EC2 instance runs in a VPC network, and the network must be secured by a solutions architect. The EC2 instances contain highly sensitive data and have been launched in private subnets. Company policy restricts EC2 instances that run in the VPC from accessing the internet. The instances need to access the software repositories using a third-party URL to download and install software product updates. All other internet traffic must be blocked, with no exceptions.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>The AWS Network Firewall is a managed service that makes it easy to deploy essential network protections for all your Amazon Virtual Private Clouds, and you can then use domain list rules to block HTTP or HTTPS traffic to domains identified as low-reputation, or that are known or suspected to be associated with malware or botnets.</p><p><strong>CORRECT: </strong>\"Configure the route table for the private subnet so that it routes the outbound traffic to an AWS Network Firewall firewall then configure domain list rule groups\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF web ACL. Filter traffic requests based on source and destination IP address ranges with custom rules\" is incorrect. AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. It is designed to protect your applications from malicious traffic, not your VPC.</p><p><strong>INCORRECT:</strong> \"Establish strict inbound rules for your security groups. Specify the URLs of the authorized software repositories on the internet in your outbound rule\" is incorrect. You cannot specify URLs in security group rules so this would not work.</p><p><strong>INCORRECT:</strong> \"Place an Application Load Balancer in front of your EC2 instances. Direct all outbound traffic to the ALB. For outbound access to the internet, use a URL-based rule listener in the ALB's target group\" is incorrect. The ALB would not work as this sits within the VPC and is unable to control traffic entering and leaving the VPC itself.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/network-firewall/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc\">https://aws.amazon.com/network-firewall/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 10966,
                        "content": "<p>Place an Application Load Balancer in front of your EC2 instances. Direct all outbound traffic to the ALB. For outbound access to the internet, use a URL-based rule listener in the ALB's target group.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10967,
                        "content": "<p>Create an AWS WAF web ACL. Filter traffic requests based on source and destination IP address ranges with custom rules.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10968,
                        "content": "<p>Configure the route table for the private subnet so that it routes the outbound traffic to an AWS Network Firewall firewall then configure domain list rule groups.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10969,
                        "content": "<p>Establish strict inbound rules for your security groups. Specify the URLs of the authorized software repositories on the internet in your outbound rule.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2624,
            "attributes": {
                "createdAt": "2023-09-09T20:39:55.962Z",
                "updatedAt": "2023-09-09T20:39:55.962Z",
                "content": "<p>A web application is being deployed on an Amazon ECS cluster using the Fargate launch type. The application is expected to receive a large volume of traffic initially. The company wishes to ensure that performance is good for the launch and that costs reduce as demand decreases</p><p>What should a solutions architect recommend?</p>",
                "answerExplanation": "<p>Amazon ECS uses the AWS Application Auto Scaling service to scales tasks. This is configured through Amazon ECS using Amazon ECS Service Auto Scaling.</p><p>A Target Tracking Scaling policy increases or decreases the number of tasks that your service runs based on a target value for a specific metric. For example, in the image below the tasks will be scaled when the average CPU breaches 80% (as reported by CloudWatch):</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-11-27_10-01-16-332b18084ef8decec084431b6cc2ec4c.jpg\"></p><p><strong>CORRECT: </strong>\"Use Amazon ECS Service Auto Scaling with target tracking policies to scale when an Amazon CloudWatch alarm is breached\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 Auto Scaling with simple scaling policies to scale when an Amazon CloudWatch alarm is breached\" is incorrect</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 Auto Scaling to scale out on a schedule and back in once the load decreases\" is incorrect</p><p><strong>INCORRECT:</strong> \"Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon CloudWatch alarm\" is incorrect</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 10970,
                        "content": "<p>Use Amazon EC2 Auto Scaling with simple scaling policies to scale when an Amazon CloudWatch alarm is breached.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10971,
                        "content": "<p>Use Amazon EC2 Auto Scaling to scale out on a schedule and back in once the load decreases.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10972,
                        "content": "<p>Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon CloudWatch alarm.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10973,
                        "content": "<p>Use Amazon ECS Service Auto Scaling with target tracking policies to scale when an Amazon CloudWatch alarm is breached.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2625,
            "attributes": {
                "createdAt": "2023-09-09T20:39:56.070Z",
                "updatedAt": "2023-09-09T20:39:56.070Z",
                "content": "<p>A company is architecting a shared storage solution for an AWS-hosted gaming application. The company needs the ability to use Lustre clients to access data. The solution must be fully managed.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>Amazon FSx for Lustre provides fully managed shared storage with the scalability and performance of the popular Lustre file system. It is fully managed and will allow the company to attach the file system to the origin server and connect the application server to the file system.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-27_10-35-12-bdf1d22b1d9df57fe18a62c76e1321e6.jpg\"><p><strong>CORRECT: </strong>\"Create an Amazon FSx for Lustre file system. Connect the file system to the origin server. Ensure that the file system is connected to the application server” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Assign the AWS DataSync task to share the data as a mountable file system. Sync the file system with the application server” is incorrect. The solution requires a managed Lustre file system, so this would not work.</p><p><strong>INCORRECT:</strong> \"Create a file gateway with AWS Storage Gateway. Create a client-side file share using the required protocol. Share the file with the application server” is incorrect. The solution requires a managed Lustre file system, so this would not work.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Elastic File System (Amazon EFS) file system and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system” is incorrect. The solution requires a managed Lustre file system, so this would not work.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/lustre/\">https://aws.amazon.com/fsx/lustre/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 10974,
                        "content": "<p>Assign the AWS DataSync task to share the data as a mountable file system. Sync the file system with the application server.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10975,
                        "content": "<p>Create an Amazon Elastic File System (Amazon EFS) file system and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10976,
                        "content": "<p>Create an Amazon FSx for Lustre file system. Connect the file system to the origin server. Ensure that the file system is connected to the application server.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10977,
                        "content": "<p>Create a file gateway with AWS Storage Gateway. Create a client-side file share using the required protocol. Share the file with the application server.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2626,
            "attributes": {
                "createdAt": "2023-09-09T20:39:56.173Z",
                "updatedAt": "2023-09-09T20:39:56.173Z",
                "content": "<p>A small Python application is used by a company to process JSON documents and output the results to a SQL database which currently lives on-premises. The application is run thousands of times every day, and the company wants to move the application to the AWS Cloud. To maximize scalability and minimize operational overhead, the company needs a highly available solution.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>Firstly, S3 is a highly available and durable place to store these JSON documents that will be written once and read many times (WORM). As this application runs thousands of times per day, AWS Lambda would be ideal to use as it will scale whenever the application needs to be ran, and Python is a runtime environment that is natively supported by AWS Lambda, whenever the events arrive in the S3 bucket, and this could be easily achieved using S3 event notifications. Finally Amazon Aurora is a highly available and durable AWS managed database. Amazon Aurora automatically maintains six copies of your data across three Availability Zones (AZs) to adhere to your redundancy requirements.</p><p><strong>CORRECT: </strong>\"Put the JSON documents in an Amazon S3 bucket. As documents arrive in the S3 bucket, create an AWS Lambda function that runs Python code to process them. Use Amazon Aurora DB clusters to store the results\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Build an S3 bucket to place the JSON documents in. Run the Python code on multiple Amazon EC2 instances to process the documents. Store the results in a database using the Amazon Aurora Database engine” is incorrect.</p><p>Multiple EC2 instances could work, however if you wanted to use EC2 to process the JSON documents you would need to either leave the EC2 instances running all the time (not cost effective) or have them spin up and spin down thousands of times per day (this would be slow and not ideal).</p><p><strong>INCORRECT:</strong> \"Create an Amazon Elastic Block Store (Amazon EBS) volume for the JSON documents. Attach the volume to multiple Amazon EC2 instances using the EBS Multi-Attach feature. Process the documents with Python code on the EC2 instances and then extract the results to an Amazon RDS DB instance\" is incorrect.</p><p>EBS is not optimized for write once read many use-cases, and if you wanted to use EC2 to process the JSON documents you would need to either leave the EC2 instances running all the time (not cost effective) or have them spin up and spin down thousands of times per day (this would be slow and not ideal).</p><p><strong>INCORRECT:</strong> \"The JSON documents should be queued as messages in the Amazon Simple Queue Service (Amazon SQS). Using the Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type, deploy the Python code as a container. The container can be used to process SQS messages. Using Amazon RDS, store the results” is incorrect.</p><p>A queue within Amazon SQS is not designed to be used for write once read many solutions, and it is designed to be used to decouple separate layers of your architecture. Secondly, ECS for EC2 is not ideal as you would need to either leave the EC2 instances running all the time (not cost effective) or have them spin up and spin down thousands of times per day (this would be slow and not ideal) if you wanted to use ECS for EC2.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 10978,
                        "content": "<p>Build an S3 bucket to place the JSON documents in. Run the Python code on multiple Amazon EC2 instances to process the documents. Store the results in a database using the Amazon Aurora Database engine.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10979,
                        "content": "<p>Create an Amazon Elastic Block Store (Amazon EBS) volume for the JSON documents. Attach the volume to multiple Amazon EC2 instances using the EBS Multi-Attach feature. Process the documents with Python code on the EC2 instances and then extract the results to an Amazon RDS DB instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10980,
                        "content": "<p>The JSON documents should be queued as messages in the Amazon Simple Queue Service (Amazon SQS). Using the Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type, deploy the Python code as a container. The container can be used to process SQS messages. Using Amazon RDS, store the results.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10981,
                        "content": "<p>Put the JSON documents in an Amazon S3 bucket. As documents arrive in the S3 bucket, create an AWS Lambda function that runs Python code to process them. Use Amazon Aurora DB clusters to store the results.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2627,
            "attributes": {
                "createdAt": "2023-09-09T20:39:56.301Z",
                "updatedAt": "2023-09-09T20:39:56.301Z",
                "content": "<p>A stock trading startup company has a custom web application to sell trading data to its users online. The company uses Amazon DynamoDB to store its data and wants to build a new service that sends an alert to the managers of four internal teams every time a new trading event is recorded. The company does not want this new service to affect the performance of the current application.</p><p>What should a solutions architect do to meet these requirements with the LEAST amount of operational overhead?</p>",
                "answerExplanation": "<p>DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real time. This is the native way to handle this within DynamoDB, therefore will incur the least amount of operational overhead.</p><p><strong>CORRECT: </strong>\"On the table, enable Amazon DynamoDB Streams. Subscriptions can be made to a single Amazon Simple Notification Service (Amazon SNS) topic using triggers” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Write new event data to the table using DynamoDB transactions. The transactions should be configured to notify internal teams” is incorrect. With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing TransactWriteItems or TransactGetItems operation. The following sections describe API operations, capacity management, best practices, and other details about using transactional operations in DynamoDB. This is not suitable for this use case.</p><p><strong>INCORRECT:</strong> \"Use the current application to publish a message to four Amazon Simple Notification Service (Amazon SNS) topics. Each team should subscribe to one topic” is incorrect. Using four separate SNS topics will take a significant amount of overhead, and this functionality can be managed natively within DynamoDB using DynamoDB streams.</p><p><strong>INCORRECT:</strong> \"Create a custom attribute for each record to flag new items. A cron job can be written to scan the table every minute for new items and notify an Amazon Simple Queue Service (Amazon SQS) queue” is incorrect. Writing a CRON job also takes significant overhead compared to using DynamoDB streams.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 10982,
                        "content": "<p>Create a custom attribute for each record to flag new items. A cron job can be written to scan the table every minute for new items and notify an Amazon Simple Queue Service (Amazon SQS) queue.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10983,
                        "content": "<p>On the table, enable Amazon DynamoDB Streams. Subscriptions can be made to a single Amazon Simple Notification Service (Amazon SNS) topic using triggers.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10984,
                        "content": "<p>Write new event data to the table using DynamoDB transactions. The transactions should be configured to notify internal teams.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10985,
                        "content": "<p>Use the current application to publish a message to four Amazon Simple Notification Service (Amazon SNS) topics. Each team should subscribe to one topic.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2628,
            "attributes": {
                "createdAt": "2023-09-09T20:39:56.401Z",
                "updatedAt": "2023-09-09T20:39:56.401Z",
                "content": "<p>A solutions architect is designing an application on AWS. The compute layer will run in parallel across EC2 instances. The compute layer should scale based on the number of jobs to be processed. The compute layer is stateless. The solutions architect must ensure that the application is loosely coupled and the job items are durably stored.</p><p>Which design should the solutions architect use?</p>",
                "answerExplanation": "<p>In this case we need to find a durable and loosely coupled solution for storing jobs. Amazon SQS is ideal for this use case and can be configured to use dynamic scaling based on the number of jobs waiting in the queue.</p><p>To configure this scaling you can use the <em>backlog per instance</em> metric with the target value being the <em>acceptable backlog per instance</em> to maintain. You can calculate these numbers as follows:</p><p><strong>Backlog per instance</strong>: To calculate your backlog per instance, start with the ApproximateNumberOfMessages queue attribute to determine the length of the SQS queue (number of messages available for retrieval from the queue). Divide that number by the fleet's running capacity, which for an Auto Scaling group is the number of instances in the InService state, to get the backlog per instance.</p><p><strong>Acceptable backlog per instance</strong>: To calculate your target value, first determine what your application can accept in terms of latency. Then, take the acceptable latency value and divide it by the average time that an EC2 instance takes to process a message.</p><p>This solution will scale EC2 instances using Auto Scaling based on the number of jobs waiting in the SQS queue.</p><p><strong>CORRECT: </strong>\"Create an Amazon SQS queue to hold the jobs that needs to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on network usage\" is incorrect as scaling on network usage does not relate to the number of jobs waiting to be processed.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on CPU usage\" is incorrect. Amazon SNS is a notification service so it delivers notifications to subscribers. It does store data durably but is less suitable than SQS for this use case. Scaling on CPU usage is not the best solution as it does not relate to the number of jobs waiting to be processed.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of messages published to the SNS topic\" is incorrect. Amazon SNS is a notification service so it delivers notifications to subscribers. It does store data durably but is less suitable than SQS for this use case. Scaling on the number of notifications in SNS is not possible.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 10986,
                        "content": "<p>Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on network usage</p>",
                        "isValid": false
                    },
                    {
                        "id": 10987,
                        "content": "<p>Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of messages published to the SNS topic</p>",
                        "isValid": false
                    },
                    {
                        "id": 10988,
                        "content": "<p>Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on CPU usage</p>",
                        "isValid": false
                    },
                    {
                        "id": 10989,
                        "content": "<p>Create an Amazon SQS queue to hold the jobs that needs to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2629,
            "attributes": {
                "createdAt": "2023-09-09T20:39:56.542Z",
                "updatedAt": "2023-09-09T20:39:56.542Z",
                "content": "<p>A company requires a solution to allow customers to customize images that are stored in an online catalog. The image customization parameters will be sent in requests to Amazon API Gateway. The customized image will then be generated on-demand and can be accessed online.</p><p>The solutions architect requires a highly available solution. Which solution will be MOST cost-effective?</p>",
                "answerExplanation": "<p>All solutions presented are highly available. The key requirement that must be satisfied is that the solution should be cost-effective and you must choose the most cost-effective option.</p><p>Therefore, it’s best to eliminate services such as Amazon EC2 and ELB as these require ongoing costs even when they’re not used. Instead, a fully serverless solution should be used. AWS Lambda, Amazon S3 and CloudFront are the best services to use for these requirements.</p><p><strong>CORRECT: </strong>\"Use AWS Lambda to manipulate the original images to the requested customization. Store the original and manipulated images in Amazon S3. Configure an Amazon CloudFront distribution with the S3 bucket as the origin\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 instances to manipulate the original images into the requested customization. Store the original and manipulated images in Amazon S3. Configure an Elastic Load Balancer in front of the EC2 instances\" is incorrect. This is not the most cost-effective option as the ELB and EC2 instances will incur costs even when not used.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda to manipulate the original images to the requested customization. Store the original images in Amazon S3 and the manipulated images in Amazon DynamoDB. Configure an Elastic Load Balancer in front of the Amazon EC2 instances\" is incorrect. This is not the most cost-effective option as the ELB will incur costs even when not used. Also, Amazon DynamoDB will incur RCU/WCUs when running and is not the best choice for storing images.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 instances to manipulate the original images into the requested customization. Store the original images in Amazon S3 and the manipulated images in Amazon DynamoDB. Configure an Amazon CloudFront distribution with the S3 bucket as the origin\" is incorrect. This is not the most cost-effective option as the EC2 instances will incur costs even when not used</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/serverless/\">https://aws.amazon.com/serverless/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 10990,
                        "content": "<p>Use AWS Lambda to manipulate the original images to the requested customization. Store the original and manipulated images in Amazon S3. Configure an Amazon CloudFront distribution with the S3 bucket as the origin</p>",
                        "isValid": true
                    },
                    {
                        "id": 10991,
                        "content": "<p>Use Amazon EC2 instances to manipulate the original images into the requested customization. Store the original images in Amazon S3 and the manipulated images in Amazon DynamoDB. Configure an Amazon CloudFront distribution with the S3 bucket as the origin</p>",
                        "isValid": false
                    },
                    {
                        "id": 10992,
                        "content": "<p>Use AWS Lambda to manipulate the original images to the requested customization. Store the original images in Amazon S3 and the manipulated images in Amazon DynamoDB. Configure an Elastic Load Balancer in front of the Amazon EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 10993,
                        "content": "<p>Use Amazon EC2 instances to manipulate the original images into the requested customization. Store the original and manipulated images in Amazon S3. Configure an Elastic Load Balancer in front of the EC2 instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2630,
            "attributes": {
                "createdAt": "2023-09-09T20:39:56.671Z",
                "updatedAt": "2023-09-09T20:39:56.671Z",
                "content": "<p>A company is storing a large quantity of small files in an Amazon S3 bucket. An application running on an Amazon EC2 instance needs permissions to access and process the files in the S3 bucket.</p><p>Which action will MOST securely grant the EC2 instance access to the S3 bucket?</p>",
                "answerExplanation": "<p>IAM roles should be used in place of storing credentials on Amazon EC2 instances. This is the most secure way to provide permissions to EC2 as no credentials are stored and short-lived credentials are obtained using AWS STS. Additionally, the policy attached to the role should provide least privilege permissions.</p><p><strong>CORRECT: </strong>\"Create an IAM role with least privilege permissions and attach it to the EC2 instance profile\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Generate access keys and store the credentials on the EC2 instance for use in making API calls\" is incorrect. This is not best practice, IAM roles are preferred.</p><p><strong>INCORRECT:</strong> \"Create an IAM user for the application with specific permissions to the S3 bucket\" is incorrect. Instances should use IAM Roles for delegation not user accounts.</p><p><strong>INCORRECT:</strong> \"Create a bucket ACL on the S3 bucket and configure the EC2 instance ID as a grantee\" is incorrect. You cannot configure an EC2 instance ID on a bucket ACL and bucket ACLs cannot be used to restrict access in this scenario.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 10994,
                        "content": "<p>Create an IAM user for the application with specific permissions to the S3 bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10995,
                        "content": "<p>Create a bucket ACL on the S3 bucket and configure the EC2 instance ID as a grantee.</p>",
                        "isValid": false
                    },
                    {
                        "id": 10996,
                        "content": "<p>Create an IAM role with least privilege permissions and attach it to the EC2 instance profile.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10997,
                        "content": "<p>Generate access keys and store the credentials on the EC2 instance for use in making API calls.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2631,
            "attributes": {
                "createdAt": "2023-09-09T20:39:56.764Z",
                "updatedAt": "2023-09-09T20:39:56.764Z",
                "content": "<p>A company hosts a serverless application on AWS. The application consists of Amazon API Gateway, AWS Lambda, and Amazon RDS for PostgreSQL. During times of peak traffic and when traffic spikes are experienced, the company notices an increase in application errors caused by database connection timeouts. The company is looking for a solution that will reduce the number of application failures with the least amount of code changes.</p><p>What should a solutions architect do to meet these requirements?</p>",
                "answerExplanation": "<p>Amazon RDS Proxy is a fully managed, highly available database proxy for Amazon Relational Database Service (RDS) that makes applications more scalable, more resilient to database failures, and more secure. Amazon RDS Proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability.</p><p>Amazon RDS Proxy can be enabled for most applications with no code changes so this solution requires the least amount of code changes.</p><p><strong>CORRECT: </strong>\"Enable an RDS Proxy instance on your RDS Database\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Reduce the concurrency rate for your Lambda Function\" is incorrect. Concurrency is the number of requests that your function is serving at any given time. The errors are caused by an increase in connection timeouts, so editing the concurrency of your Lambda function would not solve the problem.</p><p><strong>INCORRECT:</strong> \"Change the class of the instance of your database to allow more connections\" is incorrect. Resizing the instance might help, but there will be some inevitable downtime with a PostgreSQL database on RDS. RDS Proxy is specifically designed for this reason and would incur no downtime.</p><p><strong>INCORRECT:</strong> \"Change the database to an Amazon DynamoDB database with on-demand scaling\" is incorrect as this would require significant application changes to accommodate the NoSQL database structure.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/proxy/\">https://aws.amazon.com/rds/proxy/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-database/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-database/</a></p>",
                "options": [
                    {
                        "id": 10998,
                        "content": "<p>Enable an RDS Proxy instance on your RDS Database.</p>",
                        "isValid": true
                    },
                    {
                        "id": 10999,
                        "content": "<p>Change the class of the instance of your database to allow more connections.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11000,
                        "content": "<p>Change the database to an Amazon DynamoDB database with on-demand scaling.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11001,
                        "content": "<p>Reduce the concurrency rate for your Lambda Function.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2632,
            "attributes": {
                "createdAt": "2023-09-09T20:39:56.867Z",
                "updatedAt": "2023-09-09T20:39:56.867Z",
                "content": "<p>A company migrated a two-tier application from its on-premises data center to AWS Cloud. A Multi-AZ Amazon RDS for Oracle deployment is used for the data tier, along with 12 TB of General Purpose SSD Amazon EBS storage. With an average document size of 6 MB, the application processes, and stores documents as binary large objects (blobs) in the database.</p><p>Over time, the database size has grown, which has reduced performance and increased storage costs. A highly available and resilient solution is needed to improve database performance.</p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
                "answerExplanation": "<p>Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. The key in this question is the reference to binary large objects (blobs) which are stored in the database. Amazon S3 is an easy to use and very cost-effective solution for Write Once Read Many (WORM) applications and use cases.</p><p><strong>CORRECT: </strong>\"Set up an Amazon S3 bucket. The application should be updated to use S3 buckets to store documents. Store the object metadata in the existing database” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Increase the RDS DB instance size. Increase the storage capacity to 24 TiB. Change the storage type to Provisioned IOPS” is incorrect. Doing this will increase the performance of your application, however the cost will go up and not go down.</p><p><strong>INCORRECT:</strong> \"Reduce the size of the RDS DB instance. Increase the storage capacity to 24 TiB. Magnetic storage should be selected” is incorrect. Reducing the instance size will only decrease the performance of your application, alongside changing your EBS volume to a Magnetic volume.</p><p><strong>INCORRECT:</strong> \"Create a table in Amazon DynamoDB and update the application to use DynamoDB. Migrate Oracle data to DynamoDB using AWS Database Migration Service (AWS DMS)” is incorrect. DynamoDB is more expensive than Amazon S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/\">https://aws.amazon.com/s3/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11002,
                        "content": "<p>Increase the RDS DB instance size. Increase the storage capacity to 24 TiB. Change the storage type to Provisioned IOPS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11003,
                        "content": "<p>Reduce the size of the RDS DB instance. Increase the storage capacity to 24 TiB. Magnetic storage should be selected.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11004,
                        "content": "<p>Create a table in Amazon DynamoDB and update the application to use DynamoDB. Migrate Oracle data to DynamoDB using AWS Database Migration Service (AWS DMS).</p>",
                        "isValid": false
                    },
                    {
                        "id": 11005,
                        "content": "<p>Set up an Amazon S3 bucket. The application should be updated to use S3 buckets to store documents. Store the object metadata in the existing database.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2633,
            "attributes": {
                "createdAt": "2023-09-09T20:39:56.971Z",
                "updatedAt": "2023-09-09T20:39:56.971Z",
                "content": "<p>A website runs on a Microsoft Windows server in an on-premises data center. The web server is being migrated to Amazon EC2 Windows instances in multiple Availability Zones on AWS. The web server currently uses data stored in an on-premises network-attached storage (NAS) device.</p><p>Which replacement to the NAS file share is MOST resilient and durable?</p>",
                "answerExplanation": "<p>Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. It offers single-AZ and multi-AZ deployment options, fully managed backups, and encryption of data at rest and in transit.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-28-32-63d08e70f92de39652bd9053f98f90df.jpg\"></p><p>This is the only solution presented that provides resilient storage for Windows instances.</p><p><strong>CORRECT: </strong>\"Migrate the file share to Amazon FSx for Windows File Server\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Migrate the file share to Amazon Elastic File System (Amazon EFS)\" is incorrect as you cannot use Windows instances with Amazon EFS.</p><p><strong>INCORRECT:</strong> \"Migrate the file share to Amazon EBS\" is incorrect as this is not a shared storage solution for multi-AZ deployments.</p><p><strong>INCORRECT:</strong> \"Migrate the file share to AWS Storage Gateway\" is incorrect as with Storage Gateway replicated files end up on Amazon S3. The replacement storage solution should be a file share, not an object-based storage system.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11006,
                        "content": "<p>Migrate the file share to AWS Storage Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 11007,
                        "content": "<p>Migrate the file share to Amazon FSx for Windows File Server</p>",
                        "isValid": true
                    },
                    {
                        "id": 11008,
                        "content": "<p>Migrate the file share to Amazon Elastic File System (Amazon EFS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11009,
                        "content": "<p>Migrate the file share to Amazon EBS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2634,
            "attributes": {
                "createdAt": "2023-09-09T20:39:57.087Z",
                "updatedAt": "2023-09-09T20:39:57.087Z",
                "content": "<p>An organization plans to deploy a higher performance computing (HPC) workload on AWS using Linux. The HPC workload will use many Amazon EC2 instances and will generate a large quantity of small output files that must be stored in persistent storage for future use.</p><p>A Solutions Architect must design a solution that will enable the EC2 instances to access data using native file system interfaces and to store output files in cost-effective long-term storage.</p><p>Which combination of AWS services meets these requirements?</p>",
                "answerExplanation": "<p>Amazon FSx for Lustre is ideal for high performance computing (HPC) workloads running on Linux instances. FSx for Lustre provides a native file system interface and works as any file system does with your Linux operating system.</p><p>When linked to an Amazon S3 bucket, FSx for Lustre transparently presents objects as files, allowing you to run your workload without managing data transfer from S3.</p><p>This solution provides all requirements as it enables Linux workloads to use the native file system interfaces and to use S3 for long-term and cost-effective storage of output files.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-11-27_10-07-01-3904cc64f7ccc25ee4a538dcf782af68.jpg\"></p><p><strong>CORRECT: </strong>\"Amazon FSx for Lustre with Amazon S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon FSx for Windows File Server with Amazon S3\" is incorrect. This service should be used with Windows instances and does not integrate with S3.</p><p><strong>INCORRECT:</strong> \"Amazon EBS volumes with Amazon S3 Glacier\" is incorrect. EBS volumes do not provide the shared, high performance storage solution using file system interfaces.</p><p><strong>INCORRECT:</strong> \"AWS DataSync with Amazon S3 Intelligent tiering\" is incorrect. AWS DataSync is used for migrating / synchronizing data.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/lustre/\">https://aws.amazon.com/fsx/lustre/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 11010,
                        "content": "<p>AWS DataSync with Amazon S3 Intelligent tiering.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11011,
                        "content": "<p>Amazon FSx for Windows File Server with Amazon S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11012,
                        "content": "<p>Amazon FSx for Lustre with Amazon S3.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11013,
                        "content": "<p>Amazon EBS volumes with Amazon S3 Glacier.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2635,
            "attributes": {
                "createdAt": "2023-09-09T20:39:57.186Z",
                "updatedAt": "2023-09-09T20:39:57.186Z",
                "content": "<p>Data from 45 TB of data is used for reporting by a company. The company wants to move this data from on premises into the AWS cloud. A custom application in the company's data center runs a weekly data transformation job and the company plans to pause the application until the data transfer is complete and needs to begin the transfer process as soon as possible.</p><p>The data center bandwidth is saturated, and a solutions architect has been tasked to transfer the data and must configure the transformation job to continue to run in the AWS Cloud.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>As the network is saturated, the solutions architect will have to use a physical solution, i.e. a member of the snow family to achieve this requirement quickly. As the data transformation job needs to be completed in the cloud, using AWS Glue will suit this requirement also. AWS Glue is a managed data transformation service.</p><p><strong>CORRECT: </strong>\"Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. and create a custom transformation job by using AWS Glue\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The data can be moved using AWS DataSync. Using AWS Glue, create a custom transformation job” is incorrect. As the network is saturated, AWS DataSync will not work as it is primarily an online data transfer service to transfer data between a data center and AWS.</p><p><strong>INCORRECT:</strong> \"The data will be moved using an AWS Snowcone device. The transformation application should be deployed to the device” is incorrect. You would not be able to deploy a transformation service locally to the Snowcone device as it is not optimized for compute operations.</p><p><strong>INCORRECT:</strong> \"Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Transfer the data to the device. Launch a new EC2 instance to run the transformation application” is incorrect. Using an EC2 instance instead of a managed service like AWS Glue will include more operational overhead for the organization.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-glue/\">https://digitalcloud.training/aws-glue/</a></p>",
                "options": [
                    {
                        "id": 11014,
                        "content": "<p>Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Transfer the data to the device. Launch a new EC2 instance to run the transformation application.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11015,
                        "content": "<p>The data will be moved using an AWS Snowcone device. The transformation application should be deployed to the device.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11016,
                        "content": "<p>The data can be moved using AWS DataSync. Using AWS Glue, create a custom transformation job.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11017,
                        "content": "<p>Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. and create a custom transformation job by using AWS Glue.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2636,
            "attributes": {
                "createdAt": "2023-09-09T20:39:57.282Z",
                "updatedAt": "2023-09-09T20:39:57.282Z",
                "content": "<p>An application uses Amazon EC2 instances and an Amazon RDS MySQL database. The database is not currently encrypted. A solutions architect needs to apply encryption to the database for all new and existing data.</p><p>How should this be accomplished?</p>",
                "answerExplanation": "<p>There are some <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations\">limitations for encrypted Amazon RDS DB Instances</a>: you can't modify an existing unencrypted Amazon RDS DB instance to make the instance encrypted, and you can't create an encrypted read replica from an unencrypted instance.</p><p>However, you can use the Amazon RDS snapshot feature to encrypt an unencrypted snapshot that's taken from the RDS database that you want to encrypt. Restore a new RDS DB instance from the encrypted snapshot to deploy a new encrypted DB instance. Finally, switch your connections to the new DB instance.</p><p><strong>CORRECT: </strong>\"Take a snapshot of the RDS instance. Create an encrypted copy of the snapshot. Restore the RDS instance from the encrypted snapshot\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon ElastiCache cluster and encrypt data using the cache nodes\" is incorrect as you cannot encrypt an RDS database using an ElastiCache cache node.</p><p><strong>INCORRECT:</strong> \"Enable encryption for the database using the API. Take a full snapshot of the database. Delete old snapshots\" is incorrect as you cannot enable encryption for an existing database.</p><p><strong>INCORRECT:</strong> \"Create an RDS read replica with encryption at rest enabled. Promote the read replica to master and switch the application over to the new master. Delete the old RDS instance\" is incorrect as you cannot create an encrypted read replica from an unencrypted database instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-encrypt-instance-mysql-mariadb/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-encrypt-instance-mysql-mariadb/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 11018,
                        "content": "<p>Create an Amazon ElastiCache cluster and encrypt data using the cache nodes</p>",
                        "isValid": false
                    },
                    {
                        "id": 11019,
                        "content": "<p>Take a snapshot of the RDS instance. Create an encrypted copy of the snapshot. Restore the RDS instance from the encrypted snapshot</p>",
                        "isValid": true
                    },
                    {
                        "id": 11020,
                        "content": "<p>Create an RDS read replica with encryption at rest enabled. Promote the read replica to master and switch the application over to the new master. Delete the old RDS instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 11021,
                        "content": "<p>Enable encryption for the database using the API. Take a full snapshot of the database. Delete old snapshots</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2637,
            "attributes": {
                "createdAt": "2023-09-09T20:39:57.374Z",
                "updatedAt": "2023-09-09T20:39:57.374Z",
                "content": "<p>An application is running on Amazon EC2 behind an Elastic Load Balancer (ELB). Content is being published using Amazon CloudFront and you need to restrict the ability for users to circumvent CloudFront and access the content directly through the ELB.</p><p>How can you configure this solution?</p>",
                "answerExplanation": "<p>The only way to get this working is by using a VPC Security Group for the ELB that is configured to allow only the internal service IP ranges associated with CloudFront. As these are updated from time to time, you can use AWS Lambda to automatically update the addresses. This is done using a trigger that is triggered when AWS issues an SNS topic update when the addresses are changed.</p><p><strong>CORRECT: </strong>\"Create a VPC Security Group for the ELB and use AWS Lambda to automatically update the CloudFront internal service IP addresses when they change\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Origin Access Identity (OAI) and associate it with the distribution\" is incorrect. You can use an OAI to restrict access to content in Amazon S3 but not on EC2 or ELB.</p><p><strong>INCORRECT:</strong> \"Use signed URLs or signed cookies to limit access to the content\" is incorrect. Signed cookies and URLs are used to limit access to files but this does not stop people from circumventing CloudFront and accessing the ELB directly.</p><p><strong>INCORRECT:</strong> \"Use a Network ACL to restrict access to the ELB\" is incorrect. A Network ACL can be used to restrict access to an ELB but it is recommended to use security groups and this solution is incomplete as it does not account for the fact that the internal service IP ranges change over time.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-automatically-update-your-security-groups-for-amazon-cloudfront-and-aws-waf-by-using-aws-lambda/\">https://aws.amazon.com/blogs/security/how-to-automatically-update-your-security-groups-for-amazon-cloudfront-and-aws-waf-by-using-aws-lambda/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 11022,
                        "content": "<p>Use a Network ACL to restrict access to the ELB</p>",
                        "isValid": false
                    },
                    {
                        "id": 11023,
                        "content": "<p>Use signed URLs or signed cookies to limit access to the content</p>",
                        "isValid": false
                    },
                    {
                        "id": 11024,
                        "content": "<p>Create an Origin Access Identity (OAI) and associate it with the distribution</p>",
                        "isValid": false
                    },
                    {
                        "id": 11025,
                        "content": "<p>Create a VPC Security Group for the ELB and use AWS Lambda to automatically update the CloudFront internal service IP addresses when they change</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2638,
            "attributes": {
                "createdAt": "2023-09-09T20:39:57.489Z",
                "updatedAt": "2023-09-09T20:39:57.489Z",
                "content": "<p>An automotive company plans to implement IoT sensors in manufacturing equipment that will send data to AWS in real time. The solution must receive events in an ordered manner from each asset and ensure that the data is saved for future processing.</p><p>Which solution would be MOST efficient?</p>",
                "answerExplanation": "<p>Amazon Kinesis Data Streams is the ideal service for receiving streaming data. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream. Therefore, a separate partition (rather than shard) should be used for each equipment asset.</p><p>Amazon Kinesis Firehose can be used to receive streaming data from Data Streams and then load the data into Amazon S3 for future processing.</p><p><strong>CORRECT: </strong>\"Use Amazon Kinesis Data Streams for real-time events with a partition for each equipment asset. Use Amazon Kinesis Data Firehose to save data to Amazon S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams for real-time events with a shard for each equipment asset. Use Amazon Kinesis Data Firehose to save data to Amazon EBS\" is incorrect. A partition should be used rather than a shard as explained above.</p><p><strong>INCORRECT:</strong> \"Use an Amazon SQS FIFO queue for real-time events with one queue for each equipment asset. Trigger an AWS Lambda function for the SQS queue to save data to Amazon EFS\" is incorrect. Amazon SQS cannot be used for real-time use cases.</p><p><strong>INCORRECT:</strong> \"Use an Amazon SQS standard queue for real-time events with one queue for each equipment asset. Trigger an AWS Lambda function from the SQS queue to save data to Amazon S3\" is incorrect. Amazon SQS cannot be used for real-time use cases.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p><p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 11026,
                        "content": "<p>Use Amazon Kinesis Data Streams for real-time events with a shard for each equipment asset. Use Amazon Kinesis Data Firehose to save data to Amazon EBS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11027,
                        "content": "<p>Use an Amazon SQS FIFO queue for real-time events with one queue for each equipment asset. Trigger an AWS Lambda function for the SQS queue to save data to Amazon EFS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11028,
                        "content": "<p>Use an Amazon SQS standard queue for real-time events with one queue for each equipment asset. Trigger an AWS Lambda function from the SQS queue to save data to Amazon S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11029,
                        "content": "<p>Use Amazon Kinesis Data Streams for real-time events with a partition for each equipment asset. Use Amazon Kinesis Data Firehose to save data to Amazon S3.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2639,
            "attributes": {
                "createdAt": "2023-09-09T20:39:57.590Z",
                "updatedAt": "2023-09-09T20:39:57.590Z",
                "content": "<p>A solutions architect has created a new AWS account and must secure AWS account root user access.</p><p>Which combination of actions will accomplish this? (Select TWO.)</p>",
                "answerExplanation": "<p>There are several security best practices for securing the root user account:</p><p>· Lock away root user access keys OR delete them if possible</p><p>· Use a strong password</p><p>· Enable multi-factor authentication (MFA)</p><p>The root user automatically has full privileges to the account and these privileges cannot be restricted so it is extremely important to follow best practice advice about securing the root user account.</p><p><strong>CORRECT: </strong>\"Ensure the root user uses a strong password\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Enable multi-factor authentication to the root user\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Store root user access keys in an encrypted Amazon S3 bucket\" is incorrect as the best practice is to lock away or delete the root user access keys. An S3 bucket is not a suitable location for storing them, even if encrypted.</p><p><strong>INCORRECT:</strong> \"Add the root user to a group containing administrative permissions\" is incorrect as this does not restrict access and is unnecessary.</p><p><strong>INCORRECT:</strong> \"Delete the root user account\" is incorrect as you cannot delete the root user account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 11030,
                        "content": "<p>Delete the root user account</p>",
                        "isValid": false
                    },
                    {
                        "id": 11031,
                        "content": "<p>Store root user access keys in an encrypted Amazon S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 11032,
                        "content": "<p>Add the root user to a group containing administrative permissions</p>",
                        "isValid": false
                    },
                    {
                        "id": 11033,
                        "content": "<p>Ensure the root user uses a strong password</p>",
                        "isValid": true
                    },
                    {
                        "id": 11034,
                        "content": "<p>Enable multi-factor authentication for the root user</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2640,
            "attributes": {
                "createdAt": "2023-09-09T20:39:57.740Z",
                "updatedAt": "2023-09-09T20:39:57.740Z",
                "content": "<p>A company runs an application on Amazon EC2 instances which requires access to sensitive data in an Amazon S3 bucket. All traffic between the EC2 instances and the S3 bucket must not traverse the internet and must use private IP addresses. Additionally, the bucket must only allow access from services in the VPC.</p><p>Which combination of actions should a Solutions Architect take to meet these requirements? (Select TWO.)</p>",
                "answerExplanation": "<p>Private access to public services such as Amazon S3 can be achieved by creating a VPC endpoint in the VPC. For S3 this would be a gateway endpoint. The bucket policy can then be configured to restrict access to the S3 endpoint only which will ensure that only services originating from the VPC will be granted access.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-15-18-8ca86e9211911c46e6ebba4877ad1dca.jpg\"></p><p><strong>CORRECT: </strong>\"Create a VPC endpoint for Amazon S3\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Apply a bucket policy to restrict access to the S3 endpoint\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Enable default encryption on the bucket\" is incorrect. This will encrypt data at rest but does not restrict access.</p><p><strong>INCORRECT:</strong> \"Create a peering connection to the S3 bucket VPC\" is incorrect. You cannot create a peering connection to S3 as it is a public service and does not run in a VPC.</p><p><strong>INCORRECT:</strong> \"Apply an IAM policy to a VPC peering connection\" is incorrect. You cannot apply an IAM policy to a peering connection.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11035,
                        "content": "<p>Create a VPC endpoint for Amazon S3.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11036,
                        "content": "<p>Apply a bucket policy to restrict access to the S3 endpoint.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11037,
                        "content": "<p>Enable default encryption on the bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11038,
                        "content": "<p>Apply an IAM policy to a VPC peering connection.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11039,
                        "content": "<p>Create a peering connection to the S3 bucket VPC.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2641,
            "attributes": {
                "createdAt": "2023-09-09T20:39:57.845Z",
                "updatedAt": "2023-09-09T20:39:57.845Z",
                "content": "<p>An application upgrade caused some issues with stability. The application owner enabled logging and has generated a 5 GB log file in an Amazon S3 bucket. The log file must be securely shared with the application vendor to troubleshoot the issues.</p><p>What is the MOST secure way to share the log file?</p>",
                "answerExplanation": "<p>A presigned URL gives you access to the object identified in the URL. When you create a presigned URL, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (PUT for uploading objects), and an expiration date and time. The presigned URLs are valid only for the specified duration. That is, you must start the action before the expiration date and time.</p><p>This is the most secure way to provide the vendor with time-limited access to the log file in the S3 bucket.</p><p><strong>CORRECT: </strong>\"Generate a presigned URL and ask the vendor to download the log file before the URL expires\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an IAM user for the vendor to provide access to the S3 bucket and the application. Enforce multi-factor authentication\" is incorrect. This is less secure as you have to create an account to access AWS and then ensure you lock down the account appropriately.</p><p><strong>INCORRECT:</strong> \"Create access keys using an administrative account and share the access key ID and secret access key with the vendor\" is incorrect. This is extremely insecure as the access keys will provide administrative permissions to AWS and should never be shared.</p><p><strong>INCORRECT:</strong> \"Enable default encryption for the bucket and public access. Provide the S3 URL of the file to the vendor\" is incorrect. Encryption does not assist here as the bucket would be public and anyone could access it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11040,
                        "content": "<p>Create an IAM user for the vendor to provide access to the S3 bucket and the application. Enforce multi-factor authentication.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11041,
                        "content": "<p>Create access keys using an administrative account and share the access key ID and secret access key with the vendor.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11042,
                        "content": "<p>Enable default encryption for the bucket and public access. Provide the S3 URL of the file to the vendor.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11043,
                        "content": "<p>Generate a presigned URL and ask the vendor to download the log file before the URL expires.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2642,
            "attributes": {
                "createdAt": "2023-09-09T20:39:57.953Z",
                "updatedAt": "2023-09-09T20:39:57.953Z",
                "content": "<p>A company has divested a single business unit and needs to move the AWS account owned by the business unit to another AWS Organization. How can this be achieved?</p>",
                "answerExplanation": "<p>Accounts can be migrated between organizations. To do this you must have root or IAM access to both the member and master accounts. Resources will remain under the control of the migrated account.</p><p><strong>CORRECT: </strong>\"Migrate the account using the AWS Organizations console\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a new account in the destination AWS Organization and migrate resources\" is incorrect. You do not need to create a new account in the destination AWS Organization as you can just migrate the existing account.</p><p><strong>INCORRECT:</strong> \"Create a new account in the destination AWS Organization and share the original resources using AWS Resource Access Manager\" is incorrect. You do not need to create a new account in the destination AWS Organization as you can just migrate the existing account.</p><p><strong>INCORRECT:</strong> \"Migrate the account using AWS CloudFormation\" is incorrect. You do not need to use AWS CloudFormation. You can use the Organizations API or AWS CLI for when there are many accounts to migrate and therefore you could use CloudFormation for any additional automation but it is not necessary for this scenario.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/\">https://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
                "options": [
                    {
                        "id": 11044,
                        "content": "<p>Create a new account in the destination AWS Organization and migrate resources</p>",
                        "isValid": false
                    },
                    {
                        "id": 11045,
                        "content": "<p>Migrate the account using the AWS Organizations console</p>",
                        "isValid": true
                    },
                    {
                        "id": 11046,
                        "content": "<p>Create a new account in the destination AWS Organization and share the original resources using AWS Resource Access Manager</p>",
                        "isValid": false
                    },
                    {
                        "id": 11047,
                        "content": "<p>Migrate the account using AWS CloudFormation</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2643,
            "attributes": {
                "createdAt": "2023-09-09T20:39:58.068Z",
                "updatedAt": "2023-09-09T20:39:58.068Z",
                "content": "<p>There are badge readers located at every entrance of an organization’s warehouses. A message is sent over HTTPS when badges are scanned to indicate who tried to access the entrance.</p><p>A solutions architect must design a system to process these messages. A highly available solution is required. The solution must store results in a durable data store for later analysis.</p><p>Which system architecture should the solutions architect recommend?</p>",
                "answerExplanation": "<p>Amazon API Gateway would be ideal for providing a secure entry point for your application, and for traffic to be sent via HTTPS. AWS Lambda would integrate seamlessly with API Gateway to process the data, as an event-driven solution like this would be perfect when designing a scalable system based on sporadic use. Finally, DynamoDB is highly scalable and is a perfect repository for data to be stored for future analysis.</p><p><strong>CORRECT: </strong>\"Set up an HTTPS endpoint in Amazon API Gateway. To process the messages and save the results to Amazon DynamoDB, configure an API Gateway endpoint to invoke an AWS Lambda function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon EC2 instance to serve as the HTTPS endpoint and to process messages. An Amazon S3 bucket should be configured for the EC2 instance to save the results\" is incorrect. As the action of a badge being read to initiate access to a warehouse should only take a few seconds, spinning up an EC2 instance to serve as a HTTPS endpoint would take minutes, and is not suitable for this use case.</p><p><strong>INCORRECT:</strong> \"Direct incoming messages from the sensor to an AWS Lambda function using Amazon Route 53. Create a Lambda function that processes messages and saves results to Amazon DynamoDB” is incorrect. Amazon Route 53 is a managed DNS service, and DNS is not required in this instance as the badge reader does not have a DNS name.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon S3 gateway endpoint in your VPC. Connect the facility network to the VPC via a Site-to-Site VPN connection so that sensor data can be written directly to an S3 bucket\" is incorrect. VPC endpoints are designed to facilitate traffic across the AWS backbone network between AWS services and are not used to create connections between external endpoints outside of the AWS network and an Amazon S3 bucket.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 11048,
                        "content": "<p>Set up an HTTPS endpoint in Amazon API Gateway. To process the messages and save the results to Amazon DynamoDB, configure an API Gateway endpoint to invoke an AWS Lambda function.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11049,
                        "content": "<p>Create an Amazon EC2 instance to serve as the HTTPS endpoint and to process messages. An Amazon S3 bucket should be configured for the EC2 instance to save the results.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11050,
                        "content": "<p>Set up an Amazon S3 gateway endpoint in your VPC. Connect the facility network to the VPC via a Site-to-Site VPN connection so that sensor data can be written directly to an S3 bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11051,
                        "content": "<p>Direct incoming messages from the sensor to an AWS Lambda function using Amazon Route 53. Create a Lambda function that processes messages and saves results to Amazon DynamoDB.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2644,
            "attributes": {
                "createdAt": "2023-09-09T20:39:58.188Z",
                "updatedAt": "2023-09-09T20:39:58.188Z",
                "content": "<p>An application runs on-premises and produces data that must be stored in a locally accessible file system that servers can mount using the NFS protocol. The data must be subsequently analyzed by Amazon EC2 instances in the AWS Cloud.</p><p>How can these requirements be met?</p>",
                "answerExplanation": "<p>The best solution for this requirement is to use an AWS Storage Gateway file gateway. This will provide a local NFS mount point for the data and a local cache. The data is then replicated to Amazon S3 where it can be analyzed by the Amazon EC2 instances in the AWS Cloud.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-19-32-45d0ef2489b4ea2fb6fae5b15c844aa0.jpg\"></p><p><strong>CORRECT: </strong>\"Use an AWS Storage Gateway file gateway to provide a locally accessible file system that replicates data to the cloud, then analyze the data in the AWS Cloud\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an AWS Storage Gateway tape gateway to take a backup of the local data and store it on AWS, then perform analytics on this data in the AWS Cloud\" is incorrect. A tape gateway does not provide a local NFS mount point, it is simply a backup solution not a file system.</p><p><strong>INCORRECT:</strong> \"Use an AWS Storage Gateway volume gateway in stored mode to regularly take snapshots of the local data, then copy the data to AWS\" is incorrect. Volume gateways use block-based protocols not NFS.</p><p><strong>INCORRECT:</strong> \"Use an AWS Storage Gateway volume gateway in cached mode to back up all the local storage in the AWS Cloud, then perform analytics on this data in the cloud\" is incorrect. Volume gateways use block-based protocols not NFS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
                "options": [
                    {
                        "id": 11052,
                        "content": "<p>Use an AWS Storage Gateway volume gateway in stored mode to regularly take snapshots of the local data, then copy the data to AWS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11053,
                        "content": "<p>Use an AWS Storage Gateway tape gateway to take a backup of the local data and store it on AWS, then perform analytics on this data in the AWS Cloud.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11054,
                        "content": "<p>Use an AWS Storage Gateway volume gateway in cached mode to back up all the local storage in the AWS Cloud, then perform analytics on this data in the cloud.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11055,
                        "content": "<p>Use an AWS Storage Gateway file gateway to provide a locally accessible file system that replicates data to the cloud, then analyze the data in the AWS Cloud.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2645,
            "attributes": {
                "createdAt": "2023-09-09T20:39:58.296Z",
                "updatedAt": "2023-09-09T20:39:58.296Z",
                "content": "<p>An application on Amazon Elastic Container Service (ECS) performs data processing in two parts. The second part takes much longer to complete. How can an Architect decouple the data processing from the backend application component?</p>",
                "answerExplanation": "<p>Processing each part using a separate ECS task may not be essential but means you can separate the processing of the data. An Amazon Simple Queue Service (SQS) is used for decoupling applications. It is a message queue on which you place messages for processing by application components. In this case you can process each data processing part in separate ECS tasks and have them write an Amazon SQS queue. That way the backend can pick up the messages from the queue when they’re ready and there is no delay due to the second part not being complete.</p><p><strong>CORRECT: </strong>\"Process each part using a separate ECS task. Create an Amazon SQS queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Process both parts using the same ECS task. Create an Amazon Kinesis Firehose stream\" is incorrect. Amazon Kinesis Firehose is used for streaming data. This is not an example of streaming data. In this case SQS is better as a message can be placed on a queue to indicate that the job is complete and ready to be picked up by the backend application component.</p><p><strong>INCORRECT:</strong> \"Process each part using a separate ECS task. Create an Amazon SNS topic and send a notification when the processing completes\" is incorrect. Amazon Simple Notification Service (SNS) can be used for sending notifications. It is useful when you need to notify multiple AWS services. In this case an Amazon SQS queue is a better solution as there is no mention of multiple AWS services and this is an ideal use case for SQS.</p><p><strong>INCORRECT:</strong> \"Create an Amazon DynamoDB table and save the output of the first part to the table\" is incorrect. Amazon DynamoDB is unlikely to be a good solution for this requirement. There is a limit on the maximum amount of data that you can store in an entry in a DynamoDB table.</p><p><strong>References:</strong></p><p><a href=\"https://digitalcloud.training/certification-training/aws-solutions-architect-associate/application-integration/amazon-sqs/\">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/application-integration/amazon-sqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 11056,
                        "content": "<p>Process each part using a separate ECS task. Create an Amazon SNS topic and send a notification when the processing completes</p>",
                        "isValid": false
                    },
                    {
                        "id": 11057,
                        "content": "<p>Process each part using a separate ECS task. Create an Amazon SQS queue</p>",
                        "isValid": true
                    },
                    {
                        "id": 11058,
                        "content": "<p>Process both parts using the same ECS task. Create an Amazon Kinesis Firehose stream</p>",
                        "isValid": false
                    },
                    {
                        "id": 11059,
                        "content": "<p>Create an Amazon DynamoDB table and save the output of the first part to the table</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2646,
            "attributes": {
                "createdAt": "2023-09-09T20:39:58.405Z",
                "updatedAt": "2023-09-09T20:39:58.405Z",
                "content": "<p>An application runs on Amazon EC2 instances across multiple Availability Zones. The instances run in an Amazon EC2 Auto Scaling group behind an Application Load Balancer. The application performs best when the CPU utilization of the EC2 instances is at or near 40%.</p><p>What should a solutions architect do to maintain the desired performance across all instances in the group?</p>",
                "answerExplanation": "<p>With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value.</p><p>The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to the changes in the metric due to a changing load pattern.</p><p><strong>CORRECT: </strong>\"Use a target tracking policy to dynamically scale the Auto Scaling group\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a simple scaling policy to dynamically scale the Auto Scaling group\" is incorrect as target tracking is a better way to keep the aggregate CPU usage at around 40%</p><p><strong>INCORRECT:</strong> \"Use an AWS Lambda function to update the desired Auto Scaling group capacity\" is incorrect as this can be done automatically.</p><p><strong>INCORRECT:</strong> \"Use scheduled scaling actions to scale up and scale down the Auto Scaling group\" is incorrect as dynamic scaling is required to respond to changes in utilization.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 11060,
                        "content": "<p>Use an AWS Lambda function to update the desired Auto Scaling group capacity</p>",
                        "isValid": false
                    },
                    {
                        "id": 11061,
                        "content": "<p>Use scheduled scaling actions to scale up and scale down the Auto Scaling group</p>",
                        "isValid": false
                    },
                    {
                        "id": 11062,
                        "content": "<p>Use a simple scaling policy to dynamically scale the Auto Scaling group</p>",
                        "isValid": false
                    },
                    {
                        "id": 11063,
                        "content": "<p>Use a target tracking policy to dynamically scale the Auto Scaling group</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2647,
            "attributes": {
                "createdAt": "2023-09-09T20:39:58.518Z",
                "updatedAt": "2023-09-09T20:39:58.518Z",
                "content": "<p>A gaming company collects real-time data and stores it in an on-premises database system. The company are migrating to AWS and need better performance for the database. A solutions architect has been asked to recommend an in-memory database that supports data replication.</p><p>Which database should a solutions architect recommend?</p>",
                "answerExplanation": "<p>Amazon ElastiCache is an in-memory database. With ElastiCache Memcached there is no data replication or high availability. As you can see in the diagram, each node is a separate partition of data:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_02-01-20-6b625ae592ca124e03dd8f3ac8c2a94d.jpg\"></p><p>Therefore, the Redis engine must be used which does support both data replication and clustering. The following diagram shows a Redis architecture with cluster mode enabled:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_02-01-34-c19e5fe437e8f141a5fe0a655e0990bd.jpg\"></p><p><strong>CORRECT: </strong>\"Amazon ElastiCache for Redis\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon ElastiCache for Memcached\" is incorrect as Memcached does not support data replication or high availability.</p><p><strong>INCORRECT:</strong> \"Amazon RDS for MySQL\" is incorrect as this is not an in-memory database.</p><p><strong>INCORRECT:</strong> \"Amazon RDS for PostgreSQL\" is incorrect as this is not an in-memory database.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 11064,
                        "content": "<p>Amazon RDS for PostgreSQL</p>",
                        "isValid": false
                    },
                    {
                        "id": 11065,
                        "content": "<p>Amazon ElastiCache for Memcached</p>",
                        "isValid": false
                    },
                    {
                        "id": 11066,
                        "content": "<p>Amazon RDS for MySQL</p>",
                        "isValid": false
                    },
                    {
                        "id": 11067,
                        "content": "<p>Amazon ElastiCache for Redis</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2648,
            "attributes": {
                "createdAt": "2023-09-09T20:39:58.616Z",
                "updatedAt": "2023-09-09T20:39:58.616Z",
                "content": "<p>A Solutions Architect must design a solution to allow many Amazon EC2 instances across multiple subnets to access a shared data store. The data must be accessed by all instances simultaneously and access should use the NFS protocol. The solution must also be highly scalable and easy to implement.</p><p>Which solution best meets these requirements?</p>",
                "answerExplanation": "<p>The Amazon Elastic File System (EFS) is a perfect solution for this requirement. Amazon EFS filesystems are accessed using the NFS protocol and can be mounted by many instances across multiple subnets simultaneously. EFS filesystems are highly scalable and very easy to implement.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-08-11-6d39438584e2d604d169e4d38f77f00e.jpg\"></p><p><strong>CORRECT: </strong>\"Create an Amazon EFS file system. Configure a mount target in each Availability Zone. Attach each instance to the appropriate mount target\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure an additional EC2 instance as a file server. Create a role in AWS IAM that grants permissions to the file share and attach the role to the EC2 instances\" is incorrect. You cannot use IAM roles to grant permissions to a file share created within the operating system of an EC2 instance. Also, this solution is not as highly scalable or easy to implement as Amazon EFS.</p><p><strong>INCORRECT:</strong> \"Create an Amazon S3 bucket and configure a Network ACL. Grant the EC2 instances permission to access the bucket using the NFS protocol\" is incorrect. A Network ACL is created to restrict traffic in and out of subnets, it is not used to control access to S3 buckets (use a bucket policy or bucket ACL instead). You cannot grant permission to access an S3 bucket using a protocol, and NFS is not supported for S3 as it is an object-based storage system.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EBS volume and create a resource-based policy that grants an AWS IAM role access to the data. Attach the role to the EC2 instances\" is incorrect. You cannot configure a resource-based policy on an Amazon EBS volume.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html\">https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
                "options": [
                    {
                        "id": 11068,
                        "content": "<p>Create an Amazon EFS file system. Configure a mount target in each Availability Zone. Attach each instance to the appropriate mount target.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11069,
                        "content": "<p>Create an Amazon EBS volume and create a resource-based policy that grants an AWS IAM role access to the data. Attach the role to the EC2 instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11070,
                        "content": "<p>Create an Amazon S3 bucket and configure a Network ACL. Grant the EC2 instances permission to access the bucket using the NFS protocol.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11071,
                        "content": "<p>Configure an additional EC2 instance as a file server. Create a role in AWS IAM that grants permissions to the file share and attach the role to the EC2 instances.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2649,
            "attributes": {
                "createdAt": "2023-09-09T20:39:58.712Z",
                "updatedAt": "2023-09-09T20:39:58.712Z",
                "content": "<p>An Architect needs to find a way to automatically and repeatably create many member accounts within an AWS Organization. The accounts also need to be moved into an OU and have VPCs and subnets created.</p><p>What is the best way to achieve this?</p>",
                "answerExplanation": "<p>The best solution is to use a combination of scripts and AWS CloudFormation. You will also leverage the AWS Organizations API. This solution can provide all of the requirements.</p><p><strong>CORRECT: </strong>\"Use CloudFormation with scripts\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use the AWS Organizations API\" is incorrect. You can create member accounts with the AWS Organizations API. However, you cannot use that API to configure the account and create VPCs and subnets.</p><p><strong>INCORRECT:</strong> \"Use the AWS Management Console\" is incorrect. Using the AWS Management Console is not a method of automatically creating the resources.</p><p><strong>INCORRECT:</strong> \"Use the AWS CLI\" is incorrect. You can do all tasks using the AWS CLI but it is better to automate the process using AWS CloudFormation.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-aws-organizations-to-automate-end-to-end-account-creation/\">https://aws.amazon.com/blogs/security/how-to-use-aws-organizations-to-automate-end-to-end-account-creation/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
                "options": [
                    {
                        "id": 11072,
                        "content": "<p>Use the AWS Organizations API</p>",
                        "isValid": false
                    },
                    {
                        "id": 11073,
                        "content": "<p>Use the AWS CLI</p>",
                        "isValid": false
                    },
                    {
                        "id": 11074,
                        "content": "<p>Use CloudFormation with scripts</p>",
                        "isValid": true
                    },
                    {
                        "id": 11075,
                        "content": "<p>Use the AWS Management Console</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2650,
            "attributes": {
                "createdAt": "2023-09-09T20:39:58.804Z",
                "updatedAt": "2023-09-09T20:39:58.804Z",
                "content": "<p>An organization is extending a secure development environment into AWS. They have already secured the VPC including removing the Internet Gateway and setting up a Direct Connect connection. What else needs to be done to add encryption?</p>",
                "answerExplanation": "<p>A VPG is used to setup an AWS VPN which you can use in combination with Direct Connect to encrypt all data that traverses the Direct Connect link. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections.</p><p><strong>CORRECT: </strong>\"Setup a Virtual Private Gateway (VPG)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable IPSec encryption on the Direct Connect connection\" is incorrect. There is no option to enable IPSec encryption on the Direct Connect connection.</p><p><strong>INCORRECT:</strong> \"Setup the Border Gateway Protocol (BGP) with encryption\" is incorrect. The BGP protocol is not used to enable encryption for Direct Connect, it is used for routing.</p><p><strong>INCORRECT:</strong> \"Configure an AWS Direct Connect Gateway\" is incorrect. An AWS Direct Connect Gateway is used to connect to VPCs across multiple AWS regions. It is not involved with encryption.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html</a></p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11076,
                        "content": "<p>Configure an AWS Direct Connect Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 11077,
                        "content": "<p>Setup the Border Gateway Protocol (BGP) with encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 11078,
                        "content": "<p>Setup a Virtual Private Gateway (VPG)</p>",
                        "isValid": true
                    },
                    {
                        "id": 11079,
                        "content": "<p>Enable IPSec encryption on the Direct Connect connection</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2651,
            "attributes": {
                "createdAt": "2023-09-09T20:39:58.902Z",
                "updatedAt": "2023-09-09T20:39:58.902Z",
                "content": "<p>An application has multiple components for receiving requests that must be processed and subsequently processing the requests. The company requires a solution for decoupling the application components. The application receives around 10,000 requests per day and requests can take up to 2 days to process. Requests that fail to process must be retained.</p><p>Which solution meets these requirements most efficiently?</p>",
                "answerExplanation": "<p>The Amazon Simple Queue Service (SQS) is ideal for decoupling the application components. Standard queues can support up to 120,000 in flight messages and messages can be retained for up to 14 days in the queue.</p><p>To ensure the retention of requests (messages) that fail to process, a dead-letter queue can be configured. Messages that fail to process are sent to the dead-letter queue (based on the redrive policy) and can be subsequently dealt with.</p><p><strong>CORRECT: </strong>\"Decouple the application components with an Amazon SQS queue. Configure a dead-letter queue to collect the requests that failed to process\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Decouple the application components with an Amazon SQS Topic. Configure the receiving component to subscribe to the SNS Topic\" is incorrect. SNS does not store requests, it immediately forwards all notifications to subscribers.</p><p><strong>INCORRECT:</strong> \"Use an Amazon Kinesis data stream to decouple application components and integrate the processing component with the Kinesis Client Library (KCL)\" is incorrect. This is a less efficient solution and will likely be less cost-effective compared to using Amazon SQS. There is also no option for retention of requests that fail to process.</p><p><strong>INCORRECT:</strong> \"Create an Amazon DynamoDB table and enable DynamoDB streams. Configure the processing component to process requests from the stream\" is incorrect. This solution does not offer any way of retaining requests that fail to process of removal of items from the table and is therefore less efficient.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 11080,
                        "content": "<p>Decouple the application components with an Amazon SQS Topic. Configure the receiving component to subscribe to the SNS Topic.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11081,
                        "content": "<p>Create an Amazon DynamoDB table and enable DynamoDB streams. Configure the processing component to process requests from the stream.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11082,
                        "content": "<p>Use an Amazon Kinesis data stream to decouple application components and integrate the processing component with the Kinesis Client Library (KCL).</p>",
                        "isValid": false
                    },
                    {
                        "id": 11083,
                        "content": "<p>Decouple the application components with an Amazon SQS queue. Configure a dead-letter queue to collect the requests that failed to process.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2652,
            "attributes": {
                "createdAt": "2023-09-09T20:39:58.998Z",
                "updatedAt": "2023-09-09T20:39:58.998Z",
                "content": "<p>A company is planning to use Amazon S3 to store documents uploaded by its customers. The images must be encrypted at rest in Amazon S3. The company does not want to spend time managing and rotating the keys, but it does want to control who can access those keys.</p><p>What should a solutions architect use to accomplish this?</p>",
                "answerExplanation": "<p>SSE-KMS requires that AWS manage the data key but you manage the <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\">customer master key</a> (CMK) in AWS KMS. You can choose a <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk\">customer managed CMK</a> or the <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk\">AWS managed CMK</a> for Amazon S3 in your account.</p><p><em>Customer managed CMKs</em> are CMKs in your AWS account that you create, own, and manage. You have full control over these CMKs, including establishing and maintaining their <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/control-access.html\">key policies, IAM policies, and grants</a>, <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/enabling-keys.html\">enabling and disabling</a> them, <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\">rotating their cryptographic material</a>, <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/tagging-keys.html\">adding tags</a>, <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/programming-aliases.html\">creating aliases</a> that refer to the CMK, and <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html\">scheduling the CMKs for deletion</a>.</p><p>For this scenario, the solutions architect should use SSE-KMS with a customer managed CMK. That way KMS will manage the data key but the company can configure key policies defining who can access the keys.</p><p><strong>CORRECT: </strong>\"Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Server-Side Encryption with keys stored in an S3 bucket\" is incorrect as you cannot store your keys in a bucket with server-side encryption</p><p><strong>INCORRECT:</strong> \"Server-Side Encryption with Customer-Provided Keys (SSE-C)\" is incorrect as the company does not want to manage the keys.</p><p><strong>INCORRECT:</strong> \"Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)\" is incorrect as the company needs to manage access control for the keys which is not possible when they’re managed by Amazon.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html#sse\">https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html#sse</a></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
                "options": [
                    {
                        "id": 11084,
                        "content": "<p>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11085,
                        "content": "<p>Server-Side Encryption with Customer-Provided Keys (SSE-C)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11086,
                        "content": "<p>Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS)</p>",
                        "isValid": true
                    },
                    {
                        "id": 11087,
                        "content": "<p>Server-Side Encryption with keys stored in an S3 bucket</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2653,
            "attributes": {
                "createdAt": "2023-09-09T20:39:59.097Z",
                "updatedAt": "2023-09-09T20:39:59.097Z",
                "content": "<p>A company has created a disaster recovery solution for an application that runs behind an Application Load Balancer (ALB). The DR solution consists of a second copy of the application running behind a second ALB in another Region. The Solutions Architect requires a method of automatically updating the DNS record to point to the ALB in the second Region.</p><p>What action should the Solutions Architect take?</p>",
                "answerExplanation": "<p>Amazon Route 53 health checks monitor the health and performance of your web applications, web servers, and other resources. Each health check that you create can monitor one of the following:</p><p>&nbsp; •&nbsp; The health of a specified resource, such as a web server</p><p>&nbsp; •&nbsp; The status of other health checks</p><p>&nbsp; •&nbsp; The status of an Amazon CloudWatch alarm</p><p>Health checks can be used with other configurations such as a failover routing policy. In this case a failover routing policy will direct traffic to the ALB of the primary Region unless health checks fail at which time it will direct traffic to the secondary record for the DR ALB.</p><p><strong>CORRECT: </strong>\"Enable an Amazon Route 53 health check\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable an ALB health check\" is incorrect. This will simply perform health checks of the instances behind the ALB, rather than the ALB itself. This could be used in combination with Route 53 health checks.</p><p><strong>INCORRECT:</strong> \"Use Amazon EventBridge to cluster the ALBs\" is incorrect. You cannot cluster ALBs in any way.</p><p><strong>INCORRECT:</strong> \"Configure an alarm on a CloudTrail trail\" is incorrect. CloudTrail records API activity so this does not help.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
                "options": [
                    {
                        "id": 11088,
                        "content": "<p>Enable an Amazon Route 53 health check.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11089,
                        "content": "<p>Use Amazon EventBridge to cluster the ALBs.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11090,
                        "content": "<p>Enable an ALB health check.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11091,
                        "content": "<p>Configure an alarm on a CloudTrail trail.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2654,
            "attributes": {
                "createdAt": "2023-09-09T20:39:59.190Z",
                "updatedAt": "2023-09-09T20:39:59.190Z",
                "content": "<p>A High Performance Computing (HPC) application needs storage that can provide 135,000 IOPS. The storage layer is replicated across all instances in a cluster.</p><p>What is the optimal storage solution that provides the required performance and is cost-effective?</p>",
                "answerExplanation": "<p>Instance stores offer very high performance and low latency. As long as you can afford to lose an instance, i.e. you are replicating your data, these can be a good solution for high performance/low latency requirements. Also, the cost of instance stores is included in the instance charges so it can also be more cost-effective than EBS Provisioned IOPS.</p><p><strong>CORRECT: </strong>\"Use Amazon Instance Store\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon EBS Provisioned IOPS volume with 135,000 IOPS\" is incorrect. In the case of a HPC cluster that replicates data between nodes you don’t necessarily need a shared storage solution such as Amazon EBS Provisioned IOPS – this would also be a more expensive solution as the Instance Store is included in the cost of the HPC instance.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 with byte-range fetch\" is incorrect. Amazon S3 is not a solution for this HPC application as in this case it will require block-based storage to provide the required IOPS.</p><p><strong>INCORRECT:</strong> \"Enhanced networking provides higher bandwidth and lower latency and is implemented using an Elastic Network Adapter (ENA). However, using an ENA with an HDD Throughput Optimized volume is not recommended and the volume will not provide the performance required for this use case.\" is incorrect</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 11092,
                        "content": "<p>Use Amazon S3 with byte-range fetch</p>",
                        "isValid": false
                    },
                    {
                        "id": 11093,
                        "content": "<p>Use Amazon EBS Provisioned IOPS volume with 135,000 IOPS</p>",
                        "isValid": false
                    },
                    {
                        "id": 11094,
                        "content": "<p>Use Amazon Instance Store</p>",
                        "isValid": true
                    },
                    {
                        "id": 11095,
                        "content": "<p>Use Amazon EC2 Enhanced Networking with an EBS HDD Throughput Optimized volume</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2655,
            "attributes": {
                "createdAt": "2023-09-09T20:39:59.285Z",
                "updatedAt": "2023-09-09T20:39:59.285Z",
                "content": "<p>A Financial Services company currently stores data in Amazon S3. Each bucket contains items which have different access patterns. The Chief Financial officer of the organization wants to reduce costs, as they have noticed a sharp increase in their S3 bill. The Chief Financial Officer wants to reduce the S3 spend as quickly as possible.</p><p>What is the quickest way to reduce the S3 spend with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:</p><p>● Transition actions – These actions define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after creating them, or archive objects to the S3 Glacier Flexible Retrieval storage class one year after creating them. For more information, see Using Amazon S3 storage classes.</p><p>● Expiration actions – These actions define when objects expire. Amazon S3 deletes expired objects on your behalf.</p><p><strong>CORRECT: </strong>\"Transition the objects to the appropriate storage class by using an S3 Lifecycle configuration” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Automate the move of your S3 objects to the best storage class with AWS Trusted Advisor” is incorrect. Trusted Advisor does not automatically transfer objects into the most appropriate buckets. You can use Trusted Advisor to review cost optimization options, and check for public access to your buckets but you cannot automatically transition objects.</p><p><strong>INCORRECT:</strong> \"Create a Lambda function to scan your S3 buckets, check which objects are stored in the appropriate buckets, and move them there” is incorrect. You could perhaps build a Lambda function to do this, however the easiest way to do this would be to use an S3 Lifecycle configuration.</p><p><strong>INCORRECT:</strong> \"Place all objects in S3 Glacier Instant Retrieval” is incorrect. It states in the question that each bucket contains items which have different access patterns, therefore S3 Glacier is not a suitable use case.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11096,
                        "content": "<p>Automate the move of your S3 objects to the best storage class with AWS Trusted Advisor.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11097,
                        "content": "<p>Place all objects in S3 Glacier Instant Retrieval.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11098,
                        "content": "<p>Transition the objects to the appropriate storage class by using an S3 Lifecycle configuration.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11099,
                        "content": "<p>Create a Lambda function to scan your S3 buckets, check which objects are stored in the appropriate buckets, and move them there.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2656,
            "attributes": {
                "createdAt": "2023-09-09T20:39:59.406Z",
                "updatedAt": "2023-09-09T20:39:59.406Z",
                "content": "<p>A company has deployed an application that consists of several microservices running on Amazon EC2 instances behind an Amazon API Gateway API. A Solutions Architect is concerned that the microservices are not designed to elastically scale when large increases in demand occur.</p><p>Which solution addresses this concern?</p>",
                "answerExplanation": "<p>The individual microservices are not designed to scale. Therefore, the best way to ensure they are not overwhelmed by requests is to decouple the requests from the microservices. An Amazon SQS queue can be created, and the API Gateway can be configured to add incoming requests to the queue. The microservices can then pick up the requests from the queue when they are ready to process them.</p><p><strong>CORRECT: </strong>\"Create an Amazon SQS queue to store incoming requests. Configure the microservices to retrieve the requests from the queue for processing\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch alarms to notify operations staff when the microservices are suffering high CPU utilization\" is incorrect. This solution requires manual intervention and does not help the application to elastically scale.</p><p><strong>INCORRECT:</strong> \"Spread the microservices across multiple Availability Zones and configure Amazon Data Lifecycle Manager to take regular snapshots\" is incorrect. This does not automate the elasticity of the application.</p><p><strong>INCORRECT:</strong> \"Use an Elastic Load Balancer to distribute the traffic between the microservices. Configure Amazon CloudWatch metrics to monitor traffic to the microservices\" is incorrect. You cannot use an ELB spread traffic across many different individual microservices as the requests must be directed to individual microservices. Therefore, you would need a target group per microservice, and you would need Auto Scaling to scale the microservices.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/understanding-asynchronous-messaging-for-microservices/\">https://aws.amazon.com/blogs/compute/understanding-asynchronous-messaging-for-microservices/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 11100,
                        "content": "<p>Spread the microservices across multiple Availability Zones and configure Amazon Data Lifecycle Manager to take regular snapshots.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11101,
                        "content": "<p>Use an Elastic Load Balancer to distribute the traffic between the microservices. Configure Amazon CloudWatch metrics to monitor traffic to the microservices.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11102,
                        "content": "<p>Create an Amazon SQS queue to store incoming requests. Configure the microservices to retrieve the requests from the queue for processing.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11103,
                        "content": "<p>Use Amazon CloudWatch alarms to notify operations staff when the microservices are suffering high CPU utilization.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2657,
            "attributes": {
                "createdAt": "2023-09-09T20:39:59.502Z",
                "updatedAt": "2023-09-09T20:39:59.502Z",
                "content": "<p>A Solutions Architect works for a company looking to centralize its Machine Learning Operations. Currently they have a large amount of existing cloud storage to store their operational data which is used for machine learning analysis. There is some data which exists within an Amazon RDS MySQL database, and they need a solution which can easily retrieve data from the database.</p><p>Which service can be used to build a centralized data repository to be used for Machine Learning purposes?</p>",
                "answerExplanation": "<p>AWS Lake Formation is a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and prepared for analysis. With AWS Lake Formation, you can import data from MySQL, PostgreSQL, SQL Server, MariaDB, and Oracle databases running in Amazon Relational Database Service (RDS) or hosted in Amazon Elastic Compute Cloud (EC2). Both bulk and incremental data loading are supported.</p><p><strong>CORRECT: </strong>\"AWS Lake Formation\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Amazon S3\" is incorrect. Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. It is not however suitable for directly retrieving data from MySQL on RDS and using the data for a Machine learning use case.</p><p><strong>INCORRECT:</strong> \"Amazon Quantum Ledger Database\" is incorrect. Amazon Quantum Ledger Database (QLDB) is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log. It is not suitable for directly retrieving data from MySQL on RDS and using the data for a Machine learning use case.</p><p><strong>INCORRECT:</strong> \"Amazon Neptune\" is incorrect. Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications. It is not suitable for directly retrieving data from MySQL on RDS and using the data for a Machine learning use case.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/lake-formation/features/\">https://aws.amazon.com/lake-formation/features/</a></p><p><a href=\"https://aws.amazon.com/lake-formation/features/\"><br></a><strong>Save time with our AWS cheat sheets:</strong></p><p><strong><br></strong><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-associate/aws-storage-saa/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-associate/aws-storage-saa/</a></p>",
                "options": [
                    {
                        "id": 11104,
                        "content": "<p>Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 11105,
                        "content": "<p>Amazon Neptune</p>",
                        "isValid": false
                    },
                    {
                        "id": 11106,
                        "content": "<p>AWS Lake Formation</p>",
                        "isValid": true
                    },
                    {
                        "id": 11107,
                        "content": "<p>Amazon Quantum Ledger Database (QLDB)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2658,
            "attributes": {
                "createdAt": "2023-09-09T20:39:59.655Z",
                "updatedAt": "2023-09-09T20:39:59.655Z",
                "content": "<p>An application consists of a web tier in a public subnet and a MySQL cluster hosted on Amazon EC2 instances in a private subnet. The MySQL instances must retrieve product data from a third-party provider over the internet. A Solutions Architect must determine a strategy to enable this access with maximum security and minimum operational overhead.</p><p>What should the Solutions Architect do to meet these requirements?</p>",
                "answerExplanation": "<p>The MySQL clusters instances need to access a service on the internet. The most secure method of enabling this access with low operational overhead is to create a NAT gateway. When deploying a NAT gateway, the gateway itself should be deployed in a public subnet whilst the route table in the private subnet must be updated to point traffic to the NAT gateway ID.</p><p>The configuration can be seen in the diagram below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-02-45-d46799d3b11819240a38a0c28fcd0171.jpg\"></p><p><strong>CORRECT: </strong>\"Deploy a NAT gateway in the public subnet. Modify the route table in the private subnet to direct all internet traffic to the NAT gateway\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Deploy a NAT instance in the private subnet. Direct all internet traffic to the NAT instance\" is incorrect. NAT instances require more operational overhead and need to be deployed in a public subnet.</p><p><strong>INCORRECT:</strong> \"Create an internet gateway and attach it to the VPC. Modify the private subnet route table to direct internet traffic to the internet gateway\" is incorrect. You cannot point the instances in the private subnet to an internet gateway as they do not have public IP addresses which is required to use an internet gateway.</p><p><strong>INCORRECT:</strong> \"Create a virtual private gateway and attach it to the VPC. Modify the private subnet route table to direct internet traffic to the virtual private gateway\" is incorrect. A virtual private gateway (VGW) is used with a VPN connection, not for connecting instances in private subnets to the internet.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11108,
                        "content": "<p>Create an internet gateway and attach it to the VPC. Modify the private subnet route table to direct internet traffic to the internet gateway.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11109,
                        "content": "<p>Deploy a NAT instance in the private subnet. Direct all internet traffic to the NAT instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11110,
                        "content": "<p>Deploy a NAT gateway in the public subnet. Modify the route table in the private subnet to direct all internet traffic to the NAT gateway.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11111,
                        "content": "<p>Create a virtual private gateway and attach it to the VPC. Modify the private subnet route table to direct internet traffic to the virtual private gateway.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2659,
            "attributes": {
                "createdAt": "2023-09-09T20:39:59.771Z",
                "updatedAt": "2023-09-09T20:39:59.771Z",
                "content": "<p>A company needs to migrate a large quantity of data from an on-premises environment to Amazon S3. The company is connected via an AWS Direct Connect (DX) connection. The company requires a fully managed solution that will keep the data private and automate and accelerate the replication of the data to AWS storage services.</p><p>Which solution should a Solutions Architect recommend?</p>",
                "answerExplanation": "<p>AWS DataSync can be used to automate and accelerate the replication of data to AWS storage services. Note that Storage Gateway is used for hybrid scenarios where servers need local access to data with various options for storing and synchronizing the data to AWS storage services. Storage Gateway does not accelerate replication of data.</p><p>To deploy DataSync an agent must be installed. Then a task must be configured to replicated data to AWS. The task requires a connection to a service endpoint. To keep the data private and send it across the DX connection, a VPC endpoint should be used.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-04-32-b583aab06b8f4559436be3906fdb3054.jpg\"></p><p><strong>CORRECT: </strong>\"Deploy an AWS DataSync agent for the on-premises environment. Configure a task to replicate the data and connect it to a VPC endpoint\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS DataSync agent for the on-premises environment. Configure a task to replicate the data and connect it to a public endpoint\" is incorrect. A public endpoint will send data over the public internet which should be avoided in this scenario.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Storage Gateway volume gateway in stored volume mode and take point-in-time copies of the volumes using AWS Backup\" is incorrect. Storage Gateway will not accelerate replication and a volume gateway will create EBS snapshots (not S3 objects).</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Storage Gateway file gateway with a local cache and store the primary data set in Amazon S3\" is incorrect. Storage Gateway will not accelerate replication and a file gateway should be used for providing NFS or CIFS/SMB access to data locally which is not required.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/datasync/latest/userguide/choose-service-endpoint.html\">https://docs.aws.amazon.com/datasync/latest/userguide/choose-service-endpoint.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 11112,
                        "content": "<p>Deploy an AWS Storage Gateway file gateway with a local cache and store the primary data set in Amazon S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11113,
                        "content": "<p>Deploy an AWS DataSync agent for the on-premises environment. Configure a task to replicate the data and connect it to a public endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11114,
                        "content": "<p>Deploy an AWS DataSync agent for the on-premises environment. Configure a task to replicate the data and connect it to a VPC endpoint.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11115,
                        "content": "<p>Deploy an AWS Storage Gateway volume gateway in stored volume mode and take point-in-time copies of the volumes using AWS Backup.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2660,
            "attributes": {
                "createdAt": "2023-09-09T20:39:59.878Z",
                "updatedAt": "2023-09-09T20:39:59.878Z",
                "content": "<p>A Solutions Architect is designing an application that will run on Amazon EC2 instances. The application will use Amazon S3 for storing image files and an Amazon DynamoDB table for storing customer information. The security team require that traffic between the EC2 instances and AWS services must not traverse the public internet.</p><p>How can the Solutions Architect meet the security team’s requirements?</p>",
                "answerExplanation": "<p>A VPC endpoint enables private connections between your VPC and supported AWS services and VPC endpoint services powered by AWS PrivateLink. A gateway endpoint is used for Amazon S3 and Amazon DynamoDB. You specify a gateway endpoint as a route table target for traffic that is destined for the supported AWS services.</p><p><strong>CORRECT: </strong>\"Create gateway VPC endpoints for Amazon S3 and DynamoDB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a NAT gateway in a public subnet and configure route tables\" is incorrect. A NAT gateway is used for enabling internet connectivity for instances in private subnets. Connections will traverse the internet.</p><p><strong>INCORRECT:</strong> \"Create interface VPC endpoints for Amazon S3 and DynamoDB\" is incorrect. You should use a gateway VPC endpoint for S3 and DynamoDB.</p><p><strong>INCORRECT:</strong> \"Create a virtual private gateway and configure VPC route tables\" is incorrect VGWs are used for VPN connections, they do not allow access to AWS services from a VPC.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11116,
                        "content": "<p>Create a virtual private gateway and configure VPC route tables.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11117,
                        "content": "<p>Create gateway VPC endpoints for Amazon S3 and DynamoDB.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11118,
                        "content": "<p>Create interface VPC endpoints for Amazon S3 and DynamoDB.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11119,
                        "content": "<p>Create a NAT gateway in a public subnet and configure route tables.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2661,
            "attributes": {
                "createdAt": "2023-09-09T20:39:59.975Z",
                "updatedAt": "2023-09-09T20:39:59.975Z",
                "content": "<p>An online store uses an Amazon Aurora database. The database is deployed as a Multi-AZ deployment. Recently, metrics have shown that database read requests are high and causing performance issues which result in latency for write requests.</p><p>What should the solutions architect do to separate the read requests from the write requests?</p>",
                "answerExplanation": "<p>Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region.</p><p>The DB cluster volume is made up of multiple copies of the data for the DB cluster. However, the data in the cluster volume is represented as a single, logical volume to the primary instance and to Aurora Replicas in the DB cluster.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-56-02-d92494e987b1b8e80c6782e078540b97.jpg\"></p><p>As well as providing scaling for reads, Aurora Replicas are also targets for multi-AZ. In this case the solutions architect can update the application to read from the Aurora Replica</p><p><strong>CORRECT: </strong>\"Update the application to read from the Aurora Replica\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a read replica and modify the application to use the appropriate endpoint\" is incorrect. An Aurora Replica is both a standby in a Multi-AZ configuration and a target for read traffic. The architect simply needs to direct traffic to the Aurora Replica.</p><p><strong>INCORRECT:</strong> \"Enable read through caching on the Amazon Aurora database.\" is incorrect as this is not a feature of Amazon Aurora.</p><p><strong>INCORRECT:</strong> \"Create a second Amazon Aurora database and link it to the primary database as a read replica\" is incorrect as an Aurora Replica already exists as this is a Multi-AZ configuration and the standby is an Aurora Replica that can be used for read traffic.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
                "options": [
                    {
                        "id": 11120,
                        "content": "<p>Update the application to read from the Aurora Replica</p>",
                        "isValid": true
                    },
                    {
                        "id": 11121,
                        "content": "<p>Create a read replica and modify the application to use the appropriate endpoint</p>",
                        "isValid": false
                    },
                    {
                        "id": 11122,
                        "content": "<p>Enable read through caching on the Amazon Aurora database</p>",
                        "isValid": false
                    },
                    {
                        "id": 11123,
                        "content": "<p>Create a second Amazon Aurora database and link it to the primary database as a read replica</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2662,
            "attributes": {
                "createdAt": "2023-09-09T20:40:00.141Z",
                "updatedAt": "2023-09-09T20:40:00.141Z",
                "content": "<p>An application allows users to upload and download files. Files older than 2 years will be accessed less frequently. A solutions architect needs to ensure that the application can scale to any number of files while maintaining high availability and durability.</p><p>Which scalable solutions should the solutions architect recommend?</p>",
                "answerExplanation": "<p>S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance make S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files.</p><p><strong>CORRECT: </strong>\"Store the files on Amazon S3 with a lifecycle policy that moves objects older than 2 years to S3 Standard Infrequent Access (S3 Standard-IA)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Store the files on Amazon Elastic File System (EFS) with a lifecycle policy that moves objects older than 2 years to EFS Infrequent Access (EFS IA)\" is incorrect. With EFS you can transition files to EFS IA after a file has not been accessed for a specified period of time with options up to 90 days. You cannot transition based on an age of 2 years.</p><p><strong>INCORRECT:</strong> \"Store the files in Amazon Elastic Block Store (EBS) volumes. Schedule snapshots of the volumes. Use the snapshots to archive data older than 2 years\" is incorrect. You cannot identify the age of data and archive snapshots in this way with EBS.</p><p><strong>INCORRECT:</strong> \"Store the files in Amazon Elastic Block Store (EBS) volumes. Create a lifecycle policy to move files older than 2 years to Amazon S3 Glacier\" is incorrect. You cannot archive files from an EBS volume to Glacier using lifecycle policies.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11124,
                        "content": "<p>Store the files on Amazon Elastic File System (EFS) with a lifecycle policy that moves objects older than 2 years to EFS Infrequent Access (EFS IA)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11125,
                        "content": "<p>Store the files in Amazon Elastic Block Store (EBS) volumes. Schedule snapshots of the volumes. Use the snapshots to archive data older than 2 years</p>",
                        "isValid": false
                    },
                    {
                        "id": 11126,
                        "content": "<p>Store the files in Amazon Elastic Block Store (EBS) volumes. Create a lifecycle policy to move files older than 2 years to Amazon S3 Glacier</p>",
                        "isValid": false
                    },
                    {
                        "id": 11127,
                        "content": "<p>Store the files on Amazon S3 with a lifecycle policy that moves objects older than 2 years to S3 Standard Infrequent Access (S3 Standard-IA)</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2663,
            "attributes": {
                "createdAt": "2023-09-09T20:40:00.265Z",
                "updatedAt": "2023-09-09T20:40:00.265Z",
                "content": "<p>An application requires a MySQL database which will only be used several times a week for short periods. The database needs to provide automatic instantiation and scaling. Which database service is most suitable?</p>",
                "answerExplanation": "<p>Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora. The database automatically starts up, shuts down, and scales capacity up or down based on application needs. This is an ideal database solution for infrequently-used applications.</p><p><strong>CORRECT: </strong>\"Amazon Aurora Serverless\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon RDS MySQL\" is incorrect as this service requires an instance to be running all the time which is more costly.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 instance with MySQL database installed\" is incorrect as this service requires an instance to be running all the time which is more costly.</p><p><strong>INCORRECT:</strong> \"Amazon Aurora\" is incorrect as this service requires an instance to be running all the time which is more costly.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/serverless/\">https://aws.amazon.com/rds/aurora/serverless/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
                "options": [
                    {
                        "id": 11128,
                        "content": "<p>Amazon Aurora</p>",
                        "isValid": false
                    },
                    {
                        "id": 11129,
                        "content": "<p>Amazon EC2 instance with MySQL database installed</p>",
                        "isValid": false
                    },
                    {
                        "id": 11130,
                        "content": "<p>Amazon Aurora Serverless</p>",
                        "isValid": true
                    },
                    {
                        "id": 11131,
                        "content": "<p>Amazon RDS MySQL</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2664,
            "attributes": {
                "createdAt": "2023-09-09T20:40:00.382Z",
                "updatedAt": "2023-09-09T20:40:00.382Z",
                "content": "<p>A company copies 250 TB of data from a recent land survey onto multiple AWS Snowball Edge Storage Optimized devices. The company has a high-performance computing (HPC) cluster that is hosted within AWS to look for items of archaeological interest. A solutions architect must provide the cluster with consistent low latency and high-throughput access to the data which is hosted on the Snowball Edge Storage Optimized devices. The company is sending the devices back to AWS.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>Using an Amazon FSX for Lustre file system is ideal as it is designed for High Performance Compute workloads. The native connection between Snowball and Amazon S3 ensures this solution meets the stated requirements.</p><p><strong>CORRECT: </strong>\"Set up an Amazon S3 bucket. Configure an Amazon FSx for Lustre file system and integrate it with the S3 bucket after importing the data then access the FSx for Lustre file system from the HPC cluster instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a bucket in Amazon S3 and import the data into the S3 bucket. Set up an AWS Storage Gateway file gateway to use the S3 bucket and access the file gateway from the HPC cluster instances” is incorrect. AWS Storage Gateway File Gateway is not designed to allow extremely low latency file systems. It is a hybrid cloud storage service not designed for this application.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon Elastic File System (Amazon EFS) file system and an Amazon S3 bucket. Upload the data to the S3 bucket. Using the EFS file system, copy the data from the S3 bucket and access the EFS file system from the HPC cluster instances” is incorrect. Although this would work, a standard EFS File System would not provide enough performance to fit the applications requirements.</p><p><strong>INCORRECT:</strong> \"Create an Amazon FSx for Lustre file system and import the data directly into the FSx for Lustre file system and access the FSx for Lustre file system from the HPC cluster instances” is incorrect. You cannot access the FSx for Lustre file system from the HPC cluster instances and this is only possible via S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/lustre/\">https://aws.amazon.com/fsx/lustre/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 11132,
                        "content": "<p>Set up an Amazon Elastic File System (Amazon EFS) file system and an Amazon S3 bucket. Upload the data to the S3 bucket. Using the EFS file system, copy the data from the S3 bucket and access the EFS file system from the HPC cluster instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11133,
                        "content": "<p>Create an Amazon FSx for Lustre file system and import the data directly into the FSx for Lustre file system and access the FSx for Lustre file system from the HPC cluster instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11134,
                        "content": "<p>Create a bucket in Amazon S3 and import the data into the S3 bucket. Set up an AWS Storage Gateway file gateway to use the S3 bucket and access the file gateway from the HPC cluster instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11135,
                        "content": "<p>Set up an Amazon S3 bucket. Configure an Amazon FSx for Lustre file system and integrate it with the S3 bucket after importing the data then access the FSx for Lustre file system from the HPC cluster instances.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2665,
            "attributes": {
                "createdAt": "2023-09-09T20:40:00.505Z",
                "updatedAt": "2023-09-09T20:40:00.505Z",
                "content": "<p>An application runs on Amazon EC2 instances backed by Amazon EBS volumes and an Amazon RDS database. The application is highly sensitive and security compliance requirements mandate that all personally identifiable information (PII) be encrypted at rest.</p><p>Which solution should a Solutions Architect choose to this requirement?</p>",
                "answerExplanation": "<p>The data must be encrypted at rest on both the EC2 instance’s attached EBS volumes and the RDS database. Both storage locations can be encrypted using AWS KMS keys. With RDS, KMS uses a customer master key (CMK) to encrypt the DB instance, all logs, backups, and snapshots.</p><p><strong>CORRECT: </strong>\"Configure Amazon EBS encryption and Amazon RDS encryption with AWS KMS keys to encrypt instance and database volumes\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable encryption on Amazon RDS during creation. Use Amazon Macie to identify sensitive data\" is incorrect. This does not encrypt the EBS volumes attached to the EC2 instance and Macie cannot be used with RDS.</p><p><strong>INCORRECT:</strong> \"Configure SSL/TLS encryption using AWS KMS customer master keys (CMKs) to encrypt database volumes\" is incorrect. SSL encryption encrypts data in transit but not at rest.</p><p><strong>INCORRECT:</strong> \"Deploy AWS CloudHSM, generate encryption keys, and use the customer master key (CMK) to encrypt database volumes\" is incorrect. CloudHSM is not required for this solution, and we need to encrypt the database volumes and the EBS volumes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 11136,
                        "content": "<p>Configure SSL/TLS encryption using AWS KMS customer master keys (CMKs) to encrypt database volumes.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11137,
                        "content": "<p>Configure Amazon EBS encryption and Amazon RDS encryption with AWS KMS keys to encrypt instance and database volumes.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11138,
                        "content": "<p>Enable encryption on Amazon RDS during creation. Use Amazon Macie to identify sensitive data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11139,
                        "content": "<p>Deploy AWS CloudHSM, generate encryption keys, and use the customer master key (CMK) to encrypt database volumes.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2666,
            "attributes": {
                "createdAt": "2023-09-09T20:40:00.622Z",
                "updatedAt": "2023-09-09T20:40:00.622Z",
                "content": "<p>A company runs a financial application using an Amazon EC2 Auto Scaling group behind an Application Load Balancer (ALB). When running month-end reports on a specific day and time each month the application becomes unacceptably slow. Amazon CloudWatch metrics show the CPU utilization hitting 100%.</p><p>What should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?</p>",
                "answerExplanation": "<p>Scheduled scaling allows you to set your own scaling schedule. In this case the scaling action can be scheduled to occur just prior to the time that the reports will be run each month. Scaling actions are performed automatically as a function of time and date. This will ensure that there are enough EC2 instances to serve the demand and prevent the application from slowing down.</p><p><strong>CORRECT: </strong>\"Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon CloudFront distribution in front of the ALB\" is incorrect as this would be more suitable for providing access to global users by caching content.</p><p><strong>INCORRECT:</strong> \"Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization\" is incorrect as this would not prevent the slow-down from occurring as there would be a delay between when the CPU hits 100% and the metric being reported and additional instances being launched.</p><p><strong>INCORRECT:</strong> \"Configure Amazon ElastiCache to remove some of the workload from the EC2 instances\" is incorrect as ElastiCache is a database cache, it cannot replace the compute functions of an EC2 instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 11140,
                        "content": "<p>Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule</p>",
                        "isValid": true
                    },
                    {
                        "id": 11141,
                        "content": "<p>Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization</p>",
                        "isValid": false
                    },
                    {
                        "id": 11142,
                        "content": "<p>Configure Amazon ElastiCache to remove some of the workload from the EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 11143,
                        "content": "<p>Configure an Amazon CloudFront distribution in front of the ALB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2667,
            "attributes": {
                "createdAt": "2023-09-09T20:40:00.733Z",
                "updatedAt": "2023-09-09T20:40:00.733Z",
                "content": "<p>A company is creating a solution that must offer disaster recovery across multiple AWS Regions. The solution requires a relational database that can support a Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of 1 minute.</p><p>Which AWS solution can achieve this?</p>",
                "answerExplanation": "<p>Aurora Global Database lets you easily scale database reads across the world and place your applications close to your users. Your applications enjoy quick data access regardless of the number and location of secondary regions, with typical cross-region replication latencies below 1 second.</p><p>If your primary region suffers a performance degradation or outage, you can promote one of the secondary regions to take read/write responsibilities. An Aurora cluster can recover in less than 1 minute even in the event of a complete regional outage. This provides your application with an effective Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute, providing a strong foundation for a global business continuity plan.</p><p><strong>CORRECT: </strong>\"Amazon Aurora Global Database\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon RDS for with Multi-AZ enabled\" is incorrect. RDS Multi-AZ is across availability zones, not across Regions.</p><p><strong>INCORRECT:</strong> \"Amazon RDS for with a cross-Region replica\" is incorrect. A cross-Region replica for RDS cannot provide an RPO of 1 second as there is typically more latency. You also cannot achieve a minute RPO as it takes much longer to promote a replica to a master.</p><p><strong>INCORRECT:</strong> \"Amazon DynamoDB global tables\" is incorrect. This is not a relational database; it is a non-relational database (NoSQL).</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
                "options": [
                    {
                        "id": 11144,
                        "content": "<p>Amazon RDS for with Multi-AZ enabled.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11145,
                        "content": "<p>Amazon RDS for with a cross-Region replica.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11146,
                        "content": "<p>Amazon Aurora Global Database.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11147,
                        "content": "<p>Amazon DynamoDB global tables.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2668,
            "attributes": {
                "createdAt": "2023-09-09T20:40:00.846Z",
                "updatedAt": "2023-09-09T20:40:00.846Z",
                "content": "<p>A Solutions Architect created the following policy and associated to an AWS IAM group containing several administrative users:</p><p>{</p><p>&nbsp; &nbsp;\"Version\": \"2012-10-17\",</p><p>&nbsp; &nbsp; \"Statement\": [</p><p>&nbsp; &nbsp; {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Effect\": \"Allow\",</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Action\": \"ec2:TerminateInstances\",</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Resource\": \"*\",</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Condition\": {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"IpAddress\": {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"aws:SourceIp\": \"10.1.2.0/24\"</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}</p><p>&nbsp; &nbsp; },</p><p>&nbsp; &nbsp;{</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Effect\": \"Deny\",</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Action\": \"ec2:*\",</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Resource\": \"*\",</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Condition\": {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"StringNotEquals\": {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"ec2:Region\": \"us-east-1\"</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]</p><p>&nbsp; &nbsp;}</p><p>What is the effect of this policy?</p>",
                "answerExplanation": "<p>The Condition element (or Condition <em>block</em>) lets you specify conditions for when a policy is in effect. The Condition element is optional. In the Condition element, you build expressions in which you use condition operators (equal, less than, etc.) to match the condition keys and values in the policy against keys and values in the request context.</p><p>In this policy statement the first block allows the \"ec2:TerminateInstances\" API action only if the IP address of the requester is within the \"10.1.2.0/24\" range. This is specified using the \"aws:SourceIp\" condition.</p><p>The second block denies all EC2 API actions with a conditional operator (StringNotEquals) that checks the Region the request is being made in (\"ec2:Region\"). If the Region is any value other than us-east-1 the request will be denied. If the Region the request is being made in is us-east-1 the request will not be denied.</p><p><strong>CORRECT: </strong>\"Administrators can terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.1.2.28\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Administrators cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.1.2.28\" is incorrect. This is not true; the conditions allow this action.</p><p><strong>INCORRECT:</strong> \"Administrators can terminate an EC2 instance in any AWS Region except us-east-1\" is incorrect. The API action to terminate instances only has a condition of the source IP. If the source IP is in the range it will allow. The second block only denies API actions if the Region is NOT us-east-1. Therefore, the user can terminate instances in us-east-1</p><p><strong>INCORRECT:</strong> \"Administrators can terminate an EC2 instance with the IP address 10.1.2.5 in the us-east-1 Region\" is incorrect. The aws:SourceIp condition is checking the IP address of the requester (where you’re making the call from), not the resource you want to terminate.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 11148,
                        "content": "<p>Administrators can terminate an EC2 instance in any AWS Region except us-east-1.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11149,
                        "content": "<p>Administrators can terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.1.2.28.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11150,
                        "content": "<p>Administrators can terminate an EC2 instance with the IP address 10.1.2.5 in the us-east-1 Region.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11151,
                        "content": "<p>Administrators cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.1.2.28.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2669,
            "attributes": {
                "createdAt": "2023-09-09T20:40:00.959Z",
                "updatedAt": "2023-09-09T20:40:00.959Z",
                "content": "<p>A company operates a production web application that uses an Amazon RDS MySQL database. The database has automated, non-encrypted daily backups. To increase the security of the data, it has been recommended that encryption should be enabled for backups. Unencrypted backups will be destroyed after the first encrypted backup has been completed.</p><p>What should be done to enable encryption for future backups?</p>",
                "answerExplanation": "<p>Amazon RDS uses snapshots for backup. Snapshots are encrypted when created only if the database is encrypted and you can only select encryption for the database when you first create it. In this case the database, and hence the snapshots, ad unencrypted.</p><p>However, you can create an encrypted copy of a snapshot. You can restore using that snapshot which creates a new DB instance that has encryption enabled. From that point on encryption will be enabled for all snapshots.</p><p><strong>CORRECT: </strong>\"Create a snapshot of the database. Copy it to an encrypted snapshot. Restore the database from the encrypted snapshot\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable an encrypted read replica on RDS for MySQL. Promote the encrypted read replica to primary. Remove the original database instance\" is incorrect as you cannot create an encrypted read replica from an unencrypted master.</p><p><strong>INCORRECT:</strong> \"Modify the backup section of the database configuration to toggle the Enable encryption check box\" is incorrect as you cannot add encryption for an existing database.</p><p><strong>INCORRECT:</strong> \"Enable default encryption for the Amazon S3 bucket where backups are stored\" is incorrect because you do not have access to the S3 bucket in which snapshots are stored.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 11152,
                        "content": "<p>Create a snapshot of the database. Copy it to an encrypted snapshot. Restore the database from the encrypted snapshot</p>",
                        "isValid": true
                    },
                    {
                        "id": 11153,
                        "content": "<p>Modify the backup section of the database configuration to toggle the Enable encryption check box</p>",
                        "isValid": false
                    },
                    {
                        "id": 11154,
                        "content": "<p>Enable an encrypted read replica on RDS for MySQL. Promote the encrypted read replica to primary. Remove the original database instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 11155,
                        "content": "<p>Enable default encryption for the Amazon S3 bucket where backups are stored</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2670,
            "attributes": {
                "createdAt": "2023-09-09T20:40:01.072Z",
                "updatedAt": "2023-09-09T20:40:01.072Z",
                "content": "<p>An application runs on Amazon EC2 Linux instances. The application generates log files which are written using standard API calls. A storage solution is required that can be used to store the files indefinitely and must allow concurrent access to all files.</p><p>Which storage service meets these requirements and is the MOST cost-effective?</p>",
                "answerExplanation": "<p>The application is writing the files using API calls which means it will be compatible with Amazon S3 which uses a REST API. S3 is a massively scalable key-based object store that is well-suited to allowing concurrent access to the files from many instances.</p><p>Amazon S3 will also be the most cost-effective choice. A rough calculation using the AWS pricing calculator shows the cost differences between 1TB of storage on EBS, EFS, and S3 Standard.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-55-11-6ae407730f9451342d76fb514c903b8a.jpg\"></p><p><strong>CORRECT: </strong>\"Amazon S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EFS\" is incorrect as though this does offer concurrent access from many EC2 Linux instances, it is not the most cost-effective solution.</p><p><strong>INCORRECT:</strong> \"Amazon EBS\" is incorrect. The Elastic Block Store (EBS) is not a good solution for concurrent access from many EC2 instances and is not the most cost-effective option either. EBS volumes are mounted to a single instance except when using multi-attach which is a new feature and has several constraints.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 instance store\" is incorrect as this is an ephemeral storage solution which means the data is lost when powered down. Therefore, this is not an option for long-term data storage.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11156,
                        "content": "<p>Amazon EFS</p>",
                        "isValid": false
                    },
                    {
                        "id": 11157,
                        "content": "<p>Amazon EBS</p>",
                        "isValid": false
                    },
                    {
                        "id": 11158,
                        "content": "<p>Amazon EC2 instance store</p>",
                        "isValid": false
                    },
                    {
                        "id": 11159,
                        "content": "<p>Amazon S3</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2671,
            "attributes": {
                "createdAt": "2023-09-09T20:40:01.201Z",
                "updatedAt": "2023-09-09T20:40:01.201Z",
                "content": "<p>An application is deployed on multiple AWS regions and accessed from around the world. The application exposes static public IP addresses. Some users are experiencing poor performance when accessing the application over the Internet.</p><p>What should a solutions architect recommend to reduce internet latency?</p>",
                "answerExplanation": "<p>AWS Global Accelerator is a service in which you create <em>accelerators</em> to improve availability and performance of your applications for local and global users. Global Accelerator directs traffic to optimal endpoints over the AWS global network. This improves the availability and performance of your internet applications that are used by a global audience. Global Accelerator is a global service that supports endpoints in multiple AWS Regions, which are listed in the <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/\">AWS Region Table</a>.</p><p>By default, Global Accelerator provides you with two static IP addresses that you associate with your accelerator. (Or, instead of using the IP addresses that Global Accelerator provides, you can configure these entry points to be IPv4 addresses from your own IP address ranges that you bring to Global Accelerator.)</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-57-03-6258b515dd5b68a3f0c87a51702006a3.jpg\"></p><p>The static IP addresses are anycast from the AWS edge network and distribute incoming application traffic across multiple endpoint resources in multiple AWS Regions, which increases the availability of your applications. Endpoints can be Network Load Balancers, Application Load Balancers, EC2 instances, or Elastic IP addresses that are located in one AWS Region or multiple Regions.</p><p><strong>CORRECT: </strong>\"Set up AWS Global Accelerator and add endpoints\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Set up AWS Direct Connect locations in multiple Regions\" is incorrect as this is used to connect from an on-premises data center to AWS. It does not improve performance for users who are not connected to the on-premises data center.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon CloudFront distribution to access an application\" is incorrect as CloudFront cannot expose static public IP addresses.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon Route 53 geoproximity routing policy to route traffic\" is incorrect as this does not reduce internet latency as well as using Global Accelerator. GA will direct users to the closest edge location and then use the AWS global network.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html\">https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-global-accelerator/\">https://digitalcloud.training/aws-global-accelerator/</a></p>",
                "options": [
                    {
                        "id": 11160,
                        "content": "<p>Set up AWS Direct Connect locations in multiple Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 11161,
                        "content": "<p>Set up an Amazon Route 53 geoproximity routing policy to route traffic</p>",
                        "isValid": false
                    },
                    {
                        "id": 11162,
                        "content": "<p>Set up AWS Global Accelerator and add endpoints</p>",
                        "isValid": true
                    },
                    {
                        "id": 11163,
                        "content": "<p>Set up an Amazon CloudFront distribution to access an application</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2672,
            "attributes": {
                "createdAt": "2023-09-09T20:40:01.317Z",
                "updatedAt": "2023-09-09T20:40:01.317Z",
                "content": "<p>A security team wants to limit access to specific services or actions in all of the team's AWS accounts. All accounts belong to a large organization in AWS Organizations. The solution must be scalable and there must be a single point where permissions can be maintained.</p><p>What should a solutions architect do to accomplish this?</p>",
                "answerExplanation": "<p>Service control policies (SCPs) offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-57-51-f9ea69c694961184b290662661f770a4.jpg\"></p><p>SCPs alone are not sufficient for allowing access in the accounts in your organization. Attaching an SCP to an AWS Organizations entity (root, OU, or account) defines a guardrail for what actions the principals can perform. You still need to attach <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html\">identity-based or resource-based policies</a> to principals or resources in your organization's accounts to actually grant permissions to them.</p><p><strong>CORRECT: </strong>\"Create a service control policy in the root organizational unit to deny access to the services or actions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an ACL to provide access to the services or actions\" is incorrect as access control lists are not used for permissions associated with IAM. Permissions policies are used with IAM.</p><p><strong>INCORRECT:</strong> \"Create a security group to allow accounts and attach it to user groups\" is incorrect as security groups are instance level firewalls. They do not limit service actions.</p><p><strong>INCORRECT:</strong> \"Create cross-account roles in each account to deny access to the services or actions\" is incorrect as this is a complex solution and does not provide centralized control</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
                "options": [
                    {
                        "id": 11164,
                        "content": "<p>Create cross-account roles in each account to deny access to the services or actions</p>",
                        "isValid": false
                    },
                    {
                        "id": 11165,
                        "content": "<p>Create a security group to allow accounts and attach it to user groups</p>",
                        "isValid": false
                    },
                    {
                        "id": 11166,
                        "content": "<p>Create a service control policy in the root organizational unit to deny access to the services or actions</p>",
                        "isValid": true
                    },
                    {
                        "id": 11167,
                        "content": "<p>Create an ACL to provide access to the services or actions</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2673,
            "attributes": {
                "createdAt": "2023-09-09T20:40:01.422Z",
                "updatedAt": "2023-09-09T20:40:01.422Z",
                "content": "<p>A web application in a three-tier architecture runs on a fleet of Amazon EC2 instances. Performance issues have been reported and investigations point to insufficient swap space. The operations team requires monitoring to determine if this is correct.</p><p>What should a solutions architect recommend?</p>",
                "answerExplanation": "<p>You can use the CloudWatch agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The agent supports both Windows Server and Linux, and enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU core.</p><p>There is now a unified agent and previously there were monitoring scripts. Both of these tools can capture SwapUtilization metrics and send them to CloudWatch. This is the best way to get memory utilization metrics from Amazon EC2 instnaces.</p><p><strong>CORRECT: </strong>\"Install an Amazon CloudWatch agent on the instances. Run an appropriate script on a set schedule. Monitor SwapUtilization metrics in CloudWatch\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable detailed monitoring in the EC2 console. Create an Amazon CloudWatch SwapUtilization custom metric. Monitor SwapUtilization metrics in CloudWatch\" is incorrect as you do not create custom metrics in the console, you must configure the instances to send the metric information to CloudWatch.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon CloudWatch SwapUsage metric dimension. Monitor the SwapUsage dimension in the EC2 metrics in CloudWatch\" is incorrect as there is no SwapUsage metric in CloudWatch. All memory metrics must be custom metrics.</p><p><strong>INCORRECT:</strong> \"Use EC2 metadata to collect information, then publish it to Amazon CloudWatch custom metrics. Monitor SwapUsage metrics in CloudWatch\" is incorrect as performance related information is not stored in metadata.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 11168,
                        "content": "<p>Use EC2 metadata to collect information, then publish it to Amazon CloudWatch custom metrics. Monitor SwapUsage metrics in CloudWatch</p>",
                        "isValid": false
                    },
                    {
                        "id": 11169,
                        "content": "<p>Install an Amazon CloudWatch agent on the instances. Run an appropriate script on a set schedule. Monitor SwapUtilization metrics in CloudWatch</p>",
                        "isValid": true
                    },
                    {
                        "id": 11170,
                        "content": "<p>Enable detailed monitoring in the EC2 console. Create an Amazon CloudWatch SwapUtilization custom metric. Monitor SwapUtilization metrics in CloudWatch</p>",
                        "isValid": false
                    },
                    {
                        "id": 11171,
                        "content": "<p>Configure an Amazon CloudWatch SwapUsage metric dimension. Monitor the SwapUsage dimension in the EC2 metrics in CloudWatch</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2674,
            "attributes": {
                "createdAt": "2023-09-09T20:40:01.520Z",
                "updatedAt": "2023-09-09T20:40:01.520Z",
                "content": "<p>A company has a Production VPC and a Pre-Production VPC. The Production VPC uses VPNs through a customer gateway to connect to a single device in an on-premises data center. The Pre-Production VPC uses a virtual private gateway attached to two AWS Direct Connect (DX) connections. Both VPCs are connected using a single VPC peering connection.</p><p>How can a Solutions Architect improve this architecture to remove any single point of failure?</p>",
                "answerExplanation": "<p>The only single point of failure in this architecture is the customer gateway device in the on-premises data center. A customer gateway device is the on-premises (client) side of the connection into the VPC. The customer gateway configuration is created within AWS, but the actual device is a physical or virtual device running in the on-premises data center. If this device is a single device, then if it fails the VPN connections will fail. The AWS side of the VPN link is the virtual private gateway, and this is a redundant device.</p><p><strong>CORRECT: </strong>\"Add additional VPNs to the Production VPC from a second customer gateway device\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Add an additional VPC peering connection between the two VPCs\" is incorrect. VPC peering connections are already redundant, you do not need multiple connections.</p><p><strong>INCORRECT:</strong> \"Add a set of VPNs between the Production and Pre-Production VPCs\" is incorrect. You cannot create VPN connections between VPCs (using AWS VPNs).</p><p><strong>INCORRECT:</strong> \"Add a second virtual private gateway and attach it to the Production VPC\" is incorrect. Virtual private gateways (VGWs) are redundant devices so a second one is not necessary.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11172,
                        "content": "<p>Add additional VPNs to the Production VPC from a second customer gateway device.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11173,
                        "content": "<p>Add a second virtual private gateway and attach it to the Production VPC.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11174,
                        "content": "<p>Add an additional VPC peering connection between the two VPCs.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11175,
                        "content": "<p>Add a set of VPNs between the Production and Pre-Production VPCs.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2675,
            "attributes": {
                "createdAt": "2023-09-09T20:40:01.641Z",
                "updatedAt": "2023-09-09T20:40:01.641Z",
                "content": "<p>An application runs on Amazon EC2 instances in a private subnet. The EC2 instances process data that is stored in an Amazon S3 bucket. The data is highly confidential and a private and secure connection is required between the EC2 instances and the S3 bucket.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>A gateway VPC endpoint can be used to access an Amazon S3 bucket using private IP addresses. To further secure the solution an S3 bucket policy can be created that restricts access to the VPC endpoint so connections cannot be made to the bucket from other sources.</p><p><strong>CORRECT: </strong>\"Set up S3 bucket policies to allow access from a VPC endpoint\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Set up an IAM policy to grant read-write access to the S3 bucket\" is incorrect. This does not enable private access from EC2. A gateway VPC endpoint is required.</p><p><strong>INCORRECT:</strong> \"Configure encryption for the S3 bucket using an AWS KMS key\" is incorrect. This will encrypt data at rest but does not secure the connection to the bucket or ensure private connections must be made.</p><p><strong>INCORRECT:</strong> \"Configure a custom SSL/TLS certificate on the S3 bucket\" is incorrect. You cannot add a custom SSL/TLS certificate to Amazon S3.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11176,
                        "content": "<p>Configure encryption for the S3 bucket using an AWS KMS key.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11177,
                        "content": "<p>Set up S3 bucket policies to allow access from a VPC endpoint.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11178,
                        "content": "<p>Set up an IAM policy to grant read-write access to the S3 bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11179,
                        "content": "<p>Configure a custom SSL/TLS certificate on the S3 bucket.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2676,
            "attributes": {
                "createdAt": "2023-09-09T20:40:01.741Z",
                "updatedAt": "2023-09-09T20:40:01.741Z",
                "content": "<p>A company is deploying a solution for sharing media files around the world using Amazon CloudFront with an Amazon S3 origin configured as a static website. The company requires that all traffic for the website must be inspected by AWS WAF.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>The AWS Web Application Firewall (WAF) can be attached to an Amazon CloudFront distribution to enable protection from web exploits. In this case the distribution uses an S3 origin, and the question is stating that all traffic must be inspected by AWS WAF. This means we need to ensure that requests cannot circumvent AWS WAF and hit the S3 bucket directly.</p><p>This can be achieved by configuring an origin access identity (OAI) which is a special type of CloudFront user that is created within the distribution and configured in an S3 bucket policy. The policy will only allow requests that come from the OAI which means all requests must come via the distribution and cannot hit S3 directly.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-32-57-0cec77f550d1e2e6100046094949925b.jpg\"></p><p><strong>CORRECT: </strong>\"Deploy CloudFront with an S3 origin and configure an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the CloudFront distribution\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a Network ACL that limits access to the S3 bucket to the CloudFront IP addresses. Attach a WebACL to the CloudFront distribution\" is incorrect. Network ACLs restrict traffic in/out of subnets but S3 is a public service.</p><p><strong>INCORRECT:</strong> \"Use an Amazon Route 53 Alias record to forward traffic for the website to AWS WAF. Configure AWS WAF to inspect traffic and attach the CloudFront distribution\" is incorrect. You cannot direct traffic to AWS WAF using an Alias record.</p><p><strong>INCORRECT:</strong> \"Create an S3 bucket policy with a condition that only allows requests that originate from AWS WAF\" is incorrect. This cannot be done. Instead use an OAI in the bucket policy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 11180,
                        "content": "<p>Use an Amazon Route 53 Alias record to forward traffic for the website to AWS WAF. Configure AWS WAF to inspect traffic and attach the CloudFront distribution.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11181,
                        "content": "<p>Create an S3 bucket policy with a condition that only allows requests that originate from AWS WAF.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11182,
                        "content": "<p>Deploy CloudFront with an S3 origin and configure an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the CloudFront distribution.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11183,
                        "content": "<p>Create a Network ACL that limits access to the S3 bucket to the CloudFront IP addresses. Attach a WebACL to the CloudFront distribution.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2677,
            "attributes": {
                "createdAt": "2023-09-09T20:40:01.893Z",
                "updatedAt": "2023-09-09T20:40:01.893Z",
                "content": "<p>A new application will be launched on an Amazon EC2 instance with an Elastic Block Store (EBS) volume. A solutions architect needs to determine the most cost-effective storage option. The application will have infrequent usage, with peaks of traffic for a couple of hours in the morning and evening. Disk I/O is variable with peaks of up to 3,000 IOPS.</p><p>Which solution should the solutions architect recommend?</p>",
                "answerExplanation": "<p>General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods of time.</p><p>Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver their provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB.</p><p>In this case the volume would have a baseline performance of 3 x 200 = 600 IOPS. The volume could also burst to 3,000 IOPS for extended periods. As the I/O varies, this should be suitable.</p><p><strong>CORRECT: </strong>\"Amazon EBS General Purpose SSD (gp2)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EBS Provisioned IOPS SSD (io1) \" is incorrect as this would be a more expensive option and is not required for the performance characteristics of this workload.</p><p><strong>INCORRECT:</strong> \"Amazon EBS Cold HDD (sc1)\" is incorrect as there is no IOPS SLA for HDD volumes and they would likely not perform well enough for this workload.</p><p><strong>INCORRECT:</strong> \"Amazon EBS Throughput Optimized HDD (st1)\" is incorrect as there is no IOPS SLA for HDD volumes and they would likely not perform well enough for this workload.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 11184,
                        "content": "<p>Amazon EBS Provisioned IOPS SSD (io1)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11185,
                        "content": "<p>Amazon EBS General Purpose SSD (gp2)</p>",
                        "isValid": true
                    },
                    {
                        "id": 11186,
                        "content": "<p>Amazon EBS Cold HDD (sc1)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11187,
                        "content": "<p>Amazon EBS Throughput Optimized HDD (st1)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2678,
            "attributes": {
                "createdAt": "2023-09-09T20:40:01.998Z",
                "updatedAt": "2023-09-09T20:40:01.998Z",
                "content": "<p>A company is deploying an analytics application on AWS Fargate. The application requires connected storage that offers concurrent access to files and high performance.</p><p>Which storage option should the solutions architect recommend?</p>",
                "answerExplanation": "<p>The Amazon Elastic File System offers concurrent access to a shared file system and provides high performance. You can create file system policies for controlling access and then use an IAM role that is specified in the policy for access.</p><p><strong>CORRECT: </strong>\"Create an Amazon EFS file share and establish an IAM role that allows Fargate to communicate with Amazon EFS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon S3 bucket for the application and establish an IAM role for Fargate to communicate with Amazon S3\" is incorrect. S3 uses a REST API not a file system API so access can be shared but is not concurrent.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EBS volume for the application and establish an IAM role that allows Fargate to communicate with Amazon EBS\" is incorrect. EBS volumes cannot be shared amongst Fargate tasks, they are used with EC2 instances.</p><p><strong>INCORRECT:</strong> \"Create an Amazon FSx for Lustre file share and establish an IAM role that allows Fargate to communicate with FSx for Lustre\" is incorrect. It is not supported to connect Fargate to FSx for Lustre.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html\">https://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
                "options": [
                    {
                        "id": 11188,
                        "content": "<p>Create an Amazon S3 bucket for the application and establish an IAM role for Fargate to communicate with Amazon S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11189,
                        "content": "<p>Create an Amazon FSx for Lustre file share and establish an IAM role that allows Fargate to communicate with FSx for Lustre.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11190,
                        "content": "<p>Create an Amazon EBS volume for the application and establish an IAM role that allows Fargate to communicate with Amazon EBS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11191,
                        "content": "<p>Create an Amazon EFS file share and establish an IAM role that allows Fargate to communicate with Amazon EFS.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2679,
            "attributes": {
                "createdAt": "2023-09-09T20:40:02.097Z",
                "updatedAt": "2023-09-09T20:40:02.097Z",
                "content": "<p>Health related data in Amazon S3 needs to be frequently accessed for up to 90 days. After that time the data must be retained for compliance reasons for seven years and is rarely accessed.</p><p>Which storage classes should be used?</p>",
                "answerExplanation": "<p>In this case the data is frequently accessed so must be stored in standard for the first 90 days. After that the data is still to be kept for compliance reasons but is rarely accessed so is a good use case for DEEP_ARCHIVE.</p><p><strong>CORRECT: </strong>\"Store data in STANDARD for 90 days then transition the data to DEEP_ARCHIVE\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Store data in INTELLIGENT_TIERING for 90 days then transition to STANDARD_IA\" is incorrect. You cannot transition from INTELLIGENT_TIERING to STANDARD_IA.</p><p><strong>INCORRECT:</strong> \"Store data in STANDARD for 90 days then expire the data\" is incorrect. Expiring the data is not possible as it must be retained for compliance.</p><p><strong>INCORRECT:</strong> \"Store data in STANDARD for 90 days then transition to REDUCED_REDUNDANCY\" is incorrect. You cannot transition from any storage class to REDUCED_REDUNDANCY.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11192,
                        "content": "<p>Store data in STANDARD for 90 days then transition to REDUCED_REDUNDANCY</p>",
                        "isValid": false
                    },
                    {
                        "id": 11193,
                        "content": "<p>Store data in INTELLIGENT_TIERING for 90 days then transition to STANDARD_IA</p>",
                        "isValid": false
                    },
                    {
                        "id": 11194,
                        "content": "<p>Store data in STANDARD for 90 days then transition the data to DEEP_ARCHIVE</p>",
                        "isValid": true
                    },
                    {
                        "id": 11195,
                        "content": "<p>Store data in STANDARD for 90 days then expire the data</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2680,
            "attributes": {
                "createdAt": "2023-09-09T20:40:02.241Z",
                "updatedAt": "2023-09-09T20:40:02.241Z",
                "content": "<p>A systems administrator of a company wants to detect and remediate the compromise of services such as Amazon EC2 instances and Amazon S3 buckets.</p><p>Which AWS service can the administrator use to protect the company against attacks?</p>",
                "answerExplanation": "<p>Amazon GuardDuty gives you access to built-in detection techniques that are developed and optimized for the cloud. The detection algorithms are maintained and continuously improved upon by AWS Security. The primary detection categories include reconnaissance, instance compromise, account compromise, and bucket compromise.</p><p>Amazon GuardDuty offers HTTPS APIs, CLI tools, and Amazon CloudWatch Events to support automated security responses to security findings. For example, you can automate the response workflow by using CloudWatch Events as an event source to trigger an AWS Lambda function.</p><p><strong>CORRECT: </strong>\"Amazon GuardDuty\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon Cognito\" is incorrect. Cognito provides sign up and sign services for mobiles apps.</p><p><strong>INCORRECT:</strong> \"Amazon Inspector\" is incorrect. Inspector is more about identifying vulnerabilities and evaluating against security best practices. It does not detect compromise.</p><p><strong>INCORRECT:</strong> \"Amazon Macie\" is incorrect. Macie is used for detecting and protecting sensitive data that is in Amazon S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/features/\">https://aws.amazon.com/guardduty/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/additional-aws-services/\">https://digitalcloud.training/additional-aws-services/</a></p>",
                "options": [
                    {
                        "id": 11196,
                        "content": "<p>Amazon Inspector</p>",
                        "isValid": false
                    },
                    {
                        "id": 11197,
                        "content": "<p>Amazon Cognito</p>",
                        "isValid": false
                    },
                    {
                        "id": 11198,
                        "content": "<p>Amazon GuardDuty</p>",
                        "isValid": true
                    },
                    {
                        "id": 11199,
                        "content": "<p>Amazon Macie</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2681,
            "attributes": {
                "createdAt": "2023-09-09T20:40:02.348Z",
                "updatedAt": "2023-09-09T20:40:02.348Z",
                "content": "<p>A company requires a fully managed replacement for an on-premises storage service. The company’s employees often work remotely from various locations. The solution should also be easily accessible to systems connected to the on-premises environment.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>Amazon FSx for Windows File Server (Amazon FSx) is a fully managed, highly available, and scalable file storage solution built on Windows Server that uses the Server Message Block (SMB) protocol. It allows for Microsoft Active Directory integration, data deduplication, and fully managed backups, among other critical enterprise features.</p><p>An Amazon FSx file system can be created to host the file shares. Clients can then be connected to an AWS Client VPN endpoint and gateway to enable remote access. The protocol used in this solution will be SMB.</p><p><strong>CORRECT: </strong>\"Use Amazon FSx to create an SMB file share. Connect remote clients to the file share over a client VPN\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS Transfer Acceleration to replicate files to Amazon S3 and enable public access\" is incorrect. This is simply a way of improving upload speeds to S3, it is not suitable for enabling internal and external access to a file system.</p><p><strong>INCORRECT:</strong> \"Use AWS DataSync to synchronize data between the on-premises service and Amazon S3\" is incorrect. The on-premises solution is to be replaced so this is not a satisfactory solution. Also, DataSync syncs one way, it is not bidirectional.</p><p><strong>INCORRECT:</strong> \"Use AWS Storage Gateway to create a volume gateway to store and transfer files to Amazon S3\" is incorrect. Storage Gateway volume gateways are mounted using block-based protocols (iSCSI), so this would not be workable.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/storage/accessing-smb-file-shares-remotely-with-amazon-fsx-for-windows-file-server/\">https://aws.amazon.com/blogs/storage/accessing-smb-file-shares-remotely-with-amazon-fsx-for-windows-file-server/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 11200,
                        "content": "<p>Use Amazon FSx to create an SMB file share. Connect remote clients to the file share over a client VPN.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11201,
                        "content": "<p>Use AWS Storage Gateway to create a volume gateway to store and transfer files to Amazon S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11202,
                        "content": "<p>Use AWS Transfer Acceleration to replicate files to Amazon S3 and enable public access.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11203,
                        "content": "<p>Use AWS DataSync to synchronize data between the on-premises service and Amazon S3.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2682,
            "attributes": {
                "createdAt": "2023-09-09T20:40:02.472Z",
                "updatedAt": "2023-09-09T20:40:02.472Z",
                "content": "<p>A solutions architect is designing a microservices architecture. AWS Lambda will store data in an Amazon DynamoDB table named Orders. The solutions architect needs to apply an IAM policy to the Lambda function’s execution role to allow it to put, update, and delete items in the Orders table. No other actions should be allowed.</p><p>Which of the following code snippets should be included in the IAM policy to fulfill this requirement whilst providing the LEAST privileged access?</p>",
                "answerExplanation": "<p>The key requirements are to allow the Lambda function the put, update, and delete actions on a single table. Using the principle of least privilege the answer should not allow any other access.</p><p><strong>CORRECT: </strong>The following answer is correct:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"str\">\"dynamodb:PutItem\"</span><span class=\"pun\">,</span></li><li class=\"L4\"><span class=\"str\">\"dynamodb:UpdateItem\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"str\">\"dynamodb:DeleteItem\"</span></li><li class=\"L6\"><span class=\"pun\">],</span></li><li class=\"L7\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/Orders\"</span></li></ol></pre></div></div><p>This code snippet specifies the exact actions to allow and also specified the resource to apply those permissions to.</p><p><strong>INCORRECT:</strong> the following answer is incorrect:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"str\">\"dynamodb:PutItem\"</span><span class=\"pun\">,</span></li><li class=\"L4\"><span class=\"str\">\"dynamodb:UpdateItem\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"str\">\"dynamodb:DeleteItem\"</span></li><li class=\"L6\"><span class=\"pun\">],</span></li><li class=\"L7\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/*\"</span></li></ol></pre></div></div><p>This code snippet specifies the correct list of actions but it provides a wildcard “*” instead of specifying the exact resource. Therefore, the function will be able to put, update, and delete items on any table in the account.</p><p><strong>INCORRECT:</strong> the following answer is incorrect:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"dynamodb:* \"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/Orders\"</span></li></ol></pre></div></div><p>This code snippet allows any action on DynamoDB by using a wildcard “dynamodb:*”. This does not follow the principle of least privilege.</p><p><strong>INCORRECT:</strong> the following answer is incorrect:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Deny\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"dynamodb:* \"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/Orders\"</span></li></ol></pre></div></div><p>This code snippet denies any action on the table. This does not have the desired effect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 11204,
                        "content": "<div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Deny\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"dynamodb:* \"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/Orders\"</span></li></ol></pre></div></div>",
                        "isValid": false
                    },
                    {
                        "id": 11205,
                        "content": "<div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"str\">\"dynamodb:PutItem\"</span><span class=\"pun\">,</span></li><li class=\"L4\"><span class=\"str\">\"dynamodb:UpdateItem\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"str\">\"dynamodb:DeleteItem\"</span></li><li class=\"L6\"><span class=\"pun\">],</span></li><li class=\"L7\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/Orders\"</span></li></ol></pre></div></div>",
                        "isValid": true
                    },
                    {
                        "id": 11206,
                        "content": "<div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"str\">\"dynamodb:PutItem\"</span><span class=\"pun\">,</span></li><li class=\"L4\"><span class=\"str\">\"dynamodb:UpdateItem\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"str\">\"dynamodb:DeleteItem\"</span></li><li class=\"L6\"><span class=\"pun\">],</span></li><li class=\"L7\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/*\"</span></li></ol></pre></div></div>",
                        "isValid": false
                    },
                    {
                        "id": 11207,
                        "content": "<div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"dynamodb:* \"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/Orders\"</span></li></ol></pre></div></div>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2683,
            "attributes": {
                "createdAt": "2023-09-09T20:40:02.583Z",
                "updatedAt": "2023-09-09T20:40:02.583Z",
                "content": "<p>A company runs a containerized application on an Amazon Elastic Kubernetes Service (EKS) using a microservices architecture. The company requires a solution to collect, aggregate, and summarize metrics and logs. The solution should provide a centralized dashboard for viewing information including CPU and memory utilization for EKS namespaces, services, and pods.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>Use CloudWatch Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Container Insights is available for Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), and Kubernetes platforms on Amazon EC2.</p><p>With Container Insights for EKS you can see the top contributors by memory or CPU, or the most recently active resources. This is available when you select any of the following dashboards in the drop-down box near the top of the page:</p><p>&nbsp; •&nbsp; ECS Services</p><p>&nbsp; •&nbsp; ECS Tasks</p><p>&nbsp; •&nbsp; EKS Namespaces</p><p>&nbsp; •&nbsp; EKS Services</p><p>&nbsp; •&nbsp; EKS Pods</p><p><strong>CORRECT: </strong>\"Configure Amazon CloudWatch Container Insights in the existing EKS cluster. View the metrics and logs in the CloudWatch console\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Run the Amazon CloudWatch agent in the existing EKS cluster. View the metrics and logs in the CloudWatch console\" is incorrect. Container Insights is the best way to view the required data.</p><p><strong>INCORRECT:</strong> \"Migrate the containers to Amazon ECS and enable Amazon CloudWatch Container Insights. View the metrics and logs in the CloudWatch console\" is incorrect. There is no need to migrate containers to ECS as EKS is supported for Container Insights.</p><p><strong>INCORRECT:</strong> \"Configure AWS X-Ray to enable tracing for the EKS microservices. Query the trace data using Amazon Elasticsearch\" is incorrect. X-Ray will not deliver the required statistics to a centralized dashboard.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContainerInsights.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContainerInsights.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 11208,
                        "content": "<p>Configure AWS X-Ray to enable tracing for the EKS microservices. Query the trace data using Amazon Elasticsearch.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11209,
                        "content": "<p>Run the Amazon CloudWatch agent in the existing EKS cluster. View the metrics and logs in the CloudWatch console.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11210,
                        "content": "<p>Configure Amazon CloudWatch Container Insights in the existing EKS cluster. View the metrics and logs in the CloudWatch console.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11211,
                        "content": "<p>Migrate the containers to Amazon ECS and enable Amazon CloudWatch Container Insights. View the metrics and logs in the CloudWatch console.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2684,
            "attributes": {
                "createdAt": "2023-09-09T20:40:02.680Z",
                "updatedAt": "2023-09-09T20:40:02.680Z",
                "content": "<p>A security officer requires that access to company financial reports is logged. The reports are stored in an Amazon S3 bucket. Additionally, any modifications to the log files must be detected.</p><p>Which actions should a solutions architect take?</p>",
                "answerExplanation": "<p>Amazon CloudTrail can be used to log activity on the reports. The key difference between the two answers that include CloudTrail is that one references data events whereas the other references management events.</p><p>Data events provide visibility into the resource operations performed on or within a resource. These are also known as data plane operations. Data events are often high-volume activities.</p><p>Example data events include:</p><p> • Amazon S3 object-level API activity (for example, GetObject, DeleteObject, and PutObject API operations).</p><p> • AWS Lambda function execution activity (the Invoke API).</p><p>Management events provide visibility into management operations that are performed on resources in your AWS account. These are also known as control plane operations. Example management events include:</p><p> • Configuring security (for example, IAM AttachRolePolicy API operations)</p><p> • Registering devices (for example, Amazon EC2 CreateDefaultVpc API operations).</p><p>Therefore, to log data about access to the S3 objects the solutions architect should log read and write data events.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-53-27-925a2b271e999a9b3ac4717b47ed69a3.jpg\"></p><p>Log file validation can also be enabled on the destination bucket:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-53-38-ab1c8f32e2aec2ce99d13ff85ff2a881.jpg\"></p><p><strong>CORRECT: </strong>\"Use AWS CloudTrail to create a new trail. Configure the trail to log read and write data events on the S3 bucket that houses the reports. Log these events to a new bucket, and enable log file validation\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail to create a new trail. Configure the trail to log read and write management events on the S3 bucket that houses the reports. Log these events to a new bucket, and enable log file validation\" is incorrect as data events should be logged rather than management events.</p><p><strong>INCORRECT:</strong> \"Use S3 server access logging on the bucket that houses the reports with the read and write data events and the log file validation options enabled\" is incorrect as server access logging does not have an option for choosing data events or log file validation.</p><p><strong>INCORRECT:</strong> \"Use S3 server access logging on the bucket that houses the reports with the read and write management events and log file validation options enabled\" is incorrect as server access logging does not have an option for choosing management events or log file validation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudtrail/\">https://digitalcloud.training/aws-cloudtrail/</a></p>",
                "options": [
                    {
                        "id": 11212,
                        "content": "<p>Use S3 server access logging on the bucket that houses the reports with the read and write data events and the log file validation options enabled</p>",
                        "isValid": false
                    },
                    {
                        "id": 11213,
                        "content": "<p>Use AWS CloudTrail to create a new trail. Configure the trail to log read and write data events on the S3 bucket that houses the reports. Log these events to a new bucket, and enable log file validation</p>",
                        "isValid": true
                    },
                    {
                        "id": 11214,
                        "content": "<p>Use S3 server access logging on the bucket that houses the reports with the read and write management events and log file validation options enabled</p>",
                        "isValid": false
                    },
                    {
                        "id": 11215,
                        "content": "<p>Use AWS CloudTrail to create a new trail. Configure the trail to log read and write management events on the S3 bucket that houses the reports. Log these events to a new bucket, and enable log file validation</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2685,
            "attributes": {
                "createdAt": "2023-09-09T20:40:02.779Z",
                "updatedAt": "2023-09-09T20:40:02.779Z",
                "content": "<p>A web app allows users to upload images for viewing online. The compute layer that processes the images is behind an Auto Scaling group. The processing layer should be decoupled from the front end and the ASG needs to dynamically adjust based on the number of images being uploaded.</p><p>How can this be achieved?</p>",
                "answerExplanation": "<p>The best solution is to use Amazon SQS to decouple the front end from the processing compute layer. To do this you can create a custom CloudWatch metric that measures the number of messages in the queue and then configure the ASG to scale using a target tracking policy that tracks a certain value.</p><p><strong>CORRECT: </strong>\"Create an Amazon SQS queue and custom CloudWatch metric to measure the number of messages in the queue. Configure the ASG to scale based on the number of messages in the queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS Topic to generate a notification each time a message is uploaded. Have the ASG scale based on the number of SNS messages\" is incorrect. The Amazon Simple Notification Service (SNS) is used for sending notifications using topics. Amazon SQS is a better solution for this scenario as it provides a decoupling mechanism where the actual images can be stored for processing. SNS does not provide somewhere for the images to be stored.</p><p><strong>INCORRECT:</strong> \"Create a target tracking policy that keeps the ASG at 70% CPU utilization\" is incorrect. Using a target tracking policy with the ASG that tracks CPU utilization does not allow scaling based on the number of images being uploaded.</p><p><strong>INCORRECT:</strong> \"Create a scheduled policy that scales the ASG at times of expected peak load\" is incorrect. Using a scheduled policy is less dynamic as though you may be able to predict usage patterns, it would be better to adjust dynamically based on actual usage.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 11216,
                        "content": "<p>Create a scheduled policy that scales the ASG at times of expected peak load</p>",
                        "isValid": false
                    },
                    {
                        "id": 11217,
                        "content": "<p>Create a target tracking policy that keeps the ASG at 70% CPU utilization</p>",
                        "isValid": false
                    },
                    {
                        "id": 11218,
                        "content": "<p>Create an Amazon SNS Topic to generate a notification each time a message is uploaded. Have the ASG scale based on the number of SNS messages</p>",
                        "isValid": false
                    },
                    {
                        "id": 11219,
                        "content": "<p>Create an Amazon SQS queue and custom CloudWatch metric to measure the number of messages in the queue. Configure the ASG to scale based on the number of messages in the queue</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2686,
            "attributes": {
                "createdAt": "2023-09-09T20:40:02.877Z",
                "updatedAt": "2023-09-09T20:40:02.877Z",
                "content": "<p>A high-performance file system is required for a financial modelling application. The data set will be stored on Amazon S3 and the storage solution must have seamless integration so objects can be accessed as files.</p><p>Which storage solution should be used?</p>",
                "answerExplanation": "<p>Amazon FSx for Lustre provides a high-performance file system optimized for fast processing of workloads such as machine learning, high performance computing (HPC), video processing, financial modeling, and electronic design automation (EDA). Amazon FSx works natively with Amazon S3, letting you transparently access your S3 objects as files on Amazon FSx to run analyses for hours to months.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-06-30_17-36-07-2dee8736108866a3aac56736557b7717.jpg\"><p><strong>CORRECT: </strong>\"Amazon FSx for Lustre\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon FSx for Windows File Server\" is incorrect. Amazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system so you can easily move your Windows-based applications that require shared file storage to AWS. This solution integrates with Windows file shares, not with Amazon S3.</p><p><strong>INCORRECT:</strong> \"Amazon Elastic File System (EFS)\" is incorrect. EFS and EBS are not good use cases for this solution. Neither storage solution is capable of presenting Amazon S3 objects as files to the application.</p><p><strong>INCORRECT:</strong> \"Amazon Elastic Block Store (EBS)\" is incorrect. EFS and EBS are not good use cases for this solution. Neither storage solution is capable of presenting Amazon S3 objects as files to the application.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/\">https://aws.amazon.com/fsx/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 11220,
                        "content": "<p>Amazon Elastic Block Store (EBS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11221,
                        "content": "<p>Amazon Elastic File System (EFS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11222,
                        "content": "<p>Amazon FSx for Lustre</p>",
                        "isValid": true
                    },
                    {
                        "id": 11223,
                        "content": "<p>Amazon FSx for Windows File Server</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2687,
            "attributes": {
                "createdAt": "2023-09-09T20:40:03.011Z",
                "updatedAt": "2023-09-09T20:40:03.011Z",
                "content": "<p>A company has several AWS accounts that are used by developers for development, testing and pre-production environments. The company has received large bills for Amazon EC2 instances that are underutilized. A Solutions Architect has been tasked with restricting the ability to launch large EC2 instances in all accounts.</p><p>How can the Solutions Architect meet this requirement with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts.</p><p>In this case the Solutions Architect can use an SCP to define a restriction that denies the launch of large EC2 instances. The SCP can be applied to all accounts, and this will ensure that even those users with permissions to launch EC2 instances will be restricted to smaller EC2 instance types.</p><p><strong>CORRECT: </strong>\"Create an organization in AWS Organizations that includes all accounts and create a service control policy (SCP) that denies the launch of large EC2 instances\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a service-linked role for Amazon EC2 and attach a policy the denies the launch of large EC2 instances\" is incorrect. You cannot create service-linked roles yourself; they are created by AWS with predefined policies.</p><p><strong>INCORRECT:</strong> \"Create a resource-based policy that denies the launch of large EC2 instances and attach it to Amazon EC2 in each account\" is incorrect. You cannot attach a resource-based policy to Amazon EC2.</p><p><strong>INCORRECT:</strong> \"Create an IAM role in each account that denies the launch of large EC2 instances. Grant the developers IAM group access to the role\" is incorrect. This is much less operationally efficient compared to using SCPs with AWS Organizations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
                "options": [
                    {
                        "id": 11224,
                        "content": "<p>Create a resource-based policy that denies the launch of large EC2 instances and attach it to Amazon EC2 in each account.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11225,
                        "content": "<p>Create an organization in AWS Organizations that includes all accounts and create a service control policy (SCP) that denies the launch of large EC2 instances.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11226,
                        "content": "<p>Create a service-linked role for Amazon EC2 and attach a policy the denies the launch of large EC2 instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11227,
                        "content": "<p>Create an IAM role in each account that denies the launch of large EC2 instances. Grant the developers IAM group access to the role.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2688,
            "attributes": {
                "createdAt": "2023-09-09T20:40:03.107Z",
                "updatedAt": "2023-09-09T20:40:03.107Z",
                "content": "<p>A company has experienced malicious traffic from some suspicious IP addresses. The security team discovered the requests are from different IP addresses under the same CIDR range.</p><p>What should a solutions architect recommend to the team?</p>",
                "answerExplanation": "<p>You can only create deny rules with network ACLs, it is not possible with security groups. Network ACLs process rules in order from the lowest numbered rules to the highest until they reach and allow or deny. The following table describes some of the differences between security groups and network ACLs:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_02-02-30-acd31eb94885375c9562ead5a41a3639.jpg\"></p><p>Therefore, the solutions architect should add a deny rule in the inbound table of the network ACL with a lower rule number than other rules.</p><p><strong>CORRECT: </strong>\"Add a deny rule in the inbound table of the network ACL with a lower rule number than other rules\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Add a deny rule in the outbound table of the network ACL with a lower rule number than other rules\" is incorrect as this will only block outbound traffic.</p><p><strong>INCORRECT:</strong> \"Add a rule in the inbound table of the security group to deny the traffic from that CIDR range\" is incorrect as you cannot create a deny rule with a security group.</p><p><strong>INCORRECT:</strong> \"Add a rule in the outbound table of the security group to deny the traffic from that CIDR range\" is incorrect as you cannot create a deny rule with a security group.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11228,
                        "content": "<p>Add a deny rule in the outbound table of the network ACL with a lower rule number than other rules</p>",
                        "isValid": false
                    },
                    {
                        "id": 11229,
                        "content": "<p>Add a rule in the inbound table of the security group to deny the traffic from that CIDR range</p>",
                        "isValid": false
                    },
                    {
                        "id": 11230,
                        "content": "<p>Add a rule in the outbound table of the security group to deny the traffic from that CIDR range</p>",
                        "isValid": false
                    },
                    {
                        "id": 11231,
                        "content": "<p>Add a deny rule in the inbound table of the network ACL with a lower rule number than other rules</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2689,
            "attributes": {
                "createdAt": "2023-09-09T20:40:03.211Z",
                "updatedAt": "2023-09-09T20:40:03.211Z",
                "content": "<p>A web application is running on a fleet of Amazon EC2 instances using an Auto Scaling Group. It is desired that the CPU usage in the fleet is kept at 40%.</p><p>How should scaling be configured?</p>",
                "answerExplanation": "<p>This is a perfect use case for a target tracking scaling policy. With target tracking scaling policies, you select a scaling metric and set a target value. In this case you can just set the target value to 40% average aggregate CPU utilization.</p><p><strong>CORRECT: </strong>\"Use a target tracking policy that keeps the average aggregate CPU utilization at 40%\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a simple scaling policy that launches instances when the average CPU hits 40%\" is incorrect. A simple scaling policy will add instances when 40% CPU utilization is reached, but it is not designed to maintain 40% CPU utilization across the group.</p><p><strong>INCORRECT:</strong> \"Use a step scaling policy that uses the PercentChangeInCapacity value to adjust the group size as required\" is incorrect. The step scaling policy makes scaling adjustments based on a number of factors. The PercentChangeInCapacity value increments or decrements the group size by a specified percentage. This does not relate to CPU utilization.</p><p><strong>INCORRECT:</strong> \"Use a custom CloudWatch alarm to monitor CPU usage and notify the ASG using Amazon SNS\" is incorrect. You do not need to create a custom Amazon CloudWatch alarm as the ASG can scale using a policy based on CPU utilization using standard configuration.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 11232,
                        "content": "<p>Use a step scaling policy that uses the PercentChangeInCapacity value to adjust the group size as required</p>",
                        "isValid": false
                    },
                    {
                        "id": 11233,
                        "content": "<p>Use a custom CloudWatch alarm to monitor CPU usage and notify the ASG using Amazon SNS</p>",
                        "isValid": false
                    },
                    {
                        "id": 11234,
                        "content": "<p>Use a target tracking policy that keeps the average aggregate CPU utilization at 40%</p>",
                        "isValid": true
                    },
                    {
                        "id": 11235,
                        "content": "<p>Use a simple scaling policy that launches instances when the average CPU hits 40%</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2690,
            "attributes": {
                "createdAt": "2023-09-09T20:40:03.318Z",
                "updatedAt": "2023-09-09T20:40:03.318Z",
                "content": "<p>An application runs on a fleet of Amazon EC2 instances in an Amazon EC2 Auto Scaling group behind an Elastic Load Balancer. The operations team has determined that the application performs best when the CPU utilization of the EC2 instances is at or near 60%.</p><p>Which scaling configuration should a Solutions Architect use to optimize the applications performance?</p>",
                "answerExplanation": "<p>With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value.</p><p>The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to changes in the metric due to a changing load pattern.</p><p>The following diagram shows a target tracking policy set to keep the CPU utilization of the EC2 instances at or close to 60%.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-24-12-01662811c941eb690ffe2f963ddac8c5.jpg\"></p><p><strong>CORRECT: </strong>\"Use a target tracking policy to dynamically scale the Auto Scaling group\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a simple scaling policy to dynamically scale the Auto Scaling group\" is incorrect. Simple scaling is not used for maintaining a target utilization. It is used for making simple adjustments up or down based on a threshold value.</p><p><strong>INCORRECT:</strong> \"Use a step scaling policy to dynamically scale the Auto Scaling group\" is incorrect. Step scaling is not used for maintaining a target utilization. It is used for making step adjustments that vary based on the size of the alarm breach.</p><p><strong>INCORRECT:</strong> \"Use a scheduled scaling policy to dynamically the Auto Scaling group\" is incorrect. Scheduled scaling is not used for maintaining a target utilization. It is used for scheduling changes at specific dates and times.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 11236,
                        "content": "<p>Use a scheduled scaling policy to dynamically the Auto Scaling group.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11237,
                        "content": "<p>Use a target tracking policy to dynamically scale the Auto Scaling group.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11238,
                        "content": "<p>Use a simple scaling policy to dynamically scale the Auto Scaling group.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11239,
                        "content": "<p>Use a step scaling policy to dynamically scale the Auto Scaling group.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2691,
            "attributes": {
                "createdAt": "2023-09-09T20:40:03.437Z",
                "updatedAt": "2023-09-09T20:40:03.437Z",
                "content": "<p>A company runs a business-critical application in the us-east-1 Region. The application uses an Amazon Aurora MySQL database cluster which is 2 TB in size. A Solutions Architect needs to determine a disaster recovery strategy for failover to the us-west-2 Region. The strategy must provide a recovery time objective (RTO) of 10 minutes and a recovery point objective (RPO) of 5 minutes.</p><p>Which strategy will meet these requirements?</p>",
                "answerExplanation": "<p>Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages</p><p>If your primary region suffers a performance degradation or outage, you can promote one of the secondary regions to take read/write responsibilities. An Aurora cluster can recover in less than 1 minute even in the event of a complete regional outage.</p><p>This provides your application with an effective Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute, providing a strong foundation for a global business continuity plan</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-22-11-2d4e0f2ec825327cf3bcbdbe7905c475.jpg\"></p><p><strong>CORRECT: </strong>\"Recreate the database as an Aurora global database with the primary DB cluster in us-east-1 and a secondary DB cluster in us-west-2. Use an Amazon EventBridge rule that invokes an AWS Lambda function to promote the DB cluster in us-west-2 when failure is detected\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a multi-Region Aurora MySQL DB cluster in us-east-1 and us-west-2. Use an Amazon Route 53 health check to monitor us-east-1 and fail over to us-west-2 upon failure\" is incorrect. You cannot create a multi-Region Aurora MySQL DB cluster. Options are to create MySQL Replicas (may meet the RTO objectives), or to use global database.</p><p><strong>INCORRECT:</strong> \"Create a cross-Region Aurora MySQL read replica in us-west-2 Region. Configure an Amazon EventBridge rule that invokes an AWS Lambda function that promotes the read replica in us-west-2 when failure is detected\" is incorrect. This may not meet the RTO objectives as large databases may well take more than 10 minutes to promote.</p><p><strong>INCORRECT:</strong> \"Recreate the database as an Aurora multi master cluster across the us-east-1 and us-west-2 Regions with multiple writers to allow read/write capabilities from all database instances\" is incorrect. Multi master only works within a Region it does not work across Regions.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
                "options": [
                    {
                        "id": 11240,
                        "content": "<p>Recreate the database as an Aurora global database with the primary DB cluster in us-east-1 and a secondary DB cluster in us-west-2. Use an Amazon EventBridge rule that invokes an AWS Lambda function to promote the DB cluster in us-west-2 when failure is detected.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11241,
                        "content": "<p>Create a cross-Region Aurora MySQL read replica in us-west-2 Region. Configure an Amazon EventBridge rule that invokes an AWS Lambda function that promotes the read replica in us-west-2 when failure is detected.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11242,
                        "content": "<p>Recreate the database as an Aurora multi master cluster across the us-east-1 and us-west-2 Regions with multiple writers to allow read/write capabilities from all database instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11243,
                        "content": "<p>Create a multi-Region Aurora MySQL DB cluster in us-east-1 and us-west-2. Use an Amazon Route 53 health check to monitor us-east-1 and fail over to us-west-2 upon failure.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2692,
            "attributes": {
                "createdAt": "2023-09-09T20:40:03.562Z",
                "updatedAt": "2023-09-09T20:40:03.562Z",
                "content": "<p>A company has some statistical data stored in an Amazon RDS database. The company wants to allow users to access this information using an API. A solutions architect must create a solution that allows sporadic access to the data, ranging from no requests to large bursts of traffic.</p><p>Which solution should the solutions architect suggest?</p>",
                "answerExplanation": "<p>This question is simply asking you to work out the best compute service for the stated requirements. The key requirements are that the compute service should be suitable for a workload that can range quite broadly in demand from no requests to large bursts of traffic.</p><p>AWS Lambda is an ideal solution as you pay only when requests are made and it can easily scale to accommodate the large bursts in traffic. Lambda works well with both API Gateway and Amazon RDS.</p><p><strong>CORRECT: </strong>\"Set up an Amazon API Gateway and use AWS Lambda functions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon API Gateway and use Amazon ECS\" is incorrect as Lambda is a better fit for this use case as the traffic patterns are highly dynamic.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon API Gateway and use AWS Elastic Beanstalk\" is incorrect as Lambda is a better fit for this use case as the traffic patterns are highly dynamic.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon API Gateway and use Amazon EC2 with Auto Scaling\" is incorrect as Lambda is a better fit for this use case as the traffic patterns are highly dynamic.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 11244,
                        "content": "<p>Set up an Amazon API Gateway and use Amazon ECS</p>",
                        "isValid": false
                    },
                    {
                        "id": 11245,
                        "content": "<p>Set up an Amazon API Gateway and use AWS Lambda functions</p>",
                        "isValid": true
                    },
                    {
                        "id": 11246,
                        "content": "<p>Set up an Amazon API Gateway and use Amazon EC2 with Auto Scaling</p>",
                        "isValid": false
                    },
                    {
                        "id": 11247,
                        "content": "<p>Set up an Amazon API Gateway and use AWS Elastic Beanstalk</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2693,
            "attributes": {
                "createdAt": "2023-09-09T20:40:03.681Z",
                "updatedAt": "2023-09-09T20:40:03.681Z",
                "content": "<p>A company hosts statistical data in an Amazon S3 bucket that users around the world download from their website using a URL that resolves to a domain name. The company needs to provide low latency access to users and plans to use Amazon Route 53 for hosting DNS records.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>This is a simple requirement for low latency access to the contents of an Amazon S3 bucket for global users. The best solution here is to use Amazon CloudFront to cache the content in Edge Locations around the world. This involves creating a web distribution that points to an S3 origin (the bucket) and then create an Alias record in Route 53 that resolves the applications URL to the CloudFront distribution endpoint.</p><p><strong>CORRECT: </strong>\"Create a web distribution on Amazon CloudFront pointing to an Amazon S3 origin. Create an ALIAS record in the Amazon Route 53 hosted zone that points to the CloudFront distribution, resolving to the application's URL domain name\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a web distribution on Amazon CloudFront pointing to an Amazon S3 origin. Create a CNAME record in a Route 53 hosted zone that points to the CloudFront distribution, resolving to the application's URL domain name\" is incorrect. An Alias record should be used to point to an Amazon CloudFront distribution.</p><p><strong>INCORRECT:</strong> \"Create an A record in Route 53, use a Route 53 traffic policy for the web application, and configure a geolocation rule. Configure health checks to check the health of the endpoint and route DNS queries to other endpoints if an endpoint is unhealthy\" is incorrect. There is only a single endpoint (the Amazon S3 bucket) so this strategy would not work. Much better to use CloudFront to cache in multiple locations.</p><p><strong>INCORRECT:</strong> \"Create an A record in Route 53, use a Route 53 traffic policy for the web application, and configure a geoproximity rule. Configure health checks to check the health of the endpoint and route DNS queries to other endpoints if an endpoint is unhealthy\" is incorrect. Again, there is only one endpoint so this strategy will simply not work.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
                "options": [
                    {
                        "id": 11248,
                        "content": "<p>Create a web distribution on Amazon CloudFront pointing to an Amazon S3 origin. Create a CNAME record in a Route 53 hosted zone that points to the CloudFront distribution, resolving to the application's URL domain name.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11249,
                        "content": "<p>Create a web distribution on Amazon CloudFront pointing to an Amazon S3 origin. Create an ALIAS record in the Amazon Route 53 hosted zone that points to the CloudFront distribution, resolving to the application's URL domain name.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11250,
                        "content": "<p>Create an A record in Route 53, use a Route 53 traffic policy for the web application, and configure a geoproximity rule. Configure health checks to check the health of the endpoint and route DNS queries to other endpoints if an endpoint is unhealthy.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11251,
                        "content": "<p>Create an A record in Route 53, use a Route 53 traffic policy for the web application, and configure a geolocation rule. Configure health checks to check the health of the endpoint and route DNS queries to other endpoints if an endpoint is unhealthy.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2694,
            "attributes": {
                "createdAt": "2023-09-09T20:40:03.784Z",
                "updatedAt": "2023-09-09T20:40:03.784Z",
                "content": "<p>A Solutions Architect working for a large financial institution is building an application to manage their customers financial information and their sensitive personal information. The Solutions Architect requires that the storage layer can store immutable data out of the box, with the ability to encrypt the data at rest and requires that the storage layer provides ACID properties. They also want to use a containerized solution to manage the compute layer.</p><p>Which solution will meet these requirements with the LEAST amount of operational overhead?</p>",
                "answerExplanation": "<p>The solution requires that the storage layer be immutable. This immutability can only be delivered by Amazon Quantum Ledger Database (QLDB), as Amazon QLDB has a built-in immutable journal that stores an accurate and sequenced entry of every data change. The journal is append-only, meaning that data can only be added to a journal, and it cannot be overwritten or deleted.<br>Secondly the compute layer needs to not only be containerized, and implemented with the least possible operational overhead. The option that best fits these requirements is Amazon ECS on AWS Fargate, as AWS Fargate is a Serverless, containerized deployment option.</p><p><strong>CORRECT: </strong>\"Set up an ECS cluster behind an Application Load Balancer on AWS Fargate. Use Amazon Quantum Ledger Database (QLDB) to manage the storage layer” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling Group with EC2 instances behind an Application Load Balancer. To manage the storage layer, use Amazon S3” is incorrect. EC2 instances are virtual machines, not a container product and Amazon S3 is an object storage service which does not act as an immutable storage layer.</p><p><strong>INCORRECT:</strong> \"Configure an ECS cluster on EC2 behind an Application Load Balancer within an Auto Scaling Group. Store data using Amazon DynamoDB” is incorrect. ECS on EC2 provides a higher level of operational overhead than using AWS Fargate, as Fargate is a Serverless service.</p><p><strong>INCORRECT:</strong> \"Create a cluster of ECS instances on AWS Fargate within an Auto Scaling Group behind an Application Load Balancer. To manage the storage layer, use Amazon S3” is incorrect. Although Fargate would be a suitable deployment option, Amazon S3 is not suitable for the storage layer as it is not immutable by default.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/qldb/features/\">https://aws.amazon.com/qldb/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-associate/aws-database-saa/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-associate/aws-database-saa/</a></p>",
                "options": [
                    {
                        "id": 11252,
                        "content": "<p>Set up an ECS cluster behind an Application Load Balancer on AWS Fargate. Use Amazon Quantum Ledger Database (QLDB) to manage the storage layer.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11253,
                        "content": "<p>Configure an ECS cluster on EC2 behind an Application Load Balancer within an Auto Scaling Group. Store data using Amazon DynamoDB.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11254,
                        "content": "<p>Create a cluster of ECS instances on AWS Fargate within an Auto Scaling Group behind an Application Load Balancer. To manage the storage layer, use Amazon S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11255,
                        "content": "<p>Create an Auto Scaling Group with EC2 instances behind an Application Load Balancer. To manage the storage layer, use Amazon S3.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2695,
            "attributes": {
                "createdAt": "2023-09-09T20:40:03.952Z",
                "updatedAt": "2023-09-09T20:40:03.952Z",
                "content": "<p>A company is deploying an application that produces data that must be processed in the order it is received. The company requires a solution for decoupling the event data from the processing layer. The solution must minimize operational overhead.</p><p>How can a Solutions Architect meet these requirements?</p>",
                "answerExplanation": "<p>Amazon SQS can be used to decouple this application using a FIFO queue. With a FIFO queue the order in which messages are sent and received is strictly preserved. You can configure an AWS Lambda function to poll the queue, or you can configure a Lambda function as a destination to asynchronously process messages from the queue.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-17-48-037e094b09540ca2dd15db19aec25f8a.jpg\"></p><p><strong>CORRECT: </strong>\"Create an Amazon SQS FIFO queue to decouple the application. Configure an AWS Lambda function to process messages from the queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SQS standard queue to decouple the application. Set up an AWS Lambda function to process messages from the queue independently\" is incorrect. A standard queue only offers best-effort ordering so it may not preserve the order of the data.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic to decouple the application. Configure an AWS Lambda function as a subscriber\" is incorrect. Amazon SQS is better for this use case as there are a sequence of events for which the order must be maintained, and these events can be queued for processing whereas SNS delivers them for immediate processing.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic to decouple the application. Configure an Amazon SQS queue as a subscriber\" is incorrect. As above an SQS queue would be preferred for queuing the messages.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-lambda-function-trigger.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-lambda-function-trigger.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 11256,
                        "content": "<p>Create an Amazon SNS topic to decouple the application. Configure an Amazon SQS queue as a subscriber.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11257,
                        "content": "<p>Create an Amazon SQS standard queue to decouple the application. Set up an AWS Lambda function to process messages from the queue independently.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11258,
                        "content": "<p>Create an Amazon SNS topic to decouple the application. Configure an AWS Lambda function as a subscriber.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11259,
                        "content": "<p>Create an Amazon SQS FIFO queue to decouple the application. Configure an AWS Lambda function to process messages from the queue.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2696,
            "attributes": {
                "createdAt": "2023-09-09T20:40:04.071Z",
                "updatedAt": "2023-09-09T20:40:04.071Z",
                "content": "<p>A Solutions Architect is designing a solution for an application that requires very low latency between the client and the backend. The application uses the UDP protocol, and the backend is hosted on Amazon EC2 instances. The solution must be highly available across multiple Regions and users around the world should be directed to the most appropriate Region based on performance.</p><p>How can the Solutions Architect meet these requirements?</p>",
                "answerExplanation": "<p>An NLB is ideal for latency-sensitive applications and can listen on UDP for incoming requests. As Elastic Load Balancers are region-specific it is necessary to have an NLB in each Region in front of the EC2 instances.</p><p>To direct traffic based on optimal performance, AWS Global Accelerator can be used. GA will ensure traffic is routed across the AWS global network to the most optimal endpoint based on performance.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-37-03-06957cedd24132e515e0704262ea64fe.jpg\"></p><p><strong>CORRECT: </strong>\"Deploy a Network Load Balancer in front of the EC2 instances in each Region. Use AWS Global Accelerator to route traffic to the most optimal Regional endpoint\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Deploy an Application Load Balancer in front of the EC2 instances in each Region. Use AWS WAF to direct traffic to the most optimal Regional endpoint\" is incorrect. You cannot use WAF to direct traffic to endpoints based on performance.</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon CloudFront distribution with a custom origin pointing to Amazon EC2 instances in multiple Regions\" is incorrect. CloudFront cannot listen on UDP, it is used for HTTP/HTTPS.</p><p><strong>INCORRECT:</strong> \"Deploy Amazon EC2 instances in multiple Regions. Create a multivalue answer routing record in Amazon Route 53 that includes all EC2 endpoints\" is incorrect. This configuration would not route incoming requests to the most optimal endpoint based on performance, it would provide multiple records in answers and traffic would be distributed across multiple Regions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-how-it-works.html\">https://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-how-it-works.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-global-accelerator/\">https://digitalcloud.training/aws-global-accelerator/</a></p>",
                "options": [
                    {
                        "id": 11260,
                        "content": "<p>Deploy an Application Load Balancer in front of the EC2 instances in each Region. Use AWS WAF to direct traffic to the most optimal Regional endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11261,
                        "content": "<p>Deploy a Network Load Balancer in front of the EC2 instances in each Region. Use AWS Global Accelerator to route traffic to the most optimal Regional endpoint.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11262,
                        "content": "<p>Deploy Amazon EC2 instances in multiple Regions. Create a multivalue answer routing record in Amazon Route 53 that includes all EC2 endpoints.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11263,
                        "content": "<p>Deploy an Amazon CloudFront distribution with a custom origin pointing to Amazon EC2 instances in multiple Regions.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2697,
            "attributes": {
                "createdAt": "2023-09-09T20:40:04.189Z",
                "updatedAt": "2023-09-09T20:40:04.189Z",
                "content": "<p>A Solutions Architect has been tasked with migrating 30 TB of data from an on-premises data center within 20 days. The company has an internet connection that is limited to 25 Mbps and the data transfer cannot use more than 50% of the connection speed.</p><p>What should a Solutions Architect do to meet these requirements?</p>",
                "answerExplanation": "<p>This is a simple case of working out roughly how long it will take to migrate the data using the 12.5 Mbps of bandwidth that is available for transfer and seeing which options are feasible. Transferring 30 TB of data across a 25 Mbps connection could take upwards of 200 days.</p><p>Therefore, we know that using the Internet connection will not meet the requirements and we can rule out any solution that will use the internet (all options except for Snowball). AWS Snowball is a physical device that is shipped to your office or data center. You can then load data onto it and ship it back to AWS where the data is uploaded to Amazon S3.</p><p>Snowball is the only solution that will achieve the data migration requirements within the 20-day period.</p><p><strong>CORRECT: </strong>\"Use AWS Snowball\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS DataSync\" is incorrect. This uses the internet which will not meet the 20-day deadline.</p><p><strong>INCORRECT:</strong> \"Use AWS Storage Gateway\" is incorrect. This uses the internet which will not meet the 20-day deadline.</p><p><strong>INCORRECT:</strong> \"Use a site-to-site VPN\" is incorrect. This uses the internet which will not meet the 20-day deadline.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11264,
                        "content": "<p>Use AWS Snowball.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11265,
                        "content": "<p>Use a site-to-site VPN.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11266,
                        "content": "<p>Use AWS Storage Gateway.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11267,
                        "content": "<p>Use AWS DataSync.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2698,
            "attributes": {
                "createdAt": "2023-09-09T20:40:04.292Z",
                "updatedAt": "2023-09-09T20:40:04.292Z",
                "content": "<p>An e-commerce web application needs a highly scalable key-value database. Which AWS database service should be used?</p>",
                "answerExplanation": "<p>A key-value database is a type of nonrelational (NoSQL) database that uses a simple key-value method to store data. A key-value database stores data as a collection of key-value pairs in which a key serves as a unique identifier. Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability – this is the best database for these requirements.</p><p><strong>CORRECT: </strong>\"Amazon DynamoDB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon RDS\" is incorrect. Amazon RDS is a relational (SQL) type of database, not a key-value / nonrelational database.</p><p><strong>INCORRECT:</strong> \"Amazon RedShift\" is incorrect. Amazon RedShift is a data warehouse service used for online analytics processing (OLAP) workloads.</p><p><strong>INCORRECT:</strong> \"Amazon ElastiCache\" is incorrect. Amazon ElastiCache is an in-memory caching database. This is not a nonrelational key-value database.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/nosql/key-value/\">https://aws.amazon.com/nosql/key-value/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 11268,
                        "content": "<p>Amazon ElastiCache</p>",
                        "isValid": false
                    },
                    {
                        "id": 11269,
                        "content": "<p>Amazon RDS</p>",
                        "isValid": false
                    },
                    {
                        "id": 11270,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 11271,
                        "content": "<p>Amazon RedShift</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2699,
            "attributes": {
                "createdAt": "2023-09-09T20:40:04.399Z",
                "updatedAt": "2023-09-09T20:40:04.399Z",
                "content": "<p>A company has created a duplicate of its environment in another AWS Region. The application is running in warm standby mode. There is an Application Load Balancer (ALB) in front of the application. Currently, failover is manual and requires updating a DNS alias record to point to the secondary ALB.</p><p>How can a solutions architect automate the failover process?</p>",
                "answerExplanation": "<p>You can use Route 53 to check the health of your resources and only return healthy resources in response to DNS queries. There are three types of DNS failover configurations:</p><p>1. Active-passive: Route 53 actively returns a primary resource. In case of failure, Route 53 returns the backup resource. Configured using a failover policy.</p><p>2. Active-active: Route 53 actively returns more than one resource. In case of failure, Route 53 fails back to the healthy resource. Configured using any routing policy besides failover.</p><p>3. Combination: Multiple routing policies (such as latency-based, weighted, etc.) are combined into a tree to configure more complex DNS failover.</p><p>In this case an alias already exists for the secondary ALB. Therefore, the solutions architect just needs to enable a failover configuration with an Amazon Route 53 health check.</p><p>The configuration would look something like this:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_02-03-25-80669d24056e76a9d92f8db887b73ccc.jpg\"></p><p><strong>CORRECT: </strong>\"Enable an Amazon Route 53 health check\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable an ALB health check\" is incorrect. The point of an ALB health check is to identify the health of targets (EC2 instances). It cannot redirect clients to another Region.</p><p><strong>INCORRECT:</strong> \"Create a CNAME record on Amazon Route 53 pointing to the ALB endpoint\" is incorrect as an Alias record already exists and is better for mapping to an ALB.</p><p><strong>INCORRECT:</strong> \"Create a latency based routing policy on Amazon Route 53\" is incorrect as this will only take into account latency, it is not used for failover.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/route-53-dns-health-checks/\">https://aws.amazon.com/premiumsupport/knowledge-center/route-53-dns-health-checks/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
                "options": [
                    {
                        "id": 11272,
                        "content": "<p>Enable an ALB health check</p>",
                        "isValid": false
                    },
                    {
                        "id": 11273,
                        "content": "<p>Create a CNAME record on Amazon Route 53 pointing to the ALB endpoint</p>",
                        "isValid": false
                    },
                    {
                        "id": 11274,
                        "content": "<p>Enable an Amazon Route 53 health check</p>",
                        "isValid": true
                    },
                    {
                        "id": 11275,
                        "content": "<p>Create a latency based routing policy on Amazon Route 53</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2700,
            "attributes": {
                "createdAt": "2023-09-09T20:40:04.505Z",
                "updatedAt": "2023-09-09T20:40:04.505Z",
                "content": "<p>A Solutions Architect needs a solution for hosting a website that will be used by a development team. The website contents will consist of HTML, CSS, client-side JavaScript, and images.</p><p>Which solution is MOST cost-effective?</p>",
                "answerExplanation": "<p>Amazon S3 can be used for hosting static websites and cannot be used for dynamic content. In this case the content is purely static with client-side code running. Therefore, an S3 static website will be the most cost-effective solution for hosting this website.</p><p><strong>CORRECT: </strong>\"Create an Amazon S3 bucket and host the website there\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Launch an Amazon EC2 instance and host the website there\" is incorrect. This will be more expensive as it uses an EC2 instances.</p><p><strong>INCORRECT:</strong> \"Use a Docker container to host the website on AWS Fargate\" is incorrect. A static website on S3 is sufficient for this use case and will be more cost-effective than Fargate.</p><p><strong>INCORRECT:</strong> \"Create an Application Load Balancer with an AWS Lambda target\" is incorrect. This is also a more expensive solution and unnecessary for this use case.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11276,
                        "content": "<p>Create an Amazon S3 bucket and host the website there.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11277,
                        "content": "<p>Use a Docker container to host the website on AWS Fargate.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11278,
                        "content": "<p>Launch an Amazon EC2 instance and host the website there.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11279,
                        "content": "<p>Create an Application Load Balancer with an AWS Lambda target.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2701,
            "attributes": {
                "createdAt": "2023-09-09T20:40:04.600Z",
                "updatedAt": "2023-09-09T20:40:04.600Z",
                "content": "<p>A production application runs on an Amazon RDS MySQL DB instance. A solutions architect is building a new reporting tool that will access the same data. The reporting tool must be highly available and not impact the performance of the production application.</p><p>How can this be achieved?</p>",
                "answerExplanation": "<p>You can create a read replica as a Multi-AZ DB instance. Amazon RDS creates a standby of your replica in another Availability Zone for failover support for the replica. Creating your read replica as a Multi-AZ DB instance is independent of whether the source database is a Multi-AZ DB instance.</p><p><strong>CORRECT: </strong>\"Create a Multi-AZ RDS Read Replica of the production RDS DB instance\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a Single-AZ RDS Read Replica of the production RDS DB instance. Create a second Single-AZ RDS Read Replica from the replica\" is incorrect. Read replicas are primarily used for horizontal scaling. The best solution for high availability is to use a Multi-AZ read replica.</p><p><strong>INCORRECT:</strong> \"Create a cross-region Multi-AZ deployment and create a read replica in the second region\" is incorrect as you cannot create a cross-region Multi-AZ deployment with RDS.</p><p><strong>INCORRECT:</strong> \"Use Amazon Data Lifecycle Manager to automatically create and manage snapshots\" is incorrect as using snapshots is not the best solution for high availability.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html#USER_MySQL.Replication.ReadReplicas.MultiAZ\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html#USER_MySQL.Replication.ReadReplicas.MultiAZ</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 11280,
                        "content": "<p>Create a Single-AZ RDS Read Replica of the production RDS DB instance. Create a second Single-AZ RDS Read Replica from the replica</p>",
                        "isValid": false
                    },
                    {
                        "id": 11281,
                        "content": "<p>Create a cross-region Multi-AZ deployment and create a read replica in the second region</p>",
                        "isValid": false
                    },
                    {
                        "id": 11282,
                        "content": "<p>Create a Multi-AZ RDS Read Replica of the production RDS DB instance</p>",
                        "isValid": true
                    },
                    {
                        "id": 11283,
                        "content": "<p>Use Amazon Data Lifecycle Manager to automatically create and manage snapshots</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2702,
            "attributes": {
                "createdAt": "2023-09-09T20:40:04.720Z",
                "updatedAt": "2023-09-09T20:40:04.720Z",
                "content": "<p>A solutions architect is designing a high performance computing (HPC) application using Amazon EC2 Linux instances. All EC2 instances need to communicate to each other with low latency and high throughput network performance.</p><p>Which EC2 solution BEST meets these requirements?</p>",
                "answerExplanation": "<p>When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use <em>placement groups</em> to influence the placement of a group of <em>interdependent</em> instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies:</p><p><em> </em>• <em>Cluster</em> – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.</p><p><em> </em>• <em>Partition</em> – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.</p><p><em> </em>• <em>Spread</em> – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.</p><p>For this scenario, a cluster placement group should be used as this is the best option for providing low-latency network performance for a HPC application.</p><p><strong>CORRECT: </strong>\"Launch the EC2 instances in a cluster placement group in one Availability Zone\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instances in a spread placement group in one Availability Zone\" is incorrect as the spread placement group is used to spread instances across distinct underlying hardware.</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instances in an Auto Scaling group in two Regions. Place a Network Load Balancer in front of the instances\" is incorrect as this does not achieve the stated requirement to provide low-latency, high throughput network performance between instances. Also, you cannot use an ELB across Regions.</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instances in an Auto Scaling group spanning multiple Availability Zones\" is incorrect as this does not reduce network latency or improve performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 11284,
                        "content": "<p>Launch the EC2 instances in a cluster placement group in one Availability Zone</p>",
                        "isValid": true
                    },
                    {
                        "id": 11285,
                        "content": "<p>Launch the EC2 instances in an Auto Scaling group in two Regions. Place a Network Load Balancer in front of the instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 11286,
                        "content": "<p>Launch the EC2 instances in an Auto Scaling group spanning multiple Availability Zones</p>",
                        "isValid": false
                    },
                    {
                        "id": 11287,
                        "content": "<p>Launch the EC2 instances in a spread placement group in one Availability Zone</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2703,
            "attributes": {
                "createdAt": "2023-09-09T20:40:04.852Z",
                "updatedAt": "2023-09-09T20:40:04.852Z",
                "content": "<p>An application generates unique files that are returned to customers after they submit requests to the application. The application uses an Amazon CloudFront distribution for sending the files to customers. The company wishes to reduce data transfer costs without modifying the application.</p><p>How can this be accomplished?</p>",
                "answerExplanation": "<p>Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency. Lambda@Edge runs code in response to events generated by the Amazon CloudFront.</p><p>You simply upload your code to AWS Lambda, and it takes care of everything required to run and scale your code with high availability at an AWS location closest to your end user.</p><p>In this case Lambda@Edge can compress the files before they are sent to users which will reduce data egress costs.</p><p><strong>CORRECT: </strong>\"Use Lambda@Edge to compress the files as they are sent to users\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable caching on the CloudFront distribution to store generated files at the edge\" is incorrect. The files are unique to each customer request, so caching does not help.</p><p><strong>INCORRECT:</strong> \"Use AWS Global Accelerator to reduce application latency for customers\" is incorrect. The aim is to reduce cost not latency and AWS GA uses the same network as CloudFront so does not assist with latency anyway.</p><p><strong>INCORRECT:</strong> \"Enable Amazon S3 Transfer Acceleration to reduce the transfer times\" is incorrect. This does not lower costs.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/lambda/edge/\">https://aws.amazon.com/lambda/edge/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 11288,
                        "content": "<p>Use AWS Global Accelerator to reduce application latency for customers.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11289,
                        "content": "<p>Use Lambda@Edge to compress the files as they are sent to users.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11290,
                        "content": "<p>Enable caching on the CloudFront distribution to store generated files at the edge.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11291,
                        "content": "<p>Enable Amazon S3 Transfer Acceleration to reduce the transfer times.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2704,
            "attributes": {
                "createdAt": "2023-09-09T20:40:04.987Z",
                "updatedAt": "2023-09-09T20:40:04.987Z",
                "content": "<p>A company’s staff connect from home office locations to administer applications using bastion hosts in a single AWS Region. The company requires a resilient bastion host architecture that requires minimal ongoing operational overhead.</p><p>How can a Solutions Architect best meet these requirements?</p>",
                "answerExplanation": "<p>Bastion hosts (aka “jump hosts”) are EC2 instances in public subnets that administrators and operations staff can connect to from the internet. From the bastion host they are then able to connect to other instances and applications within AWS by using internal routing within the VPC.</p><p>All answers use a Network Load Balancer which is acceptable for forwarding incoming connections to targets. The differences are in where the connections are forwarded to. The best option is to create an Auto Scaling group with EC2 instances in multiple Availability Zones. This creates a resilient architecture within a single AWS Region which is exactly what the question asks for.</p><p><strong>CORRECT: </strong>\"Create a Network Load Balancer backed by an Auto Scaling group with instances in multiple Availability Zones\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer backed by an Auto Scaling group with instances in multiple AWS Regions\" is incorrect. You cannot have instances in an ASG across multiple Regions and you can’t have an NLB distribute connections across multiple Regions.</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer backed by Reserved Instances in a cluster placement group\" is incorrect. A cluster placement group packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly coupled node-to-node communication that is typical of HPC applications.</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer backed by the existing servers in different Availability Zones\" is incorrect. An Auto Scaling group is required to maintain instances in different AZs for resilience.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 11292,
                        "content": "<p>Create a Network Load Balancer backed by an Auto Scaling group with instances in multiple AWS Regions.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11293,
                        "content": "<p>Create a Network Load Balancer backed by an Auto Scaling group with instances in multiple Availability Zones.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11294,
                        "content": "<p>Create a Network Load Balancer backed by Reserved Instances in a cluster placement group.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11295,
                        "content": "<p>Create a Network Load Balancer backed by the existing servers in different Availability Zones.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2705,
            "attributes": {
                "createdAt": "2023-09-09T20:40:05.117Z",
                "updatedAt": "2023-09-09T20:40:05.117Z",
                "content": "<p>A company has deployed an API in a VPC behind an internal Network Load Balancer (NLB). An application that consumes the API as a client is deployed in a second account in private subnets.</p><p>Which architectural configurations will allow the API to be consumed without using the public Internet? (Select TWO.)</p>",
                "answerExplanation": "<p>You can create your own application in your VPC and configure it as an AWS PrivateLink-powered service (referred to as an <em>endpoint service</em>). Other AWS principals can create a connection from their VPC to your endpoint service using an <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html\">interface VPC endpoint</a>. You are the <em>service provider</em>, and the AWS principals that create connections to your service are <em>service consumers</em>.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-18_13-30-56-6e12fed09086b40d3a54c6810873d3c2.JPG\"></p><p>This configuration is powered by AWS PrivateLink and clients do not need to use an internet gateway, NAT device, VPN connection or AWS Direct Connect connection, nor do they require public IP addresses.</p><p>Another option is to use a VPC Peering connection. A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account.</p><p><strong>CORRECT: </strong>\"Configure a VPC peering connection between the two VPCs. Access the API using the private address\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Configure a PrivateLink connection for the API into the client VPC. Access the API using the PrivateLink address\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Configure an AWS Direct Connect connection between the two VPCs. Access the API using the private address\" is incorrect. Direct Connect is used for connecting from on-premises data centers into AWS. It is not used from one VPC to another.</p><p><strong>INCORRECT:</strong> \"Configure a ClassicLink connection for the API into the client VPC. Access the API using the ClassicLink address\" is incorrect. ClassicLink allows you to link EC2-Classic instances to a VPC in your account, within the same Region. This is not relevant to sending data between two VPCs.</p><p><strong>INCORRECT:</strong> \"Configure an AWS Resource Access Manager connection between the two accounts. Access the API using the private address\" is incorrect. AWS RAM lets you share resources that are provisioned and managed in other AWS services. However, APIs are not shareable resources with AWS RAM.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/endpoint-service.html\">https://docs.aws.amazon.com/vpc/latest/userguide/endpoint-service.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\">https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11296,
                        "content": "<p>Configure a ClassicLink connection for the API into the client VPC. Access the API using the ClassicLink address</p>",
                        "isValid": false
                    },
                    {
                        "id": 11297,
                        "content": "<p>Configure an AWS Resource Access Manager connection between the two accounts. Access the API using the private address</p>",
                        "isValid": false
                    },
                    {
                        "id": 11298,
                        "content": "<p>Configure a VPC peering connection between the two VPCs. Access the API using the private address</p>",
                        "isValid": true
                    },
                    {
                        "id": 11299,
                        "content": "<p>Configure a PrivateLink connection for the API into the client VPC. Access the API using the PrivateLink address</p>",
                        "isValid": true
                    },
                    {
                        "id": 11300,
                        "content": "<p>Configure an AWS Direct Connect connection between the two VPCs. Access the API using the private address</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2706,
            "attributes": {
                "createdAt": "2023-09-09T20:40:05.271Z",
                "updatedAt": "2023-09-09T20:40:05.271Z",
                "content": "<p>A company is planning to migrate a large quantity of important data to Amazon S3. The data will be uploaded to a versioning enabled bucket in the us-west-1 Region. The solution needs to include replication of the data to another Region for disaster recovery purposes.</p><p>How should a solutions architect configure the replication?</p>",
                "answerExplanation": "<p>Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. You can copy objects between different AWS Regions or within the same Region. Both source and destination buckets must have versioning enabled.</p><p><strong>CORRECT: </strong>\"Create an additional S3 bucket with versioning in another Region and configure cross-Region replication\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an additional S3 bucket in another Region and configure cross-Region replication\" is incorrect as the destination bucket must also have versioning enabled.</p><p><strong>INCORRECT:</strong> \"Create an additional S3 bucket in another Region and configure cross-origin resource sharing (CORS)\" is incorrect as CORS is not related to replication.</p><p><strong>INCORRECT:</strong> \"Create an additional S3 bucket with versioning in another Region and configure cross-origin resource sharing (CORS)\" is incorrect as CORS is not related to replication.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11301,
                        "content": "<p>Create an additional S3 bucket in another Region and configure cross-origin resource sharing (CORS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11302,
                        "content": "<p>Create an additional S3 bucket with versioning in another Region and configure cross-Region replication</p>",
                        "isValid": true
                    },
                    {
                        "id": 11303,
                        "content": "<p>Create an additional S3 bucket with versioning in another Region and configure cross-origin resource sharing (CORS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11304,
                        "content": "<p>Create an additional S3 bucket in another Region and configure cross-Region replication</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2707,
            "attributes": {
                "createdAt": "2023-09-09T20:40:05.424Z",
                "updatedAt": "2023-09-09T20:40:05.424Z",
                "content": "<p>A company has created an application that stores sales performance data in an Amazon DynamoDB table. A web application is being created to display the data. A Solutions Architect must design the web application using managed services that require minimal operational maintenance.</p><p>Which architectures meet these requirements? (Select TWO.)</p>",
                "answerExplanation": "<p>There are two architectures here that fulfill the requirement to create a web application that displays the data from the DynamoDB table.</p><p>The first one is to use an API Gateway REST API that invokes an AWS Lambda function. A Lambda proxy integration can be used, and this will proxy the API requests to the Lambda function which processes the request and accesses the DynamoDB table.</p><p>The second option is to use an API Gateway REST API to directly access the sales performance data. In this case a proxy for the DynamoDB query API can be created using a method in the REST API.</p><p><strong>CORRECT: </strong>\"An Amazon API Gateway REST API invokes an AWS Lambda function. The Lambda function reads data from the DynamoDB table\" is a correct answer.</p><p><strong>CORRECT: </strong>\"An Amazon API Gateway REST API directly accesses the sales performance data in the DynamoDB table\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"An Amazon Route 53 hosted zone routes requests to an AWS Lambda endpoint to invoke a Lambda function that reads data from the DynamoDB table\" is incorrect. An Alias record could be created in a hosted zone but a hosted zone itself does not route to a Lambda endpoint. Using an Alias, it is possible to route to a VPC endpoint that uses a Lambda function however there would not be a web front end so a REST API would be preferable.</p><p><strong>INCORRECT:</strong> \"An Elastic Load Balancer forwards requests to a target group with the DynamoDB table configured as the target\" is incorrect. You cannot configure DynamoDB as a target in a target group.</p><p><strong>INCORRECT:</strong> \"An Elastic Load Balancer forwards requests to a target group of Amazon EC2 instances. The EC2 instances run an application that reads data from the DynamoDB table\" is incorrect. This would not offer low operational maintenance as you must manage the EC2 instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/using-amazon-api-gateway-as-a-proxy-for-dynamodb/\">https://aws.amazon.com/blogs/compute/using-amazon-api-gateway-as-a-proxy-for-dynamodb/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 11305,
                        "content": "<p>An Amazon API Gateway REST API invokes an AWS Lambda function. The Lambda function reads data from the DynamoDB table.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11306,
                        "content": "<p>An Amazon API Gateway REST API directly accesses the sales performance data in the DynamoDB table.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11307,
                        "content": "<p>An Elastic Load Balancer forwards requests to a target group of Amazon EC2 instances. The EC2 instances run an application that reads data from the DynamoDB table.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11308,
                        "content": "<p>An Elastic Load Balancer forwards requests to a target group with the DynamoDB table configured as the target.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11309,
                        "content": "<p>An Amazon Route 53 hosted zone routes requests to an AWS Lambda endpoint to invoke a Lambda function that reads data from the DynamoDB table.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2708,
            "attributes": {
                "createdAt": "2023-09-09T20:40:05.588Z",
                "updatedAt": "2023-09-09T20:40:05.588Z",
                "content": "<p>A company requires a high-performance file system that can be mounted on Amazon EC2 Windows instances and Amazon EC2 Linux instances. Applications running on the EC2 instances perform separate processing of the same files and the solution must provide a file system that can be mounted by all instances simultaneously.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>Amazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system so you can easily move your Windows-based applications that require shared file storage to AWS. You can easily connect Linux instances to the file system by installing the cifs-utils package. The Linux instances can then mount an SMB/CIFS file system.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-13-16-ac3ea6d1bde80de577cab4ac0352a586.jpg\"></p><p><strong>CORRECT: </strong>\"Use Amazon FSx for Windows File Server for the Windows instances and the Linux instances\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon FSx for Windows File Server for the Windows instances. Use Amazon Elastic File System (Amazon EFS) with Max I/O performance mode for the Linux instances\" is incorrect. This solution results in two separate file systems and a shared file system is required.</p><p><strong>INCORRECT:</strong> \"Use Amazon Elastic File System (Amazon EFS) with General Purpose performance mode for the Windows instances and the Linux instances\" is incorrect. You cannot use Amazon EFS for Windows instances as this is not supported.</p><p><strong>INCORRECT:</strong> \"Use Amazon FSx for Windows File Server for the Windows instances. Use Amazon FSx for Lustre for the Linux instances. Link both Amazon FSx file systems to the same Amazon S3 bucket\" is incorrect. Amazon FSx for Windows File Server does not use Amazon S3 buckets, so this is another solution that results in separate file systems.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/using-file-shares.html#map-shares-linux\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/using-file-shares.html#map-shares-linux</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 11310,
                        "content": "<p>Use Amazon FSx for Windows File Server for the Windows instances. Use Amazon Elastic File System (Amazon EFS) with Max I/O performance mode for the Linux instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11311,
                        "content": "<p>Use Amazon FSx for Windows File Server for the Windows instances and the Linux instances.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11312,
                        "content": "<p>Use Amazon FSx for Windows File Server for the Windows instances. Use Amazon FSx for Lustre for the Linux instances. Link both Amazon FSx file systems to the same Amazon S3 bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11313,
                        "content": "<p>Use Amazon Elastic File System (Amazon EFS) with General Purpose performance mode for the Windows instances and the Linux instances.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2709,
            "attributes": {
                "createdAt": "2023-09-09T20:40:05.718Z",
                "updatedAt": "2023-09-09T20:40:05.718Z",
                "content": "<p>A storage company creates and emails PDF statements to their customers at the end of each month. Customers must be able to download their statements from the company website for up to 30 days from when the statements were generated. When customers close their accounts, they are emailed a ZIP file that contains all the statements.</p><p>What is the MOST cost-effective storage solution for this situation?</p>",
                "answerExplanation": "<p>The most cost-effective option is to store the PDF files in S3 Standard for 30 days where they can be easily downloaded by customers. Then, transition the objects to Amazon S3 Glacier which will reduce the storage costs. When a customer closes their account, the objects can be retrieved from S3 Glacier and provided to the customer as a ZIP file.</p><p>Be cautious of subtle changes to the answer options in questions like these as you may see several variations of similar questions on the exam. Also, be aware of the supported transitions (below) and minimum storage durations.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-06-02_23-11-44-ac0d2161c900ffdfb1f1bd955d008285.jpg\"></p><p><strong>CORRECT: </strong>\"Store the statements using the Amazon S3 Standard storage class. Create a lifecycle policy to move the statements to Amazon S3 Glacier storage after 30 days\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Store the statements using the Amazon S3 Glacier storage class. Create a lifecycle policy to move the statements to Amazon S3 Glacier Deep Archive storage after 30 days\" is incorrect. Using Glacier will not allow customers to download their statements as the files would need to be restored. Also, the minimum storage duration before you can transition from Glacier is 90 days.</p><p><strong>INCORRECT:</strong> \"Store the statements using the Amazon S3 Standard storage class. Create a lifecycle policy to move the statements to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) storage after 30 days\" is incorrect. This would work but is not as cost-effective as using Glacier for the longer-term storage.</p><p><strong>INCORRECT:</strong> \"Store the statements using the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create a lifecycle policy to move the statements to Amazon S3 Intelligent Tiering storage after 30 days\" is incorrect. This would work but is not as cost-effective as using Glacier for the longer-term storage.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11314,
                        "content": "<p>Store the statements using the Amazon S3 Standard storage class. Create a lifecycle policy to move the statements to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) storage after 30 days.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11315,
                        "content": "<p>Store the statements using the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create a lifecycle policy to move the statements to Amazon S3 Intelligent Tiering storage after 30 days.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11316,
                        "content": "<p>Store the statements using the Amazon S3 Glacier storage class. Create a lifecycle policy to move the statements to Amazon S3 Glacier Deep Archive storage after 30 days.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11317,
                        "content": "<p>Store the statements using the Amazon S3 Standard storage class. Create a lifecycle policy to move the statements to Amazon S3 Glacier storage after 30 days.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2710,
            "attributes": {
                "createdAt": "2023-09-09T20:40:05.823Z",
                "updatedAt": "2023-09-09T20:40:05.823Z",
                "content": "<p>Several workloads are being run in one AWS region by a rapidly growing retail company. A solutions architect must create disaster recovery plans that include different AWS regions. In the DR Region, the company needs its database to be kept up to date with the lowest latency possible. Infrastructure in the DR Region must run at reduced capacity and be capable of handling traffic immediately.</p><p>Which solution will meet these requirements with the LOWEST possible recovery time objective (RTO)?</p>",
                "answerExplanation": "<p>Amazon Aurora global databases span multiple AWS Regions, enabling low latency global reads and providing fast recovery from the rare outage that might affect an entire AWS Region. An Aurora global database has a primary DB cluster in one Region, and up to five secondary DB clusters in different Regions.</p><p>With the warm standby strategy the application can handle traffic (at reduced capacity levels) immediately so this will reduce the RTO.</p><p><strong>CORRECT: </strong>\"Use an Amazon Aurora global database with a warm standby disaster recovery strategy” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an Amazon Aurora global database with a pilot light disaster recovery strategy” is incorrect. With a pilot light strategy the application in the DR site may not be able to accept traffic immediately, and may need intervention and time to get the resources running.</p><p><strong>INCORRECT:</strong> \"Use an Amazon RDS Multi-AZ DB instance with a pilot light disaster recovery strategy” is incorrect as an Amazon RDS Multi-AZ DB instance will not be suitable for a multi-region disaster recovery scenario.</p><p><strong>INCORRECT:</strong> \"Use an Amazon RDS Multi-AZ DB instance with a warm standby disaster recovery strategy” is incorrect as an Amazon RDS Multi-AZ DB instance will not be suitable for a multi-region disaster recovery scenario.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html\">https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html</a></p>",
                "options": [
                    {
                        "id": 11318,
                        "content": "<p>Use an Amazon Aurora global database with a warm standby disaster recovery strategy.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11319,
                        "content": "<p>Use an Amazon Aurora global database with a pilot light disaster recovery strategy.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11320,
                        "content": "<p>Use an Amazon RDS Multi-AZ DB instance with a pilot light disaster recovery strategy.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11321,
                        "content": "<p>Use an Amazon RDS Multi-AZ DB instance with a warm standby disaster recovery strategy.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2711,
            "attributes": {
                "createdAt": "2023-09-09T20:40:05.925Z",
                "updatedAt": "2023-09-09T20:40:05.925Z",
                "content": "<p>An eCommerce company has a very popular web application that receives a large amount of traffic. The application must store customer profile data and shopping cart information in a database. A Solutions Architect must design the database solution to support peak loads of several million requests per second and millisecond response times. Operational overhead must be minimized, and scaling should not cause downtime.</p><p>Which database solution should the Solutions Architect recommend?</p>",
                "answerExplanation": "<p>Amazon DynamoDB is a non-relational database that is managed for you. It can scale without downtime and with minimal operational overhead. DynamoDB can support the request rates and response times required by this solution and is often used in eCommerce solutions and for session state use cases.</p><p><strong>CORRECT: </strong>\"Amazon DynamoDB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon Aurora\" is incorrect. Aurora will require downtime to scale as you must change the instance type.</p><p><strong>INCORRECT:</strong> \"Amazon RDS\" is incorrect. RDS will require downtime to scale as you must change the instance type.</p><p><strong>INCORRECT:</strong> \"Amazon Athena\" is incorrect. Athena is used for querying data in a data lake, it is not used for storing this type of information in a transactional database model.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 11322,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 11323,
                        "content": "<p>Amazon Athena</p>",
                        "isValid": false
                    },
                    {
                        "id": 11324,
                        "content": "<p>Amazon RDS</p>",
                        "isValid": false
                    },
                    {
                        "id": 11325,
                        "content": "<p>Amazon Aurora</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2712,
            "attributes": {
                "createdAt": "2023-09-09T20:40:06.065Z",
                "updatedAt": "2023-09-09T20:40:06.065Z",
                "content": "<p>A company has two accounts in an AWS Organization. The accounts are: Prod1 and Prod2. An Amazon RDS database runs in the Prod1 account. Amazon EC2 instances run in the Prod2 account. The EC2 instances in the Prod2 account must access the RDS database.</p><p>How can a Solutions Architect meet this requirement MOST cost-effectively?</p>",
                "answerExplanation": "<p>VPC sharing makes use of the AWS Resource Access Manager (AWS RAM) service. It enables the sharing of VPCs across accounts. In this model, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations.</p><p>This scenario could be implemented with Prod1 account as the VPC owner and the Prod2 account as a VPC participant. This would allow the central control of the shared resource whilst enabling the EC2 instances in Prod2 to access the database.</p><p><strong>CORRECT: </strong>\"Set up VPC sharing with the Prod1 account as the owner and the Prod2 account as the participant to transfer the data\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function in the Prod1 account to transfer data to the Amazon EC2 instances in the Prod2 account\" is incorrect. The question is not asking for transfer of data; the EC2 instances need to access the database. Therefore, a method of connecting to a database endpoint is required.</p><p><strong>INCORRECT:</strong> \"Create a cross-Region Replica of the Amazon RD database in the Prod2 account. Point the EC2 instances to the Replica endpoint\" is incorrect. You cannot create cross-Region replicas of RDS databases in different accounts.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of the Amazon RDS database and share it with the Prod2 account. In the Prod2 account, restore the cluster using the shared snapshot\" is incorrect. This is less cost-effective as there is now a second RDS database running.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11326,
                        "content": "<p>Set up VPC sharing with the Prod1 account as the owner and the Prod2 account as the participant to transfer the data.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11327,
                        "content": "<p>Create a cross-Region Replica of the Amazon RD database in the Prod2 account. Point the EC2 instances to the Replica endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11328,
                        "content": "<p>Create an AWS Lambda function in the Prod1 account to transfer data to the Amazon EC2 instances in the Prod2 account.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11329,
                        "content": "<p>Take a snapshot of the Amazon RDS database and share it with the Prod2 account. In the Prod2 account, restore the cluster using the shared snapshot.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2713,
            "attributes": {
                "createdAt": "2023-09-09T20:40:06.189Z",
                "updatedAt": "2023-09-09T20:40:06.189Z",
                "content": "<p>A Solutions Architect has placed an Amazon CloudFront distribution in front of their web server, which is serving up a highly accessed website, serving content globally. The Solutions Architect needs to dynamically route the user to a new URL depending on where the user is accessing from, through running a particular script. This dynamic routing will happen on every request, and as a result requires the code to run at extremely low latency, and low cost.</p><p>What solution will best achieve this goal?</p>",
                "answerExplanation": "<p>With CloudFront Functions in Amazon CloudFront, you can write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. Your functions can manipulate the requests and responses that flow through CloudFront, perform basic authentication and authorization, generate HTTP responses at the edge, and more. CloudFront Functions is approximately 1/6th the cost of Lambda@Edge and is extremely low latency as the functions are run on the host in the edge location, instead of the running on a Lambda function elsewhere.</p><p><strong>CORRECT: </strong>\"At the Edge Location, run your code with CloudFront Functions” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Redirect traffic by running your code within a Lambda function using Lambda@Edge” is incorrect. Although you could achieve this using Lambda@Edge, the question states the need for the lowest latency possible, and comparatively the lowest latency option is CloudFront Functions.</p><p><strong>INCORRECT:</strong> \"Use Path Based Routing to route each user to the appropriate webpage behind an Application Load Balancer” is incorrect. This architecture does not account for the fact that custom code needs to be run to make this happen.</p><p><strong>INCORRECT:</strong> \"Use Route 53 Geo Proximity Routing to route users’ traffic to your resources based on their geographic location.'' is incorrect. This may work, however again it does not account for the fact that custom code needs to be run to make this happen.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 11330,
                        "content": "<p>At the Edge Location, run your code with CloudFront Functions.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11331,
                        "content": "<p>Redirect traffic by running your code within a Lambda function using Lambda@Edge.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11332,
                        "content": "<p>Use Route 53 Geo Proximity Routing to route users’ traffic to your resources based on their geographic location.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11333,
                        "content": "<p>Use Path Based Routing to route each user to the appropriate webpage behind an Application Load Balancer.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2714,
            "attributes": {
                "createdAt": "2023-09-09T20:40:06.517Z",
                "updatedAt": "2023-09-09T20:40:06.517Z",
                "content": "<p>A large customer services company is planning to build a highly scalable and durable application designed to aggregate data across their support communications, and extract sentiment on how successfully they are helping their customers. These communications are generated across chat, social media, emails and more. They need a solution which stores output from these communication channels, which then processes the text for sentiment analysis. The outputs must then be stored in a data warehouse for future use.</p><p><br></p><p>Which series of AWS services will provide the functionality the company is looking for?</p>",
                "answerExplanation": "<p>Amazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover valuable insights and connections in text.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-27_11-07-36-05ab4438c3d5079c3dad300424cbea3b.jpg\"><p>You could easily use Amazon Comprehend to detect customer sentiment and analyze customer interactions and automatically extract insights from customer surveys to improve your products. An S3 Data Lake also acts as an ideal data repository for Machine Learning data used by many different business units and applications.</p><p><strong>CORRECT: </strong>\"Use an Amazon S3 Data Lake as the original date store for the output from the support communications. Use Amazon Comprehend to process the text for sentiment analysis. Then store the outputs in Amazon RedShift” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an Amazon S3 Data Lake as the original date store for the output from the support communications. Use Amazon Textract to process the text for sentiment analysis. Then store the outputs in Amazon RedShift” is incorrect. Amazon Textract is a machine learning (ML) service that automatically extracts text, handwriting, and data from scanned documents, and does not output any sentiment.</p><p><strong>INCORRECT:</strong> \"Use DynamoDB as the original data store for the output from the support communications. Use Amazon Comprehend to process the text for sentiment analysis. Then store the outputs in Amazon RedShift” is incorrect. DynamoDB is not as suitable of a data repository for machine learning data like an Amazon S3 Data Lake would be.</p><p><strong>INCORRECT:</strong> \"Use DynamoDB as the original data store for the output from the support communications. Use Amazon Kendra to process the text for sentiment analysis. Then store the outputs in Amazon RedShift” is incorrect. DynamoDB is not as suitable of a data repository for machine learning data like an Amazon S3 Data Lake would be, and Amazon Kendra is a highly accurate intelligent search service powered by machine learning and does not work to understand sentiment.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/comprehend/\">https://aws.amazon.com/comprehend/</a></p>",
                "options": [
                    {
                        "id": 11334,
                        "content": "<p>Use an Amazon S3 Data Lake as the original date store for the output from the support communications. Use Amazon Textract to process the text for sentiment analysis. Then store the outputs in Amazon RedShift.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11335,
                        "content": "<p>Use DynamoDB as the original data store for the output from the support communications. Use Amazon Comprehend to process the text for sentiment analysis. Then store the outputs in Amazon RedShift.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11336,
                        "content": "<p>Use DynamoDB as the original data store for the output from the support communications. Use Amazon Kendra to process the text for sentiment analysis. Then store the outputs in Amazon RedShift.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11337,
                        "content": "<p>Use an Amazon S3 Data Lake as the original date store for the output from the support communications. Use Amazon Comprehend to process the text for sentiment analysis. Then store the outputs in Amazon RedShift.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2715,
            "attributes": {
                "createdAt": "2023-09-09T20:40:06.634Z",
                "updatedAt": "2023-09-09T20:40:06.634Z",
                "content": "<p>A company are finalizing their disaster recovery plan. A limited set of core services will be replicated to the DR site ready to seamlessly take over the in the event of a disaster. All other services will be switched off.</p><p>Which DR strategy is the company using?</p>",
                "answerExplanation": "<p>In this DR approach, you simply replicate part of your IT structure for a limited set of core services so that the AWS cloud environment seamlessly takes over in the event of a disaster.</p><p>A small part of your infrastructure is always running simultaneously syncing mutable data (as databases or documents), while other parts of your infrastructure are switched off and used only during testing.</p><p>Unlike a backup and recovery approach, you must ensure that your most critical core elements are already configured and running in AWS (the pilot light). When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core.</p><p><strong>CORRECT: </strong>\"Pilot light\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Backup and restore\" is incorrect. This is the lowest cost DR approach that simply entails creating online backups of all data and applications.</p><p><strong>INCORRECT:</strong> \"Warm standby\" is incorrect. The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud.</p><p><strong>INCORRECT:</strong> \"Multi-site\" is incorrect. A multi-site solution runs on AWS as well as on your existing on-site infrastructure in an active- active configuration.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/\">https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/</a></p>",
                "options": [
                    {
                        "id": 11338,
                        "content": "<p>Multi-site</p>",
                        "isValid": false
                    },
                    {
                        "id": 11339,
                        "content": "<p>Pilot light</p>",
                        "isValid": true
                    },
                    {
                        "id": 11340,
                        "content": "<p>Warm standby</p>",
                        "isValid": false
                    },
                    {
                        "id": 11341,
                        "content": "<p>Backup and restore</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2716,
            "attributes": {
                "createdAt": "2023-09-09T20:40:06.731Z",
                "updatedAt": "2023-09-09T20:40:06.731Z",
                "content": "<p>A large online retail company manages and runs an online e-commerce web application on AWS. This application serves hundreds of thousands of concurrent users during their peak operating hours, and as a result the company needs a highly scalable, near-real-time solution to share the order details with several other internal applications for order processing. Some additional processing to remove sensitive data also needs to occur before being stored in a document database for low-latency retrieval.</p><p>What should a solutions architect recommend to meet these requirements?</p>",
                "answerExplanation": "<p>Amazon Kinesis Data Streams is a serverless streaming data service that makes it easy to capture, process, and store data streams at any scale. When connected to Amazon DynamoDB</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-09-21_03-04-54-3709918b73b34aa11cd3cb909b599be3.jpg\"><p>as an output the customer is able to scale to hundreds of thousands of concurrent users during their peak operating hours. KDS stores records for 24 hours by default so other applications can read the data.</p><p><strong>CORRECT: </strong>\"Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the transaction data in Amazon DynamoDB. Set up a rule in DynamoDB to remove sensitive data from every transaction upon writing. Use DynamoDB Streams to share the transaction data with other applications” is incorrect. There’s no capability to write rules that remove sensitive data in DynamoDB.</p><p><strong>INCORRECT:</strong> \"Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3” is incorrect. Amazon Kinesis Data Firehose cannot load data directly to Amazon DynamoDB as it is not a supported destination.</p><p><strong>INCORRECT:</strong> \"Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DynamoDB. Other applications can consume transaction files stored in Amazon S3” is incorrect. This is highly inefficient and storing data in a DynamoDB table would be a much better solution.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 11342,
                        "content": "<p>Store the transaction data in Amazon DynamoDB. Set up a rule in DynamoDB to remove sensitive data from every transaction upon writing. Use DynamoDB Streams to share the transaction data with other applications.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11343,
                        "content": "<p>Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11344,
                        "content": "<p>Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11345,
                        "content": "<p>Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DynamoDB. Other applications can consume transaction files stored in Amazon S3.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2717,
            "attributes": {
                "createdAt": "2023-09-09T20:40:06.841Z",
                "updatedAt": "2023-09-09T20:40:06.841Z",
                "content": "<p>A company runs a streaming application on AWS that ingests data in near real-time and then processes the data. The data processing takes 30 minutes to complete. As the volume of data being ingested by the application has increased, high latency has occurred. A Solutions Architect needs to design a scalable and serverless solution to improve performance.</p><p>Which combination of steps should the Solutions Architect take? (Select TWO.)</p>",
                "answerExplanation": "<p>The application is a streaming application that ingests near real time data. This is a good fit for Amazon Kinesis Data Firehose which can ingest data and load it directly to a data store where it can be subsequently processed. We then need a serverless solution for processing the data. AWS Fargate is a serverless service that uses Amazon ECS for running Docker containers on AWS.</p><p>This solution will seamlessly scale for the data ingestion and processing. It is also fully serverless.</p><p><strong>CORRECT: </strong>\"Use Amazon Kinesis Data Firehose to ingest the data\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Use containers running on AWS Fargate to process the data\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda with AWS Step Functions to process the data\" is incorrect. Lambda has a maximum execution time of 900 seconds (15 minutes), so it is not possible to use AWS Lambda functions for processing the data.</p><p><strong>INCORRECT:</strong> \"Use Amazon Simple Queue Service (SQS) to ingest the data\" is incorrect. SQS does not ingest data, you must use an application process to place messages in the queue and then another process to consumer and process the messages from the queue.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 instances in a placement group to process the data\" is incorrect. A placement group with EC2 instances is not a serverless solution as you must manage the EC2 instances.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fargate/\">https://aws.amazon.com/fargate/</a></p><p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 11346,
                        "content": "<p>Use containers running on AWS Fargate to process the data.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11347,
                        "content": "<p>Use Amazon Kinesis Data Firehose to ingest the data.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11348,
                        "content": "<p>Use Amazon Simple Queue Service (SQS) to ingest the data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11349,
                        "content": "<p>Use AWS Lambda with AWS Step Functions to process the data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11350,
                        "content": "<p>Use Amazon EC2 instances in a placement group to process the data.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2718,
            "attributes": {
                "createdAt": "2023-09-09T20:40:06.928Z",
                "updatedAt": "2023-09-09T20:40:06.928Z",
                "content": "<p>A telecommunications company is looking to expand its 5G coverage nationwide, and as a result needs to provision and build their own private cellular network with the help of AWS.</p><p>Which solution does AWS provide to help with this?</p>",
                "answerExplanation": "<p>AWS Private 5G is a managed service that makes it easy to deploy, operate, and scale your own private cellular network, with all required hardware and software provided by AWS.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-27_10-53-42-4e3810611649053688d618d44c58bd33.jpg\"><p><strong>CORRECT: </strong>\"AWS Private 5G\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS Wavelength\" is incorrect. AWS Wavelength embeds AWS compute and storage services within 5G networks, providing mobile edge computing infrastructure for developing, deploying, and scaling ultra-low-latency applications.</p><p><strong>INCORRECT:</strong> \"AWS CloudHSM\" is incorrect. AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud and has nothing to do with 5G.</p><p><strong>INCORRECT:</strong> \"AWS Outposts\" is incorrect. AWS Outposts is a family of fully managed solutions delivering AWS infrastructure and services to virtually any on-premises or edge location for a truly consistent hybrid experience. It is not related to 5G.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/private5g/\">https://aws.amazon.com/private5g/</a></p>",
                "options": [
                    {
                        "id": 11351,
                        "content": "<p>AWS Wavelength</p>",
                        "isValid": false
                    },
                    {
                        "id": 11352,
                        "content": "<p>AWS Outposts</p>",
                        "isValid": false
                    },
                    {
                        "id": 11353,
                        "content": "<p>AWS CloudHSM</p>",
                        "isValid": false
                    },
                    {
                        "id": 11354,
                        "content": "<p>AWS Private 5G</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2719,
            "attributes": {
                "createdAt": "2023-09-09T20:40:07.098Z",
                "updatedAt": "2023-09-09T20:40:07.098Z",
                "content": "<p>A web application hosts static and dynamic content. The application runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The database tier runs on an Amazon Aurora database. A Solutions Architect needs to make the application more resilient to periodic increases in request rates.</p><p>Which architecture should the Solutions Architect implement? (Select TWO.)</p>",
                "answerExplanation": "<p>Using an Amazon CloudFront distribution can help reduce the impact of increases in requests rates as content is cached at edge locations and delivered via the AWS global network. For the database layer, Aurora Replicas will assist with serving read requests which reduces the load on the main database instance.</p><p><strong>CORRECT: </strong>\"Add Aurora Replicas\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Add an Amazon CloudFront distribution\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Add an AWS Transit Gateway\" is incorrect. This service offers no value in this situation.</p><p><strong>INCORRECT:</strong> \"Add an AWS Direct Connect link\" is incorrect. This would only improve network performance for users connecting from an on-premises location.</p><p><strong>INCORRECT:</strong> \"Add an AWS Global Accelerator\" is incorrect. CloudFront is better suited to this use case as it caches static content and improves performance for dynamic content.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 11355,
                        "content": "<p>Add an AWS Direct Connect link.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11356,
                        "content": "<p>Add an AWS Transit Gateway.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11357,
                        "content": "<p>Add an AWS Global Accelerator.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11358,
                        "content": "<p>Add an Amazon CloudFront distribution.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11359,
                        "content": "<p>Add Aurora Replicas.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2720,
            "attributes": {
                "createdAt": "2023-09-09T20:40:07.186Z",
                "updatedAt": "2023-09-09T20:40:07.186Z",
                "content": "<p>An application in a private subnet needs to query data in an Amazon DynamoDB table. Use of the DynamoDB public endpoints must be avoided. What is the most EFFICIENT and secure method of enabling access to the table?</p>",
                "answerExplanation": "<p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.</p><p>Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p><p>With a gateway endpoint you configure your route table to point to the endpoint. Amazon S3 and DynamoDB use gateway endpoints.</p><p>The table below helps you to understand the key differences between the two different types of VPC endpoint:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_19-56-38-b09d4455515fb71733786fc582abbdfa.png\"></p><p><strong>CORRECT: </strong>\"Create a gateway VPC endpoint and add an entry to the route table\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an interface VPC endpoint in the VPC with an Elastic Network Interface (ENI)\" is incorrect. This would be used for services that are supported by interface endpoints, not gateway endpoints.</p><p><strong>INCORRECT:</strong> \"Create a private Amazon DynamoDB endpoint and connect to it using an AWS VPN\" is incorrect. You cannot create an Amazon DynamoDB private endpoint and connect to it over VPN. Private endpoints are VPC endpoints and are connected to by instances in subnets via route table entries or via ENIs (depending on which service).</p><p><strong>INCORRECT:</strong> \"Create a software VPN between DynamoDB and the application in the private subnet\" is incorrect. You cannot create a software VPN between DynamoDB and an application.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11360,
                        "content": "<p>Create a software VPN between DynamoDB and the application in the private subnet</p>",
                        "isValid": false
                    },
                    {
                        "id": 11361,
                        "content": "<p>Create a private Amazon DynamoDB endpoint and connect to it using an AWS VPN</p>",
                        "isValid": false
                    },
                    {
                        "id": 11362,
                        "content": "<p>Create a gateway VPC endpoint and add an entry to the route table</p>",
                        "isValid": true
                    },
                    {
                        "id": 11363,
                        "content": "<p>Create an interface VPC endpoint in the VPC with an Elastic Network Interface (ENI)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2721,
            "attributes": {
                "createdAt": "2023-09-09T20:40:07.276Z",
                "updatedAt": "2023-09-09T20:40:07.276Z",
                "content": "<p>Three Amazon VPCs are used by a company in the same region. The company has two AWS Direct Connect connections to two separate company offices and wishes to share these with all three VPCs. A Solutions Architect has created an AWS Direct Connect gateway. How can the required connectivity be configured?</p>",
                "answerExplanation": "<p>You can manage a single connection for multiple VPCs or VPNs that are in the same Region by associating a Direct Connect gateway to a transit gateway. The solution involves the following components:</p><p>- A transit gateway that has VPC attachments.</p><p>- A Direct Connect gateway.</p><p>- An association between the Direct Connect gateway and the transit gateway.</p><p>- A transit virtual interface that is attached to the Direct Connect gateway.</p><p>The following diagram depicts this configuration:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-06-30_17-55-09-1a785f673ff4f8e0d3d249ee737a98d9.jpg\"><p><strong>CORRECT: </strong>\"Associate the Direct Connect gateway to a transit gateway\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Associate the Direct Connect gateway to a virtual private gateway in each VPC\" is incorrect. For VPCs in the same region a VPG is not necessary. A transit gateway can instead be configured.</p><p><strong>INCORRECT:</strong> \"Create a VPC peering connection between the VPCs and route entries for the Direct Connect Gateway\" is incorrect. You cannot add route entries for a Direct Connect gateway to each VPC and enable routing. Use a transit gateway instead.</p><p><strong>INCORRECT:</strong> \"Create a transit virtual interface between the Direct Connect gateway and each VPC\" is incorrect. The transit virtual interface is attached to the Direct Connect gateway on the connection side, not the VPC/transit gateway side.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-direct-connect/\">https://digitalcloud.training/aws-direct-connect/</a></p>",
                "options": [
                    {
                        "id": 11364,
                        "content": "<p>Associate the Direct Connect gateway to a transit gateway</p>",
                        "isValid": true
                    },
                    {
                        "id": 11365,
                        "content": "<p>Create a transit virtual interface between the Direct Connect gateway and each VPC</p>",
                        "isValid": false
                    },
                    {
                        "id": 11366,
                        "content": "<p>Associate the Direct Connect gateway to a virtual private gateway in each VPC</p>",
                        "isValid": false
                    },
                    {
                        "id": 11367,
                        "content": "<p>Create a VPC peering connection between the VPCs and route entries for the Direct Connect Gateway</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2722,
            "attributes": {
                "createdAt": "2023-09-09T20:40:07.348Z",
                "updatedAt": "2023-09-09T20:40:07.348Z",
                "content": "<p>A Solutions Architect for a large banking company is configuring access control within the organization for an Amazon S3 bucket containing thousands of financial records. There are 20 different teams which need to have access to this bucket, however they all need different permissions. These 20 teams correspond to 20 accounts within the banking company who are currently using AWS Organizations.</p><p><br></p><p>What is the simplest way to achieve this, whilst adhering to the principle of least privilege?</p>",
                "answerExplanation": "<p>Amazon S3 Access Points, a feature of S3, simplify data access for any AWS service or customer application that stores data in S3. With S3 Access Points, customers can create unique access control policies for each access point to easily control access to shared datasets. You can also control access point usage using AWS Organizations support for AWS SCPs.</p><p><strong>CORRECT: </strong>\"Use S3 Access points to administer different access policies to each team, and control access points using Service Control Policies within AWS Organizations” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a new AWS Organizations. Assign each team to a different Organizational Unit and apply to appropriate permissions granting access to the appropriate resources in the bucket” is incorrect. This would not only be incredibly time consuming but totally unnecessary as you can use the preexisting AWS Organizations and the Service Control policies to control access via S3 Access Points.</p><p><strong>INCORRECT:</strong> \"Copy the items from the bucket to create separate versions of each Separate the items in the bucket into new buckets. Administer Bucket policies to allow each account to access the appropriate bucket” is incorrect. This involves a lot of operational overhead and would be prone to significant error when administering the correct permissions to each account.</p><p><strong>INCORRECT:</strong> \"Create the S3 Bucket in an individual account. Configure an IAM Role for each user to enable cross account access for the S3 Bucket with a permissions policy to only access the appropriate items within the bucket” is incorrect. This is an unnecessary complexity as it would be much easier to provision separate policies per team using S3 Access Points.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/features/access-points/\">https://aws.amazon.com/s3/features/access-points/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11368,
                        "content": "<p>Copy the items from the bucket to create separate versions of each Separate the items in the bucket into new buckets. Administer Bucket policies to allow each account to access the appropriate bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11369,
                        "content": "<p>Create a new AWS Organizations. Assign each team to a different Organizational Unit and apply to appropriate permissions granting access to the appropriate resources in the bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11370,
                        "content": "<p>Create the S3 Bucket in an individual account. Configure an IAM Role for each user to enable cross account access for the S3 Bucket with a permissions policy to only access the appropriate items within the bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11371,
                        "content": "<p>Use S3 Access points to administer different access policies to each team, and control access points using Service Control Policies within AWS Organizations.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2723,
            "attributes": {
                "createdAt": "2023-09-09T20:40:07.456Z",
                "updatedAt": "2023-09-09T20:40:07.456Z",
                "content": "<p>A critical web application that runs on a fleet of Amazon EC2 Linux instances has experienced issues due to failing EC2 instances. The operations team have investigated and determined that insufficient swap space is a likely cause. The operations team require a method of monitoring the swap space on the EC2 instances.</p><p>What should a Solutions Architect recommend?</p>",
                "answerExplanation": "<p>The unified CloudWatch agent enables you to collect internal system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. The metrics that are collected include swap_free, swap_used, and swap_used_percent.</p><p><strong>CORRECT: </strong>\"Install and configure the unified CloudWatch agent on the EC2 instances. Monitor Swap Utilization metrics in CloudWatch\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a custom metric in Amazon CloudWatch that monitors Swap Usage. Monitor Swap Usage metrics in CloudWatch\" is incorrect. You cannot create a custom metric for swap utilization as this data is not collected without the CloudWatch agent.</p><p><strong>INCORRECT:</strong> \"Use EC2 metadata to collect information, then publish it to Amazon CloudWatch custom metrics. Monitor Swap Usage metrics in CloudWatch\" is incorrect. You cannot collect performance metrics with EC2 metadata.</p><p><strong>INCORRECT:</strong> \"Enable detailed monitoring in the EC2 console. Create an Amazon CloudWatch SwapUtilization custom metric and monitor the metric in CloudWatch\" is incorrect. Detailed monitoring changes the frequency of metric reporting but does not include the swap utilization data as you must install the CloudWatch agent to collect that info.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 11372,
                        "content": "<p>Enable detailed monitoring in the EC2 console. Create an Amazon CloudWatch SwapUtilization custom metric and monitor the metric in CloudWatch.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11373,
                        "content": "<p>Install and configure the unified CloudWatch agent on the EC2 instances. Monitor Swap Utilization metrics in CloudWatch.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11374,
                        "content": "<p>Use EC2 metadata to collect information, then publish it to Amazon CloudWatch custom metrics. Monitor Swap Usage metrics in CloudWatch.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11375,
                        "content": "<p>Create a custom metric in Amazon CloudWatch that monitors Swap Usage. Monitor Swap Usage metrics in CloudWatch.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2724,
            "attributes": {
                "createdAt": "2023-09-09T20:40:07.559Z",
                "updatedAt": "2023-09-09T20:40:07.559Z",
                "content": "<p>A company is using Amazon Aurora as the database for an online retail application. Data analysts run reports every fortnight that take a long time to process and cause performance degradation for the database. A Solutions Architect has reviewed performance metrics in Amazon CloudWatch and noticed that the ReadIOPS and CPUUtilization metrics are spiking when the reports run.</p><p>What is the MOST cost-effective solution to resolve the performance issues?</p>",
                "answerExplanation": "<p>You can issue queries to the Aurora Replicas to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster.</p><p>This solution is the most cost-effective method of scaling the database for reads for the regular reporting job. The reporting job will be run against the read endpoint and will not cause performance issues for the main database.</p><p><strong>CORRECT: </strong>\"Migrate the fortnightly reporting to an Aurora Replica\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Migrate the fortnightly reporting to Amazon EMR\" is incorrect. There is no need to load data into EMR to run reports, simply offloading to an Aurora Replica will suffice.</p><p><strong>INCORRECT:</strong> \"Migrate the Aurora database to a larger instance class\" is incorrect. This may be a more costly solution and may not be as effective as the long-running reporting job may just complete faster with more resources whilst still causing performance degradation.</p><p><strong>INCORRECT:</strong> \"Increase the Provisioned IOPS on the Aurora instance\" is incorrect. This will improve the storage throughput but not the CPU.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
                "options": [
                    {
                        "id": 11376,
                        "content": "<p>Migrate the fortnightly reporting to Amazon EMR.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11377,
                        "content": "<p>Migrate the Aurora database to a larger instance class.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11378,
                        "content": "<p>Migrate the fortnightly reporting to an Aurora Replica.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11379,
                        "content": "<p>Increase the Provisioned IOPS on the Aurora instance.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2725,
            "attributes": {
                "createdAt": "2023-09-09T20:40:07.679Z",
                "updatedAt": "2023-09-09T20:40:07.679Z",
                "content": "<p>A retail organization sends coupons out twice a week and this results in a predictable surge in sales traffic. The application runs on Amazon EC2 instances behind an Elastic Load Balancer. The organization is looking for ways lower costs while ensuring they meet the demands of their customers.</p><p>How can they achieve this goal?</p>",
                "answerExplanation": "<p>On-Demand Capacity Reservations enable you to reserve compute capacity for your Amazon EC2 instances in a specific Availability Zone for any duration. By creating Capacity Reservations, you ensure that you always have access to EC2 capacity when you need it, for as long as you need it. When used in combination with savings plans, you can also gain the advantages of cost reduction.</p><p><strong>CORRECT: </strong>\" Use capacity reservations with savings plans\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a mixture of spot instances and on demand instances\" is incorrect. You can mix spot and on-demand in an auto scaling group. However, there’s a risk the spot price may not be good, and this is a regular, predictable increase in traffic.</p><p><strong>INCORRECT:</strong> \"Increase the instance size of the existing EC2 instances\" is incorrect. This would add more cost all the time rather than catering for the temporary increases in traffic.</p><p><strong>INCORRECT:</strong> \"Purchase Amazon EC2 dedicated hosts\" is incorrect. This is not a way to save cost as dedicated hosts are much more expensive than shared hosts.</p><p><strong>References:</strong></p><p>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html#capacity-reservations-differences</p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 11380,
                        "content": "<p>Use capacity reservations with savings plans</p>",
                        "isValid": true
                    },
                    {
                        "id": 11381,
                        "content": "<p>Increase the instance size of the existing EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 11382,
                        "content": "<p>Use a mixture of spot instances and on demand instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 11383,
                        "content": "<p>Purchase Amazon EC2 dedicated hosts</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2726,
            "attributes": {
                "createdAt": "2023-09-09T20:40:07.796Z",
                "updatedAt": "2023-09-09T20:40:07.796Z",
                "content": "<p>A company runs a containerized application on a Kubernetes cluster in an on-premises data center. The application uses a MongoDB Database to store data. The application will be migrated to AWS, but no code changes or deployment method changes are possible at this time due to a constraint in time and resources. Operational efficiency is critical.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>The easiest way to lift this application out of the data center with minimal code changes is to use the Elastic Kubernetes Service (Amazon EKS) on Fargate for the compute tier and Amazon DocumentDB (with MongoDB compatibility) for data storage.</p><p><strong>CORRECT: </strong>\"Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and Amazon DocumentDB (with MongoDB compatibility) for data storage” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Elastic Container Service (Amazon ECS) with worker nodes on Amazon EC2 for compute, as well as and MongoDB on EC2 for data storage” is incorrect. Using Amazon ECS will take some application refactoring, so it involves code changes and is not operationally efficient.</p><p><strong>INCORRECT:</strong> \"Use Amazon Elastic Kubernetes Service (Amazon EKS) with worker nodes on Amazon EC2 for compute and Amazon DynamoDB for data storage” is incorrect. Using DynamoDB would take a refactoring of the application code and is not operationally efficient.</p><p><strong>INCORRECT:</strong> \"Use Amazon Elastic Container Service (Amazon ECS) with worker nodes on Amazon EC2 for compute, as well as and MongoDB on EC2 for data storage” Using Amazon ECS will take some application refactoring, so it is not operationally efficient.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/ecs/\">https://aws.amazon.com/ecs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-compute/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-compute/</a></p>",
                "options": [
                    {
                        "id": 11384,
                        "content": "<p>Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and Amazon DynamoDB for the data storage.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11385,
                        "content": "<p>Use Amazon Elastic Container Service (Amazon ECS) with worker nodes on Amazon EC2 for compute, as well as and MongoDB on EC2 for data storage.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11386,
                        "content": "<p>Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and Amazon DocumentDB (with MongoDB compatibility) for data storage.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11387,
                        "content": "<p>Use Amazon Elastic Kubernetes Service (Amazon EKS) with worker nodes on Amazon EC2 for compute and Amazon DynamoDB for data storage.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2727,
            "attributes": {
                "createdAt": "2023-09-09T20:40:07.889Z",
                "updatedAt": "2023-09-09T20:40:07.889Z",
                "content": "<p>A DevOps team uses an Amazon RDS MySQL database running for running resource-intensive tests each month. The instance has Performance Insights enabled and is only used once a month for up to 48 hours. As part of an effort to reduce AWS spend, the team wants to reduce the cost of running the tests without reducing the memory and compute attributes of the DB instance.</p><p>Which solution meets these requirements MOST cost-effectively?</p>",
                "answerExplanation": "<p>Taking a snapshot of the instance and storing the snapshot is the most cost-effective solution. When needed, a new database can be created from the snapshot. Performance Insights can be enabled on the new instance if needed. Note that the previous data from Performance Insights will not be associated with the new instance, however this was not a requirement.</p><p><strong>CORRECT: </strong>\"Create a snapshot of the database when the tests are completed. Terminate the DB instance. Create a new DB instance from the snapshot when required” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Stop the DB instance once all tests are completed. Start the DB instance again when required” is incorrect. You will be charged when your instance is stopped. When an instance is stopped you are charged for provisioned storage, manual snapshots, and automated backup storage within your specified retention window, but not for database instance hours. This is more costly compared to using snapshots.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group for the DB instance and reduce the desired capacity to 0 once the tests are completed” is incorrect. You cannot use Auto Scaling groups with Amazon RDS instances.</p><p><strong>INCORRECT:</strong> \"Modify the DB instance size to a smaller capacity instance when all the tests have been completed. Scale up again when required” is incorrect. This will reduce compute and memory capacity and will be more costly than taking a snapshot and terminating the DB.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 11388,
                        "content": "<p>Create a snapshot of the database when the tests are completed. Terminate the DB instance. Create a new DB instance from the snapshot when required.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11389,
                        "content": "<p>Stop the DB instance once all tests are completed. Start the DB instance again when required.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11390,
                        "content": "<p>Create an Auto Scaling group for the DB instance and reduce the desired capacity to 0 once the tests are completed.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11391,
                        "content": "<p>Modify the DB instance size to a smaller capacity instance when all the tests have been completed. Scale up again when required.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2728,
            "attributes": {
                "createdAt": "2023-09-09T20:40:07.974Z",
                "updatedAt": "2023-09-09T20:40:07.974Z",
                "content": "<p>To increase performance and redundancy for an application a company has decided to run multiple implementations in different AWS Regions behind network load balancers. The company currently advertise the application using two public IP addresses from separate /24 address ranges and would prefer not to change these. Users should be directed to the closest available application endpoint.</p><p>Which actions should a solutions architect take? (Select TWO.)</p>",
                "answerExplanation": "<p>AWS Global Accelerator uses static IP addresses as fixed entry points for your application. You can migrate up to two /24 IPv4 address ranges and choose which /32 IP addresses to use when you create your accelerator.</p><p>This solution ensures the company can continue using the same IP addresses and they are able to direct traffic to the application endpoint in the AWS Region closest to the end user. Traffic is sent over the AWS global network for consistent performance.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-06-30_17-53-24-bb370bf875b707fca85b1a16e03905ad.png\"><p><strong>CORRECT: </strong>\"Create an AWS Global Accelerator and attach endpoints in each AWS Region\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Migrate both public IP addresses to the AWS Global Accelerator\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Route 53 geolocation based routing policy\" is incorrect. With this solution new IP addresses will be required as there will be application endpoints in different regions.</p><p><strong>INCORRECT:</strong> \"Assign new static anycast IP addresses and modify any existing pointers\" is incorrect. This is unnecessary as you can bring your own IP addresses to AWS Global Accelerator and this is preferred in this scenario.</p><p><strong>INCORRECT:</strong> \"Create PTR records to map existing public IP addresses to an Alias\" is incorrect. This is not a workable solution for mapping existing IP addresses to an Amazon Route 53 Alias.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/global-accelerator/features/\">https://aws.amazon.com/global-accelerator/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-global-accelerator/\">https://digitalcloud.training/aws-global-accelerator/</a></p>",
                "options": [
                    {
                        "id": 11392,
                        "content": "<p>Migrate both public IP addresses to the AWS Global Accelerator</p>",
                        "isValid": true
                    },
                    {
                        "id": 11393,
                        "content": "<p>Create an AWS Global Accelerator and attach endpoints in each AWS Region</p>",
                        "isValid": true
                    },
                    {
                        "id": 11394,
                        "content": "<p>Create an Amazon Route 53 geolocation based routing policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 11395,
                        "content": "<p>Create PTR records to map existing public IP addresses to an Alias</p>",
                        "isValid": false
                    },
                    {
                        "id": 11396,
                        "content": "<p>Assign new static anycast IP addresses and modify any existing pointers</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2729,
            "attributes": {
                "createdAt": "2023-09-09T20:40:08.051Z",
                "updatedAt": "2023-09-09T20:40:08.051Z",
                "content": "<p>A Solutions Architect is designing an application that consists of AWS Lambda and Amazon RDS Aurora MySQL. The Lambda function must use database credentials to authenticate to MySQL and security policy mandates that these credentials must not be stored in the function code.</p><p>How can the Solutions Architect securely store the database credentials and make them available to the function?</p>",
                "answerExplanation": "<p>In this case the scenario requires that credentials are used for authenticating to MySQL. The credentials need to be securely stored outside of the function code. Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management.</p><p>You can easily reference the parameters from services including AWS Lambda as depicted in the diagram below:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_19-54-15-28bb13e5c530a63a9405297b225a7cac.png\"><p><strong>CORRECT: </strong>\"Store the credentials in Systems Manager Parameter Store and update the function code and execution role\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS Key Management Service and use environment variables in the function code pointing to KMS\" is incorrect. You cannot store credentials in KMS, it is used for creating and managing encryption keys</p><p><strong>INCORRECT:</strong> \"Use the AWSAuthenticationPlugin and associate an IAM user account in the MySQL database\" is incorrect. This is a great way to securely authenticate to RDS using IAM users or roles. However, in this case the scenario requires database credentials to be used by the function.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy and store the credentials in the policy. Attach the policy to the Lambda function execution role\" is incorrect. You cannot store credentials in IAM policies.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p>",
                "options": [
                    {
                        "id": 11397,
                        "content": "<p>Use the AWSAuthenticationPlugin and associate an IAM user account in the MySQL database</p>",
                        "isValid": false
                    },
                    {
                        "id": 11398,
                        "content": "<p>Create an IAM policy and store the credentials in the policy. Attach the policy to the Lambda function execution role</p>",
                        "isValid": false
                    },
                    {
                        "id": 11399,
                        "content": "<p>Store the credentials in AWS Key Management Service and use environment variables in the function code pointing to KMS</p>",
                        "isValid": false
                    },
                    {
                        "id": 11400,
                        "content": "<p>Store the credentials in Systems Manager Parameter Store and update the function code and execution role</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2730,
            "attributes": {
                "createdAt": "2023-09-09T20:40:08.121Z",
                "updatedAt": "2023-09-09T20:40:08.121Z",
                "content": "<p>An application is deployed using Amazon EC2 instances behind an Application Load Balancer running in an Auto Scaling group. The EC2 instances connect to an Amazon RDS database. When running performance testing on the application latency was experienced when performing queries on the database. The Amazon CloudWatch metrics for the EC2 instances do not show any performance issues.</p><p>How can a Solutions Architect resolve the application latency issues?</p>",
                "answerExplanation": "<p>The latency is most likely due to the RDS database having insufficient resources to handle the load. This can be resolved by deploying a read replica and directing queries to the replica endpoint. This offloads the performance hit of the queries from the master database which will improve overall performance and reduce the latency associated with database queries.</p><p><strong>CORRECT: </strong>\"Add read replicas for the RDS database and direct read traffic to the replicas\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Replace the EC2 instances with AWS Lambda functions\" is incorrect. If the latency is being caused by the database layer, then this will not resolve the issues.</p><p><strong>INCORRECT:</strong> \"Replace the Application Load Balancer with a Network Load Balancer\" is incorrect. If the latency is being caused by the database layer, then this will not resolve the issues.</p><p><strong>INCORRECT:</strong> \"Enable Multi-AZ for the RDS database and direct read traffic to the standby\" is incorrect. You cannot read from the standby in an Amazon RDS database cluster (you can with Aurora though).</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 11401,
                        "content": "<p>Replace the Application Load Balancer with a Network Load Balancer.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11402,
                        "content": "<p>Replace the EC2 instances with AWS Lambda functions.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11403,
                        "content": "<p>Add read replicas for the RDS database and direct read traffic to the replicas.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11404,
                        "content": "<p>Enable Multi-AZ for the RDS database and direct read traffic to the standby.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2731,
            "attributes": {
                "createdAt": "2023-09-09T20:40:08.208Z",
                "updatedAt": "2023-09-09T20:40:08.208Z",
                "content": "<p>An application that runs a computational fluid dynamics workload uses a tightly-coupled HPC architecture that uses the MPI protocol and runs across many nodes. A service-managed deployment is required to minimize operational overhead.</p><p>Which deployment option is MOST suitable for provisioning and managing the resources required for this use case?</p>",
                "answerExplanation": "<p>AWS Batch Multi-node parallel jobs enable you to run single jobs that span multiple Amazon EC2 instances. With AWS Batch multi-node parallel jobs, you can run large-scale, tightly coupled, high performance computing applications and distributed GPU model training without the need to launch, configure, and manage Amazon EC2 resources directly.</p><p>An AWS Batch multi-node parallel job is compatible with any framework that supports IP-based, internode communication, such as Apache MXNet, TensorFlow, Caffe2, or Message Passing Interface (MPI).</p><p>This is the most efficient approach to deploy the resources required and supports the application requirements most effectively.</p><p><strong>CORRECT: </strong>\"Use AWS Batch to deploy a multi-node parallel job\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 Auto Scaling to deploy instances in multiple subnets \" is incorrect. This is not the best solution for a tightly-coupled HPC workload with specific requirements such as MPI support.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to deploy a Cluster Placement Group on EC2\" is incorrect. This would deploy a cluster placement group but not manage it. AWS Batch is a better fit for large scale workloads such as this.</p><p><strong>INCORRECT:</strong> \"Use AWS Elastic Beanstalk to provision and manage the EC2 instances\" is incorrect. You can certainly provision and manage EC2 instances with Elastic Beanstalk but this scenario is for a specific workload that requires MPI support and managing a HPC deployment across a large number of nodes. AWS Batch is more suitable.</p><p><strong>References:</strong></p><p><a href=\"https://d1.awsstatic.com/whitepapers/architecture/AWS-HPC-Lens.pdf\">https://d1.awsstatic.com/whitepapers/architecture/AWS-HPC-Lens.pdf</a></p><p><a href=\"https://docs.aws.amazon.com/batch/latest/userguide/multi-node-parallel-jobs.html\">https://docs.aws.amazon.com/batch/latest/userguide/multi-node-parallel-jobs.html</a></p>",
                "options": [
                    {
                        "id": 11405,
                        "content": "<p>Use AWS Batch to deploy a multi-node parallel job</p>",
                        "isValid": true
                    },
                    {
                        "id": 11406,
                        "content": "<p>Use AWS CloudFormation to deploy a Cluster Placement Group on EC2</p>",
                        "isValid": false
                    },
                    {
                        "id": 11407,
                        "content": "<p>Use Amazon EC2 Auto Scaling to deploy instances in multiple subnets</p>",
                        "isValid": false
                    },
                    {
                        "id": 11408,
                        "content": "<p>Use AWS Elastic Beanstalk to provision and manage the EC2 instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2732,
            "attributes": {
                "createdAt": "2023-09-09T20:40:08.291Z",
                "updatedAt": "2023-09-09T20:40:08.291Z",
                "content": "<p>An application runs across a fleet of Amazon EC2 instances and uses a shared file system hosted on Amazon EFS. The file system is used for storing many files that are generated by the application. The files are only accessed for the first few days after creation but must be retained.</p><p>How can a Solutions Architect optimize storage costs for the application?</p>",
                "answerExplanation": "<p>The solution uses Amazon EFS, and the files are only accessed for a few days. To reduce storage costs the Solutions Architect can configure the AFTER_7_DAYS lifecycle policy to transition the files to the IA storage class 7 days after the files are last accessed.</p><p>You define when Amazon EFS transitions files an IA storage class by setting a lifecycle policy. A file system has one lifecycle policy that applies to the entire file system. If a file is not accessed for the period of time defined by the lifecycle policy that you choose, Amazon EFS transitions the file to the IA storage class that is applicable to your file system.</p><p><strong>CORRECT: </strong>\"Configure a lifecycle policy to move the files to the EFS Infrequent Access (IA) storage class after 7 days\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement AWS Storage Gateway and transition files to Amazon S3 after 7 days\" is incorrect. Storage Gateway is used for using cloud storage from on-premises systems. It is not used for transitioning data from EFS.</p><p><strong>INCORRECT:</strong> \"Move the files to an instance store on each Amazon EC2 instance after 7 days\" is incorrect. This would put the files at risk of loss as instance stores are ephemeral.</p><p><strong>INCORRECT:</strong> \"Configure a lifecycle policy to move the files to the S3 Standard-IA storage class after 7 days\" is incorrect. You cannot create a lifecycle policy in EFS that transitions files to Amazon S3.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/lifecycle-management-efs.html\">https://docs.aws.amazon.com/efs/latest/ug/lifecycle-management-efs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
                "options": [
                    {
                        "id": 11409,
                        "content": "<p>Move the files to an instance store on each Amazon EC2 instance after 7 days.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11410,
                        "content": "<p>Configure a lifecycle policy to move the files to the S3 Standard-IA storage class after 7 days.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11411,
                        "content": "<p>Implement AWS Storage Gateway and transition files to Amazon S3 after 7 days.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11412,
                        "content": "<p>Configure a lifecycle policy to move the files to the EFS Infrequent Access (IA) storage class after 7 days.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2733,
            "attributes": {
                "createdAt": "2023-09-09T20:40:08.438Z",
                "updatedAt": "2023-09-09T20:40:08.438Z",
                "content": "<p>An Amazon S3 bucket is going to be used by a company to store sensitive data. A Solutions Architect needs to ensure that all objects uploaded to an Amazon S3 bucket are encrypted. How can this be achieved?</p>",
                "answerExplanation": "<p>To encrypt an object at the time of upload, you need to add a header called x-amz-server-side-encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS.</p><p>To enforce object encryption, create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption header. There are two possible values for the x-amz-server-side-encryption header: AES256, which tells S3 to use S3-managed keys, and aws:kms, which tells S3 to use AWS KMS–managed keys.</p><p>The example policy below denies Put requests that do not have the correct encryption header set:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-06-02_22-55-40-e722cfe635c4da8e594c03abb6d181b5.jpg\"></p><p><strong>CORRECT: </strong>\"Create a bucket policy that denies Put requests that do not have an x-amz-server-side-encryption header set\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a bucket policy that denies Put requests that do not have an s3:x-amz-acl header set\" is incorrect. This header is not for encryption.</p><p><strong>INCORRECT:</strong> \"Create a bucket policy that denies Put requests that do not have an s3:x-amz-acl header set to private\" is incorrect. This header is not for encryption.</p><p><strong>INCORRECT:</strong> \"Create a bucket policy that denies Put requests that do not have an aws:Secure Transport header set to true\" is incorrect. This header is used for SSL/TLS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/\">https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11413,
                        "content": "<p>Create a bucket policy that denies Put requests that do not have an s3:x-amz-acl header set.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11414,
                        "content": "<p>Create a bucket policy that denies Put requests that do not have an x-amz-server-side-encryption header set.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11415,
                        "content": "<p>Create a bucket policy that denies Put requests that do not have an aws:Secure Transport header set to true.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11416,
                        "content": "<p>Create a bucket policy that denies Put requests that do not have an s3:x-amz-acl header set to private.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2734,
            "attributes": {
                "createdAt": "2023-09-09T20:40:08.558Z",
                "updatedAt": "2023-09-09T20:40:08.558Z",
                "content": "<p>A highly elastic application consists of three tiers. The application tier runs in an Auto Scaling group and processes data and writes it to an Amazon RDS MySQL database. The Solutions Architect wants to restrict access to the database tier to only accept traffic from the instances in the application tier. However, instances in the application tier are being constantly launched and terminated.</p><p>How can the Solutions Architect configure secure access to the database tier?</p>",
                "answerExplanation": "<p>The best option is to configure the database security group to only allow traffic that originates from the application security group. You can also define the destination port as the database port. This setup will allow any instance that is launched and attached to this security group to connect to the database.</p><p><strong>CORRECT: </strong>\"Configure the database security group to allow traffic only from the application security group\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure the database security group to allow traffic only from port 3306\" is incorrect. Port 3306 for MySQL should be the destination port, not the source.</p><p><strong>INCORRECT:</strong> \"Configure a Network ACL on the database subnet to deny all traffic to ports other than 3306\" is incorrect. This does not restrict access specifically to the application instances.</p><p><strong>INCORRECT:</strong> \"Configure a Network ACL on the database subnet to allow all traffic from the application subnet\" is incorrect. This does not restrict access specifically to the application instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11417,
                        "content": "<p>Configure a Network ACL on the database subnet to allow all traffic from the application subnet</p>",
                        "isValid": false
                    },
                    {
                        "id": 11418,
                        "content": "<p>Configure a Network ACL on the database subnet to deny all traffic to ports other than 3306</p>",
                        "isValid": false
                    },
                    {
                        "id": 11419,
                        "content": "<p>Configure the database security group to allow traffic only from the application security group</p>",
                        "isValid": true
                    },
                    {
                        "id": 11420,
                        "content": "<p>Configure the database security group to allow traffic only from port 3306</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2735,
            "attributes": {
                "createdAt": "2023-09-09T20:40:08.683Z",
                "updatedAt": "2023-09-09T20:40:08.683Z",
                "content": "<p>A large financial services company currently has an SMB file server in its on-premises environment. After a large file is created, it is accessed frequently by the file server for the first few days. It is rare for the files to be accessed after 30 days.</p><p>Data sizes are increasing and are approaching the company's storage capacity. Increasing a company's storage space without sacrificing access to recent files is the task of the solutions architect. The solutions architect must also provide file lifecycle management to avoid future storage issues from recurring.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>Amazon S3 File Gateway provides a seamless way to connect to the cloud to store application data files and backup images as durable objects in Amazon S3 cloud storage. Amazon S3 File Gateway offers SMB or NFS-based access to data in Amazon S3 with local caching.</p><p>It can be used for on-premises data-intensive Amazon EC2-based applications that need file protocol access to S3 object storage. Lifecycle policies can then transition the data to S3 Glacier Deep Archive after 30 days.</p><p><strong>CORRECT: </strong>\"Create an Amazon S3 File Gateway, extending the company's storage space into the cloud. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 30 days” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS DataSync to take data older than 30 days from the SMB file server to AWS” is incorrect. This solution would not distinguish between more and less frequently accessed data.</p><p><strong>INCORRECT:</strong> \"Create an Amazon FSx for Windows File Server file system to extend the file space” is incorrect. Amazon FSx for Windows File Server is not a hybrid service and is only suitable for cloud-based deployments of Windows File Server.</p><p><strong>INCORRECT:</strong> \"Install a custom program on each user's computer which will grant them access to Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 30 days\" is incorrect. This involves too much extra configuration which is unnecessary.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/storagegateway/file/s3/\">https://aws.amazon.com/storagegateway/file/s3/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>",
                "options": [
                    {
                        "id": 11421,
                        "content": "<p>Use AWS DataSync to take data older than 30 days from the SMB file server to AWS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11422,
                        "content": "<p>Install a custom program on each user's computer which will grant them access to Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 30 days.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11423,
                        "content": "<p>Create an Amazon S3 File Gateway, extending the company's storage space into the cloud. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 30 days.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11424,
                        "content": "<p>Create an Amazon FSx for Windows File Server file system to extend the file space.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2736,
            "attributes": {
                "createdAt": "2023-09-09T20:40:08.801Z",
                "updatedAt": "2023-09-09T20:40:08.801Z",
                "content": "<p>A Solutions Architect needs to select a low-cost, short-term option for adding resilience to an AWS Direct Connect connection. What is the MOST cost-effective solution to provide a backup for the Direct Connect connection?</p>",
                "answerExplanation": "<p>This is the most cost-effective solution. With this option both the Direct Connect connection and IPSec VPN are active and being advertised using the Border Gateway Protocol (BGP). The Direct Connect link will always be preferred unless it is unavailable.</p><p><strong>CORRECT: </strong>\"Implement an IPSec VPN connection and use the same BGP prefix\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement a second AWS Direct Connection\" is incorrect. This is not a short-term or low-cost option as it takes time to implement and is costly.</p><p><strong>INCORRECT:</strong> \"Configure AWS Transit Gateway with an IPSec VPN backup\" is incorrect. This is a workable solution and provides some advantages. However, you do need to pay for the Transit Gateway so it is not the most cost-effective option and probably not suitable for a short-term need.</p><p><strong>INCORRECT:</strong> \"Configure an IPSec VPN connection over the Direct Connect link\" is incorrect. This is not a solution to the problem as the VPN connection is going over the Direct Connect link. This is something you might do to add encryption to Direct Connect but it doesn’t make it more resilient.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/vpn-connection-as-a-backup-to-aws-dx-connection-example.html\">https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/vpn-connection-as-a-backup-to-aws-dx-connection-example.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-direct-connect/\">https://digitalcloud.training/aws-direct-connect/</a></p>",
                "options": [
                    {
                        "id": 11425,
                        "content": "<p>Implement an IPSec VPN connection and use the same BGP prefix</p>",
                        "isValid": true
                    },
                    {
                        "id": 11426,
                        "content": "<p>Configure AWS Transit Gateway with an IPSec VPN backup</p>",
                        "isValid": false
                    },
                    {
                        "id": 11427,
                        "content": "<p>Configure an IPSec VPN connection over the Direct Connect link</p>",
                        "isValid": false
                    },
                    {
                        "id": 11428,
                        "content": "<p>Implement a second AWS Direct Connection</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2737,
            "attributes": {
                "createdAt": "2023-09-09T20:40:08.908Z",
                "updatedAt": "2023-09-09T20:40:08.908Z",
                "content": "<p>A Solutions Architect needs to design a solution for providing a shared file system for company users in the AWS Cloud. The solution must be fault tolerant and should integrate with the company’s Microsoft Active Directory for access control.</p><p>Which storage solution meets these requirements?</p>",
                "answerExplanation": "<p>Amazon FSx for Windows File Server provides fully managed Microsoft Windows file servers, backed by a fully native Windows file system. Multi-AZ file systems provide high availability and failover support across multiple Availability Zones by provisioning and maintaining a standby file server in a separate Availability Zone within an AWS Region.</p><p>Amazon FSx works with Microsoft Active Directory (AD) to integrate with your existing Microsoft Windows environments. Active Directory is the Microsoft directory service used to store information about objects on the network and make this information easy for administrators and users to find and use.</p><p><strong>CORRECT: </strong>\"Create a file system with Amazon FSx for Windows File Server and enable Multi-AZ. Join Amazon FSx to Active Directory\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EFS file system and configure AWS Single Sign-On with Active Directory\" is incorrect. You cannot configure AWS SSO for an EFS file system with Active Directory.</p><p><strong>INCORRECT:</strong> \"Use an Amazon EC2 Windows instance to create a file share. Attach Amazon EBS volumes in different Availability Zones\" is incorrect. You cannot attach EBS volumes in different AZs to an instance.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 for storing the data and configure AWS Cognito to connect S3 to Active Directory for access control\" is incorrect. You cannot use Cognito to connect S3 to Active Directory.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 11429,
                        "content": "<p>Use an Amazon EC2 Windows instance to create a file share. Attach Amazon EBS volumes in different Availability Zones.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11430,
                        "content": "<p>Create an Amazon EFS file system and configure AWS Single Sign-On with Active Directory.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11431,
                        "content": "<p>Create a file system with Amazon FSx for Windows File Server and enable Multi-AZ. Join Amazon FSx to Active Directory.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11432,
                        "content": "<p>Use Amazon S3 for storing the data and configure AWS Cognito to connect S3 to Active Directory for access control.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2738,
            "attributes": {
                "createdAt": "2023-09-09T20:40:08.997Z",
                "updatedAt": "2023-09-09T20:40:08.997Z",
                "content": "<p>An application runs on Amazon EC2 instances. The application reads data from Amazon S3, performs processing on the data, and then writes the results to an Amazon DynamoDB table.</p><p>The application writes many temporary files during the data processing. The application requires a high-performance storage solution for the temporary files.</p><p>What would be the fastest storage option for this solution?</p>",
                "answerExplanation": "<p>As the data is only temporary it can be stored on an instance store volume which is a volume that is physically attached to the host computer on which the EC2 instance is running.</p><p>To increase aggregate IOPS, or to improve sequential disk throughput, multiple instance store volumes can be grouped together using RAID 0 (disk striping) software. This can improve the aggregate performance of the volume.</p><p><strong>CORRECT: </strong>\"Multiple instance store volumes with software RAID 0\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Multiple Amazon EBS volumes with Provisioned IOPS\" is incorrect. Multiple volumes will not provide better performance unless you can aggregate the performance across them which is what the correct answer offers with instance store volumes.</p><p><strong>INCORRECT:</strong> \"Multiple Amazon EFS volumes in Max I/O performance mode\" is incorrect. You cannot aggregate the performance of multiple EFS volumes.</p><p><strong>INCORRECT:</strong> \"Multiple Amazon S3 buckets with Transfer Acceleration\" is incorrect. Amazon S3 will not provide the best performance for this use case and transfer acceleration is used more for the upload of data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-storage-services-overview/performance-4.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-storage-services-overview/performance-4.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 11433,
                        "content": "<p>Multiple Amazon EBS volumes with Provisioned IOPS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11434,
                        "content": "<p>Multiple Amazon EFS volumes in Max I/O performance mode.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11435,
                        "content": "<p>Multiple instance store volumes with software RAID 0.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11436,
                        "content": "<p>Multiple Amazon S3 buckets with Transfer Acceleration.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2739,
            "attributes": {
                "createdAt": "2023-09-09T20:40:09.099Z",
                "updatedAt": "2023-09-09T20:40:09.099Z",
                "content": "<p>A Solutions Architect is rearchitecting an application with decoupling. The application will send batches of up to 1000 messages per second that must be received in the correct order by the consumers.</p><p>Which action should the Solutions Architect take?</p>",
                "answerExplanation": "<p>Only FIFO queues guarantee the ordering of messages and therefore a standard queue would not work. The FIFO queue supports up to 3,000 messages per second with batching so this is a supported scenario.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_19-53-23-3f1e752167769131a330719cd1b24d10.png\"></p><p><strong>CORRECT: </strong>\"Create an Amazon SQS FIFO queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SQS Standard queue\" is incorrect as it does not guarantee ordering of messages.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic\" is incorrect. SNS is a notification service and a message queue is a better fit for this use case.</p><p><strong>INCORRECT:</strong> \"Create an AWS Step Functions state machine\" is incorrect. Step Functions is a workflow orchestration service and is not useful for this scenario.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 11437,
                        "content": "<p>Create an Amazon SNS topic</p>",
                        "isValid": false
                    },
                    {
                        "id": 11438,
                        "content": "<p>Create an AWS Step Functions state machine</p>",
                        "isValid": false
                    },
                    {
                        "id": 11439,
                        "content": "<p>Create an Amazon SQS Standard queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 11440,
                        "content": "<p>Create an Amazon SQS FIFO queue</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2740,
            "attributes": {
                "createdAt": "2023-09-09T20:40:09.196Z",
                "updatedAt": "2023-09-09T20:40:09.196Z",
                "content": "<p>A large manufacturing company needs to store data in Amazon S3 and prevent the data from being modified. The company requires that all new objects uploaded to Amazon S3 should remain unchangeable for an unspecified period until the company decides to modify the objects. Only specific users within the company's AWS account should have the ability to delete the objects.</p><p>What should a solutions architect do to meet these requirements?</p>",
                "answerExplanation": "<p>With S3 Object Lock, you can store objects using a write-once-read-many (WORM) model. Object Lock can help prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. You can use Object Lock to help meet regulatory requirements that require WORM storage, or to simply add another layer of protection against object changes and deletion. Governance mode also renders the objects to be immutable.</p><p><strong>CORRECT: </strong>\"Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Add a legal hold to the objects. Add the s3:PutObjectLegalHold permission to the IAM policies of users who need to delete the objects\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an S3 Glacier vault. Apply a vault lock policy to the objects to prevent modification of the objects\" is incorrect as this would not make the objects immutable.</p><p><strong>INCORRECT:</strong> \"Create an S3 bucket. Use AWS CloudTrail to track S3 all actions to the resources within S3. Upon notification, restore the modified objects from any backup versions that the company has if a delete action has taken place\" is incorrect as this would not make the objects immutable.</p><p><strong>INCORRECT:</strong> \"Create a new S3 bucket with S3 Object Lock enabled. Enable versioning on your bucket and set a retention period of 50 years. Use governance mode as the S3 bucket's default retention mode for new objects\" is incorrect as using governance mode would not allow the files to be changed at all for the duration of this being enabled.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11441,
                        "content": "<p>Create an S3 bucket. Use AWS CloudTrail to track S3 all actions to the resources within S3. Upon notification, restore the modified objects from any backup versions that the company has if a delete action has taken place.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11442,
                        "content": "<p>Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Add a legal hold to the objects. Add the s3:PutObjectLegalHold permission to the IAM policies of users who need to delete the objects.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11443,
                        "content": "<p>Create a new S3 bucket with S3 Object Lock enabled. Enable versioning on your bucket and set a retention period of 50 years. Use governance mode as the S3 bucket's default retention mode for new objects.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11444,
                        "content": "<p>Create an S3 Glacier vault. Apply a vault lock policy to the objects to prevent modification of the objects.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2741,
            "attributes": {
                "createdAt": "2023-09-09T20:40:09.328Z",
                "updatedAt": "2023-09-09T20:40:09.328Z",
                "content": "<p>A social media company has a Microsoft .NET application that currently runs on an on-premises Windows Server. The application uses an Oracle Database Standard Edition server for its database layer.</p><p>The company is planning to migrate this application to AWS and wants to minimize development changes while moving the application, due to limited staff resources. The AWS application environment should however not compromise on being highly available.</p><p>Which two actions should the company take to meet these requirements? (Select TWO.)</p>",
                "answerExplanation": "<p>AWS Elastic Beanstalk in a multi-AZ configuration is an ideal platform for running the application. This is a PaaS service, and the developers only need to add the code. The deployment also provides the required high availability as a multi-AZ deployment will include Auto Scaling and Load Balancing.</p><p>For the database tier, AWS DMS can be used to migrate the database to an Amazon RDS managed database service using the Oracle DB engine. This requires a minimum of changes and provides high availability.</p><p><strong>CORRECT: </strong>\"Redeploy the application in Elastic Beanstalk with the .NET platform provisioned in a Multi-AZ configuration” is the correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Migrate from Oracle to Oracle on Amazon RDS using the AWS Database Migration Service (AWS DMS)” is also the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Refactor the application into a serverless solution with multiple Lambda functions running .NET Core” is incorrect. This would involve substantial application changes and would not meet the specific requirements of the migration.</p><p><strong>INCORRECT:</strong> \"Migrate the server to Amazon EC2 using an Amazon Linux Amazon Machine Image (AMI)” is incorrect. There is no high availability in this solution unless deployed using Auto Scaling and a Load Balancer.</p><p><strong>INCORRECT:</strong> \"Use the AWS Database Migration Service (AWS DMS) to migrate the database from the Oracle database to an Amazon DynamoDB table in a Multi-AZ configuration” is incorrect. DynamoDB is a NoSQL Database whereas Oracle is a SQL based database, meaning that the database’s schema would have to be radically changed to accommodate the DynamoDB database. DynamoDB also does not have a feature known as multi-AZ.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 11445,
                        "content": "<p>Redeploy the application in Elastic Beanstalk with the .NET platform provisioned in a Multi-AZ configuration.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11446,
                        "content": "<p>Use the AWS Database Migration Service (AWS DMS) to migrate the database from the Oracle database to an Amazon DynamoDB table in a Multi-AZ configuration.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11447,
                        "content": "<p>Refactor the application into a serverless solution with multiple Lambda functions running .NET Core.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11448,
                        "content": "<p>Migrate the server to Amazon EC2 using an Amazon Linux Amazon Machine Image (AMI).</p>",
                        "isValid": false
                    },
                    {
                        "id": 11449,
                        "content": "<p>Migrate from Oracle to Oracle on Amazon RDS using the AWS Database Migration Service (AWS DMS).</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2742,
            "attributes": {
                "createdAt": "2023-09-09T20:40:09.467Z",
                "updatedAt": "2023-09-09T20:40:09.467Z",
                "content": "<p>Over 500 TB of data must be analyzed using standard SQL business intelligence tools. The dataset consists of a combination of structured data and unstructured data. The unstructured data is small and stored on Amazon S3. Which AWS services are most suitable for performing analytics on the data?</p>",
                "answerExplanation": null,
                "options": [
                    {
                        "id": 11450,
                        "content": "<p>Amazon RDS MariaDB with Amazon Athena</p>",
                        "isValid": false
                    },
                    {
                        "id": 11451,
                        "content": "<p>Amazon Redshift with Amazon Redshift Spectrum</p>",
                        "isValid": true
                    },
                    {
                        "id": 11452,
                        "content": "<p>Amazon DynamoDB with Amazon DynamoDB Accelerator (DAX)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11453,
                        "content": "<p>Amazon ElastiCache for Redis with cluster mode enabled</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2743,
            "attributes": {
                "createdAt": "2023-09-09T20:40:09.562Z",
                "updatedAt": "2023-09-09T20:40:09.562Z",
                "content": "<p>A Solutions Architect is migrating a distributed application from their on-premises environment into AWS. This application consists of an Apache Cassandra NoSQL database, with a containerized SUSE Linux compute layer with an additional storage layer made up of multiple Microsoft SQL Server databases. Once in the cloud the company wants to have as little operational overhead as possible, with no schema conversion during the migration and the company wants to host the architecture in a highly available and durable way. </p><p><br></p><p>Which of the following groups of services will provide the solutions architect with the best solution ?</p>",
                "answerExplanation": "<p>Amazon Keyspaces (for Apache Cassandra) is a scalable, highly available, and managed Apache Cassandra–compatible database service. This combined with a containerized, serverless compute layer on Amazon ECS for Fargate and a RDS for Microsoft SQL Server database layer is a fully managed version of what currently exists on premises.</p><p><strong>CORRECT: </strong>\"Run the NoSQL database on Amazon Keyspaces, and the compute layer on Amazon ECS on Fargate. Use Amazon RDS for Microsoft SQL Server to host the second storage layer” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Run the NoSQL database on DynamoDB, and the compute layer on Amazon ECS on EC2. Use Amazon RDS for Microsoft SQL Server to host the second storage layer” is incorrect. DynamoDB is not a managed version of DynamoDB therefore it is not the correct answer.</p><p><strong>INCORRECT:</strong> \"Run the NoSQL database on DynamoDB, and the compute layer on Amazon ECS on Fargate. Use Amazon RDS for Microsoft SQL Server to host the second storage layer” is incorrect. DynamoDB is not a managed version of DynamoDB therefore it is not the correct answer.</p><p><strong>INCORRECT:</strong> \"Run the NoSQL database on Amazon Keyspaces, and the compute layer on Amazon ECS on Fargate. Use Amazon Aurora to host the second storage layer” is incorrect. Amazon Aurora does not have an option to run a Microsoft SQL Server database, therefore this answer is not correct.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/keyspaces/\">https://aws.amazon.com/keyspaces/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-database/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-database/</a></p>",
                "options": [
                    {
                        "id": 11454,
                        "content": "<p>Run the NoSQL database on Amazon Keyspaces, and the compute layer on Amazon ECS on Fargate. Use Amazon RDS for Microsoft SQL Server to host the second storage layer.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11455,
                        "content": "<p>Run the NoSQL database on DynamoDB, and the compute layer on Amazon ECS on Fargate. Use Amazon RDS for Microsoft SQL Server to host the second storage layer.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11456,
                        "content": "<p>Run the NoSQL database on DynamoDB, and the compute layer on Amazon ECS on EC2. Use Amazon RDS for Microsoft SQL Server to host the second storage layer.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11457,
                        "content": "<p>Run the NoSQL database on Amazon Keyspaces, and the compute layer on Amazon ECS on Fargate. Use Amazon Aurora to host the second storage layer.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2744,
            "attributes": {
                "createdAt": "2023-09-09T20:40:09.670Z",
                "updatedAt": "2023-09-09T20:40:09.670Z",
                "content": "<p>A company is deploying an Amazon ElastiCache for Redis cluster. To enhance security a password should be required to access the database. What should the solutions architect use?</p>",
                "answerExplanation": "<p>Redis authentication tokens enable Redis to require a token (password) before allowing clients to execute commands, thereby improving data security.</p><p>You can require that users enter a token on a token-protected Redis server. To do this, include the parameter --auth-token (API: AuthToken) with the correct token when you create your replication group or cluster. Also include it in all subsequent commands to the replication group or cluster.</p><p><strong>CORRECT: </strong>\"Redis AUTH command\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Directory Service\" is incorrect. This is a managed Microsoft Active Directory service and cannot add password protection to Redis.</p><p><strong>INCORRECT:</strong> \"AWS IAM Policy\" is incorrect. You cannot use an IAM policy to enforce a password on Redis.</p><p><strong>INCORRECT:</strong> \"VPC Security Group\" is incorrect. A security group protects at the network layer, it does not affect application authentication.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 11458,
                        "content": "<p>Redis AUTH command</p>",
                        "isValid": true
                    },
                    {
                        "id": 11459,
                        "content": "<p>AWS IAM Policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 11460,
                        "content": "<p>AWS Directory Service</p>",
                        "isValid": false
                    },
                    {
                        "id": 11461,
                        "content": "<p>VPC Security Group</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2745,
            "attributes": {
                "createdAt": "2023-09-09T20:40:09.773Z",
                "updatedAt": "2023-09-09T20:40:09.773Z",
                "content": "<p>A company runs a legacy application that uses an Amazon RDS MySQL database without encryption. The security team has instructed a Solutions Architect to encrypt the database due to new compliance requirements.</p><p>How can the Solutions Architect encrypt all existing and new data in the database?</p>",
                "answerExplanation": "<p>This comes up on almost every exam so be prepared! The key fact to remember is that you cannot alter the encryption state of an RDS database after you have deployed it. You also cannot create encrypted replicas from unencrypted instances.</p><p>The only solution is to create a snapshot (which will be unencrypted) and subsequently create an encrypted copy of the snapshot. You can then create a new database instance from the encrypted snapshot. The new database will be encrypted and will have a new endpoint address.</p><p><strong>CORRECT: </strong>\"Take a snapshot of the RDS instance. Create an encrypted copy of the snapshot. Create a new RDS instance from the encrypted snapshot\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon S3 bucket with server-side encryption enabled. Move all the data to Amazon S3. Delete the RDS instance\" is incorrect. S3 is an object storage system and not a replacement for a relational MySQL database.</p><p><strong>INCORRECT:</strong> \"Enable Multi-AZ mode for the database and enable encryption at rest. Perform a manual failover to the standby and delete the original database instance\" is incorrect. You cannot enable encryption after creation of the database, and this includes for any instances created from the database such as replicas and multi-AZ standby instances.</p><p><strong>INCORRECT:</strong> \"Add an RDS read replica with encryption at rest enabled. Promote the read replica to master and then delete the original database instance\" is incorrect. You cannot enable encryption on a read replica when the primary database is unencrypted.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 11462,
                        "content": "<p>Enable Multi-AZ mode for the database and enable encryption at rest. Perform a manual failover to the standby and delete the original database instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11463,
                        "content": "<p>Create an Amazon S3 bucket with server-side encryption enabled. Move all the data to Amazon S3. Delete the RDS instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11464,
                        "content": "<p>Take a snapshot of the RDS instance. Create an encrypted copy of the snapshot. Create a new RDS instance from the encrypted snapshot.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11465,
                        "content": "<p>Add an RDS read replica with encryption at rest enabled. Promote the read replica to master and then delete the original database instance.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2746,
            "attributes": {
                "createdAt": "2023-09-09T20:40:09.870Z",
                "updatedAt": "2023-09-09T20:40:09.870Z",
                "content": "<p>A company plans to provide developers with individual AWS accounts. The company will use AWS Organizations to provision the accounts. A Solutions Architect must implement secure auditing using AWS CloudTrail so that all events from all AWS accounts are logged. The developers must not be able to use root-level permissions to alter the AWS CloudTrail configuration in any way or access the log files in the S3 bucket. The auditing solution and security controls must automatically apply to all new developer accounts that are created.</p><p>Which action should the Solutions Architect take?</p>",
                "answerExplanation": "<p>You can create a CloudTrail trail in the management account with the organization trails option enabled and this will create the trail in all AWS accounts within the organization.</p><p>Member accounts can see the organization trail but can't modify or delete it. By default, member accounts don't have access to the log files for the organization trail in the Amazon S3 bucket.</p><p><strong>CORRECT: </strong>\"Create a new trail in CloudTrail from within the management account with the organization trails option enabled\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy that prohibits changes to CloudTrail and attach it to the root user\" is incorrect. You cannot restrict the root user this way and should use the organization trails option or an SCP instead.</p><p><strong>INCORRECT:</strong> \"Create a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the management account\" is incorrect. You cannot create service-linked roles, these are created by AWS for you.</p><p><strong>INCORRECT:</strong> \"Create a service control policy (SCP) that prohibits changes to CloudTrail and attach it to the developer accounts\" is incorrect. An SCP can achieve the required outcome of limiting the ability to change the CloudTrail configuration, but the trail must still be created in each account and the SCP must be attached which is not automatic.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
                "options": [
                    {
                        "id": 11466,
                        "content": "<p>Create an IAM policy that prohibits changes to CloudTrail and attach it to the root user.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11467,
                        "content": "<p>Create a service control policy (SCP) that prohibits changes to CloudTrail and attach it to the developer accounts.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11468,
                        "content": "<p>Create a new trail in CloudTrail from within the management account with the organization trails option enabled.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11469,
                        "content": "<p>Create a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the management account.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2747,
            "attributes": {
                "createdAt": "2023-09-09T20:40:09.948Z",
                "updatedAt": "2023-09-09T20:40:09.948Z",
                "content": "<p>A Solutions Architect is tasked with designing a fully Serverless, Microservices based web application which requires the use of a GraphQL API to provide a single entry point to the application.</p><p>Which AWS managed service could the Solutions Architect use?</p>",
                "answerExplanation": "<p>AWS AppSync is a serverless GraphQL and Pub/Sub API service that simplifies building modern web and mobile applications.</p><p>AWS AppSync GraphQL APIs simplify application development by providing a single endpoint to securely query or update data from multiple databases, microservices, and APIs.</p><p><strong>CORRECT: </strong>\"AWS AppSync\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"API Gateway\" is incorrect. You cannot create GraphQL APIs on API Gateway.</p><p><strong>INCORRECT:</strong> \"Amazon Athena\" is incorrect. Amazon Athena is a Serverless query service where you can query S3 using SQL statements.</p><p><strong>INCORRECT:</strong> \"AWS Lambda\" is incorrect. AWS Lambda is a serverless compute service and is not designed to build APIs.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/appsync/\">https://aws.amazon.com/appsync/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-networking-content-delivery/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-networking-content-delivery/</a></p>",
                "options": [
                    {
                        "id": 11470,
                        "content": "<p>AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 11471,
                        "content": "<p>Amazon Athena</p>",
                        "isValid": false
                    },
                    {
                        "id": 11472,
                        "content": "<p>AWS AppSync</p>",
                        "isValid": true
                    },
                    {
                        "id": 11473,
                        "content": "<p>API Gateway</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2748,
            "attributes": {
                "createdAt": "2023-09-09T20:40:10.032Z",
                "updatedAt": "2023-09-09T20:40:10.032Z",
                "content": "<p>A large accounting company needs to store all its accounting records in Amazon S3. The records must be accessible for 1 year with immediate notice, and then must be archived for a further 9 years due to compliance requirements. No one at the company, under any circumstances, should be able to delete the records over the entire 10-year period. The records must be stored with maximum resiliency to prevent data loss.</p><p>Which solution will most elegantly meet these requirements?</p>",
                "answerExplanation": "<p>Using an S3 Lifecycle policy is the easiest way to transition your storage class to an archival tier after one year, with no manual intervention. S3 Standard is suitable for the first year as the customer requires maximum resiliency and immediate retrieval for the first year.</p><p>Secondly the customer requires the data to be deleted at no point, and under no circumstances. With S3 Object Lock, you can store objects using a <em>write-once-read-many</em> (WORM) model. In <em>compliance</em> mode, a protected object version can't be overwritten or deleted by any user, including the root user in your AWS account.</p><p><strong>CORRECT: </strong>\"Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the records exclusively in S3 Glacier Deep Archive for the entire 10-year period. Use an access control policy to prevent deletion of the records for 10 years” is incorrect as the customer needs to have the data immediately retrievable for the first year. This is not possible with S3 Glacier Deep Archive.</p><p><strong>INCORRECT:</strong> \"Store the records within S3 Intelligent-Tiering. Use a Bucket policy to deny deletion of the records, and after 10 years, change the Bucket policy to allow deletion manually” is incorrect. A Bucket policy can be easily changed to allow deletion of the resources within the S3 Bucket, so this is not suitable.</p><p><strong>INCORRECT:</strong> \"Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 year. Use S3 Object Lock in governance mode for a period of 10 years” is incorrect. S3 One-Zone-IA is not a suitable storage class as the customer requires as much redundancy and possible, which will not be achieved through a single AZ. Secondly, Governance mode can be overturned with certain permissions, unlike Compliance mode.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11474,
                        "content": "<p>Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 year. Use S3 Object Lock in governance mode for a period of 10 years.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11475,
                        "content": "<p>Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11476,
                        "content": "<p>Store the records exclusively in S3 Glacier Deep Archive for the entire 10-year period. Use an access control policy to prevent deletion of the records for 10 years.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11477,
                        "content": "<p>Store the records within S3 Intelligent-Tiering. Use a Bucket policy to deny deletion of the records, and after 10 years, change the Bucket policy to allow deletion manually.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2749,
            "attributes": {
                "createdAt": "2023-09-09T20:40:10.106Z",
                "updatedAt": "2023-09-09T20:40:10.106Z",
                "content": "<p>A financial services company provides users with downloadable reports in PDF format. The company requires a solution that can seamlessly scale to meet the demands of a growing, global user base. The solution must be cost-effective and minimize operational overhead.</p><p>Which combination of services should a Solutions Architect recommend to meet these requirements?</p>",
                "answerExplanation": "<p>The most cost-effective option is to use Amazon S3 for storing the PDF files and Amazon CloudFront for caching the files around the world in edge locations. This combination of services will provide seamless scalability and is cost-effective. This is also a serverless solution so operational overhead is minimized.</p><p><strong>CORRECT: </strong>\"Amazon CloudFront and Amazon S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Lambda and Amazon DynamoDB\" is incorrect. AWS Lambda can be used to process requests and serve traffic from DynamoDB. However, a front end like API Gateway may be required and DynamoDB would be less cost-effective compared to using S3.</p><p><strong>INCORRECT:</strong> \"Application Load Balancer with AWS Auto Scaling\" is incorrect. This would use Amazon EC2 instances and load balancers which is more expensive.</p><p><strong>INCORRECT:</strong> \"Amazon Route 53 with Network Load Balancers\" is incorrect. This would use Amazon EC2 instances and load balancers which is more expensive.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11478,
                        "content": "<p>Amazon CloudFront and Amazon S3.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11479,
                        "content": "<p>AWS Lambda and Amazon DynamoDB.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11480,
                        "content": "<p>Amazon Route 53 with Network Load Balancers.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11481,
                        "content": "<p>Application Load Balancer with AWS Auto Scaling.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2750,
            "attributes": {
                "createdAt": "2023-09-09T20:40:10.227Z",
                "updatedAt": "2023-09-09T20:40:10.227Z",
                "content": "<p>The Chief Financial Officer of a large corporation is looking for an AWS native tool which will help reduce their cloud spend. After receiving a budget alarm, the company has decided that they need to reduce their spend across their different areas of compute and need insights into their spend to decide where they can reduce cost. </p><p>What is the easiest way to achieve this goal?</p>",
                "answerExplanation": "<p>AWS Compute Optimizer helps you identify the optimal AWS resource configurations, such as Amazon Elastic Compute Cloud (EC2) instance types, Amazon Elastic Block Store (EBS) volume configurations, and AWS Lambda function memory sizes, using machine learning to analyze historical utilization metrics. AWS Compute Optimizer provides a set of APIs and a console experience to help you reduce costs and increase workload performance by recommending the optimal AWS resources for your AWS workloads.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-27_10-50-15-dfca99833259eb55e7d690aacd0593d3.jpg\"><p><strong>CORRECT: </strong>\"AWS Compute Optimizer\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS Trusted Advisor\" is incorrect. Whilst you will get some cost recommendations using Trusted Advisor, when working with reducing cost for compute specifically, AWS Compute Optimizer is a better choice.</p><p><strong>INCORRECT:</strong> \"Cost and Usage Reports\" is incorrect. Cost and Usage Reports are a highly detailed report of your spend and usage across your entire AWS Environment. Whilst it can be used to understand cost, it does not make recommendations.</p><p><strong>INCORRECT:</strong> \"AWS Cost Explorer\" is incorrect. Cost Explorer gives you insight into your spend and usage in a graphical format, which can be filtered and grouped by parameters like Region, instance type and can use Tags to further group resources. It does not however make any recommendations on how to reduce spend.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/compute-optimizer/faqs/\">https://aws.amazon.com/compute-optimizer/faqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-billing-and-pricing/\">https://digitalcloud.training/aws-billing-and-pricing/</a></p>",
                "options": [
                    {
                        "id": 11482,
                        "content": "<p>Cost and Usage Reports</p>",
                        "isValid": false
                    },
                    {
                        "id": 11483,
                        "content": "<p>AWS Compute Optimizer</p>",
                        "isValid": true
                    },
                    {
                        "id": 11484,
                        "content": "<p>AWS Cost Explorer</p>",
                        "isValid": false
                    },
                    {
                        "id": 11485,
                        "content": "<p>AWS Trusted Advisor</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2751,
            "attributes": {
                "createdAt": "2023-09-09T20:40:10.317Z",
                "updatedAt": "2023-09-09T20:40:10.317Z",
                "content": "<p>A financial institution with many departments wants to migrate to the AWS Cloud from their data center. Each department should have their own established AWS accounts with preconfigured, Limited access to authorized services, based on each team's needs, by the principle of least privilege.</p><p>What actions should be taken to ensure compliance with these security requirements?</p>",
                "answerExplanation": "<p>AWS Control Tower automates the setup of a new landing zone using best practices blueprints for identity, federated access, and account structure.</p><p>The account factory automates provisioning of new accounts in your organization. As a configurable account template, it helps you standardize the provisioning of new accounts with pre-approved account configurations. You can configure your account factory with pre-approved network configuration and region selections.</p><p><strong>CORRECT: </strong>\"Deploy a Landing Zone within AWS Control Tower. Allow department administrators to use the Landing Zone to create new member accounts and networking. Grant the department's AWS power user permissions on the created accounts” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to create new member accounts and networking and use IAM roles to allow access to approved AWS services” is incorrect. Although you could perhaps make new AWS Accounts with AWS CloudFormation, the easiest way to do that is by using AWS Control Tower.</p><p><strong>INCORRECT:</strong> \"Configure AWS Organizations with SCPs and create new member accounts. Use AWS CloudFormation templates to configure the member account networking” is incorrect. You can make new accounts using AWS Organizations however the easiest way to do this is by using the AWS Control Tower service.</p><p><strong>INCORRECT:</strong> \"Deploy a Landing Zone within AWS Organizations. Allow department administrators to use the Landing Zone to create new member accounts and networking. Grant the department's AWS power user permissions on the created accounts” is incorrect. Landing Zones do not get deployed within AWS Organizations.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/controltower/\">https://aws.amazon.com/controltower/</a></p>",
                "options": [
                    {
                        "id": 11486,
                        "content": "<p>Use AWS CloudFormation to create new member accounts and networking and use IAM roles to allow access to approved AWS services.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11487,
                        "content": "<p>Deploy a Landing Zone within AWS Control Tower. Allow department administrators to use the Landing Zone to create new member accounts and networking. Grant the department's AWS power user permissions on the created accounts.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11488,
                        "content": "<p>Deploy a Landing Zone within AWS Organizations. Allow department administrators to use the Landing Zone to create new member accounts and networking. Grant the department's AWS power user permissions on the created accounts.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11489,
                        "content": "<p>Configure AWS Organizations with SCPs and create new member accounts. Use AWS CloudFormation templates to configure the member account networking.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2752,
            "attributes": {
                "createdAt": "2023-09-09T20:40:10.398Z",
                "updatedAt": "2023-09-09T20:40:10.398Z",
                "content": "<p>A large financial services organization has a workflow for ingesting data. It currently consists of an Amazon Simple Notification Service (Amazon SNS) topic for receiving notifications about new deliveries of data, and an AWS Lambda function to process the data and record metadata.</p><p>Network connectivity issues occasionally cause the ingestion workflow to fail. When such a failure occurs, the Lambda function does not ingest the corresponding data and the team must manually re-run the Lambda function.</p><p>Which combination of actions should a solutions architect take to ensure that the data is ingested even if there is a network outage. (Select TWO.)</p>",
                "answerExplanation": "<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using Amazon SQS here will allow us to decouple the different parts of our architecture to ensure that even if there is a network outage, the Lambda function will not have to be retried, as the SQS queue will persistently store the request.</p><p><strong>CORRECT: </strong>\"Set up an Amazon SQS queue and subscribe it to the SNS topic” is the correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Modify the Lambda function so it reads from an Amazon SQS queue” is also the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the Lambda function across multiple Availability Zones within the Region” is incorrect. AWS Lambda automatically maintains compute capacity across multiple Availability Zones (AZs) in each AWS Region to help protect your code against individual machine or data center facility failures.</p><p><strong>INCORRECT:</strong> \"Attach the Lambda function to an Amazon VPC and deploy a NAT gateway. Enable multi-AZ for the VPC” is incorrect. There is no such thing as enabling multi-AZ for a VPC and attaching the function to a VPC does not assist with solving the issue in this case.</p><p><strong>INCORRECT:</strong> \"Increase throughput for the Lambda function and increase the provisioned CPU and memory” is incorrect. This will increase the performance of the function but will not help if there is a network outage.</p><p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 11490,
                        "content": "<p>Set up an Amazon SQS queue and subscribe it to the SNS topic.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11491,
                        "content": "<p>Modify the Lambda function so it reads from an Amazon SQS queue.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11492,
                        "content": "<p>Increase throughput for the Lambda function and increase the provisioned CPU and memory.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11493,
                        "content": "<p>Deploy the Lambda function across multiple Availability Zones within the Region.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11494,
                        "content": "<p>Attach the Lambda function to an Amazon VPC and deploy a NAT gateway. Enable multi-AZ for the VPC.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2753,
            "attributes": {
                "createdAt": "2023-09-09T20:40:10.485Z",
                "updatedAt": "2023-09-09T20:40:10.485Z",
                "content": "<p>A company requires that IAM users must rotate their access keys every 60 days. If an access key is found to older it must be removed. A Solutions Architect must create an automated solution that checks the age of access keys and removes any keys that exceed the maximum age defined.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>Amazon EventBridge uses the same underlying service and API as Amazon CloudWatch Events. You can use EventBridge to detect and react to changes in the status of AWS Config events. You can create a rule that runs whenever there is a state transition, or when there is a transition to one or more states that are of interest. Then, based on rules you create, Amazon EventBridge invokes one or more target actions when an event matches the values you specify in a rule. Depending on the type of event, you might want to send notifications, capture event information, take corrective action, initiate events, or take other actions.</p><p>The AWS Config rule can be configured using the “access-keys-rotated” managed rule which checks if the active access keys are rotated within the number of days specified in maxAccessKeyAge. The rule is NON_COMPLIANT if the access keys have not been rotated for more than maxAccessKeyAge number of days.</p><p>Amazon EventBridge can react to the change of state to NON_COMPLIANT and trigger an AWS Lambda function that invalidates and removes the access key.</p><p><strong>CORRECT: </strong>\"Create an AWS Config rule to check for the key age. Define an Amazon EventBridge rule to execute an AWS Lambda function that removes the key\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an AWS Config rule to check for the key age. Configure the AWS Config rule to trigger an Amazon SNS notification\" is incorrect. This solution notifies the user or administrator but does not automatically remove the key.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule to check for the key age. Configure the rule to trigger an AWS Config remediation that removes the key\" is incorrect. Config rules will check the key age and then EventBridge should react to the state change, not the other way around.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule to check for the key age. Define an Amazon EventBridge rule to execute an AWS Lambda function that removes the key\" is incorrect. Again, EventBridge does not check the key age, AWS Config must be used for that purpose.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/monitor-config-with-cloudwatchevents.html\">https://docs.aws.amazon.com/config/latest/developerguide/monitor-config-with-cloudwatchevents.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/access-keys-rotated.html\">https://docs.aws.amazon.com/config/latest/developerguide/access-keys-rotated.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p>",
                "options": [
                    {
                        "id": 11495,
                        "content": "<p>Create an AWS Config rule to check for the key age. Configure the AWS Config rule to trigger an Amazon SNS notification.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11496,
                        "content": "<p>Create an AWS Config rule to check for the key age. Define an Amazon EventBridge rule to execute an AWS Lambda function that removes the key.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11497,
                        "content": "<p>Create an Amazon EventBridge rule to check for the key age. Configure the rule to trigger an AWS Config remediation that removes the key.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11498,
                        "content": "<p>Create an Amazon EventBridge rule to check for the key age. Define an Amazon EventBridge rule to execute an AWS Lambda function that removes the key.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2754,
            "attributes": {
                "createdAt": "2023-09-09T20:40:10.577Z",
                "updatedAt": "2023-09-09T20:40:10.577Z",
                "content": "<p>A team of scientists are collecting environmental data to assess the impact of pollution in a small regional town. The scientists collect data from various sensors and cameras. The data must be immediately processed to validate its accuracy, but the scientists have limited local storage space on their laptops and intermittent and unreliable connectivity to their Amazon EC2 instances and S3 buckets.</p><p>What should a Solutions Architect recommend?</p>",
                "answerExplanation": "<p>AWS Snowball Edge is a type of Snowball device with on-board storage and compute power for select AWS capabilities. Snowball Edge can do local processing and edge-computing workloads in addition to transferring data between your local environment and the AWS Cloud.</p><p>You can run Amazon EC2 compute instances on a Snowball Edge device using the Amazon EC2 compatible endpoint, which supports a subset of the Amazon EC2 API operations. Data can subsequently be transferred to Amazon S3 for storage and additional processing.</p><p><strong>CORRECT: </strong>\"Use AWS Snowball Edge devices to process the data locally\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Upload the data to Amazon SQS in batches and process the messages using Amazon EC2 instances\" is incorrect. The internet connectivity is unreliable so this could result in data loss and delays for the team.</p><p><strong>INCORRECT:</strong> \"Configure Amazon Kinesis Data Firehose to load data directly to a Snowball device and process locally with Lambda@Edge\" is incorrect. KDF cannot load data to Snowball devices and Lambda@Edge is used with CloudFront for processing data.</p><p><strong>INCORRECT:</strong> \"Use AWS DataSync on the scientists’ laptops to synchronize the data to Amazon S3. Process the data with Amazon EC2 instances\" is incorrect. Due to the unreliable connectivity this does not solve the problem.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/snowball/latest/developer-guide/whatisedge.html\">https://docs.aws.amazon.com/snowball/latest/developer-guide/whatisedge.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11499,
                        "content": "<p>Configure Amazon Kinesis Data Firehose to load data directly to a Snowball device and process locally with Lambda@Edge.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11500,
                        "content": "<p>Use AWS Snowball Edge devices to process the data locally.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11501,
                        "content": "<p>Upload the data to Amazon SQS in batches and process the messages using Amazon EC2 instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11502,
                        "content": "<p>Use AWS DataSync on the scientists’ laptops to synchronize the data to Amazon S3. Process the data with Amazon EC2 instances.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2755,
            "attributes": {
                "createdAt": "2023-09-09T20:40:10.665Z",
                "updatedAt": "2023-09-09T20:40:10.665Z",
                "content": "<p>A large MongoDB database running on-premises must be migrated to Amazon DynamoDB within the next few weeks. The database is too large to migrate over the company’s limited internet bandwidth so an alternative solution must be used. What should a Solutions Architect recommend?</p>",
                "answerExplanation": "<p>Larger data migrations with AWS DMS can include many terabytes of information. This process can be cumbersome due to network bandwidth limits or just the sheer amount of data. AWS DMS can use Snowball Edge and Amazon S3 to migrate large databases more quickly than by other methods.</p><p>When you're using an Edge device, the data migration process has the following stages:</p><p>1. You use the AWS Schema Conversion Tool (AWS SCT) to extract the data locally and move it to an Edge device.</p><p>2. You ship the Edge device or devices back to AWS.</p><p>3. After AWS receives your shipment, the Edge device automatically loads its data into an Amazon S3 bucket.</p><p>4. AWS DMS takes the files and migrates the data to the target data store. If you are using change data capture (CDC), those updates are written to the Amazon S3 bucket and then applied to the target data store.</p><p><strong>CORRECT: </strong>\"Use the Schema Conversion Tool (SCT) to extract and load the data to an AWS Snowball Edge device. Use the AWS Database Migration Service (DMS) to migrate the data to Amazon DynamoDB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Setup an AWS Direct Connect and migrate the database to Amazon DynamoDB using the AWS Database Migration Service (DMS)\" is incorrect as Direct Connect connections can take several weeks to implement.</p><p><strong>INCORRECT:</strong> \"Enable compression on the MongoDB database and use the AWS Database Migration Service (DMS) to directly migrate the database to Amazon DynamoDB\" is incorrect. It’s unlikely that compression is going to make the difference and the company want to avoid the internet link as stated in the scenario.</p><p><strong>INCORRECT:</strong> \"Use the AWS Database Migration Service (DMS) to extract and load the data to an AWS Snowball Edge device. Complete the migration to Amazon DynamoDB using AWS DMS in the AWS Cloud\" is incorrect. This is the wrong method, the Solutions Architect should use the SCT to extract and load to Snowball Edge and then AWS DMS in the AWS Cloud.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html</a></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.DynamoDB.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.DynamoDB.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 11503,
                        "content": "<p>Setup an AWS Direct Connect and migrate the database to Amazon DynamoDB using the AWS Database Migration Service (DMS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11504,
                        "content": "<p>Enable compression on the MongoDB database and use the AWS Database Migration Service (DMS) to directly migrate the database to Amazon DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 11505,
                        "content": "<p>Use the AWS Database Migration Service (DMS) to extract and load the data to an AWS Snowball Edge device. Complete the migration to Amazon DynamoDB using AWS DMS in the AWS Cloud</p>",
                        "isValid": false
                    },
                    {
                        "id": 11506,
                        "content": "<p>Use the Schema Conversion Tool (SCT) to extract and load the data to an AWS Snowball Edge device. Use the AWS Database Migration Service (DMS) to migrate the data to Amazon DynamoDB</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2756,
            "attributes": {
                "createdAt": "2023-09-09T20:40:10.763Z",
                "updatedAt": "2023-09-09T20:40:10.763Z",
                "content": "<p>A Solutions Architect has been tasked with building an application which stores images to be used for a website. The website will be accessed by thousands of customers. The images within the application need to be able to be transformed and processed as they are being retrieved. The solutions architect would prefer to use managed services to achieve this, and the solution should be highly available and scalable, and be able to serve users from around the world with low latency.</p><p><br></p><p>Which scenario represents the easiest solution for this task?</p>",
                "answerExplanation": "<p>With S3 Object Lambda you can add your own code to S3 GET requests to modify and process data as it is returned to an application. For the first time, you can use custom code to modify the data returned by standard S3 GET requests to filter rows, dynamically resize images, redact confidential data, and much more. Powered by AWS Lambda functions, your code runs on infrastructure that is fully managed by AWS, eliminating the need to create and store derivative copies of your data or to run expensive proxies, all with no changes required to your applications.</p><p><strong>CORRECT: </strong>\"Store the images in Amazon S3, behind a CloudFront distribution. Use S3 Object Lambda to transform and process the images whenever a GET request is initiated on an object” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the images in a DynamoDB table, with DynamoDB Global Tables enabled. Provision a Lambda function to process the data on demand as it leaves the table” is incorrect. DynamoDB is not as well designed for Write Once Read Many workloads and adding a Lambda function to the DynamoDB table takes more manual provisioning of resources than using S3 Object Lambda.</p><p><strong>INCORRECT:</strong> \"Store the images in Amazon S3, behind a CloudFront distribution. Use S3 Event Notifications to connect to a Lambda function to process and transform the images when a GET request is initiated on an object” is incorrect. This would work; however it is easier to use S3 Object Lambda as this manages the Lambda function for you.</p><p><strong>INCORRECT:</strong> \"Store the images in a DynamoDB table, with DynamoDB Accelerator enabled. Use Amazon EventBridge to pass the data into an event bus as it is retrieved from DynamoDB and use AWS Lambda to process the data” is incorrect. DynamoDB is not as well designed for Write Once Read Many workloads and adding a Lambda function to the DynamoDB table takes more manual provisioning of resources than using S3 Object Lambda.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/features/object-lambda/\">https://aws.amazon.com/s3/features/object-lambda/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 11507,
                        "content": "<p>Store the images in a DynamoDB table, with DynamoDB Global Tables enabled. Provision a Lambda function to process the data on demand as it leaves the table.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11508,
                        "content": "<p>Store the images in Amazon S3, behind a CloudFront distribution. Use S3 Event Notifications to connect to a Lambda function to process and transform the images when a GET request is initiated on an object.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11509,
                        "content": "<p>Store the images in a DynamoDB table, with DynamoDB Accelerator enabled. Use Amazon EventBridge to pass the data into an event bus as it is retrieved from DynamoDB and use AWS Lambda to process the data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11510,
                        "content": "<p>Store the images in Amazon S3, behind a CloudFront distribution. Use S3 Object Lambda to transform and process the images whenever a GET request is initiated on an object.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2757,
            "attributes": {
                "createdAt": "2023-09-09T20:40:10.840Z",
                "updatedAt": "2023-09-09T20:40:10.840Z",
                "content": "<p>A company is migrating a decoupled application to AWS. The application uses a message broker based on the MQTT protocol. The application will be migrated to Amazon EC2 instances and the solution for the message broker must not require rewriting application code.</p><p>Which AWS service can be used for the migrated message broker?</p>",
                "answerExplanation": "<p>Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Connecting current applications to Amazon MQ is easy because it uses industry-standard APIs and protocols for messaging, including JMS, NMS, AMQP, STOMP, MQTT, and WebSocket. Using standards means that in most cases, there’s no need to rewrite any messaging code when you migrate to AWS.</p><p><strong>CORRECT: </strong>\"Amazon MQ\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon SQS\" is incorrect. This is an Amazon proprietary service and does not support industry-standard messaging APIs and protocols.</p><p><strong>INCORRECT:</strong> \"Amazon SNS\" is incorrect. This is a notification service not a message bus.</p><p><strong>INCORRECT:</strong> \"AWS Step Functions\" is incorrect. This is a workflow orchestration service, not a message bus.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/amazon-mq/\">https://aws.amazon.com/amazon-mq/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11511,
                        "content": "<p>AWS Step Functions</p>",
                        "isValid": false
                    },
                    {
                        "id": 11512,
                        "content": "<p>Amazon SNS</p>",
                        "isValid": false
                    },
                    {
                        "id": 11513,
                        "content": "<p>Amazon MQ</p>",
                        "isValid": true
                    },
                    {
                        "id": 11514,
                        "content": "<p>Amazon SQS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2758,
            "attributes": {
                "createdAt": "2023-09-09T20:40:10.936Z",
                "updatedAt": "2023-09-09T20:40:10.936Z",
                "content": "<p>A company has 200 TB of video files stored in an on-premises data center that must be moved to the AWS Cloud within the next four weeks. The company has around 50 Mbps of available bandwidth on an Internet connection for performing the transfer.</p><p>What is the MOST cost-effective solution for moving the data within the required timeframe?</p>",
                "answerExplanation": "<p>To move 200 TB of data over a 50 Mbps link would take over 300 days. Therefore, the solution must avoid the Internet link. The most cost-effective solution is to use multiple AWS Snowball devices to migrate the data to AWS.</p><p>Snowball devices are shipped to your data center where you can load the data and then ship it back to AWS. This avoids the Internet connection and utilizes local high-bandwidth network connections to load the data.</p><p><strong>CORRECT: </strong>\"Order multiple AWS Snowball devices to migrate the data to AWS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 Transfer Acceleration to securely upload the data\" is incorrect. This would use the Internet link and would not meet the required timeframe.</p><p><strong>INCORRECT:</strong> \"Create a virtual private gateway and connect a VPN to upload the data\" is incorrect. This would also use the Internet link and would not meet the required timeframe.</p><p><strong>INCORRECT:</strong> \"Use AWS Snowmobile to migrate the data to AWS\" is incorrect. This is a very large device on the back of a truck that is used for moving huge quantities of data. It would be very expensive for moving just 200 TB of data.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/snowball/features/\">https://aws.amazon.com/snowball/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11515,
                        "content": "<p>Use AWS Snowmobile to migrate the data to AWS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11516,
                        "content": "<p>Order multiple AWS Snowball devices to migrate the data to AWS.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11517,
                        "content": "<p>Use Amazon S3 Transfer Acceleration to securely upload the data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11518,
                        "content": "<p>Create a virtual private gateway and connect a VPN to upload the data.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2759,
            "attributes": {
                "createdAt": "2023-09-09T20:40:11.028Z",
                "updatedAt": "2023-09-09T20:40:11.028Z",
                "content": "<p>A company needs to transfer data from an Amazon EC2 instance to an Amazon S3 bucket. The company must prevent API calls and data from being routed over the public internet and must use a private connection. Only the single EC2 instance can have access to upload data to the S3 bucket.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>You can use two types of VPC endpoints to access Amazon S3: <em>gateway endpoints</em> and <em>interface endpoints</em> (using AWS PrivateLink). A <em>gateway endpoint</em> is a gateway that you specify in your route table to access Amazon S3 from your VPC over the AWS network. <em>Interface endpoints</em> extend the functionality of gateway endpoints by using private IP addresses to route requests to Amazon S3 from within your VPC, on premises, or from a VPC in another AWS Region using VPC peering or AWS Transit Gateway.</p><p>Using an Interface endpoint to grant access to your S3 bucket from your EC2 instance is a safe and reliable way of traversing the AWS Global Backbone, instead of moving data over the public internet. Adding a resource policy to only allow the EC2 instance IAM role will lockdown the access to this EC2 instance only.</p><p><strong>CORRECT: </strong>\"Create an Amazon S3 interface VPC endpoint in the subnet where the EC2 instance is located. Add a resource policy to the S3 bucket to allow only the EC2 instance's IAM role access” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Attach the appropriate security groups to the endpoint and use an S3 Bucket Policy on your S3 bucket to only allow the EC2 instance's IAM role access to the bucket” is incorrect. This would not prevent you from sending your traffic over the public internet, and you would not meet the requirements.</p><p><strong>INCORRECT:</strong> \"Run the nslookup tool from inside your EC2 instance to obtain the private IP address of the S3 bucket's service API endpoint. Create a route in your VPCs route table to provide the EC2 instance with direct access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access” is incorrect also. You would not need to use nslookup - as using an interface endpoint manages all of this for you.</p><p><strong>INCORRECT:</strong> \"Obtain the private IP address of the S3 bucket's service API endpoint through the management console. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access” is incorrect. Finding the private IP address of the S3 bucket's service API endpoint is not possible through the console. Also this would not prevent you from sending your traffic over the public internet, and you would not meet the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11519,
                        "content": "<p>Attach the appropriate security groups to the endpoint and use an S3 Bucket Policy on your S3 bucket to only allow the EC2 instance's IAM role access to the bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11520,
                        "content": "<p>Create an Amazon S3 interface VPC endpoint in the subnet where the EC2 instance is located. Add a resource policy to the S3 bucket to allow only the EC2 instance's IAM role access.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11521,
                        "content": "<p>Run the nslookup tool from inside your EC2 instance to obtain the private IP address of the S3 bucket's service API endpoint. Create a route in your VPCs route table to provide the EC2 instance with direct access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11522,
                        "content": "<p>Obtain the private IP address of the S3 bucket's service API endpoint through the management console. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2760,
            "attributes": {
                "createdAt": "2023-09-09T20:40:11.099Z",
                "updatedAt": "2023-09-09T20:40:11.099Z",
                "content": "<p>As part of a company’s shift to the AWS cloud, they need to gain an insight into their total on-premises footprint. They have discovered that they are currently struggling with managing their software licenses. They would like to maintain a hybrid cloud setup, with some of their licenses stored in the cloud with some stored on-premises.</p><p><br></p><p>What actions should be taken to ensure they are managing the licenses appropriately going forward?</p>",
                "answerExplanation": "<p>AWS License Manager makes it easier to manage your software licenses from vendors such as Microsoft, SAP, Oracle, and IBM across AWS and on-premises environments. AWS License Manager lets administrators create customized licensing rules that mirror the terms of their licensing agreements.</p><p><strong>CORRECT: </strong>\"Use AWS License Manager to manage the software licenses\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Secrets Manager to store the licenses as secrets to ensure they are stored securely\" is incorrect. AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. This does not include license keys.</p><p><strong>INCORRECT:</strong> \"Use the AWS Key Management Service to treat the license key safely and store it securely\" is incorrect. AWS Key Management Service (AWS KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications, not license keys.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 with governance lock to manage the storage of the licenses\" is incorrect. Amazon S3 is not designed to store software licenses.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/license-manager/\">https://aws.amazon.com/license-manager/</a></p>",
                "options": [
                    {
                        "id": 11523,
                        "content": "<p>Use AWS License Manager to manage the software licenses</p>",
                        "isValid": true
                    },
                    {
                        "id": 11524,
                        "content": "<p>Use Amazon S3 with governance lock to manage the storage of the licenses</p>",
                        "isValid": false
                    },
                    {
                        "id": 11525,
                        "content": "<p>Use AWS Secrets Manager to store the licenses as secrets to ensure they are stored securely</p>",
                        "isValid": false
                    },
                    {
                        "id": 11526,
                        "content": "<p>Use the AWS Key Management Service to treat the license key safely and store it securely</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2761,
            "attributes": {
                "createdAt": "2023-09-09T20:40:11.185Z",
                "updatedAt": "2023-09-09T20:40:11.185Z",
                "content": "<p>A Solutions Architect is designing an application that will run on an Amazon EC2 instance. The application must asynchronously invoke an AWS Lambda function to analyze thousands of .CSV files. The services should be decoupled.</p><p>Which service can be used to decouple the compute services?</p>",
                "answerExplanation": "<p>You can use a Lambda function to process Amazon Simple Notification Service notifications. Amazon SNS supports Lambda functions as a target for messages sent to a topic. This solution decouples the Amazon EC2 application from Lambda and ensures the Lambda function is invoked.</p><p><strong>CORRECT: </strong>\"Amazon SNS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon SWF\" is incorrect. The Simple Workflow Service (SWF) is used for process automation. It is not well suited to this requirement.</p><p><strong>INCORRECT:</strong> \"Amazon Kinesis\" is incorrect as this service is used for ingesting and processing real time streaming data, it is not a suitable service to be used solely for invoking a Lambda function.</p><p><strong>INCORRECT:</strong> \"Amazon OpsWorks\" is incorrect as this service is used for configuration management of systems using Chef or Puppet.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 11527,
                        "content": "<p>Amazon Kinesis</p>",
                        "isValid": false
                    },
                    {
                        "id": 11528,
                        "content": "<p>Amazon SNS</p>",
                        "isValid": true
                    },
                    {
                        "id": 11529,
                        "content": "<p>Amazon SWF</p>",
                        "isValid": false
                    },
                    {
                        "id": 11530,
                        "content": "<p>Amazon OpsWorks</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2762,
            "attributes": {
                "createdAt": "2023-09-09T20:40:11.278Z",
                "updatedAt": "2023-09-09T20:40:11.278Z",
                "content": "<p>A company is migrating an eCommerce application into the AWS Cloud. The application uses an SQL database, and the database will be migrated to Amazon RDS. A Solutions Architect has been asked to recommend a method to attain sub-millisecond responses to common read requests.</p><p>What should the solutions architect recommend?</p>",
                "answerExplanation": "<p>Amazon ElastiCache is a fully managed in-memory data store and cache service. ElastiCache can be used to cache requests to an Amazon RDS database through application configuration. This can greatly improve performance as ElastiCache can return responses to queries with sub-millisecond latency.</p><p><strong>CORRECT: </strong>\"Deploy a database cache using Amazon ElastiCache\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Deploy a database cache using Amazon DynamoDB Accelerator\" is incorrect. DynamoDB DAX cannot be used with RDS or SQL databases.</p><p><strong>INCORRECT:</strong> \"Deploy Amazon RDS read replicas\" is incorrect. Read replicas will not provide sub-millisecond response times to queries.</p><p><strong>INCORRECT:</strong> \"Use Amazon EBS Provisioned IOPS volumes\" is incorrect. This will not improve response times for queries to the database to the level required.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 11531,
                        "content": "<p>Deploy Amazon RDS read replicas.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11532,
                        "content": "<p>Deploy a database cache using Amazon DynamoDB Accelerator.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11533,
                        "content": "<p>Deploy a database cache using Amazon ElastiCache.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11534,
                        "content": "<p>Use Amazon EBS Provisioned IOPS volumes.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2763,
            "attributes": {
                "createdAt": "2023-09-09T20:40:11.349Z",
                "updatedAt": "2023-09-09T20:40:11.349Z",
                "content": "<p>A company runs a legacy application on an Amazon EC2 Linux instance. The application code cannot be modified, and the system cannot run on more than one instance. A Solutions Architect must design a resilient solution that can improve the recovery time for the system.</p><p>What should the Solutions Architect recommend to meet these requirements?</p>",
                "answerExplanation": "<p>A RAID array uses multiple EBS volumes to improve performance or redundancy. When fault tolerance is more important than I/O performance a RAID 1 array should be used which creates a mirror of your data for extra redundancy.</p><p>The following table summarizes the differences between RAID 0 and RAID 1:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-06-02_22-58-17-415cd9d4684946806d0e3e4daa6dfe99.jpg\"></p><p><strong>CORRECT: </strong>Launch the EC2 instance with two Amazon EBS volumes and configure RAID 1is the correct answer.</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instance with two Amazon EBS volumes and configure RAID 0\" is incorrect. RAID 0 is used for striping which improves performance but not redundancy.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm to recover the EC2 instance in case of failure\" is incorrect. This does not improve recovery time it just attempts to fix issues relating to the underlying hardware.</p><p><strong>INCORRECT:</strong> \"Deploy the EC2 instance in a cluster placement group in an Availability Zone\" is incorrect. You cannot gain any advantages by deploying a single instance into a cluster placement group.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 11535,
                        "content": "<p>Create an Amazon CloudWatch alarm to recover the EC2 instance in case of failure.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11536,
                        "content": "<p>Launch the EC2 instance with two Amazon EBS volumes and configure RAID 0.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11537,
                        "content": "<p>Deploy the EC2 instance in a cluster placement group in an Availability Zone.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11538,
                        "content": "<p>Launch the EC2 instance with two Amazon EBS volumes and configure RAID 1.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2764,
            "attributes": {
                "createdAt": "2023-09-09T20:40:11.429Z",
                "updatedAt": "2023-09-09T20:40:11.429Z",
                "content": "<p>A company is migrating an application that comprises a web tier and a MySQL database into the AWS Cloud. The web tier will run on EC2 instances, and the database tier will run on an Amazon RDS for MySQL DB instance. Customers access the application via the Internet using dynamic IP addresses.</p><p>How should the Solutions Architect configure the security groups to enable connectivity to the application?</p>",
                "answerExplanation": "<p>The customers are connecting from dynamic IP addresses so we must assume they will be changing regularly. Therefore, it is not possible to restrict access from the IP addresses of the customers. The security group for the web tier must allow 443 (HTTPS) from 0.0.0.0/0, which means any IP source IP address.</p><p>For the database tier, this can best be secured by restricting access to the web tier security group. The port required to be opened is 3306 for MySQL.</p><p><strong>CORRECT: </strong>\"Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0. Configure the security group for the DB tier to allow inbound traffic on port 3306 from the web tier security group\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure the security group for the web tier to allow inbound traffic on port 443 from the IP addresses of the customers. Configure the security group for the DB tier to allow inbound traffic on port 3306 from the IP addresses of the customers\" is incorrect.</p><p>The customer IP addresses are dynamic, so it is not possible to restrict access using IP addresses. Access to the DB tier should be restricted to the web tier, there is no need to enable end-user access.</p><p><strong>INCORRECT:</strong> \"Configure the security group for the web tier to allow inbound traffic on port 443 from the IP addresses of the customers. Configure the security group for the DB tier to allow inbound traffic on port 3306 from the web tier security group\" is incorrect.</p><p>The customer IP addresses are dynamic, so it is not possible to restrict access using IP addresses.</p><p><strong>INCORRECT:</strong> \"Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0. Configure the security group for the DB tier to allow inbound traffic on port 3306 from 0.0.0.0/0\" is incorrect.</p><p>Access to the DB tier should be restricted to the web tier, there is no need to enable access from the internet.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11539,
                        "content": "<p>Configure the security group for the web tier to allow inbound traffic on port 443 from the IP addresses of the customers. Configure the security group for the DB tier to allow inbound traffic on port 3306 from the IP addresses of the customers.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11540,
                        "content": "<p>Configure the security group for the web tier to allow inbound traffic on port 443 from the IP addresses of the customers. Configure the security group for the DB tier to allow inbound traffic on port 3306 from the web tier security group.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11541,
                        "content": "<p>Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0. Configure the security group for the DB tier to allow inbound traffic on port 3306 from 0.0.0.0/0.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11542,
                        "content": "<p>Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0. Configure the security group for the DB tier to allow inbound traffic on port 3306 from the web tier security group.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2765,
            "attributes": {
                "createdAt": "2023-09-09T20:40:11.517Z",
                "updatedAt": "2023-09-09T20:40:11.517Z",
                "content": "<p>A company has over 200 TB of log files in an Amazon S3 bucket. The company must process the files using a Linux-based software application that will extract and summarize data from the log files and store the output in a separate Amazon S3 bucket. The company needs to minimize data transfer charges associated with the processing of this data.</p><p>How can a Solutions Architect meet these requirements?</p>",
                "answerExplanation": "<p>The software application must be installed on a Linux operating system so we must use Amazon EC2 or an on-premises VM. To avoid data charges however, we must ensure that the data does not egress the AWS Region. The best solution to avoid the egress data charges is to use an Amazon EC2 instance in the same Region as the S3 bucket that contains the log files. The processed output files must also be stored in a bucket in the same Region to avoid any data going out from EC2 to another Region.</p><p><strong>CORRECT: </strong>\"Launch an Amazon EC2 instance in the same Region as the S3 bucket. Process the log files and upload the output to another S3 bucket in the same Region\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an on-premises virtual machine for processing the data. Retrieve the log files from the S3 bucket and upload the output to another S3 bucket in the same Region\" is incorrect. The data would need to egress the AWS Region incurring data transfer charges in this configuration.</p><p><strong>INCORRECT:</strong> \"Launch an Amazon EC2 instance in the same Region as the S3 bucket. Process the log files and upload the output to another S3 bucket in a different Region\" is incorrect. The processed data would be going from the EC2 instance to a bucket in a different Region which would incur data transfer charges.</p><p><strong>INCORRECT:</strong> \"Connect an AWS Lambda function to the S3 bucket via a VPC endpoint. Process the log files and store the output to another S3 bucket in the same Region\" is incorrect. You cannot install a Linux-based software application on AWS Lambda.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/pricing/\">https://aws.amazon.com/s3/pricing/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11543,
                        "content": "<p>Connect an AWS Lambda function to the S3 bucket via a VPC endpoint. Process the log files and store the output to another S3 bucket in the same Region.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11544,
                        "content": "<p>Launch an Amazon EC2 instance in the same Region as the S3 bucket. Process the log files and upload the output to another S3 bucket in the same Region.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11545,
                        "content": "<p>Use an on-premises virtual machine for processing the data. Retrieve the log files from the S3 bucket and upload the output to another S3 bucket in the same Region.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11546,
                        "content": "<p>Launch an Amazon EC2 instance in the same Region as the S3 bucket. Process the log files and upload the output to another S3 bucket in a different Region.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2766,
            "attributes": {
                "createdAt": "2023-09-09T20:40:11.596Z",
                "updatedAt": "2023-09-09T20:40:11.596Z",
                "content": "<p>A dynamic website runs on Amazon EC2 instances behind an Application Load Balancer (ALB). Users are distributed around the world, and many are reporting poor website performance. The company uses Amazon Route 53 for DNS.</p><p>Which set of actions will improve website performance while minimizing cost?</p>",
                "answerExplanation": "<p>The most cost-effective option for improving performance is to create an Amazon CloudFront distribution. CloudFront can be used to serve both static and dynamic content. This solution will ensure that wherever users are located they will experience improved performance due to the caching of content and the usage of the AWS global network.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudFront distribution and configure the ALB as an origin. Then update the Amazon Route 53 record to point to the CloudFront distribution\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Launch new EC2 instances running the website and ALBs in different Regions. Use AWS Global Accelerator to direct connections to the closest Region\" is incorrect. This is a more costly solution as there are more EC2 instances, ALBs, and Global Accelerator. Using CloudFront would be a better solution for this use case.</p><p><strong>INCORRECT:</strong> \"Create a latency-based Amazon Route 53 record for the ALB. Then launch new EC2 instances with larger instance sizes and register the instances with the ALB\" is incorrect. With only one ALB latency-based record serves no purpose. Additionally, using larger instances sizes may not assist as it does not reduce latency for global users.</p><p><strong>INCORRECT:</strong> \"Host the website in an Amazon S3 bucket and delete the ALB and EC2 instances. Enable transfer acceleration and update the Amazon Route 53 record to point to the S3 bucket\" is incorrect. Transfer acceleration offers performance benefits for uploading and downloading content to/from S3 buckets but the S3 bucket can only serve static content, not a dynamic website.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/dynamic-content/\">https://aws.amazon.com/cloudfront/dynamic-content/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11547,
                        "content": "<p>Launch new EC2 instances running the website and ALBs in different Regions. Use AWS Global Accelerator to direct connections to the closest Region.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11548,
                        "content": "<p>Create an Amazon CloudFront distribution and configure the ALB as an origin. Then update the Amazon Route 53 record to point to the CloudFront distribution.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11549,
                        "content": "<p>Create a latency-based Amazon Route 53 record for the ALB. Then launch new EC2 instances with larger instance sizes and register the instances with the ALB.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11550,
                        "content": "<p>Host the website in an Amazon S3 bucket and delete the ALB and EC2 instances. Enable transfer acceleration and update the Amazon Route 53 record to point to the S3 bucket.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2767,
            "attributes": {
                "createdAt": "2023-09-09T20:40:11.681Z",
                "updatedAt": "2023-09-09T20:40:11.681Z",
                "content": "<p>Every time an item in an Amazon DynamoDB table is modified a record must be retained for compliance reasons. What is the most efficient solution to recording this information?</p>",
                "answerExplanation": "<p>Amazon DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real time.</p><p>For example, in the diagram below a DynamoDB stream is being consumed by a Lambda function which processes the item data and records a record in CloudWatch Logs:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_19-55-43-04c370b6a009c1594a77bd12b9499c3d.png\"></p><p><strong>CORRECT: </strong>\"Enable DynamoDB Streams. Configure an AWS Lambda function to poll the stream and record the modified item data to an Amazon S3 bucket\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable Amazon CloudWatch Logs. Configure an AWS Lambda function to monitor the log files and record deleted item data to an Amazon S3 bucket\" is incorrect. The deleted item data will not be recorded in CloudWatch Logs.</p><p><strong>INCORRECT:</strong> \"Enable Amazon CloudTrail. Configure an Amazon EC2 instance to monitor activity in the CloudTrail log files and record changed items in another DynamoDB table\" is incorrect. CloudTrail records API actions so it will not record the data from the item that was modified.</p><p><strong>INCORRECT:</strong> \"Enable DynamoDB Global Tables. Enable DynamoDB streams on the multi-region table and save the output directly to an Amazon S3 bucket\" is incorrect. Global Tables is used for creating a multi-region, multi-master database. It is of no additional value for this requirement as you could just enable DynamoDB streams on the main table. You also cannot save modified data straight to an S3 bucket.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 11551,
                        "content": "<p>Enable DynamoDB Streams. Configure an AWS Lambda function to poll the stream and record the modified item data to an Amazon S3 bucket</p>",
                        "isValid": true
                    },
                    {
                        "id": 11552,
                        "content": "<p>Enable Amazon CloudWatch Logs. Configure an AWS Lambda function to monitor the log files and record deleted item data to an Amazon S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 11553,
                        "content": "<p>Enable DynamoDB Global Tables. Enable DynamoDB streams on the multi-region table and save the output directly to an Amazon S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 11554,
                        "content": "<p>Enable Amazon CloudTrail. Configure an Amazon EC2 instance to monitor activity in the CloudTrail log files and record changed items in another DynamoDB table</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2768,
            "attributes": {
                "createdAt": "2023-09-09T20:40:11.756Z",
                "updatedAt": "2023-09-09T20:40:11.756Z",
                "content": "<p>A computer scientist working for a university is looking to build a machine learning application which will use telemetry data to predict weather for a given area at a given time. This application would benefit from using managed services and will need to find a solution which uses third party data within the application.</p><p><br></p><p>Which of the following combinations of services will deliver the best solution?</p>",
                "answerExplanation": "<p>Amazon SageMaker allows you to build, train, and deploy machine learning models for any use case with fully managed infrastructure, tools, and workflows. AWS Data Exchange allows you to gain access to third party data sets across Automotive, Financial Services, Gaming, Healthcare &amp; Life Sciences, Manufacturing, Marketing, Media &amp; Entertainment, Retail, and many more industries.</p><p><strong>CORRECT: </strong>\"Use Amazon SageMaker to build the machine learning part of the application and use AWS Data Exchange to gain access to the third-party telemetry data” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon SageMaker to build the machine learning part of the application and use AWS DataSync to gain access to the third-party telemetry data” is incorrect. AWS DataSync is a secure, online service that automates and accelerates moving data between on-premises and AWS storage services. It does not give access to third party data.</p><p><strong>INCORRECT:</strong> \"Use a TensorFlow AMI from the AWS Marketplace to build the machine learning part of the application and use AWS DataSync to gain access to the third-party telemetry data” is incorrect. Building an EC2 instance from a TensorFlow AMI would not involve using managed services and AWS DataSync is a secure, online service that automates and accelerates moving data between on-premises and AWS storage services. It does not give access to third party data.</p><p><strong>INCORRECT:</strong> \"Use a TensorFlow AMI from the AWS Marketplace to build the machine learning part of the application and use AWS Data Exchange to gain access to the third-party telemetry data” is incorrect. Building an EC2 instance from a TensorFlow AMI would not involve using managed services.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/data-exchange/\">https://aws.amazon.com/data-exchange/</a></p>",
                "options": [
                    {
                        "id": 11555,
                        "content": "<p>Use Amazon SageMaker to build the machine learning part of the application and use AWS DataSync to gain access to the third-party telemetry data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11556,
                        "content": "<p>Use Amazon SageMaker to build the machine learning part of the application and use AWS Data Exchange to gain access to the third-party telemetry data.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11557,
                        "content": "<p>Use a TensorFlow AMI from the AWS Marketplace to build the machine learning part of the application and use AWS Data Exchange to gain access to the third-party telemetry data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11558,
                        "content": "<p>Use a TensorFlow AMI from the AWS Marketplace to build the machine learning part of the application and use AWS DataSync to gain access to the third-party telemetry data.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2769,
            "attributes": {
                "createdAt": "2023-09-09T20:40:11.834Z",
                "updatedAt": "2023-09-09T20:40:11.834Z",
                "content": "<p>A company is planning to use an Amazon S3 bucket to store a large volume of customer transaction data. The data will be structured into a hierarchy of objects, and they require a solution for running complex queries as quickly as possible. The solution must minimize operational overhead.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to setup or manage, and you can start analyzing data immediately. While Amazon Athena is ideal for quick, ad-hoc querying, it can also handle complex analysis, including large joins, window functions, and arrays.</p><p>Athena is the fastest way to query the data in Amazon S3 and offers the lowest operational overhead as it is a fully serverless solution.</p><p><strong>CORRECT: </strong>\"Use Amazon Athena on Amazon S3 to perform the queries\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS Data Pipeline to process and move the data to Amazon EMR and then perform the queries\" is incorrect. Amazon EMR is not required and would represent a more operationally costly solution.</p><p><strong>INCORRECT:</strong> \"Use AWS Elasticsearch on Amazon S3 to perform the queries\" is incorrect. Elasticsearch cannot perform SQL queries and join tables for data in Amazon S3.</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to transform the data into Amazon Redshift tables and then perform the queries\" is incorrect. RedShift is not required and would represent a more operationally costly solution.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/athena/faqs/\">https://aws.amazon.com/athena/faqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
                "options": [
                    {
                        "id": 11559,
                        "content": "<p>Use AWS Elasticsearch on Amazon S3 to perform the queries.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11560,
                        "content": "<p>Use AWS Data Pipeline to process and move the data to Amazon EMR and then perform the queries.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11561,
                        "content": "<p>Use AWS Glue to transform the data into Amazon Redshift tables and then perform the queries.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11562,
                        "content": "<p>Use Amazon Athena on Amazon S3 to perform the queries.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2770,
            "attributes": {
                "createdAt": "2023-09-09T20:40:11.934Z",
                "updatedAt": "2023-09-09T20:40:11.934Z",
                "content": "<p>The log files of a proprietary application must be analyzed. The log files are stored in an Amazon S3 bucket in JSON format. Query execution will be on-demand and simple. It is essential for a solutions architect to perform the analysis with minimal changes to the existing architecture.</p><p>How can a solutions architect meet these requirements with the LOWEST amount of operational overhead?</p>",
                "answerExplanation": "<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. The Solutions Architect could easily use Amazon Athena to query the logs on demand without refactoring any other parts of the application.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-09-21_03-00-54-8f24d03c7ef727d3541514ab5addccab.jpg\"><p><strong>CORRECT: </strong>\"Use Amazon Athena to query and analyze the data in Amazon S3 using standard SQL queries on demand\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Redshift to place all the content in one place and run the SQL queries as and when required” is incorrect. This would take a significant amount of refactoring by moving all the application log data into Amazon RedShift.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Logs for log storage. Run SQL queries on demand from the Amazon CloudWatch console” is incorrect. Though you can use CloudWatch Logs Insights to run queries on log files, these are not SQL queries, and this is not an efficient solution as it will require a lot of refactoring.</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries on demand” is incorrect. This would take a significant amount of refactoring by moving all the application log data into AWS Glue and using an EMR cluster to analyze the logs.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/athena\">https://aws.amazon.com/athena</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
                "options": [
                    {
                        "id": 11563,
                        "content": "<p>Use Amazon Athena to query and analyze the data in Amazon S3 using standard SQL queries on demand.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11564,
                        "content": "<p>Use Amazon Redshift to place all the content in one place and run the SQL queries as and when required.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11565,
                        "content": "<p>Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries on demand.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11566,
                        "content": "<p>Use Amazon CloudWatch Logs for log storage. Run SQL queries on demand from the Amazon CloudWatch console.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2771,
            "attributes": {
                "createdAt": "2023-09-09T20:40:12.009Z",
                "updatedAt": "2023-09-09T20:40:12.009Z",
                "content": "<p>An application is being monitored using Amazon GuardDuty. A Solutions Architect needs to be notified by email of medium to high severity events. How can this be achieved?</p>",
                "answerExplanation": "<p>A CloudWatch Events rule can be used to set up automatic email notifications for Medium to High Severity findings to the email address of your choice. You simply create an Amazon SNS topic and then associate it with an Amazon CloudWatch events rule.</p><p>Note: step by step procedures for how to set this up can be found in the article linked in the references below.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudWatch events rule that triggers an Amazon SNS topic\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon CloudWatch alarm that triggers based on a GuardDuty metric\" is incorrect. There is no metric for GuardDuty that can be used for specific findings.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Logs rule that triggers an AWS Lambda function\" is incorrect. CloudWatch logs is not the right CloudWatch service to use. CloudWatch events is used for reacting to changes in service state.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon CloudTrail alarm the triggers based on GuardDuty API activity\" is incorrect. CloudTrail cannot be used to trigger alarms based on GuardDuty API activity.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_findings_cloudwatch.html\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_findings_cloudwatch.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 11567,
                        "content": "<p>Create an Amazon CloudWatch Logs rule that triggers an AWS Lambda function</p>",
                        "isValid": false
                    },
                    {
                        "id": 11568,
                        "content": "<p>Configure an Amazon CloudTrail alarm the triggers based on GuardDuty API activity</p>",
                        "isValid": false
                    },
                    {
                        "id": 11569,
                        "content": "<p>Create an Amazon CloudWatch events rule that triggers an Amazon SNS topic</p>",
                        "isValid": true
                    },
                    {
                        "id": 11570,
                        "content": "<p>Configure an Amazon CloudWatch alarm that triggers based on a GuardDuty metric</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2772,
            "attributes": {
                "createdAt": "2023-09-09T20:40:12.082Z",
                "updatedAt": "2023-09-09T20:40:12.082Z",
                "content": "<p>A company is testing a new web application that runs on Amazon EC2 instances. A Solutions Architect is performing load testing and must be able to analyze the performance of the web application with a granularity of 1 minute.</p><p>What should the Solutions Architect do to meet this requirement?</p>",
                "answerExplanation": "<p>By default, your instance is enabled for basic monitoring. You can optionally enable detailed monitoring. After you enable detailed monitoring, the Amazon EC2 console displays monitoring graphs with a 1-minute period for the instance.</p><p>The following describes the data interval and charge for basic and detailed monitoring for instances:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-06-02_23-23-28-82670aaf7831d936e2f4a75866c1a1a1.jpg\"></p><p><strong>CORRECT: </strong>\"Enable detailed monitoring on all EC2 instances. Use Amazon CloudWatch metrics to perform the analysis\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Send Amazon CloudWatch logs to Amazon S3. Use Amazon Athena to perform the analysis\" is incorrect. You must enable detailed monitoring to get data in 1-minute periods.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function to fetch EC2 logs from Amazon CloudWatch Logs. Use Amazon CloudWatch metrics to perform the analysis\" is incorrect. There is no need to use a Lambda function to retrieve the logs and detailed monitoring must still be enabled.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudTrail trail and log data events. Use Amazon Athena to query the CloudTrail logs\" is incorrect. This will not assist with gathering the required data from EC2 instances. Detailed monitoring must be enabled instead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 11571,
                        "content": "<p>Create an AWS Lambda function to fetch EC2 logs from Amazon CloudWatch Logs. Use Amazon CloudWatch metrics to perform the analysis.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11572,
                        "content": "<p>Enable detailed monitoring on all EC2 instances. Use Amazon CloudWatch metrics to perform the analysis.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11573,
                        "content": "<p>Create an AWS CloudTrail trail and log data events. Use Amazon Athena to query the CloudTrail logs.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11574,
                        "content": "<p>Send Amazon CloudWatch logs to Amazon S3. Use Amazon Athena to perform the analysis.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2773,
            "attributes": {
                "createdAt": "2023-09-09T20:40:12.154Z",
                "updatedAt": "2023-09-09T20:40:12.154Z",
                "content": "<p>A HR application stores employment records on Amazon S3. Regulations mandate the records are retained for seven years. Once created the records are accessed infrequently for the first three months and then must be available within 10 minutes if required thereafter.</p><p>Which lifecycle action meets the requirements whilst MINIMIZING cost?</p>",
                "answerExplanation": "<p>The most cost-effective solution is to first store the data in S3 Standard-IA where it will be infrequently accessed for the first three months. Then, after three months expires, transition the data to S3 Glacier where it can be stored at lower cost for the remainder of the seven year period. Expedited retrieval can bring retrieval times down to 1-5 minutes.</p><p><strong>CORRECT: </strong>\"Store the data in S3 Standard-IA for 3 months, then transition to S3 Glacier\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Store the data in S3 Standard for 3 months, then transition to S3 Glacier\" is incorrect. S3 Standard is more costly than S3 Standard-IA and the data is only accessed infrequently.</p><p><strong>INCORRECT:</strong> \"Store the data in S3 Standard for 3 months, then transition to S3 Standard-IA\" is incorrect. Neither storage class in this answer is the most cost-effective option.</p><p><strong>INCORRECT:</strong> \"Store the data in S3 Intelligent Tiering for 3 months, then transition to S3 Standard-IA\" is incorrect. Intelligent tiering moves data between tiers based on access patterns, this is more costly and better suited to use cases that are unknown or unpredictable.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p><p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/downloading-an-archive-two-steps.html#api-downloading-an-archive-two-steps-retrieval-options\">https://docs.aws.amazon.com/amazonglacier/latest/dev/downloading-an-archive-two-steps.html#api-downloading-an-archive-two-steps-retrieval-options</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11575,
                        "content": "<p>Store the data in S3 Standard for 3 months, then transition to S3 Standard-IA</p>",
                        "isValid": false
                    },
                    {
                        "id": 11576,
                        "content": "<p>Store the data in S3 Standard for 3 months, then transition to S3 Glacier</p>",
                        "isValid": false
                    },
                    {
                        "id": 11577,
                        "content": "<p>Store the data in S3 Standard-IA for 3 months, then transition to S3 Glacier</p>",
                        "isValid": true
                    },
                    {
                        "id": 11578,
                        "content": "<p>Store the data in S3 Intelligent Tiering for 3 months, then transition to S3 Standard-IA</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2774,
            "attributes": {
                "createdAt": "2023-09-09T20:40:12.226Z",
                "updatedAt": "2023-09-09T20:40:12.226Z",
                "content": "<p>A company observed an increase in Amazon EC2 costs in its most recent bill. The billing team noticed unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions architect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause of the vertical scaling.</p><p>How should the solutions architect generate the information with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>AWS Cost Explorer would be the easiest way to graph this data. Cost Explorer can be accessed easily and has features for filtering billing data and graphing across relevant time periods.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-24-24-b30229192dd6797b5632983864cfeb31.jpg\"><p><strong>CORRECT: </strong>\"Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Budgets to create a budget report and compare EC2 costs based on instance types\" is incorrect.</p><p>AWS Budgets lets you set custom cost and usage budgets that alert you when your budget thresholds are exceeded (or forecasted to be exceeded).</p><p><strong>INCORRECT:</strong> \"Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months\" is incorrect.</p><p>The granularity required is not available in the billing and cost management dashboard unless using the cost and usage report.</p><p><strong>INCORRECT:</strong> \"Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon QuickSight with Amazon S3 as a source to generate an interactive graph based on instance types\" is incorrect. This could provide the required graphs, but it involves much more operational overhead.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/aws-cost-management/aws-cost-explorer/\">https://aws.amazon.com/aws-cost-management/aws-cost-explorer/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cost-management/\">https://digitalcloud.training/aws-cost-management/</a></p>",
                "options": [
                    {
                        "id": 11579,
                        "content": "<p>Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11580,
                        "content": "<p>Use AWS Budgets to create a budget report and compare EC2 costs based on instance types.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11581,
                        "content": "<p>Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon QuickSight with Amazon S3 as a source to generate an interactive graph based on instance types.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11582,
                        "content": "<p>Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2775,
            "attributes": {
                "createdAt": "2023-09-09T20:40:12.306Z",
                "updatedAt": "2023-09-09T20:40:12.306Z",
                "content": "<p>A company uses several Windows Servers as the operating system of choice for all their application servers hosted in their data center. The company wants to move some file servers into the cloud, and keep some in their data center, mounted to the same File System. The company also wants to maintain extremely low latency access to their on-premises data center, across a private network. The company has an AWS Direct Connect connection set up into the us-east-1 Region.</p><p>What should a solutions architect do to meet these requirements?</p>",
                "answerExplanation": "<p>The current AWS Direct connect connection will provide the ability to share a file system between on-premises servers and Amazon EC2 instances in the AWS Cloud. Direct Connect provides low latency access to their on-premises data center, and the company’s use of Windows File Servers necessitates the use of an SMB-based Amazon FSx File System.</p><p><strong>CORRECT: </strong>\"Install an SMB client on to the on-premises servers and mount an Amazon FSx file system to the servers. Mount the same file system to the EC2 instances within the Amazon VPC. Use the existing Direct Connect connection to connect the on-premises data center to the Amazon VPC\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate all the data to Amazon DynamoDB Local. Ensure all users have the appropriate IAM permissions to access the relevant files\" is incorrect. This will not give the company the use of a Windows File Server, and instead give them a NoSQL database. DynamoDB Local is not suitable for this use case.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 on Outposts and mount the S3 File Gateway on to the on-premises servers\" is incorrect. Amazon S3 on Outposts would not provide a hybrid cloud experience as required by the customer, and S3 File Gateway uses a Linux based file system, which is incompatible with the Windows setup the company currently uses.</p><p><strong>INCORRECT:</strong> \"Install an NFS client on to the on-premises servers and mount an Amazon EFS file system to the servers. Mount the same file system to the EC2 instances within the Amazon VPC. Use the existing Direct Connect connection to connect the on-premises data center to the Amazon VPC\" is incorrect.</p><p>Amazon EFS is a file system that is accessed using the NFS protocol and is suitable for Linux clients only. This is not natively supported for Window Servers, making this an unsuitable option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/efs-onpremises.html\">https://docs.aws.amazon.com/efs/latest/ug/efs-onpremises.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 11583,
                        "content": "<p>Migrate all the data to Amazon DynamoDB Local. Ensure all users have the appropriate IAM permissions to access the relevant files.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11584,
                        "content": "<p>Use Amazon S3 on Outposts and mount the S3 File Gateway on to the on-premises servers.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11585,
                        "content": "<p>Install an SMB client on to the on-premises servers and mount an Amazon FSx file system to the servers. Mount the same file system to the EC2 instances within the Amazon VPC. Use the existing Direct Connect connection to connect the on-premises data center to the Amazon VPC.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11586,
                        "content": "<p>Install an NFS client on to the on-premises servers and mount an Amazon EFS file system to the servers. Mount the same file system to the EC2 instances within the Amazon VPC. Use the existing Direct Connect connection to connect the on-premises data center to the Amazon VPC.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2776,
            "attributes": {
                "createdAt": "2023-09-09T20:40:12.379Z",
                "updatedAt": "2023-09-09T20:40:12.379Z",
                "content": "<p>A large company is currently using multiple AWS accounts as part of its cloud deployment model, and these accounts are currently structured using AWS Organizations. A Solutions Architect has been tasked with limiting access to an Amazon S3 bucket to only users of accounts that are enrolled with AWS Organizations. The Solutions Architect wants to avoid listing the many dozens of account IDs in the Bucket policy, as there are many accounts the frequent changes.</p><p>Which strategy meets these requirements with the LEAST amount of effort?</p>",
                "answerExplanation": "<p>The aws:PrincipalOrgID global key provides a simpler alternative to manually listing and updating all the account IDs for all AWS accounts that exist within an Organization. The following Amazon S3 bucket policy allows members of any account in the ‘123456789’ organization to add an object into the ‘mydctbucket’ bucket.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-24_20-46-05-1c8b6106074b932bcc68ed1a200779f1.jpg\"><p><strong>CORRECT: </strong>\"Use the global key of AWS Organizations within a bucket policy using the aws:PrincipalOrgID key to allow access only to accounts which are part of the Organization\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Attribute Based Access Control by referencing Tags of accounts which are either enrolled as part of AWS Organizations, or not\" is incorrect. This could be a viable option, however maintaining an accurate tagging policy as opposed to referencing the PrincipalOrgID would much more difficult.</p><p><strong>INCORRECT:</strong> \"Use AWS Config and AWS Lambda functions to make remediations to the bucket policy as and when new accounts are created and tagged as not being part of AWS Organizations. Update the S3 bucket policy accordingly\" is incorrect.</p><p>Every time an account is added or removed from the organization this workflow would have to fire. This solution would need to be built and maintained, whereas it is much easier to refer to the PrincipalOrgID once and avoid needing to change the Bucket Policy.</p><p><strong>INCORRECT:</strong> \"Add all the non-organizational accounts to an Organizational Unit (OU) and attached a Service Control Policy (SCP) which denies access to the specific Amazon S3 bucket\" is incorrect. You can only use Organization Units (OUs) and Service Control Policies (SCPs) with accounts that are a part of AWS Organizations – meaning this solution could not work.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
                "options": [
                    {
                        "id": 11587,
                        "content": "<p>Use AWS Config and AWS Lambda functions to make remediations to the bucket policy as and when new accounts are created and tagged as not being part of AWS Organizations. Update the S3 bucket policy accordingly.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11588,
                        "content": "<p>Use Attribute Based Access Control by referencing Tags of accounts which are either enrolled as part of AWS Organizations, or not.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11589,
                        "content": "<p>Add all the non-organizational accounts to an Organizational Unit (OU) and attached a Service Control Policy (SCP) which denies access to the specific Amazon S3 bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11590,
                        "content": "<p>Use the global key of AWS Organizations within a bucket policy using the aws:PrincipalOrgID key to allow access only to accounts which are part of the Organization.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2777,
            "attributes": {
                "createdAt": "2023-09-09T20:40:12.453Z",
                "updatedAt": "2023-09-09T20:40:12.453Z",
                "content": "<p>A media company is running a production workload on thousands of EC2 instances which run a custom solution powered by third-party software. This software is subjected to regular updates and patches by the third-party organization.</p><p>How can a solutions architect patch all the instances quickly to remediate a security exposure?</p>",
                "answerExplanation": "<img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-18-39-b8bddb759e545b93db5f1bcf39dbfd67.jpg\"><p>Patch Manager automates the process of patching Windows and Linux managed instances. Use this feature of AWS Systems Manager to scan your instances for missing patches or scan and install missing patches. You can install patches individually or to large groups of instances by using Amazon EC2 tags.</p><p><strong>CORRECT: </strong>\"Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function to apply the patch to all EC2 instances\" is incorrect. Since AWS already provides an out of the box solution of creating customizable patch groups enabling easy patching of EC2 instances, writing custom AWS Lambda is not the quickest/easiest solution.</p><p><strong>INCORRECT:</strong> \"Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances\" is incorrect. This is a valid option and would hold in case there’s a specific downtime or maintenance window when the patches are to be applied.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances\" is incorrect. This option wouldn’t work since the requirement is to have the patching done as quickly as possible and this would slow down the process.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/\">https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>",
                "options": [
                    {
                        "id": 11591,
                        "content": "<p>Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11592,
                        "content": "<p>Create an AWS Lambda function to apply the patch to all EC2 instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11593,
                        "content": "<p>Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11594,
                        "content": "<p>Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2778,
            "attributes": {
                "createdAt": "2023-09-09T20:40:12.535Z",
                "updatedAt": "2023-09-09T20:40:12.535Z",
                "content": "<p>A company wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the production environment. The software that accesses this data requires consistently high I/O performance.</p><p>A solutions architect needs to minimize the time that is required to clone the production data into the test environment.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>Amazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all their provisioned performance.</p><p><strong>CORRECT: </strong>\"Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the test environment\" is incorrect. You cannot restore EBS snapshots to instance store volumes. Instance store volumes are ephemeral storage volumes and are not used for data that requires persistence.</p><p><strong>INCORRECT:</strong> “Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots\" is incorrect.</p><p>This solution may take longer and may not have the consistent performance that is offered with the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the test environment\" is incorrect.</p><p>Amazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD ( io1 or io2 ) volume to multiple instances that are in the same Availability Zone. You can attach multiple Multi-Attach enabled volumes to an instance or set of instances. This does not help with the requirements of this solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 11595,
                        "content": "<p>Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11596,
                        "content": "<p>Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11597,
                        "content": "<p>Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the test environment.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11598,
                        "content": "<p>Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the test environment.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2779,
            "attributes": {
                "createdAt": "2023-09-09T20:40:12.617Z",
                "updatedAt": "2023-09-09T20:40:12.617Z",
                "content": "<p>An application analyzes images of people that are uploaded to an Amazon S3 bucket. The application determines demographic data which is then saved to a .CSV file in another S3 bucket. The data must be encrypted at rest and then queried using SQL. The solution should be fully serverless.</p><p>Which actions should a Solutions Architect take to encrypt and query the data?</p>",
                "answerExplanation": "<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Amazon Athena supports encrypted data for both the source data and query results, for example, using Amazon S3 with AWS KMS.</p><p><strong>CORRECT: </strong>\"Use AWS KMS encryption keys for the S3 bucket and use Amazon Athena to query the data\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 server-side encryption and use Amazon RedShift Spectrum to query the data\" is incorrect. RedShift Spectrum is not serverless as it requires a RedShift cluster which is based on EC2 instances.</p><p><strong>INCORRECT:</strong> \"Use AWS KMS encryption keys for the S3 bucket and use Amazon Kinesis Data Analytics to query the data\" is incorrect. Kinesis Data Analytics is used for analyzing real-time streaming data in Kinesis streams.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 server-side encryption and Amazon QuickSight to query the data\" is incorrect. Amazon QuickSight is an interactive dashboard, it is not a service for running queries on data.</p><p><strong>References:</strong></p><p><a href=\"https://d1.awsstatic.com/whitepapers/architecture/wellarchitected-Machine-Learning-Lens.pdf\">https://d1.awsstatic.com/whitepapers/architecture/wellarchitected-Machine-Learning-Lens.pdf</a></p><p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
                "options": [
                    {
                        "id": 11599,
                        "content": "<p>Use Amazon S3 server-side encryption and use Amazon RedShift Spectrum to query the data</p>",
                        "isValid": false
                    },
                    {
                        "id": 11600,
                        "content": "<p>Use AWS KMS encryption keys for the S3 bucket and use Amazon Kinesis Data Analytics to query the data</p>",
                        "isValid": false
                    },
                    {
                        "id": 11601,
                        "content": "<p>Use AWS KMS encryption keys for the S3 bucket and use Amazon Athena to query the data</p>",
                        "isValid": true
                    },
                    {
                        "id": 11602,
                        "content": "<p>Use Amazon S3 server-side encryption and Amazon QuickSight to query the data</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2780,
            "attributes": {
                "createdAt": "2023-09-09T20:40:12.698Z",
                "updatedAt": "2023-09-09T20:40:12.698Z",
                "content": "<p>A company has multiple Windows workloads which are .NET application servers and Microsoft SQL Server databases running on Amazon EC2 instances with Windows Server 2016. The company requires a shared file system which is highly available, durable and provides high levels of throughput and IOPS.</p><p>What is the best way to meet this requirement?</p>",
                "answerExplanation": "<img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-16-52-e0b5162f6cd22832735038248d52fbf0.jpg\"><p>As a fully managed service, FSx for Windows File Server eliminates the administrative overhead of setting up and provisioning file servers and storage volumes. Additionally, Amazon FSx keeps Windows software up to date, detects and addresses hardware failures, and performs backups.</p><p>Amazon FSx also provides rich integration with other AWS services like <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html\">AWS IAM</a>, <a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_microsoft_ad.html\">AWS Directory Service for Microsoft Active Directory</a>, <a href=\"https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces.html\">Amazon WorkSpaces</a>, <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/overview.html\">AWS Key Management Service</a>, and <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html\">AWS CloudTrail</a>.</p><p><strong>CORRECT: </strong>\"Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate all the data to Amazon S3. Set up IAM authentication for users to access files\" is incorrect. Since the workload is Windows specific, S3 wouldn’t really help as an optimal solution though S3 can be still used to backup objects.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon S3 File Gateway, mount the S3 File Gateway on the existing EC2 instances\" is incorrect. Amazon S3 File Gateway provides a seamless way to connect to the cloud to store application data files and backup images as durable objects in Amazon S3 cloud storage with SMB or NFS-based access and local caching. However, this is a solution designed for on-premises servers, not EC2 instances and is not the best option for this scenario.</p><p><strong>INCORRECT:</strong> \"Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS\" is incorrect. EFS cannot be used with Microsoft workloads using the SMB protocol as it only supports Linux and NFS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-file-shares.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-file-shares.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 11603,
                        "content": "<p>Set up an Amazon S3 File Gateway, mount the S3 File Gateway on the existing EC2 instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11604,
                        "content": "<p>Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11605,
                        "content": "<p>Migrate all the data to Amazon S3. Set up IAM authentication for users to access files.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11606,
                        "content": "<p>Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2781,
            "attributes": {
                "createdAt": "2023-09-09T20:40:12.793Z",
                "updatedAt": "2023-09-09T20:40:12.793Z",
                "content": "<p>A digital marketing agency manages numerous client websites and apps on AWS. Each AWS resource is supposed to be tagged by the account for tracking and backup purposes. The agency wants to ensure that all AWS resources, including untagged ones, are backed up properly to minimize data loss risks.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>This solution is the most operationally efficient due to the powerful combination of AWS Config and AWS Backup.</p><p><strong>AWS Config</strong>: This service enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. You can use AWS Config to review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. In this scenario, AWS Config can be utilized to identify all resources that lack proper tags.</p><p><strong>Tagging</strong>: Tags can be added to AWS resources programmatically. By tagging resources, you organize them into groups and subgroups, which can be based on purpose, owner, environment, or other criteria. In this context, tagging resources allows AWS Backup to identify and group resources that need to be backed up.</p><p><strong>AWS Backup</strong>: AWS Backup is a fully managed backup service that makes it easy to centralize and automate the back up of data across AWS services. You can use AWS Backup to protect several AWS resource types, including Amazon EBS volumes, Amazon RDS databases, Amazon DynamoDB tables, Amazon EFS file systems, and AWS Storage Gateway volumes. It offers a centralized dashboard where you can manage all backups and allows you to automate and monitor backups across AWS services using policies.</p><p>With AWS Config identifying and tagging untagged resources, and AWS Backup automating the backup of tagged resources, this solution requires minimal operational overhead while ensuring all resources are adequately backed up.</p><p><strong>CORRECT: </strong>\"Use AWS Config to identify all untagged resources and tag them programmatically. Then, use AWS Backup to automate the backup of all AWS resources based on tags\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Manually search for all untagged resources in each AWS service. Once identified, tag them appropriately and set up AWS Backup for each service separately\" is incorrect.</p><p>Searching for untagged resources manually in each service and setting up AWS Backup separately for each one would require a significant amount of operational overhead.</p><p><strong>INCORRECT:</strong> \"Rely on each account owner to identify their untagged resources and then use AWS Backup for backing up\" is incorrect.</p><p>Relying on individual account owners could result in inconsistency and increase the risk of missed resources or backups. Centralized backup management using AWS Backup is more efficient.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda to periodically scan for untagged resources, add necessary tags, and then set up AWS Backup\" is incorrect.</p><p>Although AWS Lambda could be used to scan for untagged resources and add necessary tags, this would require developing and maintaining a custom script. AWS Config can handle this process with less operational overhead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/aws-backup/latest/devguide/assigning-resources.html\">https://docs.aws.amazon.com/aws-backup/latest/devguide/assigning-resources.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p>",
                "options": [
                    {
                        "id": 11607,
                        "content": "<p>Use AWS Lambda to periodically scan for untagged resources, add necessary tags, and then set up AWS Backup.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11608,
                        "content": "<p>Use AWS Config to identify all untagged resources and tag them programmatically. Then, use AWS Backup to automate the backup of all AWS resources based on tags.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11609,
                        "content": "<p>Manually search for all untagged resources in each AWS service. Once identified, tag them appropriately and set up AWS Backup for each service separately.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11610,
                        "content": "<p>Rely on each account owner to identify their untagged resources and then use AWS Backup for backing up.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2782,
            "attributes": {
                "createdAt": "2023-09-09T20:40:12.873Z",
                "updatedAt": "2023-09-09T20:40:12.873Z",
                "content": "<p>An e-commerce company operates a containerized microservices application on a fleet of Amazon EC2 instances. As part of their infrastructure improvement efforts, the company plans to migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) for enhanced scalability and management.</p><p>As part of the security protocol, the company has configured the Amazon EKS control plane with endpoint private access enabled and public access disabled. The data plane resides within private subnets. However, the company faces an issue where nodes fail to join the cluster.</p><p>What can be done to allow the nodes to join the EKS cluster?</p>",
                "answerExplanation": "<p>When the EKS control plane is configured with private access, and the nodes are in a private subnet, you need to create VPC endpoints for Amazon EKS and ECR. This allows the nodes to communicate with the EKS control plane and pull container images from ECR.</p><p><strong>CORRECT: </strong>\"Set up VPC endpoints for Amazon EKS and ECR to enable nodes to communicate with the control plane\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Modify the associated IAM role to include permissions to the AmazonEKSClusterPolicy\" is incorrect.</p><p>IAM roles are crucial for setting up permissions, but simply modifying the associated IAM role would not solve the issue of nodes not being able to connect to the control plane.</p><p><strong>INCORRECT:</strong> \"Establish VPC peering connection for nodes to access the control plane\" is incorrect.</p><p>VPC peering is not the recommended way to allow nodes in a private subnet to access the EKS control plane. This approach might also incur additional operational overhead.</p><p><strong>INCORRECT:</strong> \"Move nodes to public subnet and configure security group rules for the EC2 nodes\" is incorrect.</p><p>Moving the nodes to public subnets contradicts the original requirement of having the data plane in private subnets. Additionally, this approach might introduce unnecessary security risks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/private-clusters.html\">https://docs.aws.amazon.com/eks/latest/userguide/private-clusters.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 11611,
                        "content": "<p>Establish VPC peering connection for nodes to access the control plane.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11612,
                        "content": "<p>Move nodes to public subnet and configure security group rules for the EC2 nodes.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11613,
                        "content": "<p>Set up VPC endpoints for Amazon EKS and ECR to enable nodes to communicate with the control plane.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11614,
                        "content": "<p>Modify the associated IAM role to include permissions to the AmazonEKSClusterPolicy.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2783,
            "attributes": {
                "createdAt": "2023-09-09T20:40:12.960Z",
                "updatedAt": "2023-09-09T20:40:12.960Z",
                "content": "<p>A financial services company has a large, multi-Region footprint on AWS. A recent security audit highlighted some issues that must be addressed. The company must track all configuration changes affecting AWS resources and have detailed records of who has accessed the AWS environment. The data should include information such as which user has logged in and which API calls they made</p><p>What actions should a Solutions Architect take to meet these requirements?</p>",
                "answerExplanation": "<p>AWS Config is a service used to track and remediation any unauthorized configuration changes made with your AWS Account. AWS Config could be used in this example with AWS AWS CloudTrail which keeps detailed logs of all API calls made within the account such as who logged in, which AWS Identity and Access Management (IAM) role is being used and also how they interact with the AWS Cloud.</p><p><strong>CORRECT: </strong>\"Use AWS Config to track configuration changes and AWS CloudTrail to record API calls and track access patterns in the AWS Cloud\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch to track configuration changes and AWS Config to record API calls and track access patterns in the AWS Cloud\" is incorrect. Amazon CloudWatch does not make track configuration changes, it tracks performance metrics and AWS Config does not track API calls, it tracks configuration changes.</p><p><strong>INCORRECT:</strong> \"Use AWS Config to track configuration changes and Amazon EventBridge to record API calls and track access patterns in the AWS Cloud\" is incorrect. Although AWS Config would work in this scenario, <em>Amazon EventBridge</em> is a serverless event bus used to build event-driven- architectures so it cannot be used for tracking API calls.</p><p><strong>INCORRECT:</strong> \"Use Amazon Macie to track configuration changes and Amazon CloudTrail to record API calls and track access patterns in the AWS Cloud\" is incorrect. Amazon Macie is used with Amazon S3 to detect sensitive PII data, which has nothing to do with tracking configuration changes.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/config\">https://aws.amazon.com/config</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p>",
                "options": [
                    {
                        "id": 11615,
                        "content": "<p>Use AWS Config to track configuration changes and Amazon EventBridge to record API calls and track access patterns in the AWS Cloud.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11616,
                        "content": "<p>Use AWS Config to track configuration changes and AWS CloudTrail to record API calls and track access patterns in the AWS Cloud.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11617,
                        "content": "<p>Use Amazon Macie to track configuration changes and Amazon CloudTrail to record API calls and track access patterns in the AWS Cloud.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11618,
                        "content": "<p>Use Amazon CloudWatch to track configuration changes and AWS Config to record API calls and track access patterns in the AWS Cloud.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2784,
            "attributes": {
                "createdAt": "2023-09-09T20:40:13.061Z",
                "updatedAt": "2023-09-09T20:40:13.061Z",
                "content": "<p>A large quantity of data is stored on a NAS device on-premises and accessed using the SMB protocol. The company require a managed service for hosting the filesystem and a tool to automate the migration.</p><p>Which actions should a Solutions Architect take?</p>",
                "answerExplanation": "<p>Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. This is the most suitable destination for this use case.</p><p>AWS DataSync can be used to move large amounts of data online between on-premises storage and Amazon S3, Amazon EFS, or Amazon FSx for Windows File Server. The source datastore can be Server Message Block (SMB) file servers.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-11-07-d909de298264e2e376a88159790c0be1.png\"></p><p><strong>CORRECT: </strong>\"Migrate the data to Amazon FSx for Windows File Server using AWS DataSync\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Migrate the data to Amazon EFS using the AWS Server Migration Service (SMS)\" is incorrect. EFS is used for hosting filesystems accessed over NFS from Linux (not Windows). The SMS service is used for migrating virtual machines, not data.</p><p><strong>INCORRECT:</strong> \"Migrate the data to Amazon FSx for Lustre using AWS DataSync\" is incorrect. Amazon FSx for Windows File Server should be used for hosting SMB shares.</p><p><strong>INCORRECT:</strong> \"Migrate the data to Amazon S3 using and AWS Snowball Edge device\" is incorrect. Amazon S3 is an object store and unsuitable for hosting an SMB filesystem. Snowball is not required in this case as the data is not going to S3 and there are no time or bandwidth limitations mentioned in the scenario.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p><p><a href=\"https://aws.amazon.com/datasync/features/\">https://aws.amazon.com/datasync/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 11619,
                        "content": "<p>Migrate the data to Amazon FSx for Windows File Server using AWS DataSync</p>",
                        "isValid": true
                    },
                    {
                        "id": 11620,
                        "content": "<p>Migrate the data to Amazon S3 using and AWS Snowball Edge device</p>",
                        "isValid": false
                    },
                    {
                        "id": 11621,
                        "content": "<p>Migrate the data to Amazon EFS using the AWS Server Migration Service (SMS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11622,
                        "content": "<p>Migrate the data to Amazon FSx for Lustre using AWS DataSync</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2785,
            "attributes": {
                "createdAt": "2023-09-09T20:40:13.142Z",
                "updatedAt": "2023-09-09T20:40:13.142Z",
                "content": "<p>A Solutions Architect is designing an application for processing and extracting data from log files. The log files are generated by an application and the number and frequency of updates varies. The files are up to 1 GB in size and processing will take around 40 seconds for each file.</p><p>Which solution is the most cost-effective?</p>",
                "answerExplanation": "<p>The question asks for the most cost-effective solution and therefor a serverless and automated solution will be the best choice.</p><p>AWS Lambda can run custom code in response to Amazon S3 bucket events. You upload your custom code to AWS Lambda and create a function. When Amazon S3 detects an event of a specific type (for example, an object created event), it can publish the event to AWS Lambda and invoke your function in Lambda. In response, AWS Lambda executes your function.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-12-32-5693ddd5be6f40334bb8d981a4099fbd.png\"></p><p><strong>CORRECT: </strong>\"Write the log files to an Amazon S3 bucket. Create an event notification to invoke an AWS Lambda function that will process the files\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Write the log files to an Amazon EC2 instance with an attached EBS volume. After processing, save the files to an Amazon S3 bucket\" is incorrect. This is not cost effective as it is not serverless.</p><p><strong>INCORRECT:</strong> \"Write the log files to an Amazon SQS queue. Use AWS Lambda to process the files from the queue and save to an Amazon S3 bucket\" is incorrect. SQS has a maximum message size of 256 KB so the message body would need to be saved in S3 anyway. Using an event source mapping from S3 would be less complex and preferable.</p><p><strong>INCORRECT:</strong> \"Write the log files to an Amazon S3 bucket. Create an event notification to invoke an Amazon ECS task to process the files and save to an Amazon S3 bucket\" is incorrect. You cannot use event notifications to process Amazon ECS tasks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 11623,
                        "content": "<p>Write the log files to an Amazon EC2 instance with an attached EBS volume. After processing, save the files to an Amazon S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 11624,
                        "content": "<p>Write the log files to an Amazon S3 bucket. Create an event notification to invoke an Amazon ECS task to process the files and save to an Amazon S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 11625,
                        "content": "<p>Write the log files to an Amazon S3 bucket. Create an event notification to invoke an AWS Lambda function that will process the files</p>",
                        "isValid": true
                    },
                    {
                        "id": 11626,
                        "content": "<p>Write the log files to an Amazon SQS queue. Use AWS Lambda to process the files from the queue and save to an Amazon S3 bucket</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2786,
            "attributes": {
                "createdAt": "2023-09-09T20:40:13.286Z",
                "updatedAt": "2023-09-09T20:40:13.286Z",
                "content": "<p>An international logistics company has web applications running on AWS in the us-west-2 Region and database servers in the eu-central-1 Region. The applications running in a VPC in us-west-2 need to communicate securely with the databases running in a VPC in eu-central-1.</p><p>Which network design will meet these requirements?</p>",
                "answerExplanation": "<p>The correct solution establishes a VPC peering connection between the two regions, and it properly sets up the inbound rule in the eu-central-1 database security group to allow traffic from the us-west-2 application server IP addresses, which is the correct way to configure this as security groups can't be referenced across regions.</p><p><strong>CORRECT: </strong>\"Configure a VPC peering connection between the us-west-2 VPC and the eu-central-1 VPC. Update the subnet route tables accordingly. Create an inbound rule in the eu-central-1 database security group that allows traffic from the us-west-2 application server IP addresses\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Establish a VPC peering connection between the us-west-2 VPC and the eu-central-1 VPC. Modify the subnet route tables accordingly. Create an inbound rule in the eu-central-1 database security group that references the security group ID of the application servers in us-west-2\" is incorrect.</p><p>You cannot reference a security group from another region. Security groups are region-specific and can only be referenced within the same region.</p><p><strong>INCORRECT:</strong> \"Create a VPC peering connection between the us-west-2 VPC and the eu-central-1 VPC. Add the appropriate routes to the subnet route tables. Create an inbound rule in the us-west-2 application security group that allows traffic from the eu-central-1 database server IP addresses\" is incorrect.</p><p>In this scenario, we want to allow traffic from the application servers in us-west-2 to the database servers in eu-central-1. The inbound rule should be configured in the eu-central-1 database security group to allow this traffic.</p><p><strong>INCORRECT:</strong> \"Establish a transit gateway with a peering attachment between the us-west-2 VPC and the eu-central-1 VPC. After the transit gateways are properly peered and routing is configured, create an inbound rule in the eu-central-1 database security group that references the security group ID of the application servers in us-west-2\" is incorrect.</p><p>You cannot reference a security group from another region. Security groups are region-specific and can only be referenced within the same region.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html\">https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11627,
                        "content": "<p>Establish a transit gateway with a peering attachment between the us-west-2 VPC and the eu-central-1 VPC. After the transit gateways are properly peered and routing is configured, create an inbound rule in the eu-central-1 database security group that references the security group ID of the application servers in us-west-2.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11628,
                        "content": "<p>Create a VPC peering connection between the us-west-2 VPC and the eu-central-1 VPC. Add the appropriate routes to the subnet route tables. Create an inbound rule in the us-west-2 application security group that allows traffic from the eu-central-1 database server IP addresses.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11629,
                        "content": "<p>Configure a VPC peering connection between the us-west-2 VPC and the eu-central-1 VPC. Update the subnet route tables accordingly. Create an inbound rule in the eu-central-1 database security group that allows traffic from the us-west-2 application server IP addresses.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11630,
                        "content": "<p>Establish a VPC peering connection between the us-west-2 VPC and the eu-central-1 VPC. Modify the subnet route tables accordingly. Create an inbound rule in the eu-central-1 database security group that references the security group ID of the application servers in us-west-2.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2787,
            "attributes": {
                "createdAt": "2023-09-09T20:40:13.415Z",
                "updatedAt": "2023-09-09T20:40:13.415Z",
                "content": "<p>An e-commerce company has developed a new application which has been successfully deployed on AWS. For an upcoming sale, the company is expecting a huge rise in traffic and while testing for the event they have encountered performance issues in the application when many requests are sent to the application.</p><p>The current application stack is Amazon Aurora PostgreSQL database with an AWS Lambda compute layer fronted by API Gateway. A solutions architect must recommend improvements scalability whilst minimizing the configuration effort.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>With Amazon SQS, you can offload tasks from one component of your application by sending them to a queue and processing them asynchronously. Lambda polls the queue and invokes your Lambda function synchronously with an event that contains the message from the SQS queue. This solution improves scalability as the message bus decouples the processing components of the application meaning it is less likely that the application will suffer outages or lost data.</p><p><strong>CORRECT: </strong>\"Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the database by using native Java Database Connectivity (JDBC) drivers\" is incorrect. You cannot run Lambda code on Amazon EC2 instances.</p><p><strong>INCORRECT:</strong> \"Change the platform from Aurora to Amazon DynamoDB. Provision a DynamoDB Accelerator (DAX) cluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster\" is incorrect. Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB. The question doesn’t talk about hot or frequently accessed data only about an increase in volume so introducing DAX might not completely solve the issues.</p><p><strong>INCORRECT:</strong> \"Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using Amazon Simple Notification Service (Amazon SNS)\" is incorrect. SNS is used for fan-out scenarios when a single event is to be broadcasted among consumers and hence is not a good fit here.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 11631,
                        "content": "<p>Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the database by using native Java Database Connectivity (JDBC) drivers.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11632,
                        "content": "<p>Change the platform from Aurora to Amazon DynamoDB. Provision a DynamoDB Accelerator (DAX) cluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11633,
                        "content": "<p>Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using Amazon Simple Notification Service (Amazon SNS).</p>",
                        "isValid": false
                    },
                    {
                        "id": 11634,
                        "content": "<p>Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2788,
            "attributes": {
                "createdAt": "2023-09-09T20:40:13.517Z",
                "updatedAt": "2023-09-09T20:40:13.517Z",
                "content": "<p>An application that is being installed on an Amazon EC2 instance requires a persistent block storage volume. The data must be encrypted at rest and regular volume-level backups must be automated.</p><p>Which solution options should be used?</p>",
                "answerExplanation": "<p>For block storage the Solutions Architect should use either Amazon EBS or EC2 instance store. However, the instance store is non-persistent so EBS must be used. With EBS you can encrypt your volume and automate volume-level backups using snapshots that are run by Data Lifecycle Manager.</p><p><strong>CORRECT: </strong>\"Use an encrypted Amazon EBS volume and use Data Lifecycle Manager to automate snapshots\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an encrypted Amazon EFS filesystem and use an Amazon CloudWatch Events rule to start a backup copy of data using AWS Lambda\" is incorrect. EFS is not block storage, it is a file-level storage service.</p><p><strong>INCORRECT:</strong> \"Use server-side encryption on an Amazon S3 bucket and use Cross-Region-Replication to backup on a schedule\" is incorrect. Amazon S3 is an object-based storage system not a block-based storage system.</p><p><strong>INCORRECT:</strong> \"Use an encrypted Amazon EC2 instance store and copy the data to another EC2 instance using a cron job and a batch script \" is incorrect as the EC2 instance store is a non-persistent volume.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 11635,
                        "content": "<p>Use server-side encryption on an Amazon S3 bucket and use Cross-Region-Replication to backup on a schedule</p>",
                        "isValid": false
                    },
                    {
                        "id": 11636,
                        "content": "<p>Use an encrypted Amazon EC2 instance store and copy the data to another EC2 instance using a cron job and a batch script</p>",
                        "isValid": false
                    },
                    {
                        "id": 11637,
                        "content": "<p>Use an encrypted Amazon EBS volume and use Data Lifecycle Manager to automate snapshots</p>",
                        "isValid": true
                    },
                    {
                        "id": 11638,
                        "content": "<p>Use an encrypted Amazon EFS filesystem and use an Amazon CloudWatch Events rule to start a backup copy of data using AWS Lambda</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2789,
            "attributes": {
                "createdAt": "2023-09-09T20:40:13.603Z",
                "updatedAt": "2023-09-09T20:40:13.603Z",
                "content": "<p>A game development company is planning to build a cloud-based game platform on AWS. The player activity patterns are unpredictable and could remain idle for extended periods. Only players who have purchased the game should have the ability to log in and play.</p><p>Which combination of steps will meet these requirements MOST cost-effectively? (Select THREE.)</p>",
                "answerExplanation": "<p>AWS Lambda is a cost-effective solution for unpredictable traffic patterns due to its pay-per-use pricing model. DynamoDB is also a cost-effective and highly scalable solution for storing user data. The API Gateway provides a HTTP-based endpoint that can be used to expose the Lambda function.</p><p>AWS Cognito User Pools provide user directory features including sign-up and sign-in services, which are suitable for managing game user authentication.</p><p>AWS Amplify simplifies the process of hosting web applications with automated deployment processes. It also integrates with CloudFront, providing a global content delivery network to efficiently serve the game interface.</p><p><strong>CORRECT: </strong>\"Implement an AWS Lambda function to fetch player information from Amazon DynamoDB. Establish an Amazon API Gateway endpoint to handle RESTful API calls, directing them to the Lambda function\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Cognito User Pools to handle user authentication\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Leverage AWS Amplify to serve the frontend game interface with HTML, CSS, and JS. Use the integrated Amazon CloudFront configuration for distribution\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Set up an Amazon Elastic Container Service (Amazon ECS) service behind an Application Load Balancer to fetch player information from Amazon RDS. Establish an Amazon API Gateway endpoint to handle RESTful API calls, directing them to the ECS service\" is incorrect.</p><p>Using Amazon ECS might be an overkill for this scenario and might not be as cost-effective compared to Lambda and DynamoDB, especially for unpredictable and possibly idle traffic.</p><p><strong>INCORRECT:</strong> \"Use AWS Cognito Identity Pools to handle user authentication\" is incorrect.</p><p>Cognito Identity Pools are used for granting access to AWS resources rather than handling user authentication.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 static web hosting with HTML, CSS, and JS. Use Amazon CloudFront to distribute the frontend game interface\" is incorrect.</p><p>While you could host a static website on S3 and use CloudFront for distribution, AWS Amplify can provide additional capabilities tailored to modern web applications. Furthermore, Amplify's automated deployment processes can provide a more streamlined and efficient approach to managing the game's frontend compared to managing separate S3 and CloudFront configurations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p><p><a href=\"https://aws.amazon.com/amplify/\">https://aws.amazon.com/amplify/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 11639,
                        "content": "<p>Use AWS Cognito Identity Pools to handle user authentication.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11640,
                        "content": "<p>Implement an AWS Lambda function to fetch player information from Amazon DynamoDB. Establish an Amazon API Gateway endpoint to handle RESTful API calls, directing them to the Lambda function.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11641,
                        "content": "<p>Use AWS Cognito User Pools to handle user authentication.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11642,
                        "content": "<p>Use Amazon S3 static web hosting with HTML, CSS, and JS. Use Amazon CloudFront to distribute the frontend game interface.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11643,
                        "content": "<p>Set up an Amazon Elastic Container Service (Amazon ECS) service behind an Application Load Balancer to fetch player information from Amazon RDS. Establish an Amazon API Gateway endpoint to handle RESTful API calls, directing them to the ECS service.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11644,
                        "content": "<p>Leverage AWS Amplify to serve the frontend game interface with HTML, CSS, and JS. Use the integrated Amazon CloudFront configuration for distribution.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2790,
            "attributes": {
                "createdAt": "2023-09-09T20:40:13.695Z",
                "updatedAt": "2023-09-09T20:40:13.695Z",
                "content": "<p>A three-tier web application is composed of a front end hosted on an Amazon EC2 instance in public subnet, application middleware hosted on EC2 in a private subnet and a database hosted on an Amazon RDS MySQL database in a private subnet. The database layer should be restricted to only allow incoming connections from the application.</p><p>Which of the following options makes sure that database can only be accessed by the application layer?</p>",
                "answerExplanation": "<p>Security groups are stateful. All inbound traffic is blocked by default in custom security groups. If you create an inbound rule allowing traffic in, that traffic is automatically allowed back out again. You cannot block specific IP address using Security groups (instead use Network Access Control Lists).</p><p>In this case the solution is to allow inbound traffic from the security group ID of the security group attached to the application layer. The rule should specify the appropriate protocol and port. This will ensure only the application layer can communicate with the database.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-44-44-d7ca54a2bdcb21dfbe441fc3f6080f3a.jpg\"><p><strong>CORRECT: </strong>\"Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a new route table that excludes the route to the public subnets' CIDR blocks. Associate the route table with the database subnets\" is incorrect. This would simply stop routing from working within the VPC.</p><p><strong>INCORRECT:</strong> \"Create a security group that denies inbound traffic from the security group that is assigned to instances in the public subnets. Attach the security group to the DB instances\" is incorrect.</p><p>You cannot create deny rules with security groups.</p><p><strong>INCORRECT:</strong> \"Create a new peering connection between the public subnets and the private subnets. Create a different peering connection between the private subnets and the database subnets\" is incorrect.</p><p>Peering is used when multiple VPC’s are to be connected with each other hence this is also an incorrect option.</p><p><strong>References</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 11645,
                        "content": "<p>Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11646,
                        "content": "<p>Create a new route table that excludes the route to the public subnets' CIDR blocks. Associate the route table with the database subnets.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11647,
                        "content": "<p>Create a new peering connection between the public subnets and the private subnets. Create a different peering connection between the private subnets and the database subnets.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11648,
                        "content": "<p>Create a security group that denies inbound traffic from the security group that is assigned to instances in the public subnets. Attach the security group to the DB instances.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2791,
            "attributes": {
                "createdAt": "2023-09-09T20:40:13.796Z",
                "updatedAt": "2023-09-09T20:40:13.796Z",
                "content": "<p>A multinational enterprise plans to transition from numerous independent AWS accounts to a structured, multi-account AWS setup. The enterprise anticipates creating multiple AWS accounts to cater to various departments. The enterprise seeks to authenticate access to these AWS accounts using a centralized corporate directory service.</p><p>What combination of steps should a solutions architect suggest to meet these needs? (Select TWO.)</p>",
                "answerExplanation": "<p>AWS Organizations provides policy-based management for multiple AWS accounts. With Organizations, you can create member accounts that are part of your organization and centrally manage your accounts.</p><p>AWS Directory Service allows you to connect your AWS resources with an existing on-premises Microsoft Active Directory or to set up a new, stand-alone directory in the AWS Cloud. AWS Identity Center makes it easy to centrally manage access to multiple AWS accounts and business applications and provide users with single sign-on access to all their assigned accounts and applications from one place.</p><p><strong>CORRECT: </strong>\"Create a new AWS Organizations entity with all features enabled. Create the new AWS accounts within the organization\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Deploy AWS Directory Service and integrate it with the corporate directory service. Set up AWS Identity Center for authentication across accounts\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install and configure AWS Control Tower for centralized account management. Incorporate AWS Identity Center to manage identity\" is incorrect.</p><p>AWS Control Tower does have certain benefits, but it doesn't directly cater to the company's need for centralized corporate directory service integration. However, it could be used in conjunction with AWS Identity Center for user access management.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon Cognito identity pool and configure AWS Identity Center to accept Amazon Cognito authentication\" is incorrect.</p><p>Amazon Cognito is primarily used to add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. It isn't typically used in multi-account management scenarios and isn't directly relevant to the requirement for corporate directory service integration.</p><p><strong>INCORRECT:</strong> \"Establish an AWS Transit Gateway for centralized network management, linking AWS accounts\" is incorrect.</p><p>AWS Transit Gateway connects VPCs and on-premises networks through a central hub. It is a network transit hub, not a user authentication and management service. It doesn't directly address the need for centralized corporate directory service integration.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/manage-your-identity-source-ad.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/manage-your-identity-source-ad.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
                "options": [
                    {
                        "id": 11649,
                        "content": "<p>Create a new AWS Organizations entity with all features enabled. Create the new AWS accounts within the organization.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11650,
                        "content": "<p>Establish an AWS Transit Gateway for centralized network management, linking AWS accounts.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11651,
                        "content": "<p>Deploy AWS Directory Service and integrate it with the corporate directory service. Set up AWS Identity Center for authentication across accounts.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11652,
                        "content": "<p>Set up an Amazon Cognito identity pool and configure AWS Identity Center to accept Amazon Cognito authentication.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11653,
                        "content": "<p>Install and configure AWS Control Tower for centralized account management. Incorporate AWS Identity Center to manage identity.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2792,
            "attributes": {
                "createdAt": "2023-09-09T20:40:13.881Z",
                "updatedAt": "2023-09-09T20:40:13.881Z",
                "content": "<p>A global financial services company is currently operating a three-tier web application to handle their main customer facing website. This application uses several Amazon EC2 instances behind an Application Load Balancer and connects directly to a DynamoDB table.</p><p>Due to recent customer complaints of slow loading times, their Solutions Architect has been asked to implement changes to solve this problem, without rearchitecting the core application components.</p><p><br></p><p>Which combination of actions should the solutions architect take to accomplish this? (Select TWO.)</p>",
                "answerExplanation": "<p>A CloudFront distribution would cache content in one of the many global edge locations, ensuring that any customer access to the content will be accessing it at a much lower latency compared to using the Application Load Balancer on its own.</p><p>Secondly, DynamoDB has a built-in caching solution known as DynamoDB Accelerator (DAX). If your application is serving traffic from a DynamoDB database and is struggling to scale, you can use the DynamoDB cache to improve application.</p><p><strong>CORRECT: </strong>\"Create a CloudFront distribution and place it in front of the Application Load Balancer\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Set up an Amazon DynamoDB Accelerator (DAX) cluster in front of the DynamoDB table\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the entire application stack to AWS Elastic Beanstalk with both web server and worker environments\" is incorrect.</p><p>Migrating the entire application to AWS Elastic Beanstalk would require rearchitecting and would not necessarily improve the latency of the application for end users.</p><p><strong>INCORRECT:</strong> \"Migrate the DynamoDB database to Amazon Aurora with a multi-AZ deployment model\" is incorrect.</p><p>Refactoring the application to move from a No-SQL database (DynamoDB) to a SQL database (Amazon Aurora) would take a significant amount of application and code changes, due to the fundamental differences between SQL and NoSQL databases.</p><p><strong>INCORRECT:</strong> \"Migrate the web application to be hosted on a containerized solution using AWS Fargate\" is incorrect.</p><p>The application does not currently use containers, and instead uses Amazon EC2 instances. Changing the application to using a containerized compute layer would also require architectural changes and would not be suitable for this use case.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 11654,
                        "content": "<p>Create a CloudFront distribution and place it in front of the Application Load Balancer.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11655,
                        "content": "<p>Migrate the web application to be hosted on a containerized solution using AWS Fargate.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11656,
                        "content": "<p>Migrate the DynamoDB database to Amazon Aurora with a multi-AZ deployment model.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11657,
                        "content": "<p>Set up an Amazon DynamoDB Accelerator (DAX) cluster in front of the DynamoDB table.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11658,
                        "content": "<p>Migrate the entire application stack to AWS Elastic Beanstalk with both web server and worker environments.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2793,
            "attributes": {
                "createdAt": "2023-09-09T20:40:13.964Z",
                "updatedAt": "2023-09-09T20:40:13.964Z",
                "content": "<p>An application is used by a large bank to ingest incoming messages. The messages are then quickly consumed by dozens of other applications and microservices. The number of messages can increase suddenly from a few messages per second up to 120,000 messages per second. In response to several recent outages and failures, the company wants to decouple this applications architecture and solution to ensure scalability.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>Amazon SNS can be used in this situation in a fanout architecture where messages sent to the SNS topic and then forwarded to multiple SQS queues that are subscribed to the topic. The messages can then be processed by different consumer applications from these queues.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-09-21_03-18-09-96e319b141d13bca0d11b7e412c0d801.jpg\"><p><strong>CORRECT: </strong>\"Post the messages to an Amazon Simple Notification Service topic with multiple Amazon Simple Queue Service subscriptions. Process messages from queues using the Consumer Applications” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Persist the messages in Amazon Kinesis Data Analytics. Make sure the consumer applications are configured to read and process the messages” is incorrect. Amazon Kinesis Data Analytics is used for analyzing data using SQL, it is not used for ingesting messages.</p><p><strong>INCORRECT:</strong> \"Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group and scale up and down based on CPU Utilization” is incorrect. This is not a scalable enough architecture for the application’s needs as scaling based on auto scaling groups can take many minutes, not seconds as is required for the application.</p><p><strong>INCORRECT:</strong> \"Write the messages to Amazon Kinesis Data Streams using one shard. Use an AWS Lambda function to process messages and place them in a DynamoDB table. The Applications can then be read from the DynamoDB table” is incorrect. A single shard is limited to 1 MB or 1000 messages/sec, therefore multiple shards would be required.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 11659,
                        "content": "<p>Write the messages to Amazon Kinesis Data Streams using one shard. Use an AWS Lambda function to process messages and place them in a DynamoDB table. The Applications can then be read from the DynamoDB table.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11660,
                        "content": "<p>Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group and scale up and down based on CPU Utilization.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11661,
                        "content": "<p>Post the messages to an Amazon Simple Notification Service topic with multiple Amazon Simple Queue Service subscriptions. Process messages from queues using the Consumer Applications.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11662,
                        "content": "<p>Persist the messages in Amazon Kinesis Data Analytics. Make sure the consumer applications are configured to read and process the messages.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2794,
            "attributes": {
                "createdAt": "2023-09-09T20:40:14.040Z",
                "updatedAt": "2023-09-09T20:40:14.040Z",
                "content": "<p>A software firm is developing a microservices-based application to be deployed on Amazon ECS. This application needs to interact with a resilient, shared filesystem capable of restoring data to a different AWS Region with a Recovery Point Objective (RPO) of 2 hours.</p><p>The filesystem is also expected to provide a mount target in each Availability Zone (AZ) within a Region. The solutions architect intends to employ AWS Backup to oversee the cross-Region data replication.</p><p>Which option will meet these requirements?</p>",
                "answerExplanation": "<p>Amazon Elastic File System (Amazon EFS) provides simple, scalable file storage for use with Amazon EC2 instances and other AWS services. It supports the Network File System (NFS) protocol and can be configured with mount points in multiple AZs.</p><p>EFS can be used with AWS Backup for automated and centralized backup across AWS services, and supports replication to another region, satisfying the given requirement.</p><p>All replication traffic stays on the AWS global backbone, and most changes are replicated within a minute, with an overall Recovery Point Objective (RPO) of 15 minutes for most file systems.</p><p><strong>CORRECT: </strong>\"Amazon Elastic File System (Amazon EFS) with the Standard storage class\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Amazon FSx for Windows File Server with a Multi-AZ deployment\" is incorrect.</p><p>Amazon FSx for Windows File Server provides fully managed, reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server and offers a native Windows file system experience for Windows-based applications. However, it doesn't support cross-Region data replication, making it unsuitable for the given requirement.</p><p><strong>INCORRECT:</strong> \"Amazon FSx for NetApp ONTAP with a Multi-AZ deployment\" is incorrect.</p><p>Amazon FSx for NetApp ONTAP is more suitable to scenarios where you want to migrate applications using ONTAP software and if you’re already using NetApp systems. It is better in this case to use Amazon EFS.</p><p><strong>INCORRECT:</strong> \"Amazon FSx for OpenZFS\" is incorrect.</p><p>Amazon FSx for OpenZFS is suitable if you have a specific requirement to use OpenZFS. For this use case Amazon EFS is a better choice and will likely be more cost-effective.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/efs-replication.html\">https://docs.aws.amazon.com/efs/latest/ug/efs-replication.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
                "options": [
                    {
                        "id": 11663,
                        "content": "<p>Amazon FSx for Windows File Server with a Multi-AZ deployment.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11664,
                        "content": "<p>Amazon FSx for NetApp ONTAP with a Multi-AZ deployment.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11665,
                        "content": "<p>Amazon FSx for OpenZFS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11666,
                        "content": "<p>Amazon Elastic File System (Amazon EFS) with the Standard storage class.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2795,
            "attributes": {
                "createdAt": "2023-09-09T20:40:14.142Z",
                "updatedAt": "2023-09-09T20:40:14.142Z",
                "content": "<p>An organization is planning their disaster recovery solution. They plan to run a scaled down version of a fully functional environment. In a DR situation the recovery time must be minimized.</p><p>Which DR strategy should a Solutions Architect recommend?</p>",
                "answerExplanation": "<p>The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud. A warm standby solution extends the pilot light elements and preparation.</p><p>It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems on AWS and have them always on.</p><p><strong>CORRECT: </strong>\"Warm standby\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Backup and restore\" is incorrect. This is the lowest cost DR approach that simply entails creating online backups of all data and applications.</p><p><strong>INCORRECT:</strong> Pilot light\"\" is incorrect. With a pilot light strategy a core minimum of services are running and the remainder are only brought online during a disaster recovery situation.</p><p><strong>INCORRECT:</strong> \"Multi-site\" is incorrect. A multi-site solution runs on AWS as well as on your existing on-site infrastructure in an active- active configuration.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/\">https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/</a></p>",
                "options": [
                    {
                        "id": 11667,
                        "content": "<p>Pilot light</p>",
                        "isValid": false
                    },
                    {
                        "id": 11668,
                        "content": "<p>Warm standby</p>",
                        "isValid": true
                    },
                    {
                        "id": 11669,
                        "content": "<p>Multi-site</p>",
                        "isValid": false
                    },
                    {
                        "id": 11670,
                        "content": "<p>Backup and restore</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2796,
            "attributes": {
                "createdAt": "2023-09-09T20:40:14.270Z",
                "updatedAt": "2023-09-09T20:40:14.270Z",
                "content": "<p>A music streaming company needs to incorporate a third-party song feed. The song feed sends a webhook to notify an external service when new songs are ready for consumption. A developer has written an AWS Lambda function to retrieve songs when the company receives a webhook callback. The developer must expose the Lambda function for the third party to invoke.</p><p>Which solution will meet these requirements with the LEAST operational complexity?</p>",
                "answerExplanation": "<p>API Gateway enables you to create, deploy, and manage a RESTful API to expose backend HTTP endpoints, AWS Lambda functions, or other AWS services. You can provide the third party with the API Gateway endpoint, and they can invoke the Lambda function through it. This solution is the most operationally efficient because it requires the fewest resources and management overhead.</p><p><strong>CORRECT: </strong>\"Generate an API Gateway endpoint for the Lambda function. Provide the API Gateway endpoint to the third party for the webhook\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy a Network Load Balancer (NLB) to distribute requests to the Lambda function. Provide the NLB URL to the third party for the webhook\" is incorrect.</p><p>While it is possible to trigger AWS Lambda from an Application Load Balancer (ALB), not NLB, using ALB would add unnecessary complexity to the solution.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Simple Notification Service (Amazon SNS) topic. Link the topic to the Lambda function. Provide the SNS topic ARN to the third party for the webhook\" is incorrect.</p><p>Amazon SNS is a pub/sub messaging service, but it is not meant to expose a public-facing endpoint for third-party webhooks. Also, providing ARN to a third party would not be possible as SNS topics cannot be invoked directly from the internet.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Simple Queue Service (Amazon SQS) queue. Connect the queue to the Lambda function. Provide the ARN of the SQS queue to the third party for the webhook\" is incorrect.</p><p>SQS is a message queuing service used to decouple and scale microservices, distributed systems, and serverless applications. It is not suitable for exposing a public-facing endpoint for third-party webhooks. Moreover, like SNS, SQS cannot be invoked directly from the internet.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 11671,
                        "content": "<p>Create an Amazon Simple Queue Service (Amazon SQS) queue. Connect the queue to the Lambda function. Provide the ARN of the SQS queue to the third party for the webhook.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11672,
                        "content": "<p>Create an Amazon Simple Notification Service (Amazon SNS) topic. Link the topic to the Lambda function. Provide the SNS topic ARN to the third party for the webhook.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11673,
                        "content": "<p>Deploy a Network Load Balancer (NLB) to distribute requests to the Lambda function. Provide the NLB URL to the third party for the webhook.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11674,
                        "content": "<p>Generate an API Gateway endpoint for the Lambda function. Provide the API Gateway endpoint to the third party for the webhook.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2797,
            "attributes": {
                "createdAt": "2023-09-09T20:40:14.361Z",
                "updatedAt": "2023-09-09T20:40:14.361Z",
                "content": "<p>A financial firm is aiming to leverage AWS Cloud for augmenting its on-premises disaster recovery (DR) architecture. The firm's main application, running on PostgreSQL, is housed on a virtual machine (VM) on-premises. The DR solution needs to align with the application's recovery point objective (RPO) of less than a minute and a recovery time objective (RTO) of within two hours, all while keeping costs to a minimum.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>Configuring a warm standby Amazon RDS for PostgreSQL database on AWS and using AWS DMS with change data capture will meet the RTO and RPO requirements. DMS can handle the ongoing replication from the on-premises PostgreSQL to the standby RDS instance, providing a near real-time replica of the data.</p><p>In a DR scenario, this standby instance can be promoted to become the new primary database, meeting the required RTO and RPO.</p><p><strong>CORRECT: </strong>\"Set up a warm standby Amazon RDS for PostgreSQL database on AWS. Configure AWS Database Migration Service (AWS DMS) to use change data capture (CDC)\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure an active-active multi-site setup between the on-premises server and AWS using PostgreSQL with a third-party high availability solution\" is incorrect.</p><p>Setting up an active-active multi-site setup between the on-premises server and AWS using PostgreSQL with a third-party high availability solution might meet the RPO and RTO requirements, but it would likely be more expensive and complex than the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS Elastic Disaster Recovery with continuous replication to act as a pilot light solution on AWS\" is incorrect.</p><p>AWS Elastic Disaster Recovery with continuous replication can provide a DR solution, but for a database, it is typically more efficient to use a service designed for that purpose, like RDS with DMS.</p><p><strong>INCORRECT:</strong> \"Utilize third-party backup software to perform daily backups and store a secondary set of backups in Amazon S3\" is incorrect.</p><p>Using third-party backup software to perform daily backups and storing a secondary set of backups in Amazon S3 would not meet the RPO of less than a minute, as this approach could lead to a data loss up to 24 hours. Also, the process of restoring from a backup might not meet the RTO of within two hours.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 11675,
                        "content": "<p>Utilize third-party backup software to perform daily backups and store a secondary set of backups in Amazon S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11676,
                        "content": "<p>Set up a warm standby Amazon RDS for PostgreSQL database on AWS. Configure AWS Database Migration Service (AWS DMS) to use change data capture (CDC).</p>",
                        "isValid": true
                    },
                    {
                        "id": 11677,
                        "content": "<p>Use AWS Elastic Disaster Recovery with continuous replication to act as a pilot light solution on AWS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11678,
                        "content": "<p>Configure an active-active multi-site setup between the on-premises server and AWS using PostgreSQL with a third-party high availability solution.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2798,
            "attributes": {
                "createdAt": "2023-09-09T20:40:14.500Z",
                "updatedAt": "2023-09-09T20:40:14.500Z",
                "content": "<p>A travel agency operates a web service in an AWS Region. The service is accessed by customers via a REST API on Amazon API Gateway. The agency uses Amazon Route 53 for DNS and wants to provide individual and secure URLs for each travel agent using the service.</p><p>Which combination of steps will meet these requirements with the LEAST operational complexity? (Select THREE.)</p>",
                "answerExplanation": "<p>Registering a wildcard custom domain name in Route 53 and creating a record pointing to API Gateway endpoint allows you to create unique URLs for each customer under the same domain name.</p><p>Requesting a wildcard certificate in the same AWS region as the REST API would provide secure URLs (https) for all customers under the same domain name. This would minimize the operational complexity of managing multiple certificates in different regions.</p><p>By creating a custom domain name in API Gateway and importing the wildcard certificate from ACM, the company can provide secure and unique URLs for each customer. API Gateway's custom domain names provide paths for API methods, helping maintain a consistent experience for customers.</p><p><strong>CORRECT: </strong>\"Register the desired domain with a domain registrar. Set up a wildcard custom domain in a Route 53 hosted zone and create a record in the zone that points to the API Gateway endpoint\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager (ACM) in the same Region\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Establish a custom domain name in API Gateway for the REST API. Import the corresponding certificate from AWS Certificate Manager (ACM)\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Request a wildcard certificate that corresponds to the custom domain name in AWS Certificate Manager (ACM), within a different Region\" is incorrect.</p><p>Requesting a wildcard certificate in a different AWS region than your API Gateway increases operational complexity and doesn't provide any significant benefit.</p><p><strong>INCORRECT:</strong> \"Create separate hosted zones in Route 53 for each travel agent as needed. Set up zone records that point to the API Gateway endpoint\" is incorrect.</p><p>Creating separate hosted zones for each travel agent can significantly increase operational complexity and cost. It would be more efficient to use a single hosted zone with a wildcard domain and use paths for differentiation.</p><p><strong>INCORRECT:</strong> \"Establish separate API endpoints in API Gateway for each travel agent\" is incorrect.</p><p>Creating separate API endpoints for each travel agent can significantly increase the complexity and management overhead. Instead, it would be more efficient to use different paths under the same API endpoint.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/DomainNameFormat.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/DomainNameFormat.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certificate-manager/\">https://digitalcloud.training/aws-certificate-manager/</a></p>",
                "options": [
                    {
                        "id": 11679,
                        "content": "<p>Register the desired domain with a domain registrar. Set up a wildcard custom domain in a Route 53 hosted zone and create a record in the zone that points to the API Gateway endpoint.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11680,
                        "content": "<p>Create separate hosted zones in Route 53 for each travel agent as needed. Set up zone records that point to the API Gateway endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11681,
                        "content": "<p>Establish separate API endpoints in API Gateway for each travel agent.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11682,
                        "content": "<p>Establish a custom domain name in API Gateway for the REST API. Import the corresponding certificate from AWS Certificate Manager (ACM).</p>",
                        "isValid": true
                    },
                    {
                        "id": 11683,
                        "content": "<p>Request a wildcard certificate that corresponds to the custom domain name in AWS Certificate Manager (ACM), within a different Region.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11684,
                        "content": "<p>Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager (ACM) in the same Region.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2799,
            "attributes": {
                "createdAt": "2023-09-09T20:40:14.617Z",
                "updatedAt": "2023-09-09T20:40:14.617Z",
                "content": "<p>A company has several AWS accounts each with multiple Amazon VPCs. The company must establish routing between all private subnets. The architecture should be simple and allow transitive routing to occur.</p><p>How should the network connectivity be configured?</p>",
                "answerExplanation": "<p>You can build a hub-and-spoke topology with AWS Transit Gateway that supports transitive routing. This simplifies the network topology and adds additional features over VPC peering. AWS Resource Access Manager can be used to share the connection with the other AWS accounts.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-10-15-06b3dc1b26269bff9b93a25c89727620.png\"></p><p><strong>CORRECT: </strong>\"Create an AWS Transit Gateway and share it with each account using AWS Resource Access Manager\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a transitive VPC peering connection between each Amazon VPC and configure route tables\" is incorrect. You cannot create transitive connections with VPC peering.</p><p><strong>INCORRECT:</strong> \"Create an AWS Managed VPN between each Amazon VPC and configure route tables\" is incorrect. This is a much more complex solution compared to AWS Transit Gateway so is not the best option.</p><p><strong>INCORRECT:</strong> \"Create a hub-and-spoke topology with AWS App Mesh and use AWS Resource Access Manager to share route tables\" is incorrect. AWS App Mesh is used for application-level networking for microservices applications.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-use-an-aws-transit-gateway-to-simplify-your-network-architecture/\">https://aws.amazon.com/blogs/aws/new-use-an-aws-transit-gateway-to-simplify-your-network-architecture/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11685,
                        "content": "<p>Create a hub-and-spoke topology with AWS App Mesh and use AWS Resource Access Manager to share route tables</p>",
                        "isValid": false
                    },
                    {
                        "id": 11686,
                        "content": "<p>Create an AWS Transit Gateway and share it with each account using AWS Resource Access Manager</p>",
                        "isValid": true
                    },
                    {
                        "id": 11687,
                        "content": "<p>Create a transitive VPC peering connection between each Amazon VPC and configure route tables</p>",
                        "isValid": false
                    },
                    {
                        "id": 11688,
                        "content": "<p>Create an AWS Managed VPN between each Amazon VPC and configure route tables</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2800,
            "attributes": {
                "createdAt": "2023-09-09T20:40:14.693Z",
                "updatedAt": "2023-09-09T20:40:14.693Z",
                "content": "<p>A data analytics company is hosting a data lake which consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization for the latest dataset and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>If you have data in sources other than Amazon S3, you can use Athena Federated Query to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3. With Athena Federated Query, you can run SQL queries across data stored in relational, non-relational, object, and custom data sources.</p><p>Athena uses <em>data source connectors</em> that run on AWS Lambda to run federated queries. A data source connector is a piece of code that can translate between your target data source and Athena. You can think of a connector as an extension of Athena's query engine. Prebuilt Athena data source connectors exist for data sources like Amazon CloudWatch Logs, Amazon DynamoDB, Amazon DocumentDB, and Amazon RDS, and JDBC-compliant relational data sources such MySQL, and PostgreSQL under the Apache 2.0 license.</p><p><strong>CORRECT: </strong>\"Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles\" is incorrect.</p><p>This would have worked for one time data set which only needed visualization. For any new data, analysis would need to be performed again. Also, you connect user and groups in your QuickSight account but not IAM Roles.</p><p><strong>INCORRECT:</strong> \"Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups \" is incorrect.</p><p>As with the previous answer, this option solves the problem of access sharing with resources but does not take care of delta in data. Also, you connect user and groups in your QuickSight account but not IAM Roles.</p><p><strong>INCORRECT:</strong> \"Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports\" is incorrect.</p><p>Amazon Athena should be used with AWS Glue to provide the required functionality as described in the explanation above and the article linked below.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html\">https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-glue/\">https://digitalcloud.training/aws-glue/</a></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
                "options": [
                    {
                        "id": 11689,
                        "content": "<p>Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11690,
                        "content": "<p>Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11691,
                        "content": "<p>Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11692,
                        "content": "<p>Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2801,
            "attributes": {
                "createdAt": "2023-09-09T20:40:14.768Z",
                "updatedAt": "2023-09-09T20:40:14.768Z",
                "content": "<p>A multinational podcast company uses Amazon CloudFront for distributing its digital content. The company wants to gradually introduce content across various regions. It also needs to ensure that listeners who are outside the regions to which the content is currently released, cannot access the content.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>By setting geographical restrictions on CloudFront content using a deny list, the company can block access to content for users outside of the released regions. If a user from a blocked region attempts to access the content, they would receive the custom error message, thereby meeting the company's requirements.</p><p><strong>CORRECT: </strong>\"Implement geographical restrictions on CloudFront content using a deny list and create a custom error message\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Establish a new URL for the restricted content, control access with signed URLs and cookies, and set up a custom error message\" is incorrect.</p><p>While signed URLs and cookies can be used to control access to content, they don't inherently consider the geographical location of the users, thus it would not guarantee that only users in the released regions could access the content.</p><p><strong>INCORRECT:</strong> \"Encrypt the company's distributed content data and establish a custom error message\" is incorrect.</p><p>Although encrypting the content data adds a layer of security, it does not restrict access based on the geographical location of the users.</p><p><strong>INCORRECT:</strong> \"Create a new URL for the restricted content and establish an expiration date-based access policy for signed URLs\" is incorrect.</p><p>Time-based access policies with signed URLs can limit access to the content after a certain time, but it does not restrict access based on the geographical location of the users.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 11693,
                        "content": "<p>Implement geographical restrictions on CloudFront content using a deny list and create a custom error message.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11694,
                        "content": "<p>Create a new URL for the restricted content and establish an expiration date-based access policy for signed URLs.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11695,
                        "content": "<p>Encrypt the company's distributed content data and establish a custom error message.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11696,
                        "content": "<p>Establish a new URL for the restricted content, control access with signed URLs and cookies, and set up a custom error message.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2802,
            "attributes": {
                "createdAt": "2023-09-09T20:40:14.851Z",
                "updatedAt": "2023-09-09T20:40:14.851Z",
                "content": "<p>A social media application is creating new functionality that will convert uploaded images to smaller, thumbnail images. When a user uploads an image through the web interface, the application should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function and store the image in its compressed form in a different S3 bucket.</p><p>The solution architect must develop a stateless, durable solution to process images automatically upon upload.</p><p>Which combination of actions will meet these requirements? (Select TWO.)</p>",
                "answerExplanation": "<p>You can use event notifications to publish an event to a destination when something happens in a bucket. Destinations include Lambda, SNS, and SQS. In this case the event notification can be configured to publish a message to an SQS queue when an object creation event occurs.</p><p>Lambda can be configured to poll the queue looking for new messages. When a message is added to the queue Lambda can process the message which will let the function know which image to resize. The resized image can then be saved to an output bucket.</p><p><strong>CORRECT: </strong>\"Create an Amazon SQS queue. Configure an event notification to add a message to the SQS queue when an image is uploaded to the S3 bucket” is the correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure the Lambda function to use the Amazon SQS queue as the event source. The Lambda function will resize the image and store it in a separate S3 Bucket\" is also the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the S3 Bucket to be an event source for a Lambda Function. When an uploaded image is detected, write the file name to a text file in memory and use the text file to keep track of the images that were processed” is incorrect. This solution saves state in memory which is not durable.</p><p><strong>INCORRECT:</strong> \"Launch an Amazon EC2 instance to connect to an Amazon SQS queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function” is incorrect. A single EC2 instance is not a durable solution, as if the single instance failed the solution would no longer work.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon EventBridge event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon SNS topic with the application owner's email address for further processing” is incorrect. An event notification should be created on the S3 bucket to publish information about object creation events. Destinations can be Lambda, SNS, or SQS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 11697,
                        "content": "<p>Create an Amazon SQS queue. Configure an event notification to add a message to the SQS queue when an image is uploaded to the S3 bucket.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11698,
                        "content": "<p>Configure the S3 Bucket to be an event source for a Lambda Function. When an uploaded image is detected, write the file name to a text file in memory and use the text file to keep track of the images that were processed.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11699,
                        "content": "<p>Configure the Lambda function to use the Amazon SQS queue as the event source. The Lambda function will resize the image and store it in a separate S3 Bucket.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11700,
                        "content": "<p>Configure an Amazon EventBridge event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon SNS topic with the application owner's email address for further processing.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11701,
                        "content": "<p>Launch an Amazon EC2 instance to connect to an Amazon SQS queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2803,
            "attributes": {
                "createdAt": "2023-09-09T20:40:14.937Z",
                "updatedAt": "2023-09-09T20:40:14.937Z",
                "content": "<p>A retail company is running an important event. The company require guaranteed capacity in two specific Availability Zones in a specific AWS Region for running Amazon EC2 instances for 5 consecutive days.</p><p>What is the best way to ensure guaranteed EC2 capacity?</p>",
                "answerExplanation": "<p>On-Demand Capacity Reservations enable you to reserve compute capacity for your Amazon EC2 instances in a specific Availability Zone for any duration.</p><p>When creating Capacity Reservations, you ensure that you always have access to EC2 capacity when you need it, for as long as you need it, in this case for 5 days. You can create Capacity Reservations at any time, without entering a one-year or three-year term commitment.</p><p>Also, when you create a Capacity Reservation, you specify:</p><p>- The Availability Zone in which to reserve the capacity.</p><p>- The number of instances for which to reserve capacity.</p><p>- The instance attributes, including the instance type, tenancy, and platform/OS.</p><p><strong>CORRECT: </strong>\"Create an On-Demand Capacity Reservation that specifies the Region and two Availability Zones needed\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Purchase Reserved Instances that specify the Region\" is incorrect. Reserved Instances do not provide guaranteed capacity and are solely a billing discount.</p><p><strong>INCORRECT:</strong> \"Create an On-Demand Capacity Reservation that specifies the Region” is incorrect as you must specify the Availability zones required when reserving capacity.</p><p><strong>INCORRECT:</strong> \"Purchase Reserved Instances that specify the Region and two Availability Zones needed” is incorrect. Reserved Instances do not provide guaranteed capacity and are solely a billing discount.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-billing-and-pricing/\">https://digitalcloud.training/aws-billing-and-pricing/</a></p>",
                "options": [
                    {
                        "id": 11702,
                        "content": "<p>Purchase Reserved Instances that specify the Region and two Availability Zones needed.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11703,
                        "content": "<p>Create an On-Demand Capacity Reservation that specifies the Region.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11704,
                        "content": "<p>Create an On-Demand Capacity Reservation that specifies the Region and two Availability Zones needed.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11705,
                        "content": "<p>Purchase Reserved Instances that specify the Region.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2804,
            "attributes": {
                "createdAt": "2023-09-09T20:40:15.028Z",
                "updatedAt": "2023-09-09T20:40:15.028Z",
                "content": "<p>A finance organization has bootstrapped a golden image for their in-house application and the resultant AMI is to be shared across various AWS accounts as a base image. This image is to be used across many applications. The company needs to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 CreateImage API operation is called within the company's account.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>You can create an Amazon EventBridge rule that triggers on an action by an AWS service that does not emit events. In this case you can base the rule on API calls made by AWS CloudTrail. The rule can trigger when the Amazon EC2 CreateImage API is called. The rule can then trigger another service or action.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule for the CreateImage API call. Configure the target as an Amazon SNS topic to send an alert when a Createlmage API call is detected\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS CloudTrail with an Amazon SNS notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on CreateImage when an API call is detected\" is incorrect.</p><p>Athena is a query analysis tool hence this option is incorrect.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a CreateImage API call is detected\" is incorrect.</p><p>Since the question asks about least operational overhead, this option becomes incorrect. This is an achievable solution but involves building custom code in Lambda and requires more effort.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon SQS FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon SNS topic when a CreateImage API call is detected\" is incorrect.</p><p>You cannot configure CloudTrail logs to be sent directly to an SQS queue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-log-api-call.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-log-api-call.html</a></p>",
                "options": [
                    {
                        "id": 11706,
                        "content": "<p>Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a CreateImage API call is detected.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11707,
                        "content": "<p>Configure AWS CloudTrail with an Amazon SNS notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on CreateImage when an API call is detected</p>",
                        "isValid": false
                    },
                    {
                        "id": 11708,
                        "content": "<p>Configure an Amazon SQS FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon SNS topic when a CreateImage API call is detected.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11709,
                        "content": "<p>Create an Amazon EventBridge rule for the CreateImage API call. Configure the target as an Amazon SNS topic to send an alert when a Createlmage API call is detected.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2805,
            "attributes": {
                "createdAt": "2023-09-09T20:40:15.115Z",
                "updatedAt": "2023-09-09T20:40:15.115Z",
                "content": "<p>An e-commerce website uses Amazon EC2 instance stores for storing session data. The company want to make sure that this data is highly available, and that the information is stored durably.</p><p>What should a solutions architect do to meet these requirements?</p>",
                "answerExplanation": "<p>Amazon DynamoDB is a NoSQL database and is ideal for storing session data. The data will be both highly available and durable and can be stored persistently. DynamoDB also offers time to live (TTL) attributes that can be used to automatically expire items from the table after specified time periods.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-09-21_03-25-17-85c40bff067bf144ef733a6f48acfe24.jpg\"><p><strong>CORRECT: </strong>\"Store the session data in an Amazon DynamoDB table\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Move the session data to Amazon ElastiCache for Memcached” is incorrect. ElastiCache Memcached does not store data durably or persistently. ElastiCache can be used for storing session data, but the Redis engine should be used instead.</p><p><strong>INCORRECT:</strong> \"Deploy a larger EC2 instance with a larger instance store” is incorrect. Instance stores use ephemeral storage which means it is non-persistent. The size of the instance store does not change anything here.</p><p><strong>INCORRECT:</strong> \" Move the session data to Amazon S3 Glacier Deep Archive” is incorrect. Glacier is an archiving solution and cannot be used for data that requires immediate access. It is unsuitable for storing session data.</p><p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 11710,
                        "content": "<p>Deploy a larger EC2 instance with a larger instance store.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11711,
                        "content": "<p>Move the session data to Amazon ElastiCache for Memcached.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11712,
                        "content": "<p>Store the session data in an Amazon DynamoDB table.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11713,
                        "content": "<p>Move the session data to Amazon S3 Glacier Deep Archive.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2806,
            "attributes": {
                "createdAt": "2023-09-09T20:40:15.200Z",
                "updatedAt": "2023-09-09T20:40:15.200Z",
                "content": "<p>A Solutions Architect is designing a migration strategy for a company moving to the AWS Cloud. The company use a shared Microsoft filesystem that uses Distributed File System Namespaces (DFSN). What will be the MOST suitable migration strategy for the filesystem?</p>",
                "answerExplanation": "<p>The destination filesystem should be Amazon FSx for Windows File Server. This supports DFSN and is the most suitable storage solution for Microsoft filesystems. AWS DataSync supports migrating to the Amazon FSx and automates the process.</p><p><strong>CORRECT: </strong>\"Use AWS DataSync to migrate to Amazon FSx for Windows File Server\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use the AWS Server Migration Service to migrate to Amazon FSx for Lustre\" is incorrect. The server migration service is used to migrate virtual machines and FSx for Lustre does not support Windows filesystems.</p><p><strong>INCORRECT:</strong> \"Use AWS DataSync to migrate to an Amazon EFS filesystem\" is incorrect. You can migrate data to EFS using DataSync but it is the wrong destination for a Microsoft filesystem (Linux only).</p><p><strong>INCORRECT:</strong> \"Use the AWS Server Migration Service to migrate to an Amazon S3 bucket\" is incorrect. The server migration service is used to migrate virtual machines and Amazon S3 is an object-based storage system and unsuitable for hosting a Microsoft filesystem.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/storage/migrate-to-amazon-fsx-for-windows-file-server-using-aws-datasync/\">https://aws.amazon.com/blogs/storage/migrate-to-amazon-fsx-for-windows-file-server-using-aws-datasync/</a></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-files-fsx.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-files-fsx.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
                "options": [
                    {
                        "id": 11714,
                        "content": "<p>Use AWS DataSync to migrate to an Amazon EFS filesystem</p>",
                        "isValid": false
                    },
                    {
                        "id": 11715,
                        "content": "<p>Use AWS DataSync to migrate to Amazon FSx for Windows File Server</p>",
                        "isValid": true
                    },
                    {
                        "id": 11716,
                        "content": "<p>Use the AWS Server Migration Service to migrate to Amazon FSx for Lustre</p>",
                        "isValid": false
                    },
                    {
                        "id": 11717,
                        "content": "<p>Use the AWS Server Migration Service to migrate to an Amazon S3 bucket</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2807,
            "attributes": {
                "createdAt": "2023-09-09T20:40:15.276Z",
                "updatedAt": "2023-09-09T20:40:15.276Z",
                "content": "<p>A media company is designing a disaster recovery (DR) solution for a business-critical application. The recovery time objective (RTO) should be 4 hours or less. The application is running on Amazon EC2 instances using the fewest possible AWS resources during normal operations.</p><p>Which of the following is recommended to implement the DR solution across regions cost-effectively?</p>",
                "answerExplanation": "<p>When you have a few hours to achieve disaster recovery, copying AMI’s across regions is an achievable solution. AWS CloudFormation can then be us</p><p>ed to quickly spin up the instances in the second region when a disaster recovery event occurs. This is the most cost-effective option as only the active site has running instances.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-08-45-2eadec4e7234a3f88da746220e9d2aad.jpg\"><p><strong>CORRECT: </strong>\"Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create Amazon Machine Images (AMI) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts\" is incorrect.</p><p>AWS CloudFormation is more suited to deploying infrastructure than using Lambda with custom scripts.</p><p><strong>INCORRECT:</strong> \"Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times\" is incorrect.</p><p>This approach can work but this is not a cost-effective choice.</p><p><strong>INCORRECT:</strong> \"Launch EC2 instances in a secondary Availability Zone. Always keep the EC2 instances in the secondary Availability Zone active\" is incorrect. As with the previous answer, this is not cost-effective.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws-part-i-strategies-for-recovery-in-the-cloud/\">https://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws-part-i-strategies-for-recovery-in-the-cloud/</a></p><p><a href=\"https://aws.amazon.com/blogs/architecture/creating-a-multi-region-application-with-aws-services-part-1-compute-and-security/\">https://aws.amazon.com/blogs/architecture/creating-a-multi-region-application-with-aws-services-part-1-compute-and-security/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>",
                "options": [
                    {
                        "id": 11718,
                        "content": "<p>Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11719,
                        "content": "<p>Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11720,
                        "content": "<p>Create Amazon Machine Images (AMI) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11721,
                        "content": "<p>Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary Availability Zone active at all times.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2808,
            "attributes": {
                "createdAt": "2023-09-09T20:40:15.358Z",
                "updatedAt": "2023-09-09T20:40:15.358Z",
                "content": "<p>A traffic law enforcement company is building a solution that has thousands of edge devices that collectively generate 1 TB of status alerts each day. These devices provide vehicle information and number plate data whenever alerts detecting red light jumps are detected. Each entry is around 2Kb in size. A solutions architect needs to implement a solution to ingest and store the alerts for future analysis.</p><p>The company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure. Additionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days.</p><p>What is the MOST operationally efficient solution that meets these requirements?</p>",
                "answerExplanation": "<p>Data ingestion is a good use case for since it is scalable and can achieve the volumes required. Also, an S3 lifecycle configuration is appropriate for the requirement for data retention.</p><p><strong>CORRECT: </strong>\"Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days\" is incorrect. Provisioning additional EC2 instances means provisioning infrastructure, and the question states that the company wants to avoid this.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the kinesis Data Firehose stream to deliver the alerts to an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Set up the Amazon Open Search Service (Amazon Elasticsearch Service) cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days\" is incorrect. This option would mean provisioning ECS clusters and since the question is asking for archival of data, S3 is a better fit (data deletion is not desired).</p><p><strong>INCORRECT:</strong> \"Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts and set the message retention period to 14 days. Configure consumers to poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete the message from the SQS queue\" is incorrect.</p><p>With an SQS queue you must have processing components adding and retrieving messages from the queue and this means additional infrastructure to manage. With Kinesis Data Firehose the data is loaded straight to the destination without any need for additional infrastructure.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a></p><p><a href=\"https://aws.amazon.com/kinesis/data-firehose/features/\">https://aws.amazon.com/kinesis/data-firehose/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 11722,
                        "content": "<p>Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11723,
                        "content": "<p>Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts and set the message retention period to 14 days. Configure consumers to poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete the message from the SQS queue.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11724,
                        "content": "<p>Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11725,
                        "content": "<p>Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the kinesis Data Firehose stream to deliver the alerts to an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Set up the Amazon Open Search Service (Amazon Elasticsearch Service) cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2809,
            "attributes": {
                "createdAt": "2023-09-09T20:40:15.438Z",
                "updatedAt": "2023-09-09T20:40:15.438Z",
                "content": "<p>A company runs an API on a Linux server in their on-premises data center. The company are planning to migrate the API to the AWS cloud. The company require a highly available, scalable and cost-effective solution. What should a Solutions Architect recommend?</p>",
                "answerExplanation": "<p>The best option is to use a fully serverless solution. This will provide high availability, scalability and be cost-effective. The components for this would be Amazon API Gateway for hosting the API and AWS Lambda for running the backend.</p><p>As you can see in the image below, API Gateway can be the frontend for multiple backend services:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-09-23-7dad959e9a620f37463b0048341769bc.png\"></p><p><strong>CORRECT: </strong>\"Migrate the API to Amazon API Gateway and use AWS Lambda as the backend\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Migrate the API to Amazon API Gateway and migrate the backend to Amazon EC2\" is incorrect. This is a less available and cost-effective solution for the backend compared to AWS Lambda.</p><p><strong>INCORRECT:</strong> \"Migrate the API server to Amazon EC2 instances in an Auto Scaling group and attach an Application Load Balancer\" is incorrect. Firstly, it may be difficult to load balance to an API. Additionally, this is a less cost-effective solution.</p><p><strong>INCORRECT:</strong> \"Migrate the API to Amazon CloudFront and use AWS Lambda as the origin\" is incorrect. You cannot migrate an API to CloudFront. You can use CloudFront in front of API Gateway but that is not what this answer specifies.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 11726,
                        "content": "<p>Migrate the API server to Amazon EC2 instances in an Auto Scaling group and attach an Application Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 11727,
                        "content": "<p>Migrate the API to Amazon API Gateway and migrate the backend to Amazon EC2</p>",
                        "isValid": false
                    },
                    {
                        "id": 11728,
                        "content": "<p>Migrate the API to Amazon CloudFront and use AWS Lambda as the origin</p>",
                        "isValid": false
                    },
                    {
                        "id": 11729,
                        "content": "<p>Migrate the API to Amazon API Gateway and use AWS Lambda as the backend</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2810,
            "attributes": {
                "createdAt": "2023-09-09T20:40:15.526Z",
                "updatedAt": "2023-09-09T20:40:15.526Z",
                "content": "<p>A law firm has recently productionized a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database servers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from the AWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can accept IP packets.</p><p>A solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>Gateway Load Balancers enable you to deploy, scale, and manage virtual appliances, such as firewalls, intrusion detection and prevention systems, and deep packet inspection systems. It combines a transparent network gateway (that is, a single entry and exit point for all traffic) and distributes traffic while scaling your virtual appliances with the demand.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-50-23-b89bd576b1370b29356a1ab3a41a3370.jpg\"><p><strong>CORRECT: </strong>\"Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection\" is incorrect.</p><p>Network load balancers work on Layer 4 of the OSI model and work on TCP, UDP and TLS protocols. They are not used for packet inspection.</p><p><strong>INCORRECT:</strong> \"Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection\" is incorrect.</p><p>Application load balancers work on Layer 7 and used with HTTP/HTTPS traffic. They are also not used for packet inspection.</p><p><strong>INCORRECT:</strong> \"Deploy a transit gateway in the inspection VPC. Configure route tables to route the incoming packets through the transit gateway\" is incorrect.</p><p>Transit Gateways are used for routing traffic and connecting networks and VPCs, they are not used for packet inspection purposes. In this case a load balancer is required to distributed connections to the virtual firewall appliances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/introduction.html</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection-using-aws-gateway-load-balancer/\">https://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection-using-aws-gateway-load-balancer/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 11730,
                        "content": "<p>Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11731,
                        "content": "<p>Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11732,
                        "content": "<p>Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11733,
                        "content": "<p>Deploy a transit gateway in the inspection VPC. Configure route tables to route the incoming packets through the transit gateway.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2811,
            "attributes": {
                "createdAt": "2023-09-09T20:40:15.640Z",
                "updatedAt": "2023-09-09T20:40:15.640Z",
                "content": "<p>A telemarketing company has developed customer call center functionality on AWS. The company plans to enhance the current application by enabling support for multiple speaker recognition and transcript generation. They also want to query the transcript files to analyze business patterns.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>Amazon Transcribe converts audio input into text, which opens the door for various text analytics applications on voice input. For instance, by using Amazon Comprehend on the converted text data from Amazon Transcribe, customers can perform sentiment analysis or extract entities and key phrases.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-39-19-22fe898d923b0e18e74ca7e8d8bc3f06.jpg\"><p><strong>CORRECT: </strong>\"Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use machine learning models for transcript file analysis\" is incorrect.</p><p>Amazon Rekognition Video can detect objects, scenes, faces, celebrities, text, and inappropriate content in videos. You can also search for faces appearing in a video using your own repository or collection of face images.</p><p><strong>INCORRECT:</strong> \"Use Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use SQL queries for transcript file analysis\" is incorrect.</p><p>Amazon Translate can provide automatic translation to enable cross-lingual communications between users for your applications.</p><p><strong>INCORRECT:</strong> \"Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use Amazon Textract for transcript file analysis\" is incorrect.</p><p>As mentioned above, Rekognition is better suited for identifying content in videos. Also,</p><p>Amazon Textract is a machine learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena/\">https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-machine-learning-services/\">https://digitalcloud.training/aws-machine-learning-services/</a></p>",
                "options": [
                    {
                        "id": 11734,
                        "content": "<p>Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use machine learning models for transcript file analysis.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11735,
                        "content": "<p>Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11736,
                        "content": "<p>Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use Amazon Textract for transcript file analysis.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11737,
                        "content": "<p>Use Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use SQL queries for transcript file analysis.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2812,
            "attributes": {
                "createdAt": "2023-09-09T20:40:15.722Z",
                "updatedAt": "2023-09-09T20:40:15.722Z",
                "content": "<p>A healthcare company maintains patient records in Amazon S3. To comply with HIPAA regulations, the stored data must not contain any protected health information (PHI). The company recently found out that some objects in the S3 buckets contain PHI. The company needs to automate the detection of PHI in the S3 buckets and notify its compliance team when such data is detected.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data like PHI. An Amazon EventBridge rule can be created to filter specific event types from Macie findings. When Macie identifies PHI in the S3 bucket, the EventBridge rule triggers an Amazon SES notification to the compliance team.</p><p><strong>CORRECT: </strong>\"Use Amazon Macie. Create an Amazon EventBridge rule to filter the ‘SensitiveData:S3Object/Health’ event type from Macie findings and trigger an Amazon Simple Email Service (Amazon SES) notification to the compliance team\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Macie. Create an AWS Lambda function to filter the ‘SensitiveData:S3Object/Personal’ event type from Macie findings and trigger an Amazon Simple Notification Service (Amazon SNS) notification to the compliance team\" is incorrect.</p><p>Although Macie can be used to discover sensitive data, this option is filtering the event type ‘SensitiveData:S3Object/Personal’ which is specific to personal data, not PHI.</p><p><strong>INCORRECT:</strong> \"Use AWS Security Hub. Create an Amazon EventBridge rule to filter the ‘Security Hub findings - High severity’ event type and send an Amazon Simple Notification Service (Amazon SNS) notification to the compliance team\" is incorrect.</p><p>AWS Security Hub gives a comprehensive view of high-priority security alerts and compliance status, but it does not offer data-specific detection like PHI in S3 objects.</p><p><strong>INCORRECT:</strong> \"Use AWS Security Hub. Create an AWS Lambda function to filter the ‘Security Hub findings - High severity’ event type and trigger an Amazon Simple Email Service (Amazon SES) notification to the compliance team\" is incorrect.</p><p>AWS Security Hub does not offer detection of specific data types like PHI in S3 objects. Therefore, using it for this purpose would not meet the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/macie/latest/user/findings-types.html\">https://docs.aws.amazon.com/macie/latest/user/findings-types.html</a></p>",
                "options": [
                    {
                        "id": 11738,
                        "content": "<p>Use AWS Security Hub. Create an Amazon EventBridge rule to filter the ‘Security Hub findings - High severity’ event type and send an Amazon Simple Notification Service (Amazon SNS) notification to the compliance team.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11739,
                        "content": "<p>Use Amazon Macie. Create an Amazon EventBridge rule to filter the ‘SensitiveData:S3Object/Health’ event type from Macie findings and trigger an Amazon Simple Email Service (Amazon SES) notification to the compliance team.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11740,
                        "content": "<p>Use Amazon Macie. Create an AWS Lambda function to filter the ‘SensitiveData:S3Object/Personal’ event type from Macie findings and trigger an Amazon Simple Notification Service (Amazon SNS) notification to the compliance team.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11741,
                        "content": "<p>Use AWS Security Hub. Create an AWS Lambda function to filter the ‘Security Hub findings - High severity’ event type and trigger an Amazon Simple Email Service (Amazon SES) notification to the compliance team.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2813,
            "attributes": {
                "createdAt": "2023-09-09T20:40:15.802Z",
                "updatedAt": "2023-09-09T20:40:15.802Z",
                "content": "<p>An online game platform company is launching a new game feature that involves a significant update to their existing API hosted on Amazon API Gateway. The company wants to minimize the impact on their existing users, and they need a deployment strategy that allows them to gradually roll out the changes while monitoring for any potential issues.</p><p>What should the company do to achieve this?</p>",
                "answerExplanation": "<p>The correct answer is to use Amazon API Gateway's canary release deployments. This allows the company to gradually roll out the new API version, initially exposing only a small percentage of their users to the new API. As they monitor the system and confirm that the new API is working as expected, they can increase the percentage of traffic directed to the new version.</p><p><strong>CORRECT: </strong>\"Use an API Gateway canary release deployment. Initially direct a small percentage of user traffic to the new API version. After API verification, promote the canary stage to the production stage\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Update the existing API directly in API Gateway with the new feature and immediately direct all traffic to the updated API\" is incorrect.</p><p>Updating the existing API directly in API Gateway and immediately redirecting all traffic to the updated API is risky. If there are any issues with the new API, it could negatively impact all users, rather than just a small subset of users.</p><p><strong>INCORRECT:</strong> \"Create a completely new API for the new game feature and redirect half of the user traffic to the new API while maintaining the other half on the existing API\" is incorrect.</p><p>Creating a completely new API and redirecting half the user traffic to the new API is not a gradual rollout strategy. This approach would immediately expose many users to potential issues with the new API.</p><p><strong>INCORRECT:</strong> \"Create a new version of the API and use Route 53 to gradually shift DNS queries from the existing API endpoint to the new API endpoint\" is incorrect.</p><p>Using Route 53 to gradually shift DNS queries from the existing API endpoint to the new API endpoint could work, but it is not as simple or efficient as using API Gateway's canary release deployments. DNS changes can also take time to propagate, potentially leading to inconsistent behavior for users.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 11742,
                        "content": "<p>Use an API Gateway canary release deployment. Initially direct a small percentage of user traffic to the new API version. After API verification, promote the canary stage to the production stage.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11743,
                        "content": "<p>Create a completely new API for the new game feature and redirect half of the user traffic to the new API while maintaining the other half on the existing API.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11744,
                        "content": "<p>Create a new version of the API and use Route 53 to gradually shift DNS queries from the existing API endpoint to the new API endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11745,
                        "content": "<p>Update the existing API directly in API Gateway with the new feature and immediately direct all traffic to the updated API.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2814,
            "attributes": {
                "createdAt": "2023-09-09T20:40:16.104Z",
                "updatedAt": "2023-09-09T20:40:16.104Z",
                "content": "<p>To trace a recent production incident a product manager needs to view logs in the Amazon CloudWatch logs. These logs are linked to events over the course of a week and may be needed in the future if incidents occur again. The product manager doesn’t have administrative access to the AWS account as it is managed by a third-party management company.</p><p>According to principal of least privilege, which option out of the below will fulfill the requirement to provide the necessary access for the product manager?</p>",
                "answerExplanation": "<p>Below is the sequence for sharing the dashboard from Cloud watch console.</p><p>CloudWatch &gt; Dashboard &gt; Select your board &gt; Share Dashboard&gt;Share your dashboard and require a username and password&gt;Enter mail address</p><p>You can share your CloudWatch dashboards with people who do not have direct access to your AWS account. This enables you to share dashboards across teams, with stakeholders, and with people external to your organization. You can even display dashboards on big screens in team areas or embed them in Wikis and other webpages.</p><p><strong>CORRECT: </strong>\"Share the dashboard from the CloudWatch console. Enter the product manager's email address and complete the sharing steps. Provide a shareable link for the dashboard to the product manager\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentials with the product manager. Share the browser URL of the correct dashboard with the product manager\" is incorrect.</p><p>If the dashboard needs to be shared with additional users, this option increases manual effort every time and hence is not an optimal option.</p><p><strong>INCORRECT:</strong> \"Create an IAM user for the company's employees. Attach the ViewOnly Access AWS managed policy to the IAM user. Share the new login credentials with the product manager. Ask the product manager to navigate to the CloudWatch console and locate the dashboard by name in the Dashboards section\" is incorrect.</p><p>This option also involves lot of manual steps and as the recipients for the dashboard increase in number, manual effort increase and hence this is not an optimal option.</p><p><strong>INCORRECT:</strong> \"Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard\" is incorrect.</p><p>Exposing bastion server isn’t required here for sharing the dashboard. Bastion servers are meant to be jump boxes to allow accesses to EC2 instances which isn’t the ask in the question hence this is also an incorrect option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 11746,
                        "content": "<p>Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentials with the product manager. Share the browser URL of the correct dashboard with the product manager.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11747,
                        "content": "<p>Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11748,
                        "content": "<p>Share the dashboard from the CloudWatch console. Enter the client’s email address and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11749,
                        "content": "<p>Create an IAM user for the company's employees. Attach the ViewOnly Access AWS managed policy to the IAM user. Share the new login credentials with the product manager. Ask the product manager to navigate to the CloudWatch console and locate the dashboard by name in the Dashboards section.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2815,
            "attributes": {
                "createdAt": "2023-09-09T20:40:16.266Z",
                "updatedAt": "2023-09-09T20:40:16.266Z",
                "content": "<p>A company has an on-premises server that uses a MySQL database to process and store customer information. The company wants to migrate to an AWS database service to achieve higher availability and to improve application performance. Additionally, the company wants to offload reporting workloads from its primary database to ensure it remains performant.</p><p>Which solution will meet these requirements in the MOST operationally efficient way?</p>",
                "answerExplanation": "<p>Amazon Aurora with MySQL compatibility is a good fit for achieving high availability and improved performance. Aurora automatically distributes the data across multiple AZs in a single region. Additionally, Aurora allows the creation of up to 15 Aurora Replicas that share the same underlying volume as the primary instance. Directing reporting functions to the Aurora Replica is an effective way to offload reporting workloads from the primary database.</p><p><strong>CORRECT: </strong>\"Use Amazon Aurora with MySQL compatibility. Direct the reporting functions to use one of the Aurora Replicas\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon RDS with MySQL in a Single-AZ deployment. Create a read replica in the same availability zone as the primary DB instance. Direct the reporting functions to the read replica\" is incorrect.</p><p>Though you can use Amazon RDS with MySQL in a Single-AZ deployment and create a read replica, it is not the most operationally efficient option as it does not provide the high availability that Aurora's architecture offers.</p><p><strong>INCORRECT:</strong> \"Use AWS Database Migration Service (AWS DMS) to create an Amazon Aurora DB cluster in multiple AWS Regions. Point the reporting functions toward a separate DB instance from the primary DB instance\" is incorrect.</p><p>Using AWS DMS to create Amazon Aurora DB clusters in multiple AWS Regions would be overkill for the requirements. It could also introduce additional complexity and doesn’t specifically address using a replica for reporting purposes.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 instances to deploy a self-managed MySQL database with a replication setup for reporting purposes. Place instances in multiple availability zones and manage backups and patching manually\" is incorrect.</p><p>Managing your own database on Amazon EC2 instances requires a significant operational overhead as you need to handle backups, patch management, and high availability yourself. This option is not the most operationally efficient compared to using a managed database service like Amazon Aurora.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
                "options": [
                    {
                        "id": 11750,
                        "content": "<p>Use Amazon Aurora with MySQL compatibility. Direct the reporting functions to use one of the Aurora Replicas.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11751,
                        "content": "<p>Use AWS Database Migration Service (AWS DMS) to create an Amazon Aurora DB cluster in multiple AWS Regions. Point the reporting functions toward a separate DB instance from the primary DB instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11752,
                        "content": "<p>Use Amazon RDS with MySQL in a Single-AZ deployment. Create a read replica in the same availability zone as the primary DB instance. Direct the reporting functions to the read replica.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11753,
                        "content": "<p>Use Amazon EC2 instances to deploy a self-managed MySQL database with a replication setup for reporting purposes. Place instances in multiple availability zones and manage backups and patching manually.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2816,
            "attributes": {
                "createdAt": "2023-09-09T20:40:16.360Z",
                "updatedAt": "2023-09-09T20:40:16.360Z",
                "content": "<p>A financial services company is currently using 500 Amazon EC2 instances to run batch-processing workloads to analyze financial information on a periodic basis. The organization needs to install a third-party tool on all these instances as quickly and as efficiently as possible and will have to carry out similar tasks on an ongoing basis going forward. The solution also needs to scale for the addition of future EC2 instances.</p><p>What should a solutions architect do to meet these requirements in the easiest way possible?</p>",
                "answerExplanation": "<p>AWS Systems Manager Run command is designed to run commands across a large group of instances without having to SSH into all your instances and run the same command multiple times. You can easily run the same command to all the managed nodes as part of the workload, without having to maintain access keys or individual access for each instance.</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Run Command to run a custom command that installs the tool on all the EC2 instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda Function which will make configuration changes to all of the EC2 instances. Validate the tool has been installed using another Lambda function\" is incorrect. Whilst this may be possible, the code that would be required to create and test this solution would be difficult to design and would not scale effectively as AWS Systems Manager Run Command.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Patch Manager to install the tool on all of the EC2 instances within a single patch\" is incorrect. AWS Systems Manager Patch Manager is designed to apply patches to EC2 instances and is not designed to run commands across a large group of instances.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Maintenance Windows to install the tool on all of the EC2 instances within a set period of time\" is incorrect. AWS Systems Manager Maintenance Windows is designed to select a defined window of time in which you EC2 instances will be patched and is not capable of running commands across multiple instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>",
                "options": [
                    {
                        "id": 11754,
                        "content": "<p>Use AWS Systems Manager Run Command to run a custom command that installs the tool on all the EC2 instances.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11755,
                        "content": "<p>Use AWS Systems Manager Maintenance Windows to install the tool on all the EC2 instances within a set period of time.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11756,
                        "content": "<p>Use AWS Systems Manager Patch Manager to install the tool on all the EC2 instances within a single patch.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11757,
                        "content": "<p>Create an AWS Lambda Function which will make configuration changes to all the EC2 instances. Validate the tool has been installed using another Lambda function.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2817,
            "attributes": {
                "createdAt": "2023-09-09T20:40:16.457Z",
                "updatedAt": "2023-09-09T20:40:16.457Z",
                "content": "<p>A law firm has recently moved an on-premises multi-tier web application to AWS. Currently, the web application is based on a containerized solution and is running inside Linux based EC2 instances which connect to a PostgreSQL database hosted on separate but dedicated EC2 instances. The company wishes to optimize operational efficiency and performance.</p><p>Which combination of actions should the solutions architect take? (Select TWO.)</p>",
                "answerExplanation": "<p>Amazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL. You already know how MySQL and PostgreSQL combine the speed and reliability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases. The code, tools, and applications you use today with your existing MySQL and PostgreSQL databases can be used with Aurora. With some workloads, Aurora can deliver up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.</p><p>Amazon ECS is a fully managed container orchestration service that makes it easy for you to deploy, manage, and scale containerized applications. This is a better hosting solution for a containerized solution rather than managing the underlying container platform yourself. In the case of Fargate, the solution is serverless, so it massively reduces operational overhead.</p><p><strong>CORRECT: </strong>\"Migrate the PostgreSQL database to Amazon Aurora and Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS)\" are the correct answers (as explained above)</p><p><strong>INCORRECT:</strong> \"Migrate the web application to the same Amazon EC2 instances as the database\" is incorrect. This might reduce cost but doesn’t offer any other advantages.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon CloudFront distribution for the web application content\" is incorrect. CloudFront helps with caching content globally for better performance but does not help reduce the operational overhead or performance of this solution.</p><p><strong>INCORRECT:</strong> \"Set up Amazon ElastiCache between the web application and the PostgreSQL database\" is incorrect. Caching will only help when you have hot data segments and does not reduce the operational overhead of this solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Updates.Versions.html#AuroraMySQL.Updates.UpgradePaths\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Updates.Versions.html#AuroraMySQL.Updates.UpgradePaths</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 11758,
                        "content": "<p>Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS).</p>",
                        "isValid": true
                    },
                    {
                        "id": 11759,
                        "content": "<p>Set up an Amazon CloudFront distribution for the web application content.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11760,
                        "content": "<p>Migrate the web application to the same Amazon EC2 instances as the database.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11761,
                        "content": "<p>Migrate the PostgreSQL database to Amazon Aurora.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11762,
                        "content": "<p>Set up Amazon ElastiCache between the web application and the PostgreSQL database.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2818,
            "attributes": {
                "createdAt": "2023-09-09T20:40:16.553Z",
                "updatedAt": "2023-09-09T20:40:16.553Z",
                "content": "<p>A retail organization is building an ecommerce application on AWS. The application sends information about new orders to a REST API hosted on Amazon API Gateway to process. The company needs the orders to be processed in the order that they are received.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Based on the application requirements of having the orders to be processed in the order that they are received, you could use a FIFO queue, which offers high throughput, exactly-once-processing, and first-in-first-out-delivery.</p><p><strong>CORRECT: </strong>\"When an order is received, use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. For processing, configure the SQS FIFO queue to invoke an AWS Lambda function” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Integrate the Amazon Simple Notification Service (Amazon SNS) with API Gateway. The Amazon SNS topic will send a message to AWS Lambda where the message will be processed. \" is incorrect. Amazon SNS is not suitable for this application as Amazon SNS is a one-to-many messaging service designed to deliver messages to subscribers using SMS, Emails etc.</p><p><strong>INCORRECT:</strong> \"While the application processes an order, API Gateway authorizers will block any requests” is incorrect. A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API, which does not change how the traffic is delivered in which order.</p><p><strong>INCORRECT:</strong> \"When an order is received, use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue. For processing, configure the SQS standard queue to invoke an AWS Lambda function” is incorrect as an SQS standard queue offers best-effort-ordering, which is not suitable for this use case.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 11763,
                        "content": "<p>When an order is received, use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. For processing, configure the SQS FIFO queue to invoke an AWS Lambda function.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11764,
                        "content": "<p>While the application processes an order, API Gateway authorizers will block any requests.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11765,
                        "content": "<p>When an order is received, use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue. For processing, configure the SQS standard queue to invoke an AWS Lambda function.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11766,
                        "content": "<p>Integrate the Amazon Simple Notification Service (Amazon SNS) with API Gateway. The Amazon SNS topic will send a message to AWS Lambda where the message will be processed.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2819,
            "attributes": {
                "createdAt": "2023-09-09T20:40:16.654Z",
                "updatedAt": "2023-09-09T20:40:16.654Z",
                "content": "<p>A finance organization wants to deploy end of day processing applications to a fleet of Amazon EC2 instances with a focus on reducing cost. These applications are stateless and can be re-triggered in case of failure. The company needs a solution that minimizes cost and operational overhead.</p><p>What should a solutions architect do to meet these requirements?</p>",
                "answerExplanation": "<p>Since by using EC2 Spot Instances, customers can access additional compute capacity between 70%-90% off On-Demand Instance pricing, we can directly eliminate two options utilizing on demand instances.</p><p>Among the two options with spot instances, since the application is stateless, the better idea is to have a containerized approach and utilize EKS to reduce operational overhead.</p><p><strong>CORRECT: </strong>\"Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers\" is incorrect. As mentioned above, EKS gives you more options towards application fleet orchestration which makes it a better choice.</p><p><strong>INCORRECT:</strong> \"Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers\" is incorrect.</p><p>As compared to spot instances, on demand instances are costlier and for end of day processing where failures can be re-triggered and are acceptable, spot instances are a better choice.</p><p><strong>INCORRECT:</strong> \"Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group\" is incorrect.</p><p>As compared to spot instances, on demand instances are more expensive and for end of day processing where failures can be re-triggered and are acceptable, spot instances are a better choice.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/best-practices-for-handling-ec2-spot-instance-interruptions/\">https://aws.amazon.com/blogs/compute/best-practices-for-handling-ec2-spot-instance-interruptions/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 11767,
                        "content": "<p>Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11768,
                        "content": "<p>Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11769,
                        "content": "<p>Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11770,
                        "content": "<p>Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2820,
            "attributes": {
                "createdAt": "2023-09-09T20:40:16.766Z",
                "updatedAt": "2023-09-09T20:40:16.766Z",
                "content": "<p>A large manufacturing company is migrating many of its on-premises applications to AWS. The applications are staged in many different AWS accounts under a payer account, using AWS Organizations. The company's security team needs to enable a single sign-on (SSO) solution across all the company's accounts, and this must be integrated with the company's existing Active Directory setup.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>AWS IAM Identity Center (successor to AWS Single Sign-On) requires a two-way trust so that it has permissions to read user and group information from your domain to synchronize user and group metadata. IAM Identity Center uses this metadata when assigning access to permission sets or applications.</p><p>User and group metadata is also used by applications for collaboration, like when you share a dashboard with another user or group. The trust from AWS Directory Service for Microsoft Active Directory to your domain permits IAM Identity Center to trust your domain for authentication. The trust in the opposite direction grants AWS permissions to read user and group metadata.</p><p><strong>CORRECT: </strong>\"Enable AWS IAM Identity Center (successor to AWS SSO). Create a two-way forest trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable AWS IAM Identity Center (successor to AWS SSO). Create a one-way domain trust to connect the company's self-managed Microsoft Active Directory by using AWS Directory Service for Microsoft Active Directory” is incorrect. A two-way trust is required by IAM Identity Center.</p><p><strong>INCORRECT:</strong> \"Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed Microsoft Active Directory” is incorrect. This solution does not enable SSO across the accounts as it does not involve IAM Identity Center.</p><p><strong>INCORRECT:</strong> \"Deploy an identity provider (IdP) on premises. Enable AWS IAM Identity Center (successor to AWS SSO) from the AWS Identity Center console” is incorrect. The IdP is already deployed as the company has Microsoft AD. This does not provide a solution for integrating and enabling SSO.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 11771,
                        "content": "<p>Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed Microsoft Active Directory.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11772,
                        "content": "<p>Deploy an identity provider (IDP) on-premises. Enable AWS IAM Identity Center (successor to AWS SSO) from the AWS Identity Center console.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11773,
                        "content": "<p>Enable AWS IAM Identity Center (successor to AWS SSO). Create a one-way domain trust to connect the company's self-managed Microsoft Active Directory by using AWS Directory Service for Microsoft Active Directory.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11774,
                        "content": "<p>Enable AWS IAM Identity Center (successor to AWS SSO). Create a two-way forest trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2821,
            "attributes": {
                "createdAt": "2023-09-09T20:40:16.867Z",
                "updatedAt": "2023-09-09T20:40:16.867Z",
                "content": "<p>A health tech company runs a multi-tier medical records application in the AWS Cloud, which operates across three Availability Zones. The application architecture includes an Application Load Balancer, a cluster of Amazon EC2 instances that handle user session states, and a PostgreSQL database running on an EC2 instance.</p><p>The company anticipates a sharp surge in application traffic due to a new partnership. The company needs to scale to accommodate future application capacity demands and ensure high availability across all three Availability Zones.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>This solution fulfills all the requirements. Amazon RDS with Multi-AZ instances provides high availability and failover support for DB instances. ElastiCache for Redis supports storing session state data and can provide sub-millisecond response times, enabling applications to achieve instant, high-speed reads and writes. Auto Scaling ensures that the application has the correct number of Amazon EC2 instances to handle the load for your application.</p><p><strong>CORRECT: </strong>\"Migrate the PostgreSQL database to Amazon RDS for PostgreSQL with a Multi-AZ DB instance deployment. Use Amazon ElastiCache for Redis with a replication group to manage session data and cache reads. Migrate the application server to an Auto Scaling group across three Availability Zones\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the PostgreSQL database to Amazon Aurora with PostgreSQL compatibility with a single AZ deployment. Use Amazon ElastiCache for Memcached to manage session data and cache reads. Migrate the application server to an Auto Scaling group across three Availability Zones\" is incorrect.</p><p>Aurora is a high-performance database service, but using a single AZ deployment doesn't provide the high availability across multiple AZs the company wants. Also, while ElastiCache for Memcached can be used for caching, it doesn't offer the durability and atomicity that Redis offers, which is particularly useful for session data.</p><p><strong>INCORRECT:</strong> \"Migrate the PostgreSQL database to Amazon DynamoDB. Use DynamoDB Accelerator (DAX) to cache reads. Store the session data in DynamoDB. Migrate the application server to an Auto Scaling group across three Availability Zones\" is incorrect.</p><p>Although DynamoDB is a high-performance, scalable NoSQL database, it is not a drop-in replacement for a relational database like PostgreSQL. It has a different data model and supports a different set of query options. This change could require significant modifications to the application code and may not support the same transactional capabilities as PostgreSQL.</p><p><strong>INCORRECT:</strong> \"Keep the PostgreSQL database on EC2 instance. Use Amazon ElastiCache for Redis to manage session data and cache reads. Migrate the application server to an Auto Scaling group across three Availability Zones\" is incorrect.</p><p>Although the EC2 instance can run the PostgreSQL database, it does not provide the same level of managed service benefits (like automatic patching, backups, and high availability with Multi-AZ deployments) as Amazon RDS. This option would likely result in higher operational overhead and doesn't fully utilize the benefits of managed AWS services.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p><p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p><p><a href=\"https://aws.amazon.com/ec2/autoscaling/\">https://aws.amazon.com/ec2/autoscaling/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 11775,
                        "content": "<p>Migrate the PostgreSQL database to Amazon DynamoDB. Use DynamoDB Accelerator (DAX) to cache reads. Store the session data in DynamoDB. Migrate the application server to an Auto Scaling group across three Availability Zones.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11776,
                        "content": "<p>Migrate the PostgreSQL database to Amazon Aurora with PostgreSQL compatibility with a single AZ deployment. Use Amazon ElastiCache for Memcached to manage session data and cache reads. Migrate the application server to an Auto Scaling group across three Availability Zones.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11777,
                        "content": "<p>Keep the PostgreSQL database on EC2 instance. Use Amazon ElastiCache for Redis to manage session data and cache reads. Migrate the application server to an Auto Scaling group across three Availability Zones.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11778,
                        "content": "<p>Migrate the PostgreSQL database to Amazon RDS for PostgreSQL with a Multi-AZ DB instance deployment. Use Amazon ElastiCache for Redis with a replication group to manage session data and cache reads. Migrate the application server to an Auto Scaling group across three Availability Zones.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2822,
            "attributes": {
                "createdAt": "2023-09-09T20:40:16.970Z",
                "updatedAt": "2023-09-09T20:40:16.970Z",
                "content": "<p>An application has been migrated from on-premises to an Amazon EC2 instance. The migration has failed due to an unknown dependency that the application must communicate with an on-premises server using private IP addresses.</p><p>Which action should a solutions architect take to quickly provision the necessary connectivity?</p>",
                "answerExplanation": "<p>A virtual private gateway is a logical, fully redundant distributed edge routing function that sits at the edge of your VPC. You must create a VPG in your VPC before you can establish an AWS Managed site-to-site VPN connection. The other end of the connection is the customer gateway which must be established on the customer side of the connection.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-08-32-b6eebdb5a8f23f26677a1be7da633443.png\"></p><p><strong>CORRECT: </strong>\"Configure a Virtual Private Gateway\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Setup an AWS Direct Connect connection\" is incorrect as this would take too long to provision.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution\" is incorrect. This is not a solution for enabling connectivity using private addresses to an on-premises site. CloudFront is a content delivery network (CDN).</p><p><strong>INCORRECT:</strong> \"Create an AWS Transit Gateway\" is incorrect. AWS Transit Gateway connects VPCs and on-premises networks through a central hub which is not a requirement of this solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11779,
                        "content": "<p>Create an AWS Transit Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 11780,
                        "content": "<p>Configure a Virtual Private Gateway</p>",
                        "isValid": true
                    },
                    {
                        "id": 11781,
                        "content": "<p>Setup an AWS Direct Connect connection</p>",
                        "isValid": false
                    },
                    {
                        "id": 11782,
                        "content": "<p>Create an Amazon CloudFront distribution</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2823,
            "attributes": {
                "createdAt": "2023-09-09T20:40:17.067Z",
                "updatedAt": "2023-09-09T20:40:17.067Z",
                "content": "<p>As a security measure, a finance-based organization want to introduce additional security measures for an existing application deployed in AWS. The application is serverless and has an Amazon API Gateway in front which is deployed in the us-east-1 Region and the eu-west-1 Region. The company requires the accounts to be secured against SQL injection and cross-site scripting attacks.</p><p>Which solution will meet these requirements with the LEAST amount of administrative effort?</p>",
                "answerExplanation": "<p>AWS Firewall Manager simplifies your administration and maintenance tasks across multiple accounts and resources for a variety of protections, including AWS WAF, AWS Shield Advanced, Amazon VPC security groups, AWS Network Firewall, and Amazon Route 53 Resolver DNS Firewall. With Firewall Manager, you set up your protections just once and the service automatically applies them across your accounts and resources, even as you add new accounts and resources.</p><p>AWS WAF is used for protecting against malicious web attacks and is the best service to use to protect against SQL injection and cross-site scripting attacks. Used in combination with AWS Firewall Manager this solution protects both Regions and requires the least administrative effort.</p><p><strong>CORRECT: </strong>\"Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage\" is incorrect. This solution requires more administrative effort in rule management.</p><p><strong>INCORRECT:</strong> \"Set up AWS Shield in both Regions. Associate Regional web ACLs with an API stage\" is incorrect. The primary difference between AWS Shield and WAF is that while AWS WAF can mitigate DDoS attacks at layer 7 of the OSI reference model, AWS Shield protects web services from DDoS attacks at layer 3 and 4 of the OSI reference model. In this case AWS WAF should be used.</p><p><strong>INCORRECT:</strong> \"Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage\" is incorrect. As mentioned above, AWS Shield is not an appropriate choice for securing the accounts from SQL injection and cross-site scripting attacks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/fms-chapter.html\">https://docs.aws.amazon.com/waf/latest/developerguide/fms-chapter.html</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>",
                "options": [
                    {
                        "id": 11783,
                        "content": "<p>Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11784,
                        "content": "<p>Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage</p>",
                        "isValid": false
                    },
                    {
                        "id": 11785,
                        "content": "<p>Set up AWS Shield in both Regions. Associate Regional web ACLs with an API stage.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11786,
                        "content": "<p>Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2824,
            "attributes": {
                "createdAt": "2023-09-09T20:40:17.160Z",
                "updatedAt": "2023-09-09T20:40:17.160Z",
                "content": "<p>An e-commerce company wants to ensure all its resources used to host its various Web Applications are tagged using the appropriate application name to allow the company to easily differentiate and group resources. The company wants to minimize effort involved and automate this task.</p><p>What should a solutions architect do to accomplish this with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>AWS Config enables AWS resource inventory and change management as well as Config Rules to confirm that resources are configured in compliance with policies that you define. This is the easiest way to automate the detection of non-compliant resources.</p><p>You can create custom Systems Manager automation documents to remediate the missing tags. The documents can be configured for automatic remediation in AWS Config.</p><p><strong>CORRECT: </strong>\"Use AWS Config to detect resources that are not properly tagged. Create a Systems Manager automation document for remediation” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Cost Explorer to display any application components that are not properly tagged. Tag those resources using a Python Script” is incorrect. Cost Explorer is not designed for configuration compliance and would not provide the required information.</p><p><strong>INCORRECT:</strong> \"Configure AWS CloudTrail to send events to an Amazon CloudWatch Logs log group. Use insights queries to detect API events that do not include TagResources actions” is incorrect. This is an unworkable and highly inefficient attempt at configuration compliance. Much better to use AWS Config which is designed for this purpose.</p><p><strong>INCORRECT:</strong> \"Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code” is incorrect as this would contain a significant amount of operational overhead.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p>",
                "options": [
                    {
                        "id": 11787,
                        "content": "<p>Configure AWS CloudTrail to send events to an Amazon CloudWatch Logs log group. Use insights queries to detect API events that do not include TagResources actions.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11788,
                        "content": "<p>Use AWS Config to detect resources that are not properly tagged. Create a Systems Manager automation document for remediation.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11789,
                        "content": "<p>Use Cost Explorer to display any application components that are not properly tagged. Tag those resources using a Python Script.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11790,
                        "content": "<p>Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2825,
            "attributes": {
                "createdAt": "2023-09-09T20:40:17.258Z",
                "updatedAt": "2023-09-09T20:40:17.258Z",
                "content": "<p>A global logistics company collects shipment tracking information, which updates every few seconds. The company wishes to perform real-time analysis on these data updates to monitor shipment progress and predict delays, after which they want the data to be ingested into their Amazon S3-based data lake. Which solution will fulfill these requirements with the MOST operational efficiency?</p>",
                "answerExplanation": "<p>Amazon Kinesis Data Firehose is ideal for ingesting high-velocity data into AWS, like the shipment tracking data in this scenario. It can capture, transform, and load streaming data into data lakes on S3. Kinesis Data Analytics can then analyze this data in real-time, making this the most operationally efficient solution.</p><p><strong>CORRECT: </strong>\"Use Amazon Kinesis Data Firehose for data ingestion and Amazon Kinesis Data Analytics for real-time analysis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams for data ingestion and AWS Lambda for real-time data analysis\" is incorrect.</p><p>Kinesis Data Streams can handle real-time data ingestion and Lambda can perform real-time processing, but this approach requires managing the stream consumers (like AWS Lambda) and ensuring they are scaled properly. This may not be the most operationally efficient solution.</p><p><strong>INCORRECT:</strong> \"Use AWS Direct Connect for data ingestion and Amazon Athena for real-time analysis\" is incorrect.</p><p>AWS Direct Connect is a networking service primarily for establishing dedicated network connections from on-premises to AWS, not typically used for high-velocity data ingestion. Amazon Athena is more suitable for ad-hoc querying on S3 data, not real-time analysis.</p><p><strong>INCORRECT:</strong> \"Use Amazon SQS for data ingestion and Amazon EMR for real-time analysis\" is incorrect.</p><p>Amazon SQS is capable of handling high-throughput workloads, but it's more suited to decoupling and scaling microservices, distributed systems, and serverless applications. Amazon EMR is a managed cluster platform that simplifies running big data frameworks, but it is not suitable for real-time data analysis.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-analytics/\">https://aws.amazon.com/kinesis/data-analytics/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 11791,
                        "content": "<p>Use Amazon SQS for data ingestion and Amazon EMR for real-time analysis.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11792,
                        "content": "<p>Use Amazon Kinesis Data Firehose for data ingestion and Amazon Kinesis Data Analytics for real-time analysis.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11793,
                        "content": "<p>Use AWS Direct Connect for data ingestion and Amazon Athena for real-time analysis.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11794,
                        "content": "<p>Use Amazon Kinesis Data Streams for data ingestion and AWS Lambda for real-time data analysis.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2826,
            "attributes": {
                "createdAt": "2023-09-09T20:40:17.356Z",
                "updatedAt": "2023-09-09T20:40:17.356Z",
                "content": "<p>A Solutions Architect is attempting to clean up unused EBS volumes and snapshots to save some space and cost. How many of the most recent snapshots of an EBS volume need to be maintained to guarantee that you can recreate the full EBS volume from the snapshot?</p>",
                "answerExplanation": "<p>Snapshots capture a point-in-time state of an instance. If you make periodic snapshots of a volume, the snapshots are incremental, which means that only the blocks on the device that have changed after your last snapshot are saved in the new snapshot.</p><p>Even though snapshots are saved incrementally, the snapshot deletion process is designed so that you need to retain only the most recent snapshot in order to restore the volume.</p><p><strong>CORRECT: </strong>\"Only the most recent snapshot. Snapshots are incremental, but the deletion process will ensure that no data is lost\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"You must retain all snapshots as the process is incremental and therefore data is required from each snapshot\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Two snapshots, the oldest and most recent snapshots\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"The oldest snapshot, as this references data in all other snapshots\" is incorrect as explained above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-deleting-snapshot.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-deleting-snapshot.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 11795,
                        "content": "<p>The oldest snapshot, as this references data in all other snapshots&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11796,
                        "content": "<p>You must retain all snapshots as the process is incremental and therefore data is required from each snapshot&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11797,
                        "content": "<p>Only the most recent snapshot. Snapshots are incremental, but the deletion process will ensure that no data is lost&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 11798,
                        "content": "<p>Two snapshots, the oldest and most recent snapshots&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2827,
            "attributes": {
                "createdAt": "2023-09-09T20:40:17.477Z",
                "updatedAt": "2023-09-09T20:40:17.477Z",
                "content": "<p>A media company has grown significantly in the past few months and the management team are concerned about compliance, governance, auditing, and security. The management team requires that configuration changes are tracked a history of API calls is recorded.</p><p>What should a solutions architect do to meet these requirements?</p>",
                "answerExplanation": "<p>As per definition of AWS CloudTrail and AWS Config:</p><p>CloudTrail is a web service that records AWS API calls for your AWS account and delivers log files to an Amazon S3 bucket. The recorded information includes the identity of the user, the start time of the AWS API call, the source IP address, the request parameters, and the response elements returned by the service.</p><p>AWS Config tracks changes in the configuration of your AWS resources, and it regularly sends updated configuration details to an Amazon S3 bucket that you specify. For each resource type that AWS Config records, it sends a configuration history file every six hours.</p><p><strong>CORRECT: </strong>\"Use AWS Config to track configuration changes and AWS CloudTrail to record API calls\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail to track configuration changes and AWS Config to record API calls \" is incorrect.</p><p>This option is the reverse of what’s needed, AWS config, as the name suggests, is used to track the configuration changes in AWS accounts.</p><p><strong>INCORRECT:</strong> \"Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls\" is incorrect. CloudWatch is used for performance monitoring, not tracking API calls.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls\" is incorrect. CloudTrail is not the right service for tracking configuration changes hence this option is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/APIReference/Welcome.html\">https://docs.aws.amazon.com/awscloudtrail/latest/APIReference/Welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/TrackingChanges.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/TrackingChanges.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p><p><a href=\"https://digitalcloud.training/aws-cloudtrail/\">https://digitalcloud.training/aws-cloudtrail/</a></p>",
                "options": [
                    {
                        "id": 11799,
                        "content": "<p>Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11800,
                        "content": "<p>Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11801,
                        "content": "<p>Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11802,
                        "content": "<p>Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2828,
            "attributes": {
                "createdAt": "2023-09-09T20:40:17.583Z",
                "updatedAt": "2023-09-09T20:40:17.583Z",
                "content": "<p>A software development company is creating a microservices-based application using Amazon Elastic Kubernetes Service (Amazon EKS). The company needs to ensure that sensitive configuration data like database credentials and API keys stored in Kubernetes ConfigMaps and Secrets are encrypted at rest.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>Amazon EKS supports using AWS KMS keys for envelope encryption of Kubernetes secrets. To meet the requirement of encrypting Kubernetes Secrets at rest, we can use a customer managed AWS KMS key and enable secrets encryption while creating or updating an EKS cluster.</p><p><strong>CORRECT: </strong>\"Create a new AWS Key Management Service (AWS KMS) key. Enable Amazon EKS KMS secrets encryption on the Amazon EKS cluster\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement AWS Secrets Manager to manage, rotate, and store all sensitive data. Integrate it with the Amazon EKS cluster\" is incorrect.</p><p>While AWS Secrets Manager can store and manage sensitive information, it doesn't directly encrypt Kubernetes Secrets and ConfigMaps stored in the etcd key-value store.</p><p><strong>INCORRECT:</strong> \"Create the Amazon EKS cluster with default options. Use the Amazon Elastic File System (Amazon EFS) Container Storage Interface (CSI) driver as an add-on\" is incorrect.</p><p>The EFS CSI driver enables Kubernetes pods to mount EFS file systems, but it does not offer a mechanism for encrypting secrets stored in Kubernetes.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 to store all sensitive data. Enable server-side encryption with a new AWS Key Management Service (AWS KMS) key\" is incorrect.</p><p>While S3 can store sensitive data and encrypt it using KMS, it does not provide a way to directly encrypt Kubernetes ConfigMaps and Secrets stored in etcd.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/enable-kms.html\">https://docs.aws.amazon.com/eks/latest/userguide/enable-kms.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
                "options": [
                    {
                        "id": 11803,
                        "content": "<p>Use Amazon S3 to store all sensitive data. Enable server-side encryption with a new AWS Key Management Service (AWS KMS) key.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11804,
                        "content": "<p>Create the Amazon EKS cluster with default options. Use the Amazon Elastic File System (Amazon EFS) Container Storage Interface (CSI) driver as an add-on.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11805,
                        "content": "<p>Create a new AWS Key Management Service (AWS KMS) key. Enable Amazon EKS KMS secrets encryption on the Amazon EKS cluster.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11806,
                        "content": "<p>Implement AWS Secrets Manager to manage, rotate, and store all sensitive data. Integrate it with the Amazon EKS cluster.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2829,
            "attributes": {
                "createdAt": "2023-09-09T20:40:17.678Z",
                "updatedAt": "2023-09-09T20:40:17.678Z",
                "content": "<p>A company runs an application using many Amazon EC2 instances for its application servers. The application using Amazon DynamoDB for its data store. The size of this table continuously grows, but the application only requires data from the most recent 30 days. The company needs a solution that minimizes cost and effort.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>Amazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. TTL is provided at no extra cost to reduce stored data volumes by retaining only the items that remain current for your workload’s needs.</p><p><strong>CORRECT: </strong>\"Add an attribute to each new item created in the table that has a value of the current timestamp plus 30 days. Configure this attribute as the TTL attribute” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the entire solution using an AWS CloudFormation template. Re-deploy the CloudFormation stack every 30 days, and then delete the original stack” is incorrect. This solution requires significant disruption and is highly inefficient.</p><p><strong>INCORRECT:</strong> \"Run a monitoring application from the AWS Marketplace using an EC2 instance configured with a Golden AMI. When a new item is created in the table, configure the monitoring application to use Amazon DynamoDB Streams to store the timestamp. For items with a timestamp older than 30 days, run a script on the EC2 instance” is incorrect as this would require more cost and operational overhead.</p><p><strong>INCORRECT:</strong> \"When a new item is created in the table, Amazon DynamoDB Streams will invoke an AWS Lambda function. Set the Lambda function to delete items in the table that are older than 30 days\" is incorrect. Whilst this is possible, it provides this entails higher operational overhead and cost.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 11807,
                        "content": "<p>Add an attribute to each new item created in the table that has a value of the current timestamp plus 30 days. Configure this attribute as the TTL attribute.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11808,
                        "content": "<p>When a new item is created in the table, Amazon DynamoDB Streams will invoke an AWS Lambda function. Set the Lambda function to delete items in the table that are older than 30 days.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11809,
                        "content": "<p>Deploy the entire solution using an AWS CloudFormation template. Re-deploy the CloudFormation stack every 30 days, and then delete the original stack.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11810,
                        "content": "<p>Run a monitoring application from the AWS Marketplace using an EC2 instance configured with a Golden AMI. When a new item is created in the table, configure the monitoring application to use Amazon DynamoDB Streams to store the timestamp. For items with a timestamp older than 30 days, run a script on the EC2 instance.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2830,
            "attributes": {
                "createdAt": "2023-09-09T20:40:17.782Z",
                "updatedAt": "2023-09-09T20:40:17.782Z",
                "content": "<p>A solutions architect in a large finance organization must restrict access for a specific S3 bucket to only users in accounts within the organization in AWS Organizations. This is due to the confidentiality of project reports data.</p><p>Which solution meets these requirements with the LEAST amount of operational overhead?</p>",
                "answerExplanation": "<img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-13-34-246c03f69e39c1e07b1d3c6f767eb480.jpg\"><p>PrincipalOrgId is used by specifying the Principal element in a <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html\">resource-based policy</a>. You can specify the <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_details.html\">organization ID</a> in the condition element. When you add and remove accounts, policies that include the aws:PrincipalOrgID key automatically include the correct accounts and don't require manual updating.</p><p>For example, the following Amazon S3 bucket policy allows members of any account in the o-xxxxxxxxxxx organization to add an object into the policy-ninja-dev bucket.</p><p><strong>CORRECT: </strong>\"Add the aws:PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy\" is incorrect.</p><p>This condition key ensures that the requester is an account member within the specified organization root or organizational units (OUs) in AWS Organizations. It is not required for this solution.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly\" is incorrect. This option would be required for monitoring but not sharing access.</p><p><strong>INCORRECT:</strong> \"Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy\" is incorrect. Since question is around cross account access, this option wouldn’t work as is.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/control-access-to-aws-resources-by-using-the-aws-organization-of-iam-principals/\">https://aws.amazon.com/blogs/security/control-access-to-aws-resources-by-using-the-aws-organization-of-iam-principals/</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/iam-share-aws-resources-groups-aws-accounts-aws-organizations/\">https://aws.amazon.com/blogs/security/iam-share-aws-resources-groups-aws-accounts-aws-organizations/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
                "options": [
                    {
                        "id": 11811,
                        "content": "<p>Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11812,
                        "content": "<p>Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11813,
                        "content": "<p>Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11814,
                        "content": "<p>Add the aws:PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2831,
            "attributes": {
                "createdAt": "2023-09-09T20:40:17.911Z",
                "updatedAt": "2023-09-09T20:40:17.911Z",
                "content": "<p>A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket.</p><p>The reporting team wants to move the files automatically to the analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines.</p><p>What should a solutions architect do to meet these requirements with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>With Amazon S3 you can configure same region replication (SRR) to automatically copy files from one bucket to another one as they are added to the source bucket. S3 event notifications can also be configured to trigger event driven responses when changes happen in an Amazon S3 bucket.</p><p>Amazon SageMaker Pipelines, the first purpose-built, continuous integration and continuous deployment (CI/CD) service for machine learning (ML), is now supported as a target for routing events in Amazon EventBridge. This enables customers to trigger the execution of the Amazon SageMaker model building pipeline based on any event in their event bus or on a schedule by selecting the pipeline as the target in Amazon EventBridge.</p><p>For example, customers can set up EventBridge to trigger the execution of the SageMaker model building pipeline when a new file with the training data set is uploaded to an Amazon S3 bucket or when the SageMaker Model Monitor indicates a deviation in model quality through alarms in Amazon CloudWatch metrics. Customers can also create rules in Amazon EventBridge that trigger the pipeline execution on an automated schedule.</p><p><strong>CORRECT: </strong>\"Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge. Configure an ObjectCreated rule in EventBridge. Configure Lambda and SageMaker Pipelines as targets for the rule \" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge. Configure an ObjectCreated rule in EventBridge. Configure Lambda and SageMaker Pipelines as targets for the rule \" is incorrect.</p><p>This is the closest option with one flaw in that it involves setting up a Lambda function which would require more effort. S3 replication is an out of the box feature from AWS which will be more efficient.</p><p><strong>INCORRECT:</strong> \"Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type\" is incorrect.</p><p>This options involves manual steps to set up Lambda and any manual intervention could be avoided with S3 replication. Hence this is an incorrect option.</p><p><strong>INCORRECT:</strong> \"Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type\" is incorrect.</p><p>This option could work but again avoiding writing a Lambda function and using EventBridge reduces the manual intervention and the effort needed.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/using-dynamic-amazon-s3-event-handling-with-amazon-eventbridge/\">https://aws.amazon.com/blogs/compute/using-dynamic-amazon-s3-event-handling-with-amazon-eventbridge/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11815,
                        "content": "<p>Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11816,
                        "content": "<p>Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge. Configure an ObjectCreated rule in EventBridge. Configure Lambda and SageMaker Pipelines as targets for the rule.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11817,
                        "content": "<p>Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge. Configure an ObjectCreated rule in EventBridge. Configure Lambda and SageMaker Pipelines as targets for the rule.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11818,
                        "content": "<p>Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2832,
            "attributes": {
                "createdAt": "2023-09-09T20:40:18.007Z",
                "updatedAt": "2023-09-09T20:40:18.007Z",
                "content": "<p>An online education platform uses Amazon CloudFront to distribute learning resources globally. The company wants to ensure that only enrolled students have access to the course materials. These materials are stored in an Amazon S3 bucket. In addition, the company occasionally provides exclusive resources to certain students for research and project work.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>CloudFront signed cookies are a method to control who can access your content. When a user authenticates and is verified as an enrolled student, the application can set a cookie in the student's browser. The cookie contains the same information that can be included in a signed URL but applies to multiple files in one or multiple directories.</p><p><strong>CORRECT: </strong>\"Implement CloudFront signed cookies for authenticated students\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create and provide S3 pre-signed URLs to authenticated students\" is incorrect.</p><p>S3 pre-signed URLs are used to grant temporary access to a specific S3 object. This could be a valid option for individual file access but would be less efficient for multiple files or directories.</p><p><strong>INCORRECT:</strong> \"Utilize Amazon S3 object-level encryption for course materials\" is incorrect.</p><p>Amazon S3 object-level encryption is mainly about securing data at rest, it won't control who can or cannot access the content.</p><p><strong>INCORRECT:</strong> \"Implement CloudFront Field-Level Encryption to block access to non-enrolled students\" is incorrect.</p><p>CloudFront Field-Level Encryption handles sensitive information in HTTP POST requests to help prevent the information from being seen by unauthorized viewers. It's not designed to control access to content.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 11819,
                        "content": "<p>Utilize Amazon S3 object-level encryption for course materials.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11820,
                        "content": "<p>Implement CloudFront Field-Level Encryption to block access to non-enrolled students.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11821,
                        "content": "<p>Implement CloudFront signed cookies for authenticated students.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11822,
                        "content": "<p>Create and provide S3 pre-signed URLs to authenticated students.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2833,
            "attributes": {
                "createdAt": "2023-09-09T20:40:18.126Z",
                "updatedAt": "2023-09-09T20:40:18.126Z",
                "content": "<p>A company stores its application logs in an Amazon CloudWatch Logs log group. A new policy requires the company to store all application logs in Amazon OpenSearch Service (Amazon Elasticsearch Service) in near-real time.</p><p>Which solution will meet this requirement with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>You can configure a CloudWatch Logs log group to stream data it receives to your Amazon OpenSearch Service cluster in near real-time through a CloudWatch Logs subscription. This is the solution that requires the least operational overhead. Subscription filters can also be created for Kinesis, Kinesis Data Firehose, and AWS Lambda.</p><p><strong>CORRECT: </strong>\" Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service) \" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery stream's source. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the delivery stream's destination\" is incorrect.</p><p>This is a possible solution but requires more operational overhead as it includes an additional service which must also be configured and managed.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service)\" is incorrect. This would require more operational overhead as you must write and manage the code for the function yourself.</p><p><strong>INCORRECT:</strong> \"Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon Kinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service)\" is incorrect. Since the requirement is to dump the logs into OpenSearch and no further computation is needed, Firehose is a better candidate here.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p><p><a href=\"https://digitalcloud.training/amazon-opensearch/\">https://digitalcloud.training/amazon-opensearch/</a></p>",
                "options": [
                    {
                        "id": 11823,
                        "content": "<p>Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon Kinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).</p>",
                        "isValid": false
                    },
                    {
                        "id": 11824,
                        "content": "<p>Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).</p>",
                        "isValid": false
                    },
                    {
                        "id": 11825,
                        "content": "<p>Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery stream's source. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the delivery stream's destination.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11826,
                        "content": "<p>Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2834,
            "attributes": {
                "createdAt": "2023-09-09T20:40:18.261Z",
                "updatedAt": "2023-09-09T20:40:18.261Z",
                "content": "<p>The database layer of an on-premises web application is being migrated to AWS. The database uses a multi-threaded, in-memory caching layer to improve performance for repeated queries. Which service would be the most suitable replacement for the database cache?</p>",
                "answerExplanation": "<p>Amazon ElastiCache with the Memcached engine is an in-memory database that can be used as a database caching layer. The memached engine supports multiple cores and threads and large nodes.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-11-45-0d3bdca18464fbf41521677922bfcec8.png\"></p><p><strong>CORRECT: </strong>\"Amazon ElastiCache Memcached\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon ElastiCache Redis\" is incorrect. The Redis engine does not support multiple CPU cores or threads.</p><p><strong>INCORRECT:</strong> \"Amazon DynamoDB DAX\" is incorrect. Amazon DynamoDB Accelerator (DAX) is a database cache that should be used with DynamoDB only.</p><p><strong>INCORRECT:</strong> \"Amazon RDS MySQL\" is incorrect as this is not an example of an in-memory database that can be used as a database caching layer.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 11827,
                        "content": "<p>Amazon ElastiCache Memcached</p>",
                        "isValid": true
                    },
                    {
                        "id": 11828,
                        "content": "<p>Amazon DynamoDB DAX</p>",
                        "isValid": false
                    },
                    {
                        "id": 11829,
                        "content": "<p>Amazon RDS MySQL</p>",
                        "isValid": false
                    },
                    {
                        "id": 11830,
                        "content": "<p>Amazon ElastiCache Redis</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2835,
            "attributes": {
                "createdAt": "2023-09-09T20:40:18.354Z",
                "updatedAt": "2023-09-09T20:40:18.354Z",
                "content": "<p>A software development firm uses AWS to run their compute instances across multiple accounts. These instances are individually billed. The company recently purchased an EC2 Reserved Instance (RI) for an ongoing project. However, due to the completion of that project, a significant number of EC2 instances were decommissioned. The company now wishes to utilize the benefits of their unused Reserved Instance across their other AWS accounts.</p><p>Which combination of steps should the company follow to achieve this? (Select TWO.)</p>",
                "answerExplanation": "<p>Just like the Savings Plans, the benefits of Reserved Instances can be applied across accounts if those accounts are part of the same AWS Organization and if sharing is enabled. This can be achieved by enabling Reserved Instance sharing in the AWS Management Console for the account that purchased the RI.</p><p>Setting up an AWS Organization from the account that purchased the Reserved Instance allows you to group your accounts. After the organization is set up, you can invite other accounts to join the organization, enabling you to share the benefits of the Reserved Instance across all accounts in the organization.</p><p><strong>CORRECT: </strong>\"Enable Reserved Instance sharing in the billing preferences section of the AWS Management Console for the account that purchased the existing RI\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Establish an AWS Organization in the AWS account that purchased the RI and hosts the remaining active EC2 instances. Invite the other AWS accounts to join this organization from the management account\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"From the AWS Organizations management account, utilize AWS Resource Access Manager (AWS RAM) to share the Reserved Instance with other accounts\" is incorrect.</p><p>AWS RAM does not apply to Reserved Instances. It is used to share other resources like Subnets, Transit Gateways, etc.</p><p><strong>INCORRECT:</strong> \"Enable Reserved Instance sharing in the billing preferences section of the AWS Management Console for the management account\" is incorrect.</p><p>Reserved Instance sharing needs to be enabled in the account that purchased the RI, not the management account.</p><p><strong>INCORRECT:</strong> \"Use AWS Organizations to establish a new payer account and invite the other accounts to join this organization\" is incorrect.</p><p>Creating a new payer account is not necessary. It would be more efficient to use the existing account that purchased the Reserved Instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
                "options": [
                    {
                        "id": 11831,
                        "content": "<p>Establish an AWS Organization in the AWS account that purchased the RI and hosts the remaining active EC2 instances. Invite the other AWS accounts to join this organization from the management account.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11832,
                        "content": "<p>Use AWS Organizations to establish a new payer account and invite the other accounts to join this organization.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11833,
                        "content": "<p>Enable Reserved Instance sharing in the billing preferences section of the AWS Management Console for the management account.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11834,
                        "content": "<p>From the AWS Organizations management account, utilize AWS Resource Access Manager (AWS RAM) to share the Reserved Instance with other accounts.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11835,
                        "content": "<p>Enable Reserved Instance sharing in the billing preferences section of the AWS Management Console for the account that purchased the existing RI.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2836,
            "attributes": {
                "createdAt": "2023-09-09T20:40:18.474Z",
                "updatedAt": "2023-09-09T20:40:18.474Z",
                "content": "<p>An application is running in a private subnet of an Amazon VPC and must have outbound internet access for downloading updates. The Solutions Architect does not want the application exposed to inbound connection attempts. Which steps should be taken?</p>",
                "answerExplanation": "<p>To enable outbound connectivity for instances in private subnets a NAT gateway can be created. The NAT gateway is created in a public subnet and a route must be created in the private subnet pointing to the NAT gateway for internet-bound traffic. An internet gateway must be attached to the VPC to facilitate outbound connections.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-07-43-70afcd404db57b167e180b7ee7fee9d7.png\"></p><p>You cannot directly connect to an instance in a private subnet from the internet. You would need to use a bastion/jump host. Therefore, the application will not be exposed to inbound connection attempts.</p><p><strong>CORRECT: </strong>\"Create a NAT gateway and attach an internet gateway to the VPC\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a NAT gateway but do not create attach an internet gateway to the VPC\" is incorrect. An internet gateway must be attached to the VPC for any outbound connections to work.</p><p><strong>INCORRECT:</strong> \"Attach an internet gateway to the private subnet and create a NAT gateway\" is incorrect. You do not attach internet gateways to subnets, you attach them to VPCs.</p><p><strong>INCORRECT:</strong> \"Attach an internet gateway to the VPC but do not create a NAT gateway\" is incorrect. Without a NAT gateway the instances in the private subnet will not be able to download updates from the internet.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11836,
                        "content": "<p>Create a NAT gateway but do not attach an internet gateway to the VPC</p>",
                        "isValid": false
                    },
                    {
                        "id": 11837,
                        "content": "<p>Attach an internet gateway to the VPC but do not create a NAT gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 11838,
                        "content": "<p>Attach an internet gateway to the private subnet and create a NAT gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 11839,
                        "content": "<p>Create a NAT gateway and attach an internet gateway to the VPC</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2837,
            "attributes": {
                "createdAt": "2023-09-09T20:40:18.599Z",
                "updatedAt": "2023-09-09T20:40:18.599Z",
                "content": "<p>A company provides a Voice over Internet Protocol (VoIP) service that uses UDP as the protocol. The service utilizes Amazon EC2 instances that are scaled automatically using an Auto Scaling group. The company currently uses multiple AWS Regions for its AWS deployments.</p><p>The company needs to route users to the appropriate Region based on the lowest latency. The company also needs automated failover between Regions.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>For UDP traffic the solution must use a Network Load Balancer as ALBs do not support UDP. The solution also requires both latency-based routing and automated failover. AWS Global Accelerator can be used to achieve both these requirements. It will direct users to the lowest latency endpoint and if an endpoint becomes unhealthy it automatically reroutes to the next best endpoint.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-09-21_03-21-33-01ff999266aba8530a30ecca0b2c32d9.jpg\"><p><strong>CORRECT: </strong>\"Set up a Network Load Balancer (NLB) and an associated target group. Assign the target group with the Auto Scaling group. In each region, use the NLB as an AWS Global Accelerator endpoint\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer (NLB) and an associated target group. Assign the target group to the Auto Scaling group and create an Amazon Route 53 latency record that points to aliases for each NLB\" is incorrect. An NLB must be used but Route 53 latency-based routing will not automatically failover the application to another endpoint unless health checks are enabled, and this is not described.</p><p><strong>INCORRECT:</strong> \"Set up an Application Load Balancer (ALB) and a target group. Associate the target group with the Auto Scaling group and use the ALB as an AWS Global Accelerator endpoint in each Region” is incorrect as Application Load Balancers balance HTTP and HTTPS traffic at Layer 7, not UDP traffic.</p><p><strong>INCORRECT:</strong> \"Deploy an Application Load Balancer (ALB) and its associated target group. Assign the target group to the Auto Scaling group and create an Amazon Route 53 weighted record that points to aliases for each ALB\" is incorrect as ALBs do not support UDP listeners and weighted routing is not used for latency or failover.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 11840,
                        "content": "<p>Deploy an Application Load Balancer (ALB) and its associated target group. Assign the target group to the Auto Scaling group and create an Amazon Route 53 weighted record that points to aliases for each ALB.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11841,
                        "content": "<p>Create a Network Load Balancer (NLB) and an associated target group. Assign the target group to the Auto Scaling group and create an Amazon Route 53 latency record that points to aliases for each NLB.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11842,
                        "content": "<p>Set up a Network Load Balancer (NLB) and an associated target group. Assign the target group with the Auto Scaling group. In each region, use the NLB as an AWS Global Accelerator endpoint.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11843,
                        "content": "<p>Set up an Application Load Balancer (ALB) and a target group. Associate the target group with the Auto Scaling group and use the ALB as an AWS Global Accelerator endpoint in each Region.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2838,
            "attributes": {
                "createdAt": "2023-09-09T20:40:18.700Z",
                "updatedAt": "2023-09-09T20:40:18.700Z",
                "content": "<p>A Solutions Architect manages multiple Amazon RDS MySQL databases. To improve security, the Solutions Architect wants to enable secure user access with short-lived credentials. How can these requirements be met?</p>",
                "answerExplanation": "<p>With MySQL, authentication is handled by AWSAuthenticationPlugin—an AWS-provided plugin that works seamlessly with IAM to authenticate your IAM users. Connect to the DB instance and issue the CREATE USER statement, as shown in the following example.</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pln\">CREATE USER jane_doe IDENTIFIED WITH </span><span class=\"typ\">AWSAuthenticationPlugin</span><span class=\"pln\"> AS </span><span class=\"str\">'RDS'</span><span class=\"pun\">;</span></li></ol></pre></div></div><p>The IDENTIFIED WITH clause allows MySQL to use the AWSAuthenticationPlugin to authenticate the database account (jane_doe). The AS 'RDS' clause refers to the authentication method, and the specified database account should have the same name as the IAM user or role. In this example, both the database account and the IAM user or role are named jane_doe.</p><p><strong>CORRECT: </strong>\"Create the MySQL user accounts to use the AWSAuthenticationPlugin with IAM\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure the MySQL databases to use the AWS Security Token Service (STS)\" is incorrect. You cannot configure MySQL to directly use the AWS STS.</p><p><strong>INCORRECT:</strong> \"Configure the application to use the AUTH command to send a unique password\" is incorrect. This is used with Redis databases, not with RDS databases.</p><p><strong>INCORRECT:</strong> \"Configure the MySQL databases to use AWS KMS data encryption keys\" is incorrect. Data encryption keys are used for data encryption not management of connections strings.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.DBAccounts.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.DBAccounts.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 11844,
                        "content": "<p>Configure the MySQL databases to use the AWS Security Token Service (STS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11845,
                        "content": "<p>Configure the application to use the AUTH command to send a unique password</p>",
                        "isValid": false
                    },
                    {
                        "id": 11846,
                        "content": "<p>Configure the MySQL databases to use AWS KMS data encryption keys</p>",
                        "isValid": false
                    },
                    {
                        "id": 11847,
                        "content": "<p>Create the MySQL user accounts to use the AWSAuthenticationPlugin with IAM</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2839,
            "attributes": {
                "createdAt": "2023-09-09T20:40:18.800Z",
                "updatedAt": "2023-09-09T20:40:18.800Z",
                "content": "<p>A digital media company uses an Amazon RDS MySQL instance for its content management system. Recently, the company has observed that their RDS instance is nearing its storage capacity due to the constant influx of new data. The company wants to ensure there's always sufficient storage without any operational interruption or manual intervention.</p><p>Which solution should the company use to address this situation with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>Amazon RDS's automatic storage scaling allows the database to automatically increase its storage capacity when the available storage is low. This feature helps to prevent out-of-storage situations and requires no operational overhead.</p><p><strong>CORRECT: </strong>\"Enable automatic storage scaling for the MySQL instance\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the database to a larger Amazon RDS MySQL instance\" is incorrect.</p><p>While this would provide more storage, it does not address the issue of potential future storage shortages and requires significant operational effort for the migration.</p><p><strong>INCORRECT:</strong> \"Implement a lifecycle policy to delete older data from the MySQL instance\" is incorrect.</p><p>While this might help free up some storage, it might not be suitable if all data is essential for business operations. Also, this does not provide a long-term solution if data growth continues.</p><p><strong>INCORRECT:</strong> \"Utilize Amazon ElastiCache to offload some read traffic and reduce database load\" is incorrect.</p><p>While ElastiCache can help to improve the database's read efficiency, it doesn't directly address the disk space concern for the RDS instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 11848,
                        "content": "<p>Enable automatic storage scaling for the MySQL instance.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11849,
                        "content": "<p>Implement a lifecycle policy to delete older data from the MySQL instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11850,
                        "content": "<p>Utilize Amazon ElastiCache to offload some read traffic and reduce database load.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11851,
                        "content": "<p>Migrate the database to a larger Amazon RDS MySQL instance.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2840,
            "attributes": {
                "createdAt": "2023-09-09T20:40:18.907Z",
                "updatedAt": "2023-09-09T20:40:18.907Z",
                "content": "<p>A corporation has a web-based multiplayer gaming service that operates using both TCP and UDP protocols. Amazon Route 53 is currently employed to direct application traffic to a set of Network Load Balancers (NLBs) in various AWS Regions. To prepare for an increase in user activity, the company must enhance application performance and reduce latency.</p><p>Which approach will best meet these requirements?</p>",
                "answerExplanation": "<p>AWS Global Accelerator is designed to improve the availability and performance of your applications for local and global users. It directs traffic to optimal endpoints over the AWS global network, thus enhancing the performance of your TCP and UDP traffic by routing packets through the AWS global network infrastructure, reducing jitter, and improving overall game performance.</p><p><strong>CORRECT: </strong>\"Implement AWS Global Accelerator ahead of the NLBs and align the Global Accelerator endpoint to use the appropriate listener ports\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Incorporate Amazon CloudFront in front of the NLBs and extend the duration of the Cache-Control max-age directive\" is incorrect.</p><p>Amazon CloudFront is a content delivery network (CDN) that speeds up the delivery of your static and dynamic web content. While it could potentially help with application performance, it doesn't directly improve TCP/UDP performance, which is the specific requirement in this case.</p><p><strong>INCORRECT:</strong> \"Substitute the NLBs with Application Load Balancers (ALBs) and set Route 53 to utilize latency-based routing\" is incorrect.</p><p>Application Load Balancers (ALBs) are layer 7 load balancers and they do not support the handling of raw TCP and UDP traffic, which is a requirement for the gaming application in the question. NLBs, on the other hand, are suitable for extreme performance needs and for TCP/UDP traffic.</p><p><strong>INCORRECT:</strong> \"Insert an Amazon API Gateway endpoint behind the NLBs, enable API caching, and customize method caching across different stages\" is incorrect.</p><p>While the API Gateway would add more control and security to the application, the caching feature is not necessarily beneficial for this real-time gaming scenario where the content is likely to change frequently and unpredictably.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-global-accelerator/\">https://digitalcloud.training/aws-global-accelerator/</a></p>",
                "options": [
                    {
                        "id": 11852,
                        "content": "<p>Substitute the NLBs with Application Load Balancers (ALBs) and set Route 53 to utilize latency-based routing.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11853,
                        "content": "<p>Incorporate Amazon CloudFront in front of the NLBs and extend the duration of the Cache-Control max-age directive.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11854,
                        "content": "<p>Implement AWS Global Accelerator ahead of the NLBs and align the Global Accelerator endpoint to use the appropriate listener ports.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11855,
                        "content": "<p>Insert an Amazon API Gateway endpoint behind the NLBs, enable API caching, and customize method caching across different stages.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2841,
            "attributes": {
                "createdAt": "2023-09-09T20:40:19.007Z",
                "updatedAt": "2023-09-09T20:40:19.007Z",
                "content": "<p>A company is developing a web-based application that will be used for real-time chat functionality. The application should use WebSocket APIs to maintain a persistent connection with the client. The backend services of the application, hosted in containers within private subnets of a VPC, need to be accessed securely.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>The requirement is for a real-time chat application, which makes the use of WebSocket APIs more suitable. Hosting the application in Amazon EKS within a private subnet allows secure and scalable management of the application. Creating a VPC link provides secure, private connectivity between API Gateway and the Amazon EKS service hosted inside the VPC.</p><p><strong>CORRECT: </strong>\"Develop a WebSocket API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Establish a private VPC link for the API Gateway to securely access the Amazon EKS cluster\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Establish a private VPC link for the API Gateway to securely access the Amazon EKS cluster\" is incorrect.</p><p>This solution does provide the secure hosting environment and private connectivity between API Gateway and the Amazon EKS cluster, but REST APIs are not suitable for real-time applications like a chat service. This is because REST APIs use request-response model which doesn't provide the continuous connection needed for real-time communication.</p><p><strong>INCORRECT:</strong> \"Develop a WebSocket API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster\" is incorrect.</p><p>This option, while correctly suggesting the use of WebSocket APIs and Amazon EKS, proposes the use of a security group for connectivity. However, security groups act as a firewall for associated Amazon EC2 instances, controlling both inbound and outbound traffic at the instance level, while access to services within VPCs is more securely managed through VPC links.</p><p><strong>INCORRECT:</strong> \"Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster\" is incorrect.</p><p>REST APIs are not suitable for a real-time chat application. Also, managing access via a security group is not the most secure method for accessing services hosted within private subnets in a VPC.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/best-practices-api-gateway-private-apis-integration/websocket-api.html\">https://docs.aws.amazon.com/whitepapers/latest/best-practices-api-gateway-private-apis-integration/websocket-api.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 11856,
                        "content": "<p>Develop a WebSocket API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Establish a private VPC link for the API Gateway to securely access the Amazon EKS cluster.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11857,
                        "content": "<p>Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11858,
                        "content": "<p>Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Establish a private VPC link for the API Gateway to securely access the Amazon EKS cluster.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11859,
                        "content": "<p>Develop a WebSocket API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2842,
            "attributes": {
                "createdAt": "2023-09-09T20:40:19.113Z",
                "updatedAt": "2023-09-09T20:40:19.113Z",
                "content": "<p>A web application runs on a series of Amazon EC2 instances behind an Application Load Balancer (ALB). A Solutions Architect is updating the configuration with a health check and needs to select the protocol to use. What options are available? (choose 2)</p>",
                "answerExplanation": "<p>An Application Load Balancer periodically sends requests to its registered targets to test their status. These tests are called <em>health checks</em>.</p><p>Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. After your target is registered, it must pass one health check to be considered healthy. After each health check is completed, the load balancer node closes the connection that was established for the health check.</p><p>If a target group contains only unhealthy registered targets, the load balancer nodes route requests across its unhealthy targets.</p><p>For an ALB the possible protocols are HTTP and HTTPS. The default is the HTTP protocol.</p><p><strong>CORRECT: </strong>\"HTTP\" is the correct answer.</p><p><strong>CORRECT: </strong>\"HTTPS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"SSL\" is incorrect as this is not supported by the ALB.</p><p><strong>INCORRECT:</strong> \"TCP\" is incorrect as this is not supported by the ALB.</p><p><strong>INCORRECT:</strong> \"ICMP\" is incorrect as this is not supported by the ALB.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 11860,
                        "content": "<p>ICMP</p>",
                        "isValid": false
                    },
                    {
                        "id": 11861,
                        "content": "<p>HTTP</p>",
                        "isValid": true
                    },
                    {
                        "id": 11862,
                        "content": "<p>SSL</p>",
                        "isValid": false
                    },
                    {
                        "id": 11863,
                        "content": "<p>HTTPS</p>",
                        "isValid": true
                    },
                    {
                        "id": 11864,
                        "content": "<p>TCP</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2843,
            "attributes": {
                "createdAt": "2023-09-09T20:40:19.213Z",
                "updatedAt": "2023-09-09T20:40:19.213Z",
                "content": "<p>A company is transitioning their web presence into the AWS cloud. As part of the migration the company will be running a web application both on-premises and in AWS for a period of time. During the period of co-existence the client would like 80% of the traffic to hit the AWS-based web servers and 20% to be directed to the on-premises web servers.</p><p>What method can a Solutions Architect use to distribute traffic as requested?</p>",
                "answerExplanation": "<p>Route 53 weighted routing policy is similar to simple but you can specify a weight per IP address. You create records that have the same name and type and assign each record a relative weight which is a numerical value that favours one IP over another (values must total 100). To stop sending traffic to a resource you can change the weight of the record to 0.</p><p><strong>CORRECT: </strong>\"Use Route 53 with a weighted routing policy and configure the respective weights\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Route 53 with a simple routing policy\" is incorrect as this will not split traffic based on weights as required.</p><p><strong>INCORRECT:</strong> \"Use an Application Load Balancer to distribute traffic based on IP address\" is incorrect. Application Load Balancer can distribute traffic to AWS and on-premise resources using IP addresses but cannot be used to distribute traffic in a weighted manner.</p><p><strong>INCORRECT:</strong> \"Use a Network Load Balancer to distribute traffic based on Instance ID\" is incorrect. Network Load Balancer can distribute traffic to AWS and on-premise resources using IP addresses (not Instance IDs).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
                "options": [
                    {
                        "id": 11865,
                        "content": "<p>Use Route 53 with a weighted routing policy and configure the respective weights&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 11866,
                        "content": "<p>Use an Application Load Balancer to distribute traffic based on IP address&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11867,
                        "content": "<p>Use a Network Load Balancer to distribute traffic based on Instance ID&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11868,
                        "content": "<p>Use Route 53 with a simple routing policy&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2844,
            "attributes": {
                "createdAt": "2023-09-09T20:40:19.317Z",
                "updatedAt": "2023-09-09T20:40:19.317Z",
                "content": "<p>A legacy application is being migrated into AWS. The application has a large amount of data that is rarely accessed. When files are accessed they are retrieved sequentially. The application will be migrated onto an Amazon EC2 instance.</p><p>What is the LEAST expensive EBS volume type for this use case?</p>",
                "answerExplanation": "<p>The cold HDD (sc1) EBS volume type is the lowest cost option that is suitable for this use case. The sc1 volume type is suitable for infrequently accessed data and use cases that are oriented towards throughput like sequential data access.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-29-08-7da5cb7bfcd2de70f1d5a02ddab9fb12.png\"></p><p><strong>CORRECT: </strong>\"Cold HDD (sc1)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Provisioned IOPS SSD (io1)\" is incorrect. This is the most expensive option and used for use cases that demand high IOPS.</p><p><strong>INCORRECT:</strong> \"General Purpose SSD (gp2)\" is incorrect. This is a more expensive SSD volume type that is used for general use cases.</p><p><strong>INCORRECT:</strong> \"Throughput Optimized HDD (st1)\" is incorrect. This is also used for throughput-oriented use cases however it is higher cost than sc1 and better for frequently accessed data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 11869,
                        "content": "<p>General Purpose SSD (gp2)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11870,
                        "content": "<p>Throughput Optimized HDD (st1)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11871,
                        "content": "<p>Cold HDD (sc1)</p>",
                        "isValid": true
                    },
                    {
                        "id": 11872,
                        "content": "<p>Provisioned IOPS SSD (io1)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2845,
            "attributes": {
                "createdAt": "2023-09-09T20:40:19.411Z",
                "updatedAt": "2023-09-09T20:40:19.411Z",
                "content": "<p>A Solutions Architect needs to capture information about the traffic that reaches an Amazon Elastic Load Balancer. The information should include the source, destination, and protocol.</p><p>What is the most secure and reliable method for gathering this data?</p>",
                "answerExplanation": "<p>You can use VPC Flow Logs to capture detailed information about the traffic going to and from your Elastic Load Balancer. Create a flow log for each network interface for your load balancer. There is one network interface per load balancer subnet.</p><p><strong>CORRECT: </strong>\"Create a VPC flow log for each network interface associated with the ELB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable Amazon CloudTrail logging and configure packet capturing\" is incorrect. CloudTrail performs auditing of API actions, it does not do packet capturing.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Logs to review detailed logging information\" is incorrect as this service does not record this information in CloudWatch logs.</p><p><strong>INCORRECT:</strong> \"Create a VPC flow log for the subnets in which the ELB is running\" is incorrect as the more secure option is to use the ELB network interfaces.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html</a><br><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-monitoring.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-monitoring.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 11873,
                        "content": "<p>Enable Amazon CloudTrail logging and configure packet capturing</p>",
                        "isValid": false
                    },
                    {
                        "id": 11874,
                        "content": "<p>Create a VPC flow log for each network interface associated with the ELB</p>",
                        "isValid": true
                    },
                    {
                        "id": 11875,
                        "content": "<p>Create a VPC flow log for the subnets in which the ELB is running</p>",
                        "isValid": false
                    },
                    {
                        "id": 11876,
                        "content": "<p>Use Amazon CloudWatch Logs to review detailed logging information</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2846,
            "attributes": {
                "createdAt": "2023-09-09T20:40:19.507Z",
                "updatedAt": "2023-09-09T20:40:19.507Z",
                "content": "<p>An Amazon DynamoDB table has a variable load, ranging from sustained heavy usage some days, to only having small spikes on others. The load is 80% read and 20% write. The provisioned throughput capacity has been configured to account for the heavy load to ensure throttling does not occur.</p><p>What would be the most efficient solution to optimize cost?</p>",
                "answerExplanation": "<p><em>Amazon DynamoDB auto scaling </em>uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This is the most efficient and cost-effective solution to optimizing for cost.</p><p><strong>CORRECT: </strong>\"Create a DynamoDB Auto Scaling scaling policy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that triggers an AWS Lambda function that adjusts the provisioned throughput\" is incorrect. Using AWS Lambda to modify the provisioned throughput is possible but it would be more cost-effective to use DynamoDB Auto Scaling as there is no cost to using it.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that notifies you of increased/decreased load, and manually adjust the provisioned throughput\" is incorrect. Manually adjusting the provisioned throughput is not efficient.</p><p><strong>INCORRECT:</strong> \"Use DynamoDB DAX to increase the performance of the database\" is incorrect. DynamoDB DAX is an in-memory cache that increases the performance of DynamoDB. However, it costs money and there is no requirement to increase performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 11877,
                        "content": "<p>Create a CloudWatch alarm that notifies you of increased/decreased load, and manually adjust the provisioned throughput&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11878,
                        "content": "<p>Create a CloudWatch alarm that triggers an AWS Lambda function that adjusts the provisioned throughput&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11879,
                        "content": "<p>Use DynamoDB DAX to increase the performance of the database&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11880,
                        "content": "<p>Create a DynamoDB Auto Scaling scaling policy&nbsp; &nbsp; </p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2847,
            "attributes": {
                "createdAt": "2023-09-09T20:40:19.619Z",
                "updatedAt": "2023-09-09T20:40:19.619Z",
                "content": "<p>A Solutions Architect needs to upload a large (2GB) file to an S3 bucket. What is the recommended way to upload a single large file to an S3 bucket?</p>",
                "answerExplanation": "<p>In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.</p><p><strong>CORRECT: </strong>\"Use Multipart Upload\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS Import/Export\" is incorrect. AWS Import/Export is a service in which you send in HDDs with data on to AWS and they import your data into S3. It is not used for single files.</p><p><strong>INCORRECT:</strong> \"Use a single PUT request to upload the large file\" is incorrect. The largest object that can be uploaded in a single PUT is 5 gigabytes.</p><p><strong>INCORRECT:</strong> \"Use Amazon Snowball\" is incorrect. Snowball is used for migrating large quantities (TB/PB) of data into AWS, it is overkill for this requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 11881,
                        "content": "<p>Use AWS Import/Export&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11882,
                        "content": "<p>Use a single PUT request to upload the large file&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11883,
                        "content": "<p>Use Amazon Snowball&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11884,
                        "content": "<p>Use Multipart Upload&nbsp; &nbsp; </p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2848,
            "attributes": {
                "createdAt": "2023-09-09T20:40:19.723Z",
                "updatedAt": "2023-09-09T20:40:19.723Z",
                "content": "<p>Three AWS accounts are owned by the same company but in different regions. Account Z has two AWS Direct Connect connections to two separate company offices. Accounts A and B require the ability to route across account Z’s Direct Connect connections to each company office. A Solutions Architect has created an AWS Direct Connect gateway in account Z. </p><p>How can the required connectivity be configured?</p>",
                "answerExplanation": "<p>You can associate an <em>AWS Direct Connect gateway</em> with either of the following gateways:</p><p>- A transit gateway when you have multiple VPCs in the same Region.</p><p>- A virtual private gateway.</p><p>In this case account Z owns the Direct Connect gateway so a VPG in accounts A and B must be associated with it to enable this configuration to work. After Account Z accepts the proposals, Account A and Account B can route traffic from their virtual private gateway to the Direct Connect gateway.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-27-50-f7c361a3527b03f1a25861bfd2e7664b.png\"></p><p><strong>CORRECT: </strong>\"Associate the Direct Connect gateway to a virtual private gateway in account A and B\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Associate the Direct Connect gateway to a transit gateway in each region\" is incorrect. This would be a good solution if the accounts were in VPCs within a region rather than across regions.</p><p><strong>INCORRECT:</strong> \"Create a VPC Endpoint to the Direct Connect gateway in account A and B\" is incorrect. You cannot create a VPC endpoint for Direct Connect gateways.</p><p><strong>INCORRECT:</strong> \"Create a PrivateLink connection in Account Z and ENIs in accounts A and B\" is incorrect. You cannot use PrivateLink connections to publish a Direct Connect gateway.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-direct-connect/\">https://digitalcloud.training/aws-direct-connect/</a></p>",
                "options": [
                    {
                        "id": 11885,
                        "content": "<p>Associate the Direct Connect gateway to a virtual private gateway in account A and B</p>",
                        "isValid": true
                    },
                    {
                        "id": 11886,
                        "content": "<p>Associate the Direct Connect gateway to a transit gateway in each region</p>",
                        "isValid": false
                    },
                    {
                        "id": 11887,
                        "content": "<p>Create a PrivateLink connection in Account Z and ENIs in accounts A and B</p>",
                        "isValid": false
                    },
                    {
                        "id": 11888,
                        "content": "<p>Create a VPC Endpoint to the Direct Connect gateway in account A and B</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2849,
            "attributes": {
                "createdAt": "2023-09-09T20:40:19.829Z",
                "updatedAt": "2023-09-09T20:40:19.829Z",
                "content": "<p>A Solutions Architect enabled Access Logs on an Application Load Balancer (ALB) and needs to process the log files using a hosted Hadoop service. What configuration changes and services can be leveraged to deliver this requirement?</p>",
                "answerExplanation": "<p>Access Logs can be enabled on ALB and configured to store data in an S3 bucket. Amazon EMR is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data. EMR utilizes a hosted Hadoop framework running on Amazon EC2 and Amazon S3.</p><p><strong>CORRECT: </strong>\"Configure Access Logs to be delivered to S3 and use EMR for processing the log files\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure Access Logs to be delivered to EC2 and install Hadoop for processing the log files\" is incorrect. EC2 does not provide a hosted Hadoop service.</p><p><strong>INCORRECT:</strong> \"Configure Access Logs to be delivered to DynamoDB and use EMR for processing the log files\" is incorrect. You cannot configure access logs to be delivered to DynamoDB.</p><p><strong>INCORRECT:</strong> \"Configure Access Logs to be delivered to S3 and use Kinesis for processing the log files\" is incorrect. Kinesis does not provide a hosted Hadoop service.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-emr/\">https://digitalcloud.training/amazon-emr/</a></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 11889,
                        "content": "<p>Configure Access Logs to be delivered to DynamoDB and use EMR for processing the log files&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11890,
                        "content": "<p>Configure Access Logs to be delivered to S3 and use Kinesis for processing the log files&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11891,
                        "content": "<p>Configure Access Logs to be delivered to EC2 and install Hadoop for processing the log files&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11892,
                        "content": "<p>Configure Access Logs to be delivered to S3 and use EMR for processing the log files&nbsp; &nbsp; </p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2850,
            "attributes": {
                "createdAt": "2023-09-09T20:40:19.936Z",
                "updatedAt": "2023-09-09T20:40:19.936Z",
                "content": "<p>An application uses a MySQL database running on an Amazon EC2 instance. The application generates high I/O and constant writes to a single table on the database. Which Amazon EBS volume type will provide the MOST consistent performance and low latency?</p>",
                "answerExplanation": "<p>The Provisioned IOPS SSD (io1) volume type will offer the most consistent performance and can be configured with the amount of IOPS required by the application. It will also provide the lowest latency of the options presented.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-30-39-f006b5d6b4ffa97e2db382aff552857e.png\"></p><p><strong>CORRECT: </strong>\"Provisioned IOPS SSD (io1)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"General Purpose SSD (gp2)\" is incorrect. This is not the best solution for when you require high I/O, consistent performance and low latency.</p><p><strong>INCORRECT:</strong> \"Throughput Optimized HDD (st1)\" is incorrect. This is a HDD type of disk and not suitable for low latency workloads that require consistent performance.</p><p><strong>INCORRECT:</strong> \"Cold HDD (sc1)\" is incorrect. This is the lowest cost option and not suitable for frequently accessed workloads.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 11893,
                        "content": "<p>General Purpose SSD (gp2)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11894,
                        "content": "<p>Provisioned IOPS SSD (io1)</p>",
                        "isValid": true
                    },
                    {
                        "id": 11895,
                        "content": "<p>Throughput Optimized HDD (st1)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11896,
                        "content": "<p>Cold HDD (sc1)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2851,
            "attributes": {
                "createdAt": "2023-09-09T20:40:20.046Z",
                "updatedAt": "2023-09-09T20:40:20.046Z",
                "content": "<p>A Solutions Architect has logged into an Amazon EC2 Linux instance using SSH and needs to determine a few pieces of information including what IAM role is assigned, the instance ID and the names of the security groups that are assigned to the instance.</p><p>From the options below, what would be the best source of this information?</p>",
                "answerExplanation": "<p><em>Instance metadata</em> is data about your instance that you can use to configure or manage the running instance. Instance metadata is divided into categories, for example, host name, events, and security groups.</p><p>Instance metadata is available at http://169.254.169.254/latest/meta-data.</p><p><strong>CORRECT: </strong>\"Metadata\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Tags\" is incorrect. Tags are used to categorize and label resources.</p><p><strong>INCORRECT:</strong> \"User data\" is incorrect. User data is used to configure the system at launch time and specify scripts.</p><p><strong>INCORRECT:</strong> \"Parameters\" is incorrect. Parameters are used in databases.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 11897,
                        "content": "<p>Tags&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11898,
                        "content": "<p>Parameters&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11899,
                        "content": "<p>Metadata&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 11900,
                        "content": "<p>User data&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2852,
            "attributes": {
                "createdAt": "2023-09-09T20:40:20.145Z",
                "updatedAt": "2023-09-09T20:40:20.145Z",
                "content": "<p>A company is looking for ways to incorporate its current AWS usage expenditure into its operational expense tracking dashboard. A solutions architect has been tasked with proposing a method that enables the company to fetch its current year's cost data and project the costs for the forthcoming 12 months programmatically.</p><p>Which approach would fulfill these needs with the MINIMUM operational burden?</p>",
                "answerExplanation": "<p>AWS Cost Explorer API provides programmatic access to AWS cost and usage information. The user can query for aggregated data such as total monthly costs or total daily usage with this API.</p><p>Also, the Cost Explorer API supports pagination for managing larger data sets, making it efficient for larger queries.</p><p><strong>CORRECT: </strong>\"Leverage the AWS Cost Explorer API to retrieve usage cost-related data, using pagination for larger data sets\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Make use of downloadable AWS Cost Explorer report files in the .csv format to access usage cost-related data\" is incorrect.</p><p>While AWS Cost Explorer does allow you to download .csv reports of your cost data, this method would not be programmatically accessible and would involve manual steps to download and process the data.</p><p><strong>INCORRECT:</strong> \"Set up AWS Budgets actions to transmit usage cost data to the corporation via FTP\" is incorrect.</p><p>AWS Budgets actions allow you to set custom cost and usage budgets that trigger actions (such as turning off EC2 instances) when the budget thresholds you set are breached. However, AWS Budgets does not support transmitting data via FTP.</p><p><strong>INCORRECT:</strong> \"Generate AWS Budgets reports on usage cost data and dispatch the data to the corporation through SMTP\" is incorrect.</p><p>AWS Budgets does not support the dispatching of data through SMTP. AWS Budgets is primarily a tool for setting up alerts on your AWS costs or usage to control your costs, rather than a tool for exporting or transmitting cost data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/aws-cost-management/latest/APIReference/API_Operations_AWS_Cost_Explorer_Service.html\">https://docs.aws.amazon.com/aws-cost-management/latest/APIReference/API_Operations_AWS_Cost_Explorer_Service.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cost-management/\">https://digitalcloud.training/aws-cost-management/</a></p>",
                "options": [
                    {
                        "id": 11901,
                        "content": "<p>Leverage the AWS Cost Explorer API to retrieve usage cost-related data, using pagination for larger data sets.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11902,
                        "content": "<p>Make use of downloadable AWS Cost Explorer report files in the .csv format to access usage cost-related data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11903,
                        "content": "<p>Generate AWS Budgets reports on usage cost data and dispatch the data to the corporation through SMTP.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11904,
                        "content": "<p>Set up AWS Budgets actions to transmit usage cost data to the corporation via FTP.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2853,
            "attributes": {
                "createdAt": "2023-09-09T20:40:20.277Z",
                "updatedAt": "2023-09-09T20:40:20.277Z",
                "content": "<p>A Solutions Architect is designing the disk configuration for an Amazon EC2 instance. The instance needs to support a MapReduce process that requires high throughput for a large dataset with large I/O sizes.</p><p>Which Amazon EBS volume is the MOST cost-effective solution for these requirements?</p>",
                "answerExplanation": "<p>EBS Throughput Optimized HDD is good for the following use cases (and is the most cost-effective option:</p><p>- Frequently accessed, throughput intensive workloads with large datasets and large I/O sizes, such as MapReduce, Kafka, log processing, data warehouse, and ETL workloads.</p><p>Throughput is measured in MB/s, and includes the ability to burst up to 250 MB/s per TB, with a baseline throughput of 40 MB/s per TB and a maximum throughput of 500 MB/s per volume.</p><p><strong>CORRECT: </strong>\"EBS Throughput Optimized HDD\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"EBS General Purpose SSD in a RAID 1 configuration\" is incorrect. This is not the best solution for the requirements or the most cost-effective.</p><p><strong>INCORRECT:</strong> \"EBS Provisioned IOPS SSD\" is incorrect. SSD disks are more expensive.</p><p><strong>INCORRECT:</strong> \"EBS General Purpose SSD\" is incorrect. SSD disks are more expensive.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 11905,
                        "content": "<p>EBS General Purpose SSD in a RAID 1 configuration&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11906,
                        "content": "<p>EBS Provisioned IOPS SSD&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11907,
                        "content": "<p>EBS General Purpose SSD&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11908,
                        "content": "<p>EBS Throughput Optimized HDD&nbsp; &nbsp; </p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2854,
            "attributes": {
                "createdAt": "2023-09-09T20:40:20.392Z",
                "updatedAt": "2023-09-09T20:40:20.392Z",
                "content": "<p>A Solutions Architect has created a VPC and is in the process of formulating the subnet design. The VPC will be used to host a two-tier application that will include Internet facing web servers, and internal-only DB servers. Zonal redundancy is required. </p><p>How many subnets are required to support this requirement?</p>",
                "answerExplanation": "<p>Zonal redundancy indicates that the architecture should be split across multiple Availability Zones. Subnets are mapped 1:1 to AZs.</p><p>A public subnet should be used for the Internet-facing web servers and a separate private subnet should be used for the internal-only DB servers. Therefore you need 4 subnets – 2 (for redundancy) per public/private subnet.</p><p><strong>CORRECT: </strong>\"4 subnets\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"2 subnets\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"6 subnets\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"2 subnet\" is incorrect as explained above.</p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11909,
                        "content": "<p>4 subnets</p>",
                        "isValid": true
                    },
                    {
                        "id": 11910,
                        "content": "<p>2 subnets</p>",
                        "isValid": false
                    },
                    {
                        "id": 11911,
                        "content": "<p>6 subnets</p>",
                        "isValid": false
                    },
                    {
                        "id": 11912,
                        "content": "<p>1 subnet</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2855,
            "attributes": {
                "createdAt": "2023-09-09T20:40:20.506Z",
                "updatedAt": "2023-09-09T20:40:20.506Z",
                "content": "<p>A software development company is deploying a microservices-based application on Amazon Elastic Kubernetes Service (Amazon EKS). The application's traffic fluctuates significantly throughout the day and the company wants to ensure that the EKS cluster scales up and down according to these traffic patterns.</p><p>Which combination of steps would satisfy these requirements with MINIMAL operational overhead? (Select TWO.)</p>",
                "answerExplanation": "<p>The Metrics Server collects resource metrics like CPU and memory usage from each node and its pods and provides these metrics to the Kubernetes API server for use by the Horizontal Pod Autoscaler, which automatically scales the number of pods in a deployment, replication controller, replica set, or stateful set based on observed CPU utilization.</p><p>The Kubernetes Cluster Autoscaler automatically adjusts the size of the Kubernetes cluster when there are pods that failed to run in the cluster due to insufficient resources or when there are nodes in the cluster that have been underutilized for an extended period and their pods can be placed on other existing nodes.</p><p><strong>CORRECT: </strong>\"Utilize the Kubernetes Metrics Server to enable horizontal pod autoscaling based on resource utilization\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Employ the Kubernetes Cluster Autoscaler for dynamically managing the quantity of nodes in the EKS cluster\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement the Kubernetes Vertical Pod Autoscaler to adjust the CPU and memory allocation for the pods\" is incorrect.</p><p>The Vertical Pod Autoscaler adjusts the resources of the pods and not the number of pods or nodes, which won't directly help with scaling according to traffic patterns.</p><p><strong>INCORRECT:</strong> \"Integrate Amazon SQS and connect it to Amazon EKS for workload management\" is incorrect.</p><p>Amazon SQS is a message queuing service, and while it can be used to manage workloads by decoupling microservices, it doesn't directly help with autoscaling an EKS cluster based on traffic patterns.</p><p><strong>INCORRECT:</strong> \"Leverage AWS X-Ray to track and analyze the application's network activity\" is incorrect.</p><p>AWS X-Ray provides insights into the behavior of your applications, but it doesn't directly help with autoscaling an EKS cluster.</p><p><strong>References:</strong></p><p><a href=\"https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-server\">https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-server</a></p><p><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html\">https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 11913,
                        "content": "<p>Utilize the Kubernetes Metrics Server to enable horizontal pod autoscaling based on resource utilization.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11914,
                        "content": "<p>Implement the Kubernetes Vertical Pod Autoscaler to adjust the CPU and memory allocation for the pods.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11915,
                        "content": "<p>Integrate Amazon SQS and connect it to Amazon EKS for workload management.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11916,
                        "content": "<p>Employ the Kubernetes Cluster Autoscaler for dynamically managing the quantity of nodes in the EKS cluster.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11917,
                        "content": "<p>Leverage AWS X-Ray to track and analyze the application's network activity.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2856,
            "attributes": {
                "createdAt": "2023-09-09T20:40:20.650Z",
                "updatedAt": "2023-09-09T20:40:20.650Z",
                "content": "<p>A company needs to ensure that they can failover between AWS Regions in the event of a disaster seamlessly with minimal downtime and data loss. The applications will run in an active-active configuration.</p><p>Which DR strategy should a Solutions Architect recommend?</p>",
                "answerExplanation": "<p>A multi-site solution runs on AWS as well as on your existing on-site infrastructure in an active- active configuration. The data replication method that you employ will be determined by the recovery point that you choose. This is either Recovery Time Objective (the maximum allowable downtime before degraded operations are restored) or Recovery Point Objective (the maximum allowable time window whereby you will accept the loss of transactions during the DR process).</p><p><strong>CORRECT: </strong>\"Multi-site\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Backup and restore\" is incorrect. This is the lowest cost DR approach that simply entails creating online backups of all data and applications.</p><p><strong>INCORRECT:</strong> \"Pilot light\" is incorrect. With a pilot light strategy a core minimum of services are running and the remainder are only brought online during a disaster recovery situation.</p><p><strong>INCORRECT:</strong> \"Warm standby\" is incorrect. The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/\">https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/</a></p>",
                "options": [
                    {
                        "id": 11918,
                        "content": "<p>Pilot light</p>",
                        "isValid": false
                    },
                    {
                        "id": 11919,
                        "content": "<p>Backup and restore</p>",
                        "isValid": false
                    },
                    {
                        "id": 11920,
                        "content": "<p>Warm standby</p>",
                        "isValid": false
                    },
                    {
                        "id": 11921,
                        "content": "<p>Multi-site</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2857,
            "attributes": {
                "createdAt": "2023-09-09T20:40:20.800Z",
                "updatedAt": "2023-09-09T20:40:20.800Z",
                "content": "<p>An application receives a high traffic load between 7:30am and 9:30am daily. The application uses an Auto Scaling group to maintain three instances most of the time but during the peak period it requires six instances.</p><p>How can a Solutions Architect configure Auto Scaling to perform a daily scale-out event at 7:30am and a scale-in event at 9:30am to account for the peak load?</p>",
                "answerExplanation": "<p>The following scaling policy options are available:</p><p><strong>Simple</strong> – maintains a current number of instances, you can manually change the ASGs min/desired/max and attach/detach instances.</p><p><strong>Scheduled</strong> – Used for predictable load changes, can be a single event or a recurring schedule</p><p><strong>Dynamic </strong>(event based) – scale in response to an event/alarm.</p><p>Step – configure multiple scaling steps in response to multiple alarms.</p><p><strong>CORRECT: </strong>\"Use a Scheduled scaling policy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a Simple scaling policy\" is incorrect. Please refer to the description above.</p><p><strong>INCORRECT:</strong> \"Use a Dynamic scaling policy\" is incorrect. Please refer to the description above.</p><p><strong>INCORRECT:</strong> \"Use a Step scaling policy\" is incorrect. Please refer to the description above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 11922,
                        "content": "<p>Use a Dynamic scaling policy&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11923,
                        "content": "<p>Use a Scheduled scaling policy&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 11924,
                        "content": "<p>Use a Step scaling policy&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11925,
                        "content": "<p>Use a Simple scaling policy&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2858,
            "attributes": {
                "createdAt": "2023-09-09T20:40:20.898Z",
                "updatedAt": "2023-09-09T20:40:20.898Z",
                "content": "<p>A company operates a critical Python-based application that analyzes incoming real-time data. The application runs every 15 minutes and takes approximately 2 minutes to complete a run. It requires 1.5 GB of memory and uses the CPU intensively during its operation. The company wants to minimize the costs associated with running this application.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>This is the most cost-effective solution. AWS Lambda is designed for running code in response to events or on a schedule, and you only pay for the compute time that you consume.</p><p>Configuring the function with 1.5GB memory would ensure the function has enough resources, and using Amazon EventBridge for scheduling would enable running the function every 15 minutes.</p><p><strong>CORRECT: </strong>\"Implement the application as an AWS Lambda function configured with 1.5 GB of memory. Use Amazon EventBridge to schedule the function to run every 15 minutes\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS App2Container (A2C) to containerize the application. Run the application as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 1 virtual CPU (vCPU) and 1.5 GB of memory\" is incorrect.</p><p>This is not the most cost-effective solution. Even though AWS App2Container (A2C) would help in containerizing the application and AWS Fargate would abstract the need to manage underlying EC2 instances, it is still an overkill for an application that runs for short durations intermittently. It would still result in paying for unused compute resources.</p><p><strong>INCORRECT:</strong> \"Use AWS App2Container (A2C) to containerize the application. Deploy the container on an Amazon EC2 instance, configure an Amazon CloudWatch alarm to stop the instance when the application is not running\" is incorrect.</p><p>AWS App2Container (A2C) is used to help containerize applications, but this does not optimize for cost because it requires running an EC2 instance continuously and stopping the instance when not in use can be complex and might not be timely, resulting in potential unnecessary costs.</p><p><strong>INCORRECT:</strong> \"Deploy the application on an Amazon EC2 instance and manually start and stop the instance in alignment with the schedule of the application run\" is incorrect.</p><p>This solution involves significant manual intervention and managing EC2 instances. While it can work, it's not an optimized way, especially in terms of cost and operation overhead. It does not take advantage of the pay-per-use model and automatic scaling provided by AWS Lambda.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 11926,
                        "content": "<p>Deploy the application on an Amazon EC2 instance and manually start and stop the instance in alignment with the schedule of the application run.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11927,
                        "content": "<p>Use AWS App2Container (A2C) to containerize the application. Deploy the container on an Amazon EC2 instance, configure an Amazon CloudWatch alarm to stop the instance when the application is not running.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11928,
                        "content": "<p>Use AWS App2Container (A2C) to containerize the application. Run the application as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 1 virtual CPU (vCPU) and 1.5 GB of memory.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11929,
                        "content": "<p>Implement the application as an AWS Lambda function configured with 1.5 GB of memory. Use Amazon EventBridge to schedule the function to run every 15 minutes.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2859,
            "attributes": {
                "createdAt": "2023-09-09T20:40:21.015Z",
                "updatedAt": "2023-09-09T20:40:21.015Z",
                "content": "<p>A cloud architect is assessing the resilience of a web application deployed on AWS. It was observed that the application experienced a downtime of about 3 minutes when a scheduled failover was performed on the application's Amazon RDS MySQL database as part of a scaling operation.</p><p>The organization wants to mitigate such downtime in future scaling exercises while minimizing operational overhead.</p><p>Which solution will be the MOST effective in achieving this?</p>",
                "answerExplanation": "<p>Amazon RDS Proxy is a fully managed, highly available database proxy for Amazon RDS that makes applications more scalable, more resilient to database failures, and more secure.</p><p>During a failover, RDS Proxy automatically connects to a standby database instance while preserving connections from your application and reducing failover times for RDS and Aurora multi-AZ databases. So, there is minimal downtime for the application.</p><p><strong>CORRECT: </strong>\"Configure an Amazon RDS Proxy for the database and modify the application to connect to the proxy endpoint\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement more RDS MySQL read replicas in the cluster to manage the load during the failover\" is incorrect.</p><p>Adding more read replicas to the cluster does not decrease the downtime during a failover. It only improves the database's ability to handle read-heavy workloads. Read replicas do not contribute to a faster failover process.</p><p><strong>INCORRECT:</strong> \"Establish a secondary RDS MySQL cluster within the same AWS Region. During any future failover, modify the application to connect to the secondary cluster's writer endpoint\" is incorrect.</p><p>This approach is operationally heavy as it involves managing two separate RDS clusters and manually updating the application's database endpoint during a failover. Moreover, it does not necessarily reduce the downtime during a failover as there might be data inconsistency issues between the primary and secondary clusters, depending on the replication latency.</p><p><strong>INCORRECT:</strong> \"Implement an Amazon ElastiCache for Redis cluster to manage the load during the failover\" is incorrect.</p><p>ElastiCache is an in-memory cache and not a relational database service. It is typically used to cache frequently accessed data to reduce latency and improve application performance, not for managing failovers.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 11930,
                        "content": "<p>Implement more RDS MySQL read replicas in the cluster to manage the load during the failover.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11931,
                        "content": "<p>Establish a secondary RDS MySQL cluster within the same AWS Region. During any future failover, modify the application to connect to the secondary cluster's writer endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11932,
                        "content": "<p>Configure an Amazon RDS Proxy for the database and modify the application to connect to the proxy endpoint.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11933,
                        "content": "<p>Implement an Amazon ElastiCache for Redis cluster to manage the load during the failover.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2860,
            "attributes": {
                "createdAt": "2023-09-09T20:40:21.200Z",
                "updatedAt": "2023-09-09T20:40:21.200Z",
                "content": "<p>The database layer of an on-premises web application is being migrated to AWS. The database currently uses an in-memory cache. A Solutions Architect must deliver a solution that supports high availability and replication for the caching layer.</p><p>Which service should the Solutions Architect recommend?</p>",
                "answerExplanation": "<p>Amazon ElastiCache Redis is an in-memory database cache and supports high availability through replicas and multi-AZ. The table below compares ElastiCache Redis with Memcached:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-32-43-a842e78974d591a0b9d23fb5d088de78.png\"></p><p><strong>CORRECT: </strong>\"Amazon ElastiCache Redis\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon ElastiCache Memcached\" is incorrect as it does not support high availability or multi-AZ.</p><p><strong>INCORRECT:</strong> \"Amazon RDS Multi-AZ\" is incorrect. This is not an in-memory database and it not suitable for use as a caching layer.</p><p><strong>INCORRECT:</strong> \"Amazon DynamoDB\" is incorrect. DynamoDB is a non-relational database. You would not use it for a caching layer. Also, the in-memory, low-latency caching for DynamoDB is implemented using DynamoDB Accelerator (DAX).</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 11934,
                        "content": "<p>Amazon ElastiCache Redis</p>",
                        "isValid": true
                    },
                    {
                        "id": 11935,
                        "content": "<p>Amazon RDS Multi-AZ</p>",
                        "isValid": false
                    },
                    {
                        "id": 11936,
                        "content": "<p>Amazon ElastiCache Memcached</p>",
                        "isValid": false
                    },
                    {
                        "id": 11937,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2861,
            "attributes": {
                "createdAt": "2023-09-09T20:40:21.302Z",
                "updatedAt": "2023-09-09T20:40:21.302Z",
                "content": "<p>A financial institution wants to use machine learning (ML) algorithms to detect potential fraudulent transactions. They need to create ML models based on their vast financial transaction data and integrate these models into their business intelligence system for real-time decision-making. The solution should require minimal operational overhead.</p><p>Which solution will best meet these requirements?</p>",
                "answerExplanation": "<p>Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. It can directly connect with data sources and has built-in algorithms to ease the ML process.</p><p>Amazon QuickSight is a business intelligence tool that can be used to create dashboards for data visualization. This combination perfectly suits the requirement.</p><p><strong>CORRECT: </strong>\"Use Amazon SageMaker to build, train, and deploy ML models, and use Amazon QuickSight for data visualization\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to perform ETL jobs on the transaction data and use Amazon Forecast for predictive analytics\" is incorrect.</p><p>AWS Glue is primarily used for ETL jobs - cleaning, preparing, and moving data. Amazon Forecast is a fully managed service for time-series forecasting, which might not be a complete solution for detecting fraudulent transactions.</p><p><strong>INCORRECT:</strong> \"Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models and use AWS Athena for data visualization\" is incorrect.</p><p>AWS Marketplace ML AMIs can be used to create and train models, but this will require manual operational effort in terms of setting up and managing the instances. Athena is a query service and does not provide data visualization capabilities that a business intelligence tool like QuickSight provides.</p><p><strong>INCORRECT:</strong> \"Use Amazon Comprehend for analyzing the transaction data and Amazon Elasticsearch for visualization\" is incorrect.</p><p>Amazon Comprehend is primarily used for natural language processing (NLP), which isn't suited for detecting fraudulent transactions. Elasticsearch is a search and analytics engine and might not be the best tool for the use case described here.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/sagemaker/\">https://aws.amazon.com/sagemaker/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-machine-learning-services/\">https://digitalcloud.training/aws-machine-learning-services/</a></p>",
                "options": [
                    {
                        "id": 11938,
                        "content": "<p>1. Use AWS Glue to perform ETL jobs on the transaction data and use Amazon Forecast for predictive analytics.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11939,
                        "content": "<p>Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models and use AWS Athena for data visualization.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11940,
                        "content": "<p>Use Amazon Comprehend for analyzing the transaction data and Amazon Elasticsearch for visualization.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11941,
                        "content": "<p>Use Amazon SageMaker to build, train, and deploy ML models, and use Amazon QuickSight for data visualization.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2862,
            "attributes": {
                "createdAt": "2023-09-09T20:40:21.417Z",
                "updatedAt": "2023-09-09T20:40:21.417Z",
                "content": "<p>A media company hosts several terabytes of multimedia content across multiple AWS accounts. The company uses AWS Lake Formation to manage its data lake. The company's marketing team needs to securely access and analyze selective data from various accounts for targeted advertisement campaigns.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>With Lake Formation tag-based access control, you can manage permissions using tags and grant cross-account permissions, which would meet the requirements with the least operational overhead.</p><p><strong>CORRECT: </strong>\"Utilize Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the marketing team accounts\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Replicate the required data to a shared account. Create an IAM access role in that account. Grant access by defining a permission policy that includes users from the marketing team accounts as trusted entities\" is incorrect.</p><p>This solution involves the unnecessary replication of data, leading to increased storage costs and operational overhead.</p><p><strong>INCORRECT:</strong> \"Use the Lake Formation permissions Grant command in each account where the data is stored to permit the required marketing team users to access the data\" is incorrect.</p><p>The Grant command would need to be manually executed in each account where data is stored, which could lead to increased operational overhead, particularly if the data is spread across many accounts.</p><p><strong>INCORRECT:</strong> \"Use AWS DataSync to synchronize the necessary data to the marketing team accounts\" is incorrect.</p><p>AWS DataSync is designed for online data transfer, not for granting access permissions to data already stored in AWS, so this would not meet the requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/tag-based-access-control.html\">https://docs.aws.amazon.com/lake-formation/latest/dg/tag-based-access-control.html</a></p>",
                "options": [
                    {
                        "id": 11942,
                        "content": "<p>Use AWS DataSync to synchronize the necessary data to the marketing team accounts.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11943,
                        "content": "<p>Utilize Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the marketing team accounts.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11944,
                        "content": "<p>Use the Lake Formation permissions Grant command in each account where the data is stored to permit the required marketing team users to access the data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11945,
                        "content": "<p>Replicate the required data to a shared account. Create an IAM access role in that account. Grant access by defining a permission policy that includes users from the marketing team accounts as trusted entities.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2863,
            "attributes": {
                "createdAt": "2023-09-09T20:40:21.516Z",
                "updatedAt": "2023-09-09T20:40:21.516Z",
                "content": "<p>The application development team in a company have developed a Java application and saved the source code in a .war file. They would like to run the application on AWS resources and are looking for a service that can handle the provisioning and management of the underlying resources it will run on.</p><p>Which AWS service should a Solutions Architect recommend the Developers use to upload the Java source code file?</p>",
                "answerExplanation": "<p>AWS Elastic Beanstalk can be used to quickly deploy and manage applications in the AWS Cloud. Developers upload applications and Elastic Beanstalk handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring</p><p>Elastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby, as well as different platform configurations for each language. To use Elastic Beanstalk, you create an application, upload an application version in the form of an application source bundle (for example, a Java .war file) to Elastic Beanstalk, and then provide some information about the application.</p><p><strong>CORRECT: </strong>\"AWS Elastic Beanstalk\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodeDeploy\" is incorrect. AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.</p><p><strong>INCORRECT:</strong> \"AWS CloudFormation\" is incorrect. AWS CloudFormation uses templates to deploy infrastructure as code. It is not a PaaS service like Elastic Beanstalk and is more focused on infrastructure than applications and management of applications.</p><p><strong>INCORRECT:</strong> \"AWS OpsWorks\" is incorrect. AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
                "options": [
                    {
                        "id": 11946,
                        "content": "<p>AWS OpsWorks&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11947,
                        "content": "<p>AWS CodeDeploy&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11948,
                        "content": "<p>AWS Elastic Beanstalk&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 11949,
                        "content": "<p>AWS CloudFormation&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2864,
            "attributes": {
                "createdAt": "2023-09-09T20:40:21.631Z",
                "updatedAt": "2023-09-09T20:40:21.631Z",
                "content": "<p>The Solutions Architect in charge of a critical application must ensure the Amazon EC2 instances are able to be launched in another AWS Region in the event of a disaster.</p><p>What steps should the Solutions Architect take? (Select TWO.)</p>",
                "answerExplanation": "<p>You can create AMIs of the EC2 instances and then copy them across Regions. This provides a point-in-time copy of the state of the EC2 instance in the remote Region.</p><p>Once you’ve created AMIs of EC2 instances and copied them to the second Region, you can then launch the EC2 instances from the AMIs in that Region.</p><p>This is a good DR strategy as you have moved stateful EC2 instances to another Region.</p><p><strong>CORRECT: </strong>\"Create AMIs of the instances and copy them to another Region\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Launch instances in the second Region from the AMIs\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Launch instances in the second Region using the S3 API\" is incorrect. Though snapshots (and EBS-backed AMIs) are stored on Amazon S3, you cannot actually access them using the S3 API. You must use the EC2 API.</p><p><strong>INCORRECT:</strong> \"Enable cross-region snapshots for the Amazon EC2 instances\" is incorrect. You cannot enable “cross-region snapshots” as this is not a feature that currently exists.</p><p><strong>INCORRECT:</strong> \"Copy the snapshots using Amazon S3 cross-region replication\" is incorrect. You cannot work with snapshots using Amazon S3 at all including leveraging the cross-region replication feature.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/ebs-snapshot-copy/\">https://aws.amazon.com/blogs/aws/ebs-snapshot-copy/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 11950,
                        "content": "<p>Enable cross-region snapshots for the Amazon EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 11951,
                        "content": "<p>Create AMIs of the instances and copy them to another Region</p>",
                        "isValid": true
                    },
                    {
                        "id": 11952,
                        "content": "<p>Launch instances in the second Region from the AMIs</p>",
                        "isValid": true
                    },
                    {
                        "id": 11953,
                        "content": "<p>Copy the snapshots using Amazon S3 cross-region replication</p>",
                        "isValid": false
                    },
                    {
                        "id": 11954,
                        "content": "<p>Launch instances in the second Region using the S3 API</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2865,
            "attributes": {
                "createdAt": "2023-09-09T20:40:21.741Z",
                "updatedAt": "2023-09-09T20:40:21.741Z",
                "content": "<p>A company needs to capture detailed information about all HTTP requests that are processed by their Internet facing Application Load Balancer (ALB). The company requires information on the requester, IP address, and request type for analyzing traffic patterns to better understand their customer base.</p><p>Which actions should a Solutions Architect recommend?</p>",
                "answerExplanation": "<p>You can enable access logs on the ALB and this will provide the information required including requester, IP, and request type. Access logs are not enabled by default. You can optionally store and retain the log files on S3.</p><p><strong>CORRECT: </strong>\"Enable Access Logs and store the data on S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure metrics in CloudWatch for the ALB\" is incorrect. CloudWatch is used for performance monitoring and CloudTrail is used for auditing API access..</p><p><strong>INCORRECT:</strong> \"Enable EC2 detailed monitoring\" is incorrect. Enabling EC2 detailed monitoring will not capture the information requested.</p><p><strong>INCORRECT:</strong> Use CloudTrail to capture all API calls made to the ALB\"\" is incorrect. CloudTrail captures API activity and would not include the requested information.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 11955,
                        "content": "<p>Use CloudTrail to capture all API calls made to the ALB&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11956,
                        "content": "<p>Enable Access Logs and store the data on S3&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 11957,
                        "content": "<p>Enable EC2 detailed monitoring&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11958,
                        "content": "<p>Configure metrics in CloudWatch for the ALB&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2866,
            "attributes": {
                "createdAt": "2023-09-09T20:40:21.839Z",
                "updatedAt": "2023-09-09T20:40:21.839Z",
                "content": "<p>A financial services company is migrating its sensitive customer data and applications to AWS. They want to ensure that the data is securely stored and managed while reducing the overall maintenance and operational overhead associated with managing databases.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>Amazon RDS makes it easy to go from project conception to deployment by managing time-consuming database administration tasks including backups, software patching, monitoring, scaling, and replication.</p><p>Amazon RDS supports encryption at rest, which ensures the security of sensitive data and meets regulatory compliance requirements. AWS Key Management Service (AWS KMS) is integrated with Amazon RDS to make it easier to create, control, and manage keys for encryption.</p><p><strong>CORRECT: </strong>\"Migrate the data and applications to Amazon RDS instances. Enable encryption at rest using AWS Key Management Service (AWS KMS)\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the applications and data to Amazon EC2 instances. Utilize the AWS Key Management Service (AWS KMS) customer managed keys for encryption\" is incorrect.</p><p>While this solution offers data encryption, it does not meet the requirement to reduce operational overhead. Managing databases on EC2 instances requires additional administrative tasks, such as managing backups and applying software patches, which Amazon RDS handles automatically.</p><p><strong>INCORRECT:</strong> \"Store the data in Amazon S3. Utilize Amazon Macie for ongoing data security and threat detection\" is incorrect.</p><p>Amazon S3 and Macie are suitable for data storage and security analysis, respectively. However, Amazon S3 is not designed to serve as a transactional database for applications, which is a key requirement in this scenario.</p><p><strong>INCORRECT:</strong> \"Migrate the data to Amazon RDS instances. Enable Amazon GuardDuty for data protection and threat detection\" is incorrect.</p><p>While Amazon RDS is a correct choice for database management and Amazon GuardDuty offers threat detection, GuardDuty is not specifically designed for data protection within databases. It's a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
                "options": [
                    {
                        "id": 11959,
                        "content": "<p>Migrate the data to Amazon RDS instances. Enable Amazon GuardDuty for data protection and threat detection.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11960,
                        "content": "<p>Migrate the data and applications to Amazon RDS instances. Enable encryption at rest using AWS Key Management Service (AWS KMS).</p>",
                        "isValid": true
                    },
                    {
                        "id": 11961,
                        "content": "<p>Migrate the applications and data to Amazon EC2 instances. Utilize the AWS Key Management Service (AWS KMS) customer managed keys for encryption.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11962,
                        "content": "<p>Store the data in Amazon S3. Utilize Amazon Macie for ongoing data security and threat detection.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2867,
            "attributes": {
                "createdAt": "2023-09-09T20:40:21.967Z",
                "updatedAt": "2023-09-09T20:40:21.967Z",
                "content": "<p>A Solutions Architect has created a new security group in an Amazon VPC. No rules have been created. Which of the statements below are correct regarding the default state of the security group? (choose 2)</p>",
                "answerExplanation": "<p>Custom security groups do not have inbound allow rules (all inbound traffic is denied by default) whereas default security groups do have inbound allow rules (allowing traffic from within the group). All outbound traffic is allowed by default in both custom and default security groups.</p><p>Security groups act like a stateful firewall at the instance level. Specifically security groups operate at the network interface level of an EC2 instance. You can only assign permit rules in a security group, you cannot assign deny rules and there is an implicit deny rule at the end of the security group. All rules are evaluated until a permit is encountered or continues until the implicit deny. You can create ingress and egress rules.</p><p><strong>CORRECT: </strong>\"There is an outbound rule that allows all traffic to all IP addresses\" is the correct answer.</p><p><strong>CORRECT: </strong>\"There are no inbound rules and traffic will be implicitly denied\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"There is an inbound rule allowing traffic from the Internet to port 22 for management\" is incorrect. This is not true.</p><p><strong>INCORRECT:</strong> \"There are is an inbound rule that allows traffic from the Internet Gateway\" is incorrect. There are no inbound allow rules by default.</p><p><strong>INCORRECT:</strong> \"There is an outbound rule allowing traffic to the Internet Gateway\" is incorrect. There is an outbound allow rule but it allows traffic to anywhere, it does not specify the internet gateway.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 11963,
                        "content": "<p>There is an outbound rule that allows all traffic to all IP addresses&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 11964,
                        "content": "<p>There is an inbound rule allowing traffic from the Internet to port 22 for management&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11965,
                        "content": "<p>There are no inbound rules and traffic will be implicitly denied&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 11966,
                        "content": "<p>There are is an inbound rule that allows traffic from the Internet Gateway&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11967,
                        "content": "<p>There is an outbound rule allowing traffic to the Internet Gateway&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2868,
            "attributes": {
                "createdAt": "2023-09-09T20:40:22.061Z",
                "updatedAt": "2023-09-09T20:40:22.061Z",
                "content": "<p>A company has launched a multi-tier application architecture. The web tier and database tier run on Amazon EC2 instances in private subnets within the same Availability Zone.</p><p>Which combination of steps should a Solutions Architect take to add high availability to this architecture? (Select TWO.)</p>",
                "answerExplanation": "<p>The Solutions Architect can use Auto Scaling group across multiple AZs with an ALB in front to create an elastic and highly available architecture. Then, migrate the database to an Amazon RDS multi-AZ deployment to create HA for the database tier. This results in a fully redundant architecture that can withstand the failure of an availability zone.</p><p><strong>CORRECT: </strong>\"Create an Amazon EC2 Auto Scaling group and Application Load Balancer (ALB) spanning multiple AZs\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Create new private subnets in the same VPC but in a different AZ. Migrate the database to an Amazon RDS multi-AZ deployment\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create new public subnets in the same AZ for high availability and move the web tier to the public subnets\" is incorrect. If subnets share the same AZ they are not suitable for splitting your tier across them for HA as the failure of a an AZ will take out both subnets.</p><p><strong>INCORRECT:</strong> \"Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer (ALB)\" is incorrect. The instances are in a single AZ so the Solutions Architect should create a new auto scaling group and launch instances across multiple AZs.</p><p><strong>INCORRECT:</strong> \"Create new private subnets in the same VPC but in a different AZ. Create a database using Amazon EC2 in one AZ\" is incorrect. A database in a single AZ will not be highly available.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-increase-availability.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-increase-availability.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 11968,
                        "content": "<p>Create new private subnets in the same VPC but in a different AZ. Migrate the database to an Amazon RDS multi-AZ deployment</p>",
                        "isValid": true
                    },
                    {
                        "id": 11969,
                        "content": "<p>Create new public subnets in the same AZ for high availability and move the web tier to the public subnets</p>",
                        "isValid": false
                    },
                    {
                        "id": 11970,
                        "content": "<p>Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer (ALB)</p>",
                        "isValid": false
                    },
                    {
                        "id": 11971,
                        "content": "<p>Create an Amazon EC2 Auto Scaling group and Application Load Balancer (ALB) spanning multiple AZs</p>",
                        "isValid": true
                    },
                    {
                        "id": 11972,
                        "content": "<p>Create new private subnets in the same VPC but in a different AZ. Create a database using Amazon EC2 in one AZ</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2869,
            "attributes": {
                "createdAt": "2023-09-09T20:40:22.172Z",
                "updatedAt": "2023-09-09T20:40:22.172Z",
                "content": "<p>A company is in the process of improving its security posture and wants to analyze and rectify a high volume of failed login attempts and unauthorized activities being logged in AWS CloudTrail.</p><p>What is the most efficient solution to help the company identify these security events with the LEAST amount of operational effort?</p>",
                "answerExplanation": "<p>Amazon Athena can directly query data from S3 (where CloudTrail logs are stored) using standard SQL, making it a powerful and efficient tool for analyzing these logs. You don't need to manage any infrastructure or write custom scripts, and you can quickly write and run queries to identify the required security events.</p><p><strong>CORRECT: </strong>\"Use Amazon Athena to directly query CloudTrail logs for failed logins and unauthorized activities\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Leverage AWS Lambda to trigger on CloudTrail log updates and use a custom script to scan for failed logins and unauthorized actions\" is incorrect.</p><p>While Lambda functions can be triggered based on CloudTrail log updates and could theoretically be used to scan for security events, this would require substantial setup and ongoing maintenance of the script. It's not the most efficient choice.</p><p><strong>INCORRECT:</strong> \"Utilize AWS Data Pipeline to regularly extract CloudTrail logs and use a custom script to identify the required security events\" is incorrect.</p><p>This solution could work, but the operational overhead of managing the extraction process and maintaining a custom script for analysis is not minimal.</p><p><strong>INCORRECT:</strong> \"Implement Amazon Elasticsearch Service with Kibana to visualize the CloudTrail logs and manually search for these events\" is incorrect.</p><p>While Elasticsearch and Kibana provide powerful search and visualization capabilities, respectively, they require a fair amount of setup and management. This option would provide more in-depth analysis and real-time monitoring, but it wouldn't be the most efficient way to simply identify the security events mentioned.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html\">https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
                "options": [
                    {
                        "id": 11973,
                        "content": "<p>Leverage AWS Lambda to trigger on CloudTrail log updates and use a custom script to scan for failed logins and unauthorized actions.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11974,
                        "content": "<p>Implement Amazon Elasticsearch Service with Kibana to visualize the CloudTrail logs and manually search for these events.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11975,
                        "content": "<p>Utilize AWS Data Pipeline to regularly extract CloudTrail logs and use a custom script to identify the required security events.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11976,
                        "content": "<p>Use Amazon Athena to directly query CloudTrail logs for failed logins and unauthorized activities.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2870,
            "attributes": {
                "createdAt": "2023-09-09T20:40:22.279Z",
                "updatedAt": "2023-09-09T20:40:22.279Z",
                "content": "<p>An application makes calls to a REST API running on Amazon EC2 instances behind an Application Load Balancer (ALB). Most API calls complete quickly. However, a single endpoint is making API calls that require much longer to complete and this is introducing overall latency into the system. What steps can a Solutions Architect take to minimize the effects of the long-running API calls?</p>",
                "answerExplanation": "<p>An Amazon Simple Queue Service (SQS) can be used to offload and decouple the long-running requests. They can then be processed asynchronously by separate EC2 instances. This is the best way to reduce the overall latency introduced by the long-running API call.</p><p><strong>CORRECT: </strong>\"Create an Amazon SQS queue and decouple the long-running API calls\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Change the EC2 instance to one with enhanced networking to reduce latency\" is incorrect. This will not reduce the latency of the API call as network latency is not the issue here, it is the latency of how long the API call takes to complete.</p><p><strong>INCORRECT:</strong> \"Increase the ALB idle timeout to allow the long-running requests to complete\" is incorrect. The issue is not the connection being interrupted, it is that the API call takes a long time to complete.</p><p><strong>INCORRECT:</strong> \"Change the ALB to a Network Load Balancer (NLB) and use SSL/TLS termination\" is incorrect. SSL/TLS termination is not of benefit here as the problem is not encryption or processing of encryption. The issue is API call latency.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 11977,
                        "content": "<p>Create an Amazon SQS queue and decouple the long-running API calls</p>",
                        "isValid": true
                    },
                    {
                        "id": 11978,
                        "content": "<p>Change the EC2 instance to one with enhanced networking to reduce latency</p>",
                        "isValid": false
                    },
                    {
                        "id": 11979,
                        "content": "<p>Increase the ALB idle timeout to allow the long-running requests to complete</p>",
                        "isValid": false
                    },
                    {
                        "id": 11980,
                        "content": "<p>Change the ALB to a Network Load Balancer (NLB) and use SSL/TLS termination</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2871,
            "attributes": {
                "createdAt": "2023-09-09T20:40:22.375Z",
                "updatedAt": "2023-09-09T20:40:22.375Z",
                "content": "<p>Several Amazon EC2 Spot instances are being used to process messages from an Amazon SQS queue and store results in an Amazon DynamoDB table. Shortly after picking up a message from the queue AWS terminated the Spot instance. The Spot instance had not finished processing the message. What will happen to the message?</p>",
                "answerExplanation": "<p>The visibility timeout is the amount of time a message is invisible in the queue after a reader picks up the message. If a job is processed within the visibility timeout the message will be deleted. If a job is not processed within the visibility timeout the message will become visible again (could be delivered twice). The maximum visibility timeout for an Amazon SQS message is 12 hours.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-16-54-ed49b8985fe33f8c30a3f5c24800aca9.png\"></p><p><strong>CORRECT: </strong>\"The message will become available for processing again after the visibility timeout expires\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The message will be lost as it would have been deleted from the queue when processed\" is incorrect. The message will not be lost and will not be immediately picked up by another instance.</p><p><strong>INCORRECT:</strong> \"The message will remain in the queue and be immediately picked up by another instance\" is incorrect. As mentioned above it will be available for processing in the queue again after the timeout expires.</p><p><strong>INCORRECT:</strong> \"The results may be duplicated in DynamoDB as the message will likely be processed multiple times\" is incorrect. As the instance had not finished processing the message it should only be fully processed once. Depending on your application process however it is possible some data was written to DynamoDB.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 11981,
                        "content": "<p>The message will be lost as it would have been deleted from the queue when processed&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11982,
                        "content": "<p>The message will remain in the queue and be immediately picked up by another instance&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11983,
                        "content": "<p>The results may be duplicated in DynamoDB as the message will likely be processed multiple times&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11984,
                        "content": "<p>The message will become available for processing again after the visibility timeout expires&nbsp; &nbsp; </p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2872,
            "attributes": {
                "createdAt": "2023-09-09T20:40:22.475Z",
                "updatedAt": "2023-09-09T20:40:22.475Z",
                "content": "<p>An Amazon EC2 instance behind an Elastic Load Balancer (ELB) is in the process of being de-registered. Which ELB feature is used to allow existing connections to close cleanly?</p>",
                "answerExplanation": "<p>Connection draining is enabled by default and provides a period of time for existing connections to close cleanly. When connection draining is in action an CLB will be in the status “InService: Instance deregistration currently in progress”.</p><p><strong>CORRECT: </strong>\"Connection Draining\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Sticky Sessions\" is incorrect. Session stickiness uses cookies and ensures a client is bound to an individual back-end instance for the duration of the cookie lifetime.</p><p><strong>INCORRECT:</strong> \"Proxy Protocol\" is incorrect. The Proxy Protocol header helps you identify the IP address of a client when you have a load balancer that uses TCP for back-end connections.</p><p><strong>INCORRECT:</strong> \"Deletion Protection\" is incorrect. Deletion protection is used to protect the ELB from deletion.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2014/03/20/elastic-load-balancing-supports-connection-draining/\">https://aws.amazon.com/about-aws/whats-new/2014/03/20/elastic-load-balancing-supports-connection-draining/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 11985,
                        "content": "<p>Proxy Protocol&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11986,
                        "content": "<p>Sticky Sessions&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11987,
                        "content": "<p>Connection Draining&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 11988,
                        "content": "<p>Deletion Protection&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2873,
            "attributes": {
                "createdAt": "2023-09-09T20:40:22.576Z",
                "updatedAt": "2023-09-09T20:40:22.576Z",
                "content": "<p>An e-commerce company operates a serverless web application that must interact with numerous Amazon DynamoDB tables to fulfill user requests. It is critical that the application's performance remains consistent and unaffected while interacting with these tables.</p><p>Which method provides the MOST operationally efficient way to fulfill these requirements?</p>",
                "answerExplanation": "<p>AWS AppSync simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one or more data sources. AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need, including from multiple DynamoDB tables.</p><p>AWS AppSync is designed for real-time and offline data access which makes it an ideal solution for this scenario.</p><p><strong>CORRECT: </strong>\"AWS AppSync with multiple data sources and resolvers\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS Lambda with Step Functions\" is incorrect.</p><p>AWS Step Functions make it easy to coordinate the components of distributed applications and microservices using visual workflows. However, while you could theoretically build a flow to retrieve data from multiple tables, it's not the most efficient solution as it introduces additional complexity and potential latency.</p><p><strong>INCORRECT:</strong> \"Amazon S3 with Lambda triggers\" is incorrect.</p><p>While you can use AWS Lambda to execute code in response to triggers like changes to data in an Amazon S3 bucket, this doesn't directly allow the application to retrieve data from multiple DynamoDB tables. This approach would also involve unnecessary data transfers and added latency.</p><p><strong>INCORRECT:</strong> \"AWS Glue with a DynamoDB connector\" is incorrect.</p><p>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load your data for analytics. However, AWS Glue isn't meant for real-time data retrieval in an application. Using it for real-time data retrieval would likely be overcomplicated and inefficient.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/appsync/product-details/\">https://aws.amazon.com/appsync/product-details/</a></p>",
                "options": [
                    {
                        "id": 11989,
                        "content": "<p>AWS AppSync with multiple data sources and resolvers.</p>",
                        "isValid": true
                    },
                    {
                        "id": 11990,
                        "content": "<p>AWS Glue with a DynamoDB connector.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11991,
                        "content": "<p>AWS Lambda with Step Functions.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11992,
                        "content": "<p>Amazon S3 with Lambda triggers.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2874,
            "attributes": {
                "createdAt": "2023-09-09T20:40:22.693Z",
                "updatedAt": "2023-09-09T20:40:22.693Z",
                "content": "<p>A development team needs to run up a few lab servers on a weekend for a new project. The servers will need to run uninterrupted for a few hours. Which EC2 pricing option would be most suitable?&nbsp; &nbsp; </p>",
                "answerExplanation": "<p>On-Demand pricing ensures that instances will not be terminated and is the most economical option. Use on-demand for ad-hoc requirements where you cannot tolerate interruption.</p><p><strong>CORRECT: </strong>\"On-Demand\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Spot\" is incorrect. Spot pricing may be the most economical option for a short duration over a weekend but you may have the instances terminated by AWS and there is a requirement that the servers run uninterrupted.</p><p><strong>INCORRECT:</strong> \"Reserved\" is incorrect. Reserved pricing provides a reduced cost for a contracted period (1 or 3 years), and is not suitable for ad hoc requirements.</p><p><strong>INCORRECT:</strong> \"Dedicated instances\" is incorrect. Dedicated instances run on hardware that’s dedicated to a single customer and are more expensive than regular On-Demand instances.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 11993,
                        "content": "<p>Spot&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 11994,
                        "content": "<p>Dedicated instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 11995,
                        "content": "<p>Reserved</p>",
                        "isValid": false
                    },
                    {
                        "id": 11996,
                        "content": "<p>On-Demand</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2875,
            "attributes": {
                "createdAt": "2023-09-09T20:40:22.787Z",
                "updatedAt": "2023-09-09T20:40:22.787Z",
                "content": "<p>A telecommunication company has an API that allows users to manage their mobile plans and services. The API experiences significant traffic spikes during specific times such as end of the month and special offer periods. The company needs to ensure low latency response time consistently to ensure a good user experience. The solution should also minimize operational overhead.</p><p>Which solution would meet these requirements MOST efficiently?</p>",
                "answerExplanation": "<p>Amazon API Gateway and AWS Lambda together make a highly scalable solution for APIs. Provisioned concurrency in Lambda ensures that there is always a warm pool of functions ready to quickly respond to API requests, thereby guaranteeing low latency even during peak traffic times.</p><p><strong>CORRECT: </strong>\"Use Amazon API Gateway along with AWS Lambda functions with provisioned concurrency\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement the API using AWS Elastic Beanstalk with auto-scaling groups\" is incorrect.</p><p>Elastic Beanstalk is a viable option for deploying applications and auto-scaling and can help handle increased traffic, but it doesn't guarantee the low latency requirement during peak traffic times.</p><p><strong>INCORRECT:</strong> \"Use Amazon API Gateway with AWS Fargate tasks to handle the API requests\" is incorrect.</p><p>API Gateway with Fargate can provide scalable compute, but this approach can result in higher operational overhead because of the need to manage the container lifecycle.</p><p><strong>INCORRECT:</strong> \"Implement the API on an Amazon EC2 instance behind an Application Load Balancer with manual scaling\" is incorrect.</p><p>This solution does not scale automatically and would require manual intervention to ensure optimal performance during traffic spikes. Therefore, it doesn't satisfy the requirement of minimizing operational overhead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 11997,
                        "content": "<p>Implement the API using AWS Elastic Beanstalk with auto-scaling groups.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11998,
                        "content": "<p>Use Amazon API Gateway with AWS Fargate tasks to handle the API requests.</p>",
                        "isValid": false
                    },
                    {
                        "id": 11999,
                        "content": "<p>Implement the API on an Amazon EC2 instance behind an Application Load Balancer with manual scaling.</p>",
                        "isValid": false
                    },
                    {
                        "id": 12000,
                        "content": "<p>Use Amazon API Gateway along with AWS Lambda functions with provisioned concurrency.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2876,
            "attributes": {
                "createdAt": "2023-09-09T20:40:22.925Z",
                "updatedAt": "2023-09-09T20:40:22.925Z",
                "content": "<p>The load on a MySQL database running on Amazon EC2 is increasing and performance has been impacted. Which of the options below would help to increase storage performance? (choose 2)</p>",
                "answerExplanation": "<p>EBS optimized instances provide dedicated capacity for Amazon EBS I/O. EBS optimized instances are designed for use with all EBS volume types.</p><p>Provisioned IOPS EBS volumes allow you to specify the amount of IOPS you require up to 50 IOPS per GB. Within this limitation you can therefore choose to select the IOPS required to improve the performance of your volume.</p><p>RAID can be used to increase IOPS, however RAID 1 does not. For example:</p><p>– RAID 0 = 0 striping – data is written across multiple disks and increases performance but no redundancy.</p><p>– RAID 1 = 1 mirroring – creates 2 copies of the data but does not increase performance, only redundancy.</p><p>HDD, Cold – (SC1) provides the lowest cost storage and low performance</p><p><strong>CORRECT: </strong>\"Use Provisioned IOPS (I01) EBS volumes\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Use EBS optimized instances\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Use a larger instance size within the instance family\" is incorrect as this may not increase storage performance.</p><p><strong>INCORRECT:</strong> \"Use HDD, Cold (SC1) EBS volumes\" is incorrect. As this will likely decrease storage performance.</p><p><strong>INCORRECT:</strong> \"Create a RAID 1 array from multiple EBS volumes\" is incorrect. As explained above, mirroring does not increase performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 12001,
                        "content": "<p>Use a larger instance size within the instance family&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12002,
                        "content": "<p>Use EBS optimized instances&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12003,
                        "content": "<p>Use HDD, Cold (SC1) EBS volumes&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12004,
                        "content": "<p>Use Provisioned IOPS (I01) EBS volumes&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12005,
                        "content": "<p>Create a RAID 1 array from multiple EBS volumes&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2877,
            "attributes": {
                "createdAt": "2023-09-09T20:40:23.064Z",
                "updatedAt": "2023-09-09T20:40:23.064Z",
                "content": "<p>A Solutions Architect needs to run a PowerShell script on a fleet of Amazon EC2 instances running Microsoft Windows. The instances have already been launched in an Amazon VPC. What tool can be run from the AWS Management Console that to execute the script on all target EC2 instances?</p>",
                "answerExplanation": "<p>Run Command is designed to support a wide range of enterprise scenarios including installing software, running ad hoc scripts or Microsoft PowerShell commands, configuring Windows Update settings, and more.</p><p>Run Command can be used to implement configuration changes across Windows instances on a consistent yet ad hoc basis and is accessible from the AWS Management Console, the AWS Command Line Interface (CLI), the AWS Tools for Windows PowerShell, and the AWS SDKs.</p><p><strong>CORRECT: </strong>\"Run Command\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodeDeploy\" is incorrect. AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.</p><p><strong>INCORRECT:</strong> \"AWS Config\" is incorrect. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It is not used for ad-hoc script execution.</p><p><strong>INCORRECT:</strong> \"AWS OpsWorks\" is incorrect. AWS OpsWorks provides instances of managed Puppet and Chef.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-ec2-run-command-remote-instance-management-at-scale/\">https://aws.amazon.com/blogs/aws/new-ec2-run-command-remote-instance-management-at-scale/</a></p>",
                "options": [
                    {
                        "id": 12006,
                        "content": "<p>AWS CodeDeploy&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12007,
                        "content": "<p>AWS OpsWorks&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12008,
                        "content": "<p>AWS Config&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12009,
                        "content": "<p>Run Command&nbsp; &nbsp; </p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2878,
            "attributes": {
                "createdAt": "2023-09-09T20:40:23.172Z",
                "updatedAt": "2023-09-09T20:40:23.172Z",
                "content": "<p>A multinational organization has a distributed application that runs on Amazon EC2 instances, which are behind an Application Load Balancer in an Auto Scaling group. The application utilizes a MySQL database hosted on Amazon Aurora. The database cluster spans across multiple Availability Zones in a single region.</p><p>The organization plans to launch its services in a new geographical area and wants to ensure maximum availability with minimal service interruption.</p><p>Which strategy should the organization adopt?</p>",
                "answerExplanation": "<p>This solution involves creating an application layer in the new region and using Amazon Aurora Global Database, which supports replicating your databases across multiple regions with minimal impact on performance.</p><p>This configuration can enhance disaster recovery capabilities and can reduce the impact of planned maintenance. Amazon Route 53 health checks with a failover routing policy can automatically route traffic to the new region in the event of a failure in the primary region, thereby ensuring high availability.</p><p>With an Aurora global database, there are two different approaches to failover depending on the scenario. You can use manual unplanned failover (detach and promote) or managed planned failover.</p><p><strong>CORRECT: </strong>\"Establish the application layer in the new region. Use Amazon Aurora Global Database for deploying the database in the primary and new regions. Apply Amazon Route 53 health checks with a failover routing policy to the new region. Perform a manual failover as required\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Replicate the application layer in the new region. Implement an Aurora MySQL Read Replica in the new region using Route 53 health checks and a failover routing policy. In case of primary failure, promote the Read Replica to primary\" is incorrect.</p><p>This solution involves creating a Read Replica in the new region, which would indeed allow for the promotion of the Read Replica to a primary instance if necessary. However, this process isn't instantaneous and could lead to service interruption, which is not what the question asked for. Aurora Global Database provides a lower RTO/RPO.</p><p><strong>INCORRECT:</strong> \"Create a similar application layer in the new region. Establish a new Aurora MySQL database in this region. Use AWS Database Migration Service (AWS DMS) for ongoing replication from the primary database to the new region. Implement Amazon Route 53 health checks with a failover routing policy to the new region\" is incorrect.</p><p>AWS Database Migration Service (AWS DMS) is primarily used for migrating databases to AWS from on-premises environments or for replicating databases for data warehousing and other use cases. It isn't as suitable for ongoing high-availability or failover scenarios as Amazon Aurora Global Database, which is specifically designed for these situations.</p><p><strong>INCORRECT:</strong> \"Expand the existing Auto Scaling group into the new Region. Utilize Amazon Aurora Global Database to extend the database across the primary and new regions. Implement Amazon Route 53 health checks with a failover routing policy directed towards the new region\" is incorrect.</p><p>It is not possible to expand an Auto Scaling group across multiple Regions. ASGs operate within a Region only.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html#aurora-global-database-failover\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html#aurora-global-database-failover</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
                "options": [
                    {
                        "id": 12010,
                        "content": "<p>Establish the application layer in the new region. Use Amazon Aurora Global Database for deploying the database in the primary and new regions. Apply Amazon Route 53 health checks with a failover routing policy to the new region. Promote the secondary to primary as needed.</p>",
                        "isValid": true
                    },
                    {
                        "id": 12011,
                        "content": "<p>Replicate the application layer in the new region. Implement an Aurora MySQL Read Replica in the new region using Route 53 health checks and a failover routing policy. In case of primary failure, promote the Read Replica to primary.</p>",
                        "isValid": false
                    },
                    {
                        "id": 12012,
                        "content": "<p>Create a similar application layer in the new region. Establish a new Aurora MySQL database in this region. Use AWS Database Migration Service (AWS DMS) for ongoing replication from the primary database to the new region. Implement Amazon Route 53 health checks with a failover routing policy to the new region.</p>",
                        "isValid": false
                    },
                    {
                        "id": 12013,
                        "content": "<p>Expand the existing Auto Scaling group into the new Region. Utilize Amazon Aurora Global Database to extend the database across the primary and new regions. Implement Amazon Route 53 health checks with a failover routing policy directed towards the new region.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2879,
            "attributes": {
                "createdAt": "2023-09-09T20:40:23.265Z",
                "updatedAt": "2023-09-09T20:40:23.265Z",
                "content": "<p>An on-premises server runs a MySQL database and will be migrated to the AWS Cloud. The company require a managed solution that supports high availability and automatic failover in the event of the outage of an Availability Zone (AZ).</p><p>Which solution is the BEST fit for these requirements?</p>",
                "answerExplanation": "<p>The AWS DMS service can be used to directly migrate the MySQL database to an Amazon RDS Multi-AZ deployment. The entire process can be online and is managed for you. There is no need to perform schema translation between MySQL and RDS (assuming you choose the MySQL RDS engine).</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-31-42-b6a87bc2209574bb6eabad34a39f6577.png\"></p><p><strong>CORRECT: </strong>\"Use the AWS Database Migration Service (DMS) to directly migrate the database to an Amazon RDS MySQL Multi-AZ deployment\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use the AWS Database Migration Service (DMS) to directly migrate the database to an Amazon EC2 MySQL Multi-AZ deployment\" is incorrect as there is no such thing as “multi-AZ” on Amazon EC2 with MySQL, you must use RDS.</p><p><strong>INCORRECT:</strong> \"Create a snapshot of the MySQL database server and use AWS DataSync to migrate the data Amazon S3. Launch a new Amazon RDS MySQL Multi-AZ deployment from the snapshot\" is incorrect. You cannot create a snapshot of a MySQL database server running on-premises.</p><p><strong>INCORRECT:</strong> \"Use the AWS Database Migration Service (DMS) to directly migrate the database to Amazon RDS MySQL. Use the Schema Conversion Tool (SCT) to enable conversion from MySQL to Amazon RDS\" is incorrect. There is no need to convert the schema when migrating from MySQL to Amazon RDS (MySQL engine).</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 12014,
                        "content": "<p>Create a snapshot of the MySQL database server and use AWS DataSync to migrate the data Amazon S3. Launch a new Amazon RDS MySQL Multi-AZ deployment from the snapshot</p>",
                        "isValid": false
                    },
                    {
                        "id": 12015,
                        "content": "<p>Use the AWS Database Migration Service (DMS) to directly migrate the database to Amazon RDS MySQL. Use the Schema Conversion Tool (SCT) to enable conversion from MySQL to Amazon RDS</p>",
                        "isValid": false
                    },
                    {
                        "id": 12016,
                        "content": "<p>Use the AWS Database Migration Service (DMS) to directly migrate the database to an Amazon EC2 MySQL Multi-AZ deployment</p>",
                        "isValid": false
                    },
                    {
                        "id": 12017,
                        "content": "<p>Use the AWS Database Migration Service (DMS) to directly migrate the database to an Amazon RDS MySQL Multi-AZ deployment</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2880,
            "attributes": {
                "createdAt": "2023-09-09T20:40:23.361Z",
                "updatedAt": "2023-09-09T20:40:23.361Z",
                "content": "<p>A Solutions Architect has created an AWS Organization with several AWS accounts. Security policy requires that use of specific API actions are limited across all accounts. The Solutions Architect requires a method of centrally controlling these actions.</p><p>What is the SIMPLEST method of achieving the requirements?</p>",
                "answerExplanation": "<p>Service control policies (SCPs) offer central control over the maximum available permissions for all accounts in your organization allowing you to ensure your accounts stay within your organization’s access control guidelines.</p><p>In the example below, a policy in OU1 restricts all users from launching EC2 instance types other than a t2.micro:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-33-24-3942465af0c21392a215faf713e45486.png\"></p><p><strong>CORRECT: </strong>\"Create a service control policy in the root organizational unit to deny access to the services or actions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a Network ACL that limits access to the services or actions and attach it to all relevant subnets\" is incorrect. Network ACLs control network traffic - not API actions.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy in the root account and attach it to users and groups in each account\" is incorrect. This is not an efficient or centrally managed method of applying the security restrictions.</p><p><strong>INCORRECT:</strong> \"Create cross-account roles in each account to limit access to the services and actions that are allowed\" is incorrect. This is another example of a complex and inefficient method of providing access across accounts and does not restrict API actions within the account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-accounts/\">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-accounts/</a></p>",
                "options": [
                    {
                        "id": 12018,
                        "content": "<p>Create a Network ACL that limits access to the services or actions and attach it to all relevant subnets</p>",
                        "isValid": false
                    },
                    {
                        "id": 12019,
                        "content": "<p>Create an IAM policy in the root account and attach it to users and groups in each account</p>",
                        "isValid": false
                    },
                    {
                        "id": 12020,
                        "content": "<p>Create a service control policy in the root organizational unit to deny access to the services or actions</p>",
                        "isValid": true
                    },
                    {
                        "id": 12021,
                        "content": "<p>Create cross-account roles in each account to limit access to the services and actions that are allowed</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2881,
            "attributes": {
                "createdAt": "2023-09-09T20:40:23.474Z",
                "updatedAt": "2023-09-09T20:40:23.474Z",
                "content": "<p>A company operates multiple AWS accounts under AWS Organizations. To better manage the costs, the company wants to allocate different budgets for each of these accounts. The company also wants to prevent additional resource provisioning in an AWS account if it reaches its allocated budget before the end of the budget period.</p><p>Which combination of solutions will meet these requirements? (Select THREE.)</p>",
                "answerExplanation": "<p>AWS Budgets is a tool that enables you to set custom cost and usage budgets. You can set your budget amount, and AWS provides you with estimated charges and forecasted costs for your AWS usage. Configuring the budgets in the Billing and Cost Management console is a recommended step.</p><p>AWS Budgets can execute budget actions (like preventing additional resource provisioning) using an IAM role with the necessary permissions.</p><p>Configuring alerts in AWS Budgets and linking a budget action to an IAM role for automatic prevention of additional resource provisioning is a correct and efficient way to manage costs.</p><p><strong>CORRECT: </strong>\"Use AWS Budgets to establish different budgets for each AWS account. Configure the budgets in the Billing and Cost Management console\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Set up an IAM role with the necessary permissions that allow AWS Budgets to execute budget actions\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure alerts in AWS Budgets to notify the company when an account is about to reach its budget threshold. Then use a budget action that links to the IAM role to prevent additional resource provisioning\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Budgets in the AWS Management Console to set up budgets and specify the cost threshold for each AWS account\" is incorrect.</p><p>While AWS Budgets can indeed be set up in the AWS Management Console, the budgets aren't set in the context of cost thresholds for each AWS account. This option is not fully accurate.</p><p><strong>INCORRECT:</strong> \"Create an IAM user with adequate permissions to allow AWS Budgets to enforce budget actions\" is incorrect.</p><p>Although you can create an IAM user with necessary permissions, using an IAM role is generally a better practice. An IAM user is an entity that you create in AWS to represent the person or service that uses it to interact with AWS, while an IAM role is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. A role does not have long-term credentials associated with it like an IAM user does.</p><p><strong>INCORRECT:</strong> \"Set up an alert in AWS Budgets to notify the company when a particular account meets its budget threshold. Implement a budget action that utilizes the IAM role to apply a Service Control Policy (SCP) that prohibits further resource provisioning\" is incorrect.</p><p>You can set up an alert in AWS Budgets but using an IAM role to apply a Service Control Policy (SCP) to prohibit resource provisioning is not correct. AWS Budgets doesn't directly enforce SCPs. SCPs are typically used in the context of AWS Organizations to manage permissions across multiple AWS accounts.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-controls.html\">https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-controls.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cost-management/\">https://digitalcloud.training/aws-cost-management/</a></p>",
                "options": [
                    {
                        "id": 12022,
                        "content": "<p>Use AWS Budgets in the AWS Management Console to set up budgets and specify the cost threshold for each AWS account.</p>",
                        "isValid": false
                    },
                    {
                        "id": 12023,
                        "content": "<p>Set up an alert in AWS Budgets to notify the company when a particular account meets its budget threshold. Implement a budget action that utilizes the IAM role to apply a Service Control Policy (SCP) that prohibits further resource provisioning.</p>",
                        "isValid": false
                    },
                    {
                        "id": 12024,
                        "content": "<p>Configure alerts in AWS Budgets to notify the company when an account is about to reach its budget threshold. Then use a budget action that links to the IAM role to prevent additional resource provisioning.</p>",
                        "isValid": true
                    },
                    {
                        "id": 12025,
                        "content": "<p>Set up an IAM role with the necessary permissions that allow AWS Budgets to execute budget actions.</p>",
                        "isValid": true
                    },
                    {
                        "id": 12026,
                        "content": "<p>Use AWS Budgets to establish different budgets for each AWS account. Configure the budgets in the Billing and Cost Management console.</p>",
                        "isValid": true
                    },
                    {
                        "id": 12027,
                        "content": "<p>Create an IAM user with adequate permissions to allow AWS Budgets to enforce budget actions.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2882,
            "attributes": {
                "createdAt": "2023-09-09T20:40:23.614Z",
                "updatedAt": "2023-09-09T20:40:23.614Z",
                "content": "<p>A healthcare company is migrating its patient record system to AWS. The company receives thousands of encrypted patient data files every day through FTP. An on-premises server processes the data files twice a day. However, the processing job takes hours to finish.</p><p>The company wants the AWS solution to process incoming data files as soon as they arrive with minimal changes to the FTP clients that send the files. The solution must delete the incoming data files after the files have been processed successfully. Processing for each file needs to take around 10 minutes.</p><p>Which solution will meet these requirements in the MOST operationally efficient way?</p>",
                "answerExplanation": "<p>AWS Transfer Family provides fully managed support for file transfers directly into and out of Amazon S3 using SFTP. Storing incoming files in S3 Standard offers high durability, availability, and performance object storage for frequently accessed data.</p><p>AWS Lambda can respond immediately to S3 events, which allows processing of files as soon as they arrive. Lambda can also delete the files after processing. This meets all requirements and is operationally efficient, as it requires minimal management and has low costs.</p><p><strong>CORRECT: </strong>\"Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Standard. Create an AWS Lambda function to process the files and to delete the files after they are processed. Use an S3 event notification to invoke the Lambda function when the files arrive\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Glacier. Configure an Amazon EC2 instance to process the files. Use Amazon EventBridge rules to invoke the EC2 instance to process the files twice a day from S3 Glacier. Delete the objects after the job has processed the objects\" is incorrect.</p><p>This option involves using Amazon S3 Glacier, which is primarily used for long-term archival storage. Accessing data for processing could take longer and be more expensive than using S3 Standard. In addition, EC2 instances need to be managed and are less efficient for this scenario compared to AWS Lambda.</p><p><strong>INCORRECT:</strong> \"Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Standard. Use Amazon EC2 instances managed by an Auto Scaling group to process the files. Set an S3 event notification to trigger an AWS Lambda function that launches the EC2 instances when the files arrive. Delete the files after they are processed\" is incorrect.</p><p>While this solution will work, it is less efficient operationally because managing EC2 instances and an Auto Scaling group is more complex and likely more expensive than simply using AWS Lambda for processing.</p><p><strong>INCORRECT:</strong> \"Use an Amazon EC2 instance that runs an SFTP server to store incoming files in Amazon S3 Standard. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to invoke the job to process the files twice a day. Delete the files after the job has processed the files\" is incorrect.</p><p>This option does not meet the requirement of processing incoming data files as soon as they arrive, as EventBridge rules would invoke the job only twice a day. It also involves managing an EC2 instance, which is less operationally efficient than the AWS Transfer Family and AWS Lambda option.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/aws-transfer-family/\">https://aws.amazon.com/aws-transfer-family/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
                "options": [
                    {
                        "id": 12028,
                        "content": "<p>Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Glacier. Configure an Amazon EC2 instance to process the files. Use Amazon EventBridge rules to invoke the EC2 instance to process the files twice a day from S3 Glacier. Delete the objects after the job has processed the objects.</p>",
                        "isValid": false
                    },
                    {
                        "id": 12029,
                        "content": "<p>Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Standard. Use Amazon EC2 instances managed by an Auto Scaling group to process the files. Set an S3 event notification to trigger an AWS Lambda function that launches the EC2 instances when the files arrive. Delete the files after they are processed.</p>",
                        "isValid": false
                    },
                    {
                        "id": 12030,
                        "content": "<p>Use an Amazon EC2 instance that runs an SFTP server to store incoming files in Amazon S3 Standard. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to invoke the job to process the files twice a day. Delete the files after the job has processed the files.</p>",
                        "isValid": false
                    },
                    {
                        "id": 12031,
                        "content": "<p>Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Standard. Create an AWS Lambda function to process the files and to delete the files after they are processed. Use an S3 event notification to invoke the Lambda function when the files arrive.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2883,
            "attributes": {
                "createdAt": "2023-09-09T20:40:23.720Z",
                "updatedAt": "2023-09-09T20:40:23.720Z",
                "content": "<p>A company is deploying a new two-tier web application that uses EC2 web servers and a DynamoDB database backend. An Internet facing ELB distributes connections between the web servers.</p><p>The Solutions Architect has created a security group for the web servers and needs to create a security group for the ELB. What rules should be added? (choose 2)</p>",
                "answerExplanation": "<p>An inbound rule should be created for the relevant protocols (HTTP/HTTPS) and the source should be set to any address (0.0.0.0/0).</p><p>The outbound rule should forward the relevant protocols (HTTP/HTTPS) and the destination should be set to the web server security group.</p><p>Note that on the web server security group you’d want to add an Inbound rule allowing HTTP/HTTPS from the ELB security group.</p><p><strong>CORRECT: </strong>\"Add an Outbound rule that allows HTTP/HTTPS, and specify the destination as the web server security group\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Add an Inbound rule that allows HTTP/HTTPS, and specify the source as 0.0.0.0/0\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Add an Outbound rule that allows ALL TCP, and specify the destination as the Internet Gateway\" is incorrect as the relevant protocol should be specified and the destination should be the web server security group.</p><p><strong>INCORRECT:</strong> \"Add an Outbound rule that allows HTTP/HTTPS, and specify the destination as VPC CIDR\" is incorrect. Using the VPC CIDR would not be secure and you cannot specify an Internet Gateway in a security group (not that you’d want to anyway).</p><p><strong>INCORRECT:</strong> \"Add an Inbound rule that allows HTTP/HTTPS, and specify the source as 0.0.0.0/32\" is incorrect. The address 0.0.0.0/32 is incorrect as the 32 mask means an exact match is required (0.0.0.0).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 12032,
                        "content": "<p>Add an Outbound rule that allows HTTP/HTTPS, and specify the destination as the web server security group&nbsp; &nbsp; &nbsp;</p>",
                        "isValid": true
                    },
                    {
                        "id": 12033,
                        "content": "<p>Add an Inbound rule that allows HTTP/HTTPS, and specify the source as 0.0.0.0/0&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12034,
                        "content": "<p>Add an Outbound rule that allows ALL TCP, and specify the destination as the Internet Gateway&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12035,
                        "content": "<p>Add an Outbound rule that allows HTTP/HTTPS, and specify the destination as VPC CIDR&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12036,
                        "content": "<p>Add an Inbound rule that allows HTTP/HTTPS, and specify the source as 0.0.0.0/32&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2884,
            "attributes": {
                "createdAt": "2023-09-09T20:40:23.821Z",
                "updatedAt": "2023-09-09T20:40:23.821Z",
                "content": "<p>An Amazon EC2 instance is generating very high packets-per-second and performance of the application stack is being impacted. A Solutions Architect needs to determine a resolution to the issue that results in improved performance.</p><p>Which action should the Architect take?</p>",
                "answerExplanation": "<p>Enhanced networking provides higher bandwidth, higher packet-per-second (PPS) performance, and consistently lower inter-instance latencies. If your packets-per-second rate appears to have reached its ceiling, you should consider moving to enhanced networking because you have likely reached the upper thresholds of the VIF driver. It is only available for certain instance types and only supported in VPC. You must also launch an HVM AMI with the appropriate drivers.</p><p>AWS currently supports enhanced networking capabilities using SR-IOV. SR-IOV provides direct access to network adapters, provides higher performance (packets-per-second) and lower latency.</p><p><strong>CORRECT: </strong>\"Use enhanced networking\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure a RAID 1 array from multiple EBS volumes\" is incorrect. You do not need to create a RAID 1 array (which is more for redundancy than performance anyway).</p><p><strong>INCORRECT:</strong> \"Create a placement group and put the EC2 instance in it\" is incorrect. A placement group is used to increase network performance between instances. In this case there is only a single instance so it won’t help.</p><p><strong>INCORRECT:</strong> \"Add multiple Elastic IP addresses to the instance\" is incorrect. Adding multiple IP addresses is not a way to increase performance of the instance as the same amount of bandwidth is available to the Elastic Network Interface (ENI).</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/enable-configure-enhanced-networking/\">https://aws.amazon.com/premiumsupport/knowledge-center/enable-configure-enhanced-networking/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 12037,
                        "content": "<p>Add multiple Elastic IP addresses to the instance&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12038,
                        "content": "<p>Configure a RAID 1 array from multiple EBS volumes&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12039,
                        "content": "<p>Use enhanced networking&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12040,
                        "content": "<p>Create a placement group and put the EC2 instance in it&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2885,
            "attributes": {
                "createdAt": "2023-09-09T20:40:23.946Z",
                "updatedAt": "2023-09-09T20:40:23.946Z",
                "content": "<p>A web application receives order processing information from customers and places the messages on an Amazon SQS queue. A fleet of Amazon EC2 instances are configured to pick up the messages, process them, and store the results in a DynamoDB table. The current configuration has been resulting in a large number of empty responses to <code>ReceiveMessage</code> API requests.</p><p>A Solutions Architect needs to eliminate empty responses to reduce operational overhead. How can this be done?&nbsp; </p>",
                "answerExplanation": "<p>The correct answer is to use Long Polling which will eliminate empty responses by allowing Amazon SQS to wait until a message is available in a queue before sending a response.</p><p>The problem does not relate to the order in which the messages are processed in and there are no concerns over messages being delivered more than once so it doesn’t matter whether you use a FIFO or standard queue.</p><p><strong>Long Polling:</strong></p><p>– Uses fewer requests and reduces cost.</p><p>– Eliminates false empty responses by querying all servers.</p><p>– SQS waits until a message is available in the queue before sending a response.</p><p><strong>Short Polling:</strong></p><p>– Does not wait for messages to appear in the queue.</p><p>– It queries only a subset of the available servers for messages (based on weighted random execution).</p><p>– Short polling is the default.</p><p>– ReceiveMessageWaitTime is set to 0.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-20-24-648b8e1ff1276f787c24226b61455dd3.png\"></p><p><strong>CORRECT: </strong>\"Configure Long Polling to eliminate empty responses by allowing Amazon SQS to wait until a message is available in a queue before sending a response\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a Standard queue to provide at-least-once delivery, which means that each message is delivered at least once\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Use a FIFO (first-in-first-out) queue to preserve the exact order in which messages are sent and received\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Configure Short Polling to eliminate empty responses by reducing the length of time a connection request remains open\" is incorrect as explained above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 12041,
                        "content": "<p>Use a FIFO (first-in-first-out) queue to preserve the exact order in which messages are sent and received&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12042,
                        "content": "<p>Configure Long Polling to eliminate empty responses by allowing Amazon SQS to wait until a message is available in a queue before sending a response&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12043,
                        "content": "<p>Use a Standard queue to provide at-least-once delivery, which means that each message is delivered at least once&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12044,
                        "content": "<p>Configure Short Polling to eliminate empty responses by reducing the length of time a connection request remains open&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2886,
            "attributes": {
                "createdAt": "2023-09-09T20:40:24.087Z",
                "updatedAt": "2023-09-09T20:40:24.087Z",
                "content": "<p>An on-premise data center will be connected to an Amazon VPC by a hardware VPN that has public and VPN-only subnets. The security team has requested that traffic hitting public subnets on AWS that’s destined to on-premise applications must be directed over the VPN to the corporate firewall.</p><p>How can this be achieved?</p>",
                "answerExplanation": "<p>Route tables determine where network traffic is directed. In your route table, you must add a route for your remote network and specify the virtual private gateway as the target. This enables traffic from your VPC that’s destined for your remote network to route via the virtual private gateway and over one of the VPN tunnels. You can enable route propagation for your route table to automatically propagate your network routes to the table for you.</p><p><strong>CORRECT: </strong>\"In the public subnet route table, add a route for your remote network and specify the virtual private gateway as the target\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"In the VPN-only subnet route table, add a route that directs all Internet traffic to the virtual private gateway\" is incorrect. You must create the route table rule in the route table attached to the public subnet, not the VPN-only subnet.</p><p><strong>INCORRECT:</strong> \"In the public subnet route table, add a route for your remote network and specify the customer gateway as the target\" is incorrect. You must select the virtual private gateway (AWS side of the VPN) not the customer gateway (customer side of the VPN) in the target in the route table.</p><p><strong>INCORRECT:</strong> \"Configure a NAT Gateway and configure all traffic to be directed via the virtual private gateway\" is incorrect. NAT Gateways are used to enable Internet access for EC2 instances in private subnets, they cannot be used to direct traffic to VPG.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_VPN.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_VPN.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario3.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario3.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 12045,
                        "content": "<p>In the VPN-only subnet route table, add a route that directs all Internet traffic to the virtual private gateway&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12046,
                        "content": "<p>Configure a NAT Gateway and configure all traffic to be directed via the virtual private gateway&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12047,
                        "content": "<p>In the public subnet route table, add a route for your remote network and specify the virtual private gateway as the target&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12048,
                        "content": "<p>In the public subnet route table, add a route for your remote network and specify the customer gateway as the target</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2887,
            "attributes": {
                "createdAt": "2023-09-09T20:40:24.187Z",
                "updatedAt": "2023-09-09T20:40:24.187Z",
                "content": "<p>A data analytics company is building a high-performance application that requires concurrent writes to a shared block storage volume from multiple Amazon EC2 instances.</p><p>The EC2 instances are Nitro-based and reside within the same Availability Zone. The company needs a storage solution that supports simultaneous connections to facilitate data resilience and high availability.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>io2 volumes are designed for I/O-intensive workloads, particularly database workloads, that require high performance and low latency. io1 and io2 volumes support Multi-Attach, which enables you to attach a single volume to multiple EC2 instances in the same Availability Zone.</p><p><strong>CORRECT: </strong>\"Use Provisioned IOPS SSD (io2) EBS volumes with Amazon EBS Multi-Attach\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon EFS with NFSv4.1 protocol across multiple EC2 instances\" is incorrect.</p><p>Amazon Elastic File System (EFS) is a scalable file storage for use with Amazon EC2. You can use an Amazon EFS file system as a common data source for workloads and applications running on multiple instances, but it does not provide the block-level storage required for high IOPS operations.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 with S3 Transfer Acceleration to enhance speed\" is incorrect.</p><p>Amazon S3 is an object storage service. While S3 Transfer Acceleration does enhance the speed of in-transit file transfers, it is not a block storage solution, it is an object storage solution and is not suitable for this use case.</p><p><strong>INCORRECT:</strong> \"Use General Purpose SSD (gp2) EBS volumes with Amazon EBS Multi-Attach\" is incorrect.</p><p>Amazon EBS Multi-Attach only supports io1 and io2 volumes, and it is not supported on gp2 volumes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 12049,
                        "content": "<p>Use Amazon EFS with NFSv4.1 protocol across multiple EC2 instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 12050,
                        "content": "<p>Use General Purpose SSD (gp2) EBS volumes with Amazon EBS Multi-Attach.</p>",
                        "isValid": false
                    },
                    {
                        "id": 12051,
                        "content": "<p>Use Provisioned IOPS SSD (io2) EBS volumes with Amazon EBS Multi-Attach.</p>",
                        "isValid": true
                    },
                    {
                        "id": 12052,
                        "content": "<p>Use Amazon S3 with S3 Transfer Acceleration to enhance speed.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2888,
            "attributes": {
                "createdAt": "2023-09-09T20:40:24.298Z",
                "updatedAt": "2023-09-09T20:40:24.298Z",
                "content": "<p>A company runs an application on premises that stores a large quantity of semi-structured data using key-value pairs. The application code will be migrated to AWS Lambda and a highly scalable solution is required for storing the data.</p><p>Which datastore will be the best fit for these requirements?</p>",
                "answerExplanation": "<p>Amazon DynamoDB is a no-SQL database that stores data using key-value pairs. It is ideal for storing large amounts of semi-structured data and is also highly scalable. This is the best solution for storing this data based on the requirements in the scenario.</p><p><strong>CORRECT: </strong>\"Amazon DynamoDB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EFS\" is incorrect. The Amazon Elastic File System (EFS) is not suitable for storing key-value pairs.</p><p><strong>INCORRECT:</strong> \"Amazon RDS MySQL\" is incorrect. Amazon Relational Database Service (RDS) is used for structured data as it is an SQL type of database.</p><p><strong>INCORRECT:</strong> \"Amazon EBS\" is incorrect. Amazon Elastic Block Store (EBS) is a block-based storage system. You attach volumes to EC2 instances. It is not used for key-value pairs or to be used by Lambda functions.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/features/\">https://aws.amazon.com/dynamodb/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 12053,
                        "content": "<p>Amazon RDS MySQL</p>",
                        "isValid": false
                    },
                    {
                        "id": 12054,
                        "content": "<p>Amazon EFS</p>",
                        "isValid": false
                    },
                    {
                        "id": 12055,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 12056,
                        "content": "<p>Amazon EBS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2889,
            "attributes": {
                "createdAt": "2023-09-09T20:40:24.433Z",
                "updatedAt": "2023-09-09T20:40:24.433Z",
                "content": "<p>A security officer has requested that all data associated with a specific customer is encrypted. The data resides on Elastic Block Store (EBS) volumes. Which of the following statements about using EBS encryption are correct? (choose 2)&nbsp; &nbsp; &nbsp;</p>",
                "answerExplanation": "<p>All EBS types and all instance <em>families</em> support encryption but not all instance <em>types</em> support encryption. There is no direct way to change the encryption state of a volume. Data in transit between an instance and an encrypted volume is also encrypted.</p><p><strong>CORRECT: </strong>\"Data in transit between an instance and an encrypted volume is also encrypted\" is the correct answer.</p><p><strong>CORRECT: </strong>\"There is no direct way to change the encryption state of a volume\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Not all EBS types support encryption\" is incorrect as all EBS volume types support encryption.</p><p><strong>INCORRECT:</strong> \"All attached EBS volumes must share the same encryption state\" is incorrect. You can have encrypted and non-encrypted EBS volumes on a single instance.</p><p><strong>INCORRECT:</strong> \"All instance types support encryption\" is incorrect. All instance families support encryption, but not all instance types.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 12057,
                        "content": "<p>There is no direct way to change the encryption state of a volume&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12058,
                        "content": "<p>All attached EBS volumes must share the same encryption state&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12059,
                        "content": "<p>Data in transit between an instance and an encrypted volume is also encrypted&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12060,
                        "content": "<p>Not all EBS types support encryption&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12061,
                        "content": "<p>All instance types support encryption&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2890,
            "attributes": {
                "createdAt": "2023-09-09T20:40:24.537Z",
                "updatedAt": "2023-09-09T20:40:24.537Z",
                "content": "<p>A Solutions Architect is launching an Amazon EC2 instance with multiple attached volumes by modifying the block device mapping. Which block device can be specified in a block device mapping to be used with an EC2 instance? (choose 2)</p>",
                "answerExplanation": "<p>Each instance that you launch has an associated root device volume, either an Amazon EBS volume or an instance store volume.</p><p>You can use block device mapping to specify additional EBS volumes or instance store volumes to attach to an instance when it’s launched. You can also attach additional EBS volumes to a running instance.</p><p>You cannot use a block device mapping to specify a snapshot, EFS volume or S3 bucket.</p><p><strong>CORRECT: </strong>\"EBS volume\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Instance store volume\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"EFS volume\" is incorrect as described above.</p><p><strong>INCORRECT:</strong> \"Snapshot\" is incorrect as described above.</p><p><strong>INCORRECT:</strong> \"S3 bucket\" is incorrect as described above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/block-device-mapping-concepts.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/block-device-mapping-concepts.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 12062,
                        "content": "<p>EFS volume&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12063,
                        "content": "<p>EBS volume&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12064,
                        "content": "<p>S3 bucket&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12065,
                        "content": "<p>Snapshot&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12066,
                        "content": "<p>Instance store volume&nbsp; &nbsp; </p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2891,
            "attributes": {
                "createdAt": "2023-09-09T20:40:24.651Z",
                "updatedAt": "2023-09-09T20:40:24.651Z",
                "content": "<p>An application runs on EC2 instances in a private subnet behind an Application Load Balancer in a public subnet. The application is highly available and distributed across multiple AZs. The EC2 instances must make API calls to an internet-based service. How can the Solutions Architect enable highly available internet connectivity?</p>",
                "answerExplanation": "<p>The only solution presented that actually works is to create a NAT gateway in the public subnet of each AZ. They must be created in the public subnet as they gain public IP addresses and use an internet gateway for internet access.</p><p>The route tables in the private subnets must then be configured with a route to the NAT gateway and then the EC2 instances will be able to access the internet (subject to security group configuration).</p><p><strong>CORRECT: </strong>\"Create a NAT gateway in the public subnet of each AZ. Update the route tables for each private subnet to direct internet-bound traffic to the NAT gateway\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a NAT gateway and attach it to the VPC. Add a route to the gateway to each private subnet route table\" is incorrect. You do not attach NAT gateways to VPCs, you add them to public subnets.</p><p><strong>INCORRECT:</strong> \"Configure an internet gateway. Add a route to the gateway to each private subnet route table\" is incorrect. You cannot add a route to an internet gateway to a private subnet route table (private EC2 instances don’t even have public IP addresses).</p><p><strong>INCORRECT:</strong> \"Create a NAT instance in the private subnet of each AZ. Update the route tables for each private subnet to direct internet-bound traffic to the NAT instance\" is incorrect. You do not create NAT instances in private subnets, they must be created in public subnets.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 12067,
                        "content": "<p>Create a NAT instance in the private subnet of each AZ. Update the route tables for each private subnet to direct internet-bound traffic to the NAT instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 12068,
                        "content": "<p>Create a NAT gateway and attach it to the VPC. Add a route to the gateway to each private subnet route table</p>",
                        "isValid": false
                    },
                    {
                        "id": 12069,
                        "content": "<p>Create a NAT gateway in the public subnet of each AZ. Update the route tables for each private subnet to direct internet-bound traffic to the NAT gateway</p>",
                        "isValid": true
                    },
                    {
                        "id": 12070,
                        "content": "<p>Configure an internet gateway. Add a route to the gateway to each private subnet route table</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2892,
            "attributes": {
                "createdAt": "2023-09-09T20:40:24.764Z",
                "updatedAt": "2023-09-09T20:40:24.764Z",
                "content": "<p>A Solutions Architect has created a new Network ACL in an Amazon VPC. No rules have been created. Which of the statements below are correct regarding the default state of the Network ACL? (choose 2)</p>",
                "answerExplanation": "<p>A VPC automatically comes with a default network ACL which allows all inbound/outbound traffic. A custom NACL denies all traffic both inbound and outbound by default.</p><p>Network ACL’s function at the subnet level and you can have permit and deny rules. Network ACLs have separate inbound and outbound rules and each rule can allow or deny traffic.</p><p>Network ACLs are stateless so responses are subject to the rules for the direction of traffic. NACLs only apply to traffic that is ingress or egress to the subnet not to traffic within the subnet.</p><p><strong>CORRECT: </strong>\"There is a default inbound rule denying all traffic\" is a correct answer.</p><p><strong>CORRECT: </strong>\"There is a default outbound rule denying all traffic\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"There is a default inbound rule allowing traffic from the VPC CIDR block\" is incorrect as inbound traffic is not allowed from anywhere by default.</p><p><strong>INCORRECT:</strong> \"There is a default outbound rule allowing traffic to the Internet Gateway\" is incorrect as outbound traffic is not allowed to anywhere by default.</p><p><strong>INCORRECT:</strong> \"There is a default outbound rule allowing all traffic\" is incorrect as all traffic is denied.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 12071,
                        "content": "<p>There is a default inbound rule denying all traffic&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12072,
                        "content": "<p>There is a default outbound rule allowing all traffic&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12073,
                        "content": "<p>There is a default outbound rule allowing traffic to the Internet Gateway&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12074,
                        "content": "<p>There is a default outbound rule denying all traffic&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12075,
                        "content": "<p>There is a default inbound rule allowing traffic from the VPC CIDR block&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2893,
            "attributes": {
                "createdAt": "2023-09-09T20:40:24.887Z",
                "updatedAt": "2023-09-09T20:40:24.887Z",
                "content": "<p>One of the departments in a company has been generating a large amount of data on Amazon S3 and costs are increasing. Data older than 90 days is rarely accessed but must be retained for several years. If this data does need to be accessed at least 24 hours notice is provided.</p><p>How can a Solutions Architect optimize the costs associated with storage of this data whilst ensuring it is accessible if required?</p>",
                "answerExplanation": "<p>To manage your objects so that they are stored cost effectively throughout their lifecycle, configure their lifecycle. A lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. Transition actions define when objects transition to another storage class.</p><p>For example, you might choose to transition objects to the STANDARD_IA storage class 30 days after you created them, or archive objects to the GLACIER storage class one year after creating them.</p><p>GLACIER retrieval times:</p><p>- Standard retrieval is 3-5 hours which is well within the requirements here.</p><p>- You can use Expedited retrievals to access data in 1 – 5 minutes.</p><p>- You can use Bulk retrievals to access up to petabytes of data in approximately 5 – 12 hours.</p><p><strong>CORRECT: </strong>\"Use S3 lifecycle policies to move data to GLACIER after 90 days\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement archival software that automatically moves the data to tape\" is incorrect as this solution can be fully automated using lifecycle policies.</p><p><strong>INCORRECT:</strong> \"Use S3 lifecycle policies to move data to the STANDARD_IA storage class\" is incorrect. STANDARD_IA is good for infrequently accessed data and provides faster access times than GLACIER but is more expensive so not the best option here.</p><p><strong>INCORRECT:</strong> \"Select the older data and manually migrate it to GLACIER\" is incorrect as a lifecycle policy can automate the process.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2016/11/access-your-amazon-glacier-data-in-minutes-with-new-retrieval-options/\">https://aws.amazon.com/about-aws/whats-new/2016/11/access-your-amazon-glacier-data-in-minutes-with-new-retrieval-options/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 12076,
                        "content": "<p>Use S3 lifecycle policies to move data to GLACIER after 90 days&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12077,
                        "content": "<p>Implement archival software that automatically moves the data to tape&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12078,
                        "content": "<p>Select the older data and manually migrate it to GLACIER&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12079,
                        "content": "<p>Use S3 lifecycle policies to move data to the STANDARD_IA storage class&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2894,
            "attributes": {
                "createdAt": "2023-09-09T20:40:25.023Z",
                "updatedAt": "2023-09-09T20:40:25.023Z",
                "content": "<p>A company runs a streaming media service and the content is stored on Amazon S3. The media catalog server pulls updated content from S3 and can issue over 1 million read operations per second for short periods. Latency must be kept under 5ms for these updates. Which solution will provide the BEST performance for the media catalog updates?</p>",
                "answerExplanation": "<p>Some applications, such as media catalog updates require high frequency reads, and consistent throughput. For such applications, customers often complement S3 with an in-memory cache, such as Amazon ElastiCache for Redis, to reduce the S3 retrieval cost and to improve performance.</p><p>ElastiCache for Redis is a fully managed, in-memory data store that provides sub-millisecond latency performance with high throughput. ElastiCache for Redis complements S3 in the following ways:</p><p>- Redis stores data in-memory, so it provides sub-millisecond latency and supports incredibly high requests per second.</p><p>- It supports key/value based operations that map well to S3 operations (for example, GET/SET =&gt; GET/PUT), making it easy to write code for both S3 and ElastiCache.</p><p>- It can be implemented as an application side cache. This allows you to use S3 as your persistent store and benefit from its durability, availability, and low cost. Your applications decide what objects to cache, when to cache them, and how to cache them.</p><p>In this example the media catalog is pulling updates from S3 so the performance between these components is what needs to be improved. Therefore, using ElastiCache to cache the content will dramatically increase the performance.</p><p><strong>CORRECT: </strong>\"Update the application code to use an Amazon ElastiCache for Redis cluster\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement Amazon CloudFront and cache the content at Edge Locations\" is incorrect. CloudFront is good for getting media closer to users but in this case we’re trying to improve performance within the data center moving data from S3 to the media catalog server.</p><p><strong>INCORRECT:</strong> \"Update the application code to use an Amazon DynamoDB Accelerator cluster\" is incorrect. DynamoDB Accelerator (DAX) is used with DynamoDB but is unsuitable for use with Amazon S3.</p><p><strong>INCORRECT:</strong> \"Implement an Instance store volume on the media catalog server\" is incorrect. This will improve local disk performance but will not improve reads from Amazon S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/storage/turbocharge-amazon-s3-with-amazon-elasticache-for-redis/\">https://aws.amazon.com/blogs/storage/turbocharge-amazon-s3-with-amazon-elasticache-for-redis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 12080,
                        "content": "<p>Implement Amazon CloudFront and cache the content at Edge Locations</p>",
                        "isValid": false
                    },
                    {
                        "id": 12081,
                        "content": "<p>Update the application code to use an Amazon DynamoDB Accelerator cluster</p>",
                        "isValid": false
                    },
                    {
                        "id": 12082,
                        "content": "<p>Implement an Instance store volume on the media catalog server</p>",
                        "isValid": false
                    },
                    {
                        "id": 12083,
                        "content": "<p>Update the application code to use an Amazon ElastiCache for Redis cluster</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2895,
            "attributes": {
                "createdAt": "2023-09-09T20:40:25.127Z",
                "updatedAt": "2023-09-09T20:40:25.127Z",
                "content": "<p>An application uses an Amazon RDS database and Amazon EC2 instances in a web tier. The web tier instances must not be directly accessible from the internet to improve security.</p><p>How can a Solutions Architect meet these requirements?</p>",
                "answerExplanation": "<p>To prevent direct connectivity to the EC2 instances from the internet you can deploy your EC2 instances in a private subnet and have the ELB in a public subnet. To configure this you must enable a public subnet in the ELB that is in the same AZ as the private subnet.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-29-50-1c44f7977941f91bda9d48cd258911c6.png\"></p><p><strong>CORRECT: </strong>\"Launch the EC2 instances in a private subnet and create an Application Load Balancer in a public subnet\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instances in a private subnet with a NAT gateway and update the route table\" is incorrect. This configuration will not allow the application to be accessible from the internet, the aim is to only prevent direct access to the EC2 instances.</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instances in a public subnet and use AWS WAF to protect the instances from internet-based attacks\" is incorrect. With the EC2 instances in a public subnet, direct access from the internet is possible. It only takes a security group misconfiguration or software exploit and the instance becomes vulnerable to attack.</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instances in a public subnet and create an Application Load Balancer in a public subnet\" is incorrect. The EC2 instances should be launched in a private subnet.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/\">https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 12084,
                        "content": "<p>Launch the EC2 instances in a public subnet and create an Application Load Balancer in a public subnet</p>",
                        "isValid": false
                    },
                    {
                        "id": 12085,
                        "content": "<p>Launch the EC2 instances in a public subnet and use AWS WAF to protect the instances from internet-based attacks</p>",
                        "isValid": false
                    },
                    {
                        "id": 12086,
                        "content": "<p>Launch the EC2 instances in a private subnet and create an Application Load Balancer in a public subnet</p>",
                        "isValid": true
                    },
                    {
                        "id": 12087,
                        "content": "<p>Launch the EC2 instances in a private subnet with a NAT gateway and update the route table</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2896,
            "attributes": {
                "createdAt": "2023-09-09T20:40:25.251Z",
                "updatedAt": "2023-09-09T20:40:25.251Z",
                "content": "<p>A tool needs to analyze data stored in an Amazon S3 bucket. Processing the data takes a few seconds and results are then written to another S3 bucket. Less than 256 MB of memory is needed to run the process. What would be the MOST cost-effective compute solutions for this use case?</p>",
                "answerExplanation": "<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Lambda has a maximum execution time of 900 seconds and memory can be allocated up to 3008 MB. Therefore, the most cost-effective solution will be AWS Lambda.</p><p><strong>CORRECT: </strong>\"AWS Lambda functions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Fargate tasks\" is incorrect. Fargate runs Docker containers and is serverless. However, you do pay for the running time of the tasks so it will not be as cost-effective.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 spot instances\" is incorrect. EC2 instances must run continually waiting for jobs to process so even with spot this would be less cost-effective (and subject to termination).</p><p><strong>INCORRECT:</strong> \"Amazon Elastic Beanstalk\" is incorrect. This services also relies on Amazon EC2 instances so would not be as cost-effective.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 12088,
                        "content": "<p>Amazon EC2 spot instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 12089,
                        "content": "<p>AWS Lambda functions</p>",
                        "isValid": true
                    },
                    {
                        "id": 12090,
                        "content": "<p>Amazon Elastic Beanstalk</p>",
                        "isValid": false
                    },
                    {
                        "id": 12091,
                        "content": "<p>AWS Fargate tasks</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2897,
            "attributes": {
                "createdAt": "2023-09-09T20:40:25.369Z",
                "updatedAt": "2023-09-09T20:40:25.369Z",
                "content": "<p>A company requires an Elastic Load Balancer (ELB) for an application they are planning to deploy on AWS. The application requires extremely high throughput and extremely low latencies. The connections will be made using the TCP protocol and the ELB must support load balancing to multiple ports on an instance. Which ELB would should the company use?</p>",
                "answerExplanation": "<p>The Network Load Balancer operates at the connection level (Layer 4), routing connections to targets – Amazon EC2 instances, containers and IP addresses based on IP protocol data. It is architected to handle millions of requests/sec, sudden volatile traffic patterns and provides extremely low latencies.</p><p>The NLB provides high throughput and extremely low latencies and is designed to handle traffic as it grows and can load balance millions of requests/second. NLB also supports load balancing to multiple ports on an instance.</p><p><strong>CORRECT: </strong>\"Network Load Balancer\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Classic Load Balancer\" is incorrect. The CLB operates using the TCP, SSL, HTTP and HTTPS protocols. It is not the best choice for requirements of extremely high throughput and low latency and does not support load balancing to multiple ports on an instance.</p><p><strong>INCORRECT:</strong> \"Application Load Balancer\" is incorrect. The ALB operates at the HTTP and HTTPS level only (does not support TCP load balancing).</p><p><strong>INCORRECT:</strong> \"Route 53\" is incorrect. Route 53 is a DNS service, it is not a type of ELB (though you can do some types of load balancing with it).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 12092,
                        "content": "<p>Route 53&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12093,
                        "content": "<p>Classic Load Balancer&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12094,
                        "content": "<p>Network Load Balancer&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12095,
                        "content": "<p>Application Load Balancer&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2898,
            "attributes": {
                "createdAt": "2023-09-09T20:40:25.569Z",
                "updatedAt": "2023-09-09T20:40:25.569Z",
                "content": "<p>A Solutions Architect has created an AWS account and selected the Asia Pacific (Sydney) region. Within the default VPC there is a default security group. What settings are configured within this security group by default? (choose 2)</p>",
                "answerExplanation": "<p>Default security groups have inbound allow rules (allowing traffic from within the group) whereas custom security groups do not have inbound allow rules (all inbound traffic is denied by default). All outbound traffic is allowed by default in custom and default security groups.</p><p><strong>CORRECT: </strong>\"There is an inbound rule that allows all traffic from the security group itself\" is a correct answer.</p><p><strong>CORRECT: </strong>\"There is an outbound rule that allows all traffic to all addresses\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"There is an inbound rule that allows all traffic from any address\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"There is an outbound rule that allows all traffic to the security group itself\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"There is an outbound rule that allows traffic to the VPC router\" is incorrect as explained above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 12096,
                        "content": "<p>There is an outbound rule that allows traffic to the VPC router&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12097,
                        "content": "<p>There is an outbound rule that allows all traffic to the security group itself&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12098,
                        "content": "<p>There is an inbound rule that allows all traffic from any address&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12099,
                        "content": "<p>There is an outbound rule that allows all traffic to all addresses&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12100,
                        "content": "<p>There is an inbound rule that allows all traffic from the security group itself&nbsp; &nbsp; </p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2899,
            "attributes": {
                "createdAt": "2023-09-09T20:40:25.670Z",
                "updatedAt": "2023-09-09T20:40:25.670Z",
                "content": "<p>A company runs a web-based application that uses Amazon EC2 instances for the web front-end and Amazon RDS for the database back-end. The web application writes transaction log files to an Amazon S3 bucket and the quantity of files is becoming quite large. It is acceptable to retain the most recent 60 days of log files and permanently delete the rest.</p><p>Which action can a Solutions Architect take to enable this to happen automatically?</p>",
                "answerExplanation": "<p>To manage your objects so that they are stored cost effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:</p><p>- Transition actions—Define when objects transition to another <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html\">storage class</a>. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them.</p><p>- Expiration actions—Define when objects expire. Amazon S3 deletes expired objects on your behalf.</p><p><strong>CORRECT: </strong>\"Use an S3 lifecycle policy with object expiration configured to automatically remove objects that are more than 60 days old\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Write a Ruby script that checks the age of objects and deletes any that are more than 60 days old\" is incorrect as the automated method is to use object expiration.</p><p><strong>INCORRECT:</strong> \"Use an S3 bucket policy that deletes objects that are more than 60 days old\" is incorrect as you cannot do this with bucket policies.</p><p><strong>INCORRECT:</strong> \"Use an S3 lifecycle policy to move the log files that are more than 60 days old to the GLACIER storage class\" is incorrect. Moving logs to Glacier may save cost but the question requests that the files are permanently deleted.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 12101,
                        "content": "<p>Use an S3 lifecycle policy to move the log files that are more than 60 days old to the GLACIER storage class&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12102,
                        "content": "<p>Write a Ruby script that checks the age of objects and deletes any that are more than 60 days old&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12103,
                        "content": "<p>Use an S3 lifecycle policy with object expiration configured to automatically remove objects that are more than 60 days old&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12104,
                        "content": "<p>Use an S3 bucket policy that deletes objects that are more than 60 days old&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2900,
            "attributes": {
                "createdAt": "2023-09-09T20:40:25.766Z",
                "updatedAt": "2023-09-09T20:40:25.766Z",
                "content": "<p>A large quantity of data that is rarely accessed is being archived onto Amazon Glacier. Your CIO wants to understand the resilience of the service. Which of the statements below is correct about Amazon Glacier storage? &nbsp;(choose 2)&nbsp; </p>",
                "answerExplanation": "<p>Glacier is designed for durability of 99.999999999% of objects across multiple Availability Zones. Data is resilient in the event of one entire Availability Zone destruction. Glacier supports SSL for data in transit and encryption of data at rest. Glacier is extremely low cost and is ideal for long-term archival.</p><p><strong>CORRECT: </strong>\"Provides 99.999999999% durability of archives\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Data is resilient in the event of one entire Availability Zone destruction\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Data is replicated globally\" is incorrect. Data is not replicated globally.</p><p><strong>INCORRECT:</strong> \"Data is resilient in the event of one entire region destruction\" is incorrect. Data is not resilient to the failure of an entire region.</p><p><strong>INCORRECT:</strong> \"Provides 99.9% availability of archives\" is incorrect. Glacier is “designed for” availability of <strong>99.99%</strong></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 12105,
                        "content": "<p>Data is replicated globally&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12106,
                        "content": "<p>Provides 99.9% availability of archives&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12107,
                        "content": "<p>Provides 99.999999999% durability of archives&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12108,
                        "content": "<p>Data is resilient in the event of one entire region destruction&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12109,
                        "content": "<p>Data is resilient in the event of one entire Availability Zone destruction&nbsp; &nbsp; </p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 2901,
            "attributes": {
                "createdAt": "2023-09-09T20:40:25.887Z",
                "updatedAt": "2023-09-09T20:40:25.887Z",
                "content": "<p>An Amazon EBS-backed EC2 instance has been launched. A requirement has come up for some high-performance ephemeral storage.</p><p>How can a Solutions Architect add a new instance store volume?</p>",
                "answerExplanation": "<p>You can specify the instance store volumes for your instance only when you launch an instance. You can’t attach instance store volumes to an instance after you’ve launched it.</p><p><strong>CORRECT: </strong>\"You can specify the instance store volumes for your instance only when you launch an instance\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"You must shutdown the instance in order to be able to add the instance store volume\" is incorrect. You can use a block device mapping to specify additional EBS volumes when you launch your instance, or you can attach additional EBS volumes after your instance is running.</p><p><strong>INCORRECT:</strong> \"You must use an Elastic Network Adapter (ENA) to add instance store volumes. First, attach an ENA, and then attach the instance store volume\" is incorrect. An Elastic Network Adapter has nothing to do with adding instance store volumes.</p><p><strong>INCORRECT:</strong> \"You can use a block device mapping to specify additional instance store volumes when you launch your instance, or you can attach additional instance store volumes after your instance is running\" is incorrect. You can’t attach instance store volumes to an instance after you’ve launched it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/add-instance-store-volumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/add-instance-store-volumes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 12110,
                        "content": "<p>You must shutdown the instance in order to be able to add the instance store volume&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12111,
                        "content": "<p>You can specify the instance store volumes for your instance only when you launch an instance&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12112,
                        "content": "<p>You must use an Elastic Network Adapter (ENA) to add instance store volumes. First, attach an ENA, and then attach the instance store volume&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12113,
                        "content": "<p>You can use a block device mapping to specify additional instance store volumes when you launch your instance, or you can attach additional instance store volumes after your instance is running&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2902,
            "attributes": {
                "createdAt": "2023-09-09T20:40:25.987Z",
                "updatedAt": "2023-09-09T20:40:25.987Z",
                "content": "<p>An organization in the agriculture sector is deploying sensors and smart devices around factory plants and fields. The devices will collect information and send it to cloud applications running on AWS. </p><p>Which AWS service will securely connect the devices to the cloud applications?</p>",
                "answerExplanation": "<p>AWS IoT Core is a managed cloud service that lets connected devices easily and securely interact with cloud applications and other devices. AWS IoT Core can support billions of devices and trillions of messages, and can process and route those messages to AWS endpoints and to other devices reliably and securely.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-24-14-48d1a8665d7d70aa01818799e7168c7e.png\"><p><strong>CORRECT: </strong>\"AWS IoT Core\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Glue\" is incorrect. AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics.</p><p><strong>INCORRECT:</strong> \"AWS DMS\" is incorrect. AWS Database Migration Service helps you migrate databases to AWS quickly and securely.</p><p><strong>INCORRECT:</strong> \"AWS Lambda\" is incorrect. AWS Lambda lets you run code without provisioning or managing servers.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/iot-core/\">https://aws.amazon.com/iot-core/</a></p>",
                "options": [
                    {
                        "id": 12114,
                        "content": "<p>AWS Glue&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12115,
                        "content": "<p>AWS DMS&nbsp; &nbsp; </p>",
                        "isValid": false
                    },
                    {
                        "id": 12116,
                        "content": "<p>AWS IoT Core&nbsp; &nbsp; </p>",
                        "isValid": true
                    },
                    {
                        "id": 12117,
                        "content": "<p>AWS Lambda&nbsp; &nbsp; </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 2903,
            "attributes": {
                "createdAt": "2023-09-09T20:40:26.081Z",
                "updatedAt": "2023-09-09T20:40:26.081Z",
                "content": "<p>An international software firm provides its clients with custom solutions and tools designed for efficient data collection and analysis on AWS. The firm intends to centrally manage and distribute a standard set of solutions and tools for its clients' self-service needs.</p><p>Which solution would best satisfy these requirements?</p>",
                "answerExplanation": "<p>AWS Service Catalog enables organizations to create and manage catalogs of IT services that are approved for use on AWS. It allows centrally managed service portfolios, which clients can use on a self-service basis.</p><p>AWS Service Catalog provides a single location where organizations can centrally manage catalogs of IT services, which simplifies the organizational process and helps ensure compliance.</p><p><strong>CORRECT: </strong>\"Create AWS Service Catalog portfolios for the clients\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create AWS CloudFormation stacks for the clients\" is incorrect.</p><p>While AWS CloudFormation is a powerful service for infrastructure as code (IaC), it doesn't provide a straightforward way for clients to discover and use shared tools or solutions for self-service needs. It lacks the management features and access control mechanisms necessary for this scenario.</p><p><strong>INCORRECT:</strong> \"Create AWS Systems Manager documents for the clients\" is incorrect.</p><p>AWS Systems Manager documents define the actions that Systems Manager performs on your managed instances. Although Systems Manager allows the central management of resources and applications, it doesn't provide an effective means for clients to self-discover and use shared tools or solutions.</p><p><strong>INCORRECT:</strong> \"Create AWS Config rules for the clients\" is incorrect.</p><p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It isn't designed to centrally manage and distribute software tools or solutions.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/servicecatalog/\">https://aws.amazon.com/servicecatalog/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-service-catalog/\">https://digitalcloud.training/aws-service-catalog/</a></p>",
                "options": [
                    {
                        "id": 12118,
                        "content": "<p>Create AWS Service Catalog portfolios for the clients.</p>",
                        "isValid": true
                    },
                    {
                        "id": 12119,
                        "content": "<p>Create AWS CloudFormation stacks for the clients.</p>",
                        "isValid": false
                    },
                    {
                        "id": 12120,
                        "content": "<p>Create AWS Systems Manager documents for the clients.</p>",
                        "isValid": false
                    },
                    {
                        "id": 12121,
                        "content": "<p>Create AWS Config rules for the clients.</p>",
                        "isValid": false
                    }
                ]
            }
        }
    ],
    "meta": {
        "pagination": {
            "page": 1,
            "pageSize": 1000,
            "pageCount": 1,
            "total": 777
        },
        "name": "Solution Architect"
    }
}