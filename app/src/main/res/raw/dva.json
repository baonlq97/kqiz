{
    "data": [
        {
            "id": 263,
            "attributes": {
                "createdAt": "2023-09-07T08:39:25.339Z",
                "updatedAt": "2023-09-07T08:39:25.339Z",
                "content": "<p>Which of the following security credentials can only be created by the AWS Account root user? </p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>For Amazon CloudFront, you use key pairs to create signed URLs for private content, such as when you want to distribute restricted content that someone paid for.</p>\n\n<p><strong>CloudFront Key Pairs</strong> - IAM users can't create CloudFront key pairs. You must log in using root credentials to create key pairs.</p>\n\n<p>To create signed URLs or signed cookies, you need a signer. A signer is either a trusted key group that you create in CloudFront, or an AWS account that contains a CloudFront key pair. AWS recommends that you use trusted key groups with signed URLs and signed cookies instead of using CloudFront key pairs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EC2 Instance Key Pairs</strong> - You use key pairs to access Amazon EC2 instances, such as when you use SSH to log in to a Linux instance. These key pairs can be created from the IAM user login and do not need root user access.</p>\n\n<p><strong>IAM User Access Keys</strong> - Access keys consist of two parts: an access key ID and a secret access key. You use access keys to sign programmatic requests that you make to AWS if you use AWS CLI commands (using the SDKs) or using AWS API operations. IAM users can create their own Access Keys, does not need root access.</p>\n\n<p><strong>IAM User passwords</strong> - Every IAM user has access to his own credentials and can reset the password whenever they need to.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html</a></p>\n",
                "options": [
                    {
                        "id": 1071,
                        "content": "<p>IAM User Access Keys</p>",
                        "isValid": false
                    },
                    {
                        "id": 1072,
                        "content": "<p>CloudFront Key Pairs</p>",
                        "isValid": true
                    },
                    {
                        "id": 1073,
                        "content": "<p>IAM User passwords</p>",
                        "isValid": false
                    },
                    {
                        "id": 1074,
                        "content": "<p>EC2 Instance Key Pairs</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 264,
            "attributes": {
                "createdAt": "2023-09-07T08:39:25.417Z",
                "updatedAt": "2023-09-07T08:39:25.417Z",
                "content": "<p>The development team at a company creates serverless solutions using AWS Lambda. Functions are invoked by clients via AWS API Gateway which anyone can access. The team lead would like to control access using a 3rd party authorization mechanism.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>\"Lambda Authorizer\"</p>\n\n<p>An Amazon API Gateway Lambda authorizer (formerly known as a custom authorizer) is a Lambda function that you provide to control access to your API. A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML. Before creating an API Gateway Lambda authorizer, you must first create the AWS Lambda function that implements the logic to authorize and, if necessary, to authenticate the caller.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"IAM permissions with sigv4\" - Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. You will still need to provide permissions but our requirements have a need for 3rd party authentication which is where Lambda Authorizer comes in to play.</p>\n\n<p>\"Cognito User Pools\" - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK. This is managed by AWS, therefore, does not meet our requirements.</p>\n\n<p>\"API Gateway User Pools\" - This is a made-up option, added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n",
                "options": [
                    {
                        "id": 1075,
                        "content": "<p>IAM permissions with sigv4</p>",
                        "isValid": false
                    },
                    {
                        "id": 1076,
                        "content": "<p>Cognito User Pools</p>",
                        "isValid": false
                    },
                    {
                        "id": 1077,
                        "content": "<p>API Gateway User Pools</p>",
                        "isValid": false
                    },
                    {
                        "id": 1078,
                        "content": "<p>Lambda Authorizer</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 265,
            "attributes": {
                "createdAt": "2023-09-07T08:39:25.496Z",
                "updatedAt": "2023-09-07T08:39:25.496Z",
                "content": "<p>A Developer at a company is working on a CloudFormation template to set up resources. Resources will be defined using code and provisioned based on certain conditions defined in the <code>Conditions</code> section.</p>\n\n<p>Which section of a CloudFormation template cannot be associated with <code>Condition</code>?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Parameters</strong></p>\n\n<p>Parameters enable you to input custom values to your CloudFormation template each time you create or update a stack. Please see this note to understand how to define a parameter in a template:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q11-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a></p>\n\n<p>The optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.</p>\n\n<p>You might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs. For the production environment, you might include Amazon EC2 instances with certain capabilities; however, for the test environment, you want to use reduced capabilities to save money.</p>\n\n<p>Conditions cannot be used within the Parameters section. After you define all your conditions, you can associate them with resources and resource properties only in the Resources and Outputs sections of a template.</p>\n\n<p>Please review this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q11-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html</a></p>\n\n<p>Please visit https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html for more information on the parameter structure.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Resources</strong> - Resources section describes the resources that you want to provision in your AWS CloudFormation stacks. You can associate conditions with the resources that you want to conditionally create.</p>\n\n<p><strong>Conditions</strong> - You actually define conditions in this section of the CloudFormation template</p>\n\n<p><strong>Outputs</strong> - The optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find. You can associate conditions with the outputs that you want to conditionally create.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html</a></p>\n",
                "options": [
                    {
                        "id": 1079,
                        "content": "<p>Conditions</p>",
                        "isValid": false
                    },
                    {
                        "id": 1080,
                        "content": "<p>Outputs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1081,
                        "content": "<p>Parameters</p>",
                        "isValid": true
                    },
                    {
                        "id": 1082,
                        "content": "<p>Resources</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 266,
            "attributes": {
                "createdAt": "2023-09-07T08:39:25.581Z",
                "updatedAt": "2023-09-07T08:39:25.581Z",
                "content": "<p>A company uses Elastic Beanstalk to manage its IT infrastructure on AWS Cloud and it would like to deploy the new application version to the EC2 instances. When the deployment is executed, some instances should serve requests with the old application version, while other instances should serve requests using the new application version until the deployment is completed.</p>\n\n<p>Which deployment meets this requirement without incurring additional costs?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Rolling</strong></p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>How Elastic BeanStalk Works:\n<img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/clearbox-flow-00.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p>\n\n<p>The rolling deployment policy deploys the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch. The cost remains the same as the number of EC2 instances does not increase. This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time.</p>\n\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q15-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Immutable</strong> - The 'Immutable' deployment policy ensures that your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails.</p>\n\n<p><strong>All at once</strong> - This policy deploys the new version to all instances simultaneously. Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time.</p>\n\n<p><strong>Rolling with additional batches</strong> - This policy deploys the new version in batches, but first launches a new batch of instances to ensure full capacity during the deployment process. This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. These increase the costs as you're adding extra instances during the deployment.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n",
                "options": [
                    {
                        "id": 1083,
                        "content": "<p>Immutable</p>",
                        "isValid": false
                    },
                    {
                        "id": 1084,
                        "content": "<p>Rolling</p>",
                        "isValid": true
                    },
                    {
                        "id": 1085,
                        "content": "<p>Rolling with additional batches</p>",
                        "isValid": false
                    },
                    {
                        "id": 1086,
                        "content": "<p>All at once</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 267,
            "attributes": {
                "createdAt": "2023-09-07T08:39:25.661Z",
                "updatedAt": "2023-09-07T08:39:25.661Z",
                "content": "<p>You have created a Java application that uses RDS for its main data storage and ElastiCache for user session storage. The application needs to be deployed using Elastic Beanstalk and every new deployment should allow the application servers to reuse the RDS database. On the other hand, user session data stored in ElastiCache can be re-created for every deployment.</p>\n\n<p>Which of the following configurations will allow you to achieve this? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>ElastiCache defined in <code>.ebextensions/</code></strong> - Any resources created as part of your <code>.ebextensions</code> is part of your Elastic Beanstalk template and will get deleted if the environment is terminated.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p><strong>RDS database defined externally and referenced through environment variables</strong> - To decouple your database instance from your environment, you can run a database instance in Amazon RDS and configure your application to connect to it on launch. This enables you to connect multiple environments to a database, terminate an environment without affecting the database, and perform seamless updates with blue-green deployments. To allow the Amazon EC2 instances in your environment to connect to an outside database, you can configure the environment's Auto Scaling group with an additional security group.</p>\n\n<p>Using Elastic Beanstalk with Amazon RDS:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q23-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>ElastiCache bundled with the application source code</strong> - ElastiCache is an AWS service and cannot be bundled with the source code.</p>\n\n<p><strong>RDS database defined in <code>.ebextensions/</code></strong> - The lifetime of the RDS instance gets tied to the lifetime of the Elastic Beanstalk environment, so this option is incorrect.</p>\n\n<p><strong>ElastiCache database defined externally and referenced through environment variables</strong> - For the given use-case, the client is fine with losing user session data and hence defining it in .ebextensions/ is more appropriate.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-environment-resources-elasticache.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-environment-resources-elasticache.html</a></p>\n",
                "options": [
                    {
                        "id": 1087,
                        "content": "<p>ElastiCache defined in <code>.ebextensions/</code></p>",
                        "isValid": true
                    },
                    {
                        "id": 1088,
                        "content": "<p>ElastiCache database defined externally and referenced through environment variables</p>",
                        "isValid": false
                    },
                    {
                        "id": 1089,
                        "content": "<p>RDS database defined in <code>.ebextensions/</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 1090,
                        "content": "<p>ElastiCache bundled with the application source code</p>",
                        "isValid": false
                    },
                    {
                        "id": 1091,
                        "content": "<p>RDS database defined externally and referenced through environment variables</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 268,
            "attributes": {
                "createdAt": "2023-09-07T08:39:25.756Z",
                "updatedAt": "2023-09-07T08:39:25.756Z",
                "content": "<p>To enable HTTPS connections for his web application deployed on the AWS Cloud, a developer is in the process of creating server certificate.</p>\n\n<p>Which AWS entities can be used to deploy SSL/TLS server certificates? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>AWS Certificate Manager</strong> - AWS Certificate Manager (ACM) is the preferred tool to provision, manage, and deploy server certificates. With ACM you can request a certificate or deploy an existing ACM or external certificate to AWS resources. Certificates provided by ACM are free and automatically renew. In a supported Region, you can use ACM to manage server certificates from the console or programmatically.</p>\n\n<p><strong>IAM</strong> - IAM is used as a certificate manager only when you must support HTTPS connections in a Region that is not supported by ACM. IAM securely encrypts your private keys and stores the encrypted version in IAM SSL certificate storage. IAM supports deploying server certificates in all Regions, but you must obtain your certificate from an external provider for use with AWS. You cannot upload an ACM certificate to IAM. Additionally, you cannot manage your certificates from the IAM Console.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Secrets Manager</strong> - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. It cannot be used to discover and protect your sensitive data in AWS.</p>\n\n<p><strong>AWS Systems Manager</strong> - AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks such as running commands, managing patches, and configuring servers across AWS Cloud as well as on-premises infrastructure.</p>\n\n<p><strong>AWS CloudFormation</strong> - AWS CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all Regions and accounts. Think infrastructure as code; think CloudFormation. You cannot use CloudFormation for running commands or managing patches on servers.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html</a></p>\n",
                "options": [
                    {
                        "id": 1092,
                        "content": "<p>IAM</p>",
                        "isValid": true
                    },
                    {
                        "id": 1093,
                        "content": "<p>AWS Secrets Manager</p>",
                        "isValid": false
                    },
                    {
                        "id": 1094,
                        "content": "<p>AWS Certificate Manager</p>",
                        "isValid": true
                    },
                    {
                        "id": 1095,
                        "content": "<p>AWS Systems Manager</p>",
                        "isValid": false
                    },
                    {
                        "id": 1096,
                        "content": "<p>AWS CloudFormation</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 269,
            "attributes": {
                "createdAt": "2023-09-07T08:39:25.840Z",
                "updatedAt": "2023-09-07T08:39:25.840Z",
                "content": "<p>A development team has configured inbound traffic for the relevant ports in both the Security Group of the EC2 instance as well as the Network Access Control List (NACL) of the subnet for the EC2 instance. The team is, however, unable to connect to the service running on the Amazon EC2 instance.</p>\n\n<p>As a developer associate, which of the following will you recommend to fix this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic</strong> - Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic.</p>\n\n<p>To enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as allow outbound traffic from ephemeral ports. When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port.</p>\n\n<p>The designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL.</p>\n\n<p>By default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive, then you need to explicitly allow traffic from the ephemeral port range.</p>\n\n<p>If you accept traffic from the internet, then you also must establish a route through an internet gateway. If you accept traffic over VPN or AWS Direct Connect, then you must establish a route through a virtual private gateway.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic</strong> - This is incorrect as already discussed.</p>\n\n<p><strong>IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs</strong> - This is a made-up option and just added as a distractor.</p>\n\n<p><strong>Rules associated with Network ACLs should never be modified from the command line. An attempt to modify rules from the command line blocks the rule and results in an erratic behavior</strong> - This option is a distractor. AWS does not support modifying rules of Network ACLs from the command line tool.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/\">https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/</a></p>\n",
                "options": [
                    {
                        "id": 1097,
                        "content": "<p>Rules associated with Network ACLs should never be modified from the command line. An attempt to modify rules from the command line blocks the rule and results in an erratic behavior</p>",
                        "isValid": false
                    },
                    {
                        "id": 1098,
                        "content": "<p>IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1099,
                        "content": "<p>Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic</p>",
                        "isValid": false
                    },
                    {
                        "id": 1100,
                        "content": "<p>Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 270,
            "attributes": {
                "createdAt": "2023-09-07T08:39:25.925Z",
                "updatedAt": "2023-09-07T08:39:25.925Z",
                "content": "<p>Your global organization has an IT infrastructure that is deployed using CloudFormation on AWS Cloud. One employee, in us-east-1 Region, has created a stack 'Application1' and made an exported output with the name 'ELBDNSName'. Another employee has created a stack for a different application 'Application2' in us-east-2 Region and also exported an output with the name 'ELBDNSName'. The first employee wanted to deploy the CloudFormation stack 'Application1' in us-east-2, but it got an error. What is the cause of the error?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>\"Exported Output Values in CloudFormation must have unique names within a single Region\"</p>\n\n<p>Using CloudFormation, you can create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.</p>\n\n<p>A CloudFormation template has an optional Outputs section which declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find.</p>\n\n<p>You can use the Export Output Values to export the name of the resource output for a cross-stack reference. For each AWS account, export names must be unique within a region. In this case, we would have a conflict within us-east-2.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q20-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Output Values in CloudFormation must have unique names across all Regions\"</p>\n\n<p>\"Exported Output Values in CloudFormation must have unique names across all Regions\"</p>\n\n<p>\"Output Values in CloudFormation must have unique names within a single Region\"</p>\n\n<p>These three options contradict the explanation provided earlier, hence these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html</a></p>\n",
                "options": [
                    {
                        "id": 1101,
                        "content": "<p>Output Values in CloudFormation must have unique names within a single Region</p>",
                        "isValid": false
                    },
                    {
                        "id": 1102,
                        "content": "<p>Exported Output Values in CloudFormation must have unique names within a single Region</p>",
                        "isValid": true
                    },
                    {
                        "id": 1103,
                        "content": "<p>Output Values in CloudFormation must have unique names across all Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 1104,
                        "content": "<p>Exported Output Values in CloudFormation must have unique names across all Regions</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 271,
            "attributes": {
                "createdAt": "2023-09-07T08:39:26.036Z",
                "updatedAt": "2023-09-07T08:39:26.036Z",
                "content": "<p>Your company has configured AWS Organizations to manage multiple AWS accounts. Within each AWS account, there are many CloudFormation scripts running. Your manager has requested that each script output the account number of the account the script was executed in.</p>\n\n<p>Which Pseudo parameter will you use to get this information?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS::AccountId</strong></p>\n\n<p>Using CloudFormation, you can create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.</p>\n\n<p>Pseudo parameters are parameters that are predefined by AWS CloudFormation. You do not declare them in your template. Use them the same way as you would a parameter, as the argument for the Ref function.</p>\n\n<p>AWS::AccountId returns the AWS account ID of the account in which the stack is being created.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS::NoValue</strong> - This removes the corresponding resource property when specified as a return value in the Fn::If intrinsic function.</p>\n\n<p><strong>AWS::Region</strong> - Returns a string representing the AWS Region in which the encompassing resource is being created, such as us-west-2.</p>\n\n<p><strong>AWS::StackName</strong> - Returns the name of the stack as specified with the aws cloudformation create-stack command, such as \"teststack\".</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html</a></p>\n",
                "options": [
                    {
                        "id": 1105,
                        "content": "<p>AWS::StackName</p>",
                        "isValid": false
                    },
                    {
                        "id": 1106,
                        "content": "<p>AWS::AccountId</p>",
                        "isValid": true
                    },
                    {
                        "id": 1107,
                        "content": "<p>AWS::NoValue</p>",
                        "isValid": false
                    },
                    {
                        "id": 1108,
                        "content": "<p>AWS::Region</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 272,
            "attributes": {
                "createdAt": "2023-09-07T08:39:26.118Z",
                "updatedAt": "2023-09-07T08:39:26.118Z",
                "content": "<p>An application is hosted by a 3rd party and exposed at yourapp.3rdparty.com. You would like to have your users access your application using www.mydomain.com, which you own and manage under Route 53.</p>\n\n<p>What Route 53 record should you create?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a CNAME record</strong></p>\n\n<p>A CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com or example.net) or subdomain (acme.example.com or zenith.example.org).</p>\n\n<p>CNAME records can be used to map one domain name to another. Although you should keep in mind that the DNS protocol does not allow you to create a CNAME record for the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You cannot create a CNAME record for example.com, but you can create CNAME records for www.example.com, newproduct.example.com, and so on.</p>\n\n<p>Please review the major differences between CNAME and Alias Records:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an A record</strong> - Used to point a domain or subdomain to an IP address. 'A record' cannot be used to map one domain name to another.</p>\n\n<p><strong>Create a PTR record</strong> - A Pointer (PTR) record resolves an IP address to a fully-qualified domain name (FQDN) as an opposite to what A record does. PTR records are also called Reverse DNS records. 'PTR record' cannot be used to map one domain name to another.</p>\n\n<p><strong>Create an Alias Record</strong> - Alias records let you route traffic to selected AWS resources, such as CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record. 3rd party websites do not qualify for these as we have no control over those. 'Alias record' cannot be used to map one domain name to another.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p>\n",
                "options": [
                    {
                        "id": 1109,
                        "content": "<p>Create a CNAME record</p>",
                        "isValid": true
                    },
                    {
                        "id": 1110,
                        "content": "<p>Create an Alias Record</p>",
                        "isValid": false
                    },
                    {
                        "id": 1111,
                        "content": "<p>Create a PTR record</p>",
                        "isValid": false
                    },
                    {
                        "id": 1112,
                        "content": "<p>Create an A record</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 273,
            "attributes": {
                "createdAt": "2023-09-07T08:39:26.192Z",
                "updatedAt": "2023-09-07T08:39:26.192Z",
                "content": "<p>As an AWS Certified Developer Associate, you are given a document written in YAML that represents the architecture of a serverless application. The first line of the document contains <code>Transform: 'AWS::Serverless-2016-10-31'</code>.</p>\n\n<p>What does the <code>Transform</code> section in the document represent?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. Templates include several major sections. The \"Resources\" section is the only required section. The optional \"Transform\" section specifies one or more macros that AWS CloudFormation uses to process your template.</p>\n\n<p>The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML.</p>\n\n<p><strong>Presence of 'Transform' section indicates it is a Serverless Application Model (SAM) template</strong> - The AWS::Serverless transform, which is a macro hosted by AWS CloudFormation, takes an entire template written in the AWS Serverless Application Model (AWS SAM) syntax and transforms and expands it into a compliant AWS CloudFormation template. So, presence of \"Transform\" section indicates, the document is a SAM template.</p>\n\n<p>Sample CloudFormation YAML template:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q63-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>It represents a Lambda function definition</strong> - Lambda function is created using \"AWS::Lambda::Function\" resource and has no connection to 'Transform' section.</p>\n\n<p><strong>It represents an intrinsic function</strong> - Intrinsic Functions in templates are used to assign values to properties that are not available until runtime. They usually start with <code>Fn::</code> or <code>!</code>. Example: <code>!Sub</code> or <code>Fn::Sub</code>.</p>\n\n<p><strong>Presence of 'Transform' section indicates it is a CloudFormation Parameter</strong> - CloudFormation parameters are part of <code>Parameters</code> block of the template, like so:</p>\n\n<p>Parameters in YAML:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q63-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p>\n",
                "options": [
                    {
                        "id": 1113,
                        "content": "<p>Presence of <code>Transform</code> section indicates it is a CloudFormation Parameter</p>",
                        "isValid": false
                    },
                    {
                        "id": 1114,
                        "content": "<p>Presence of <code>Transform</code> section indicates it is a Serverless Application Model (SAM) template</p>",
                        "isValid": true
                    },
                    {
                        "id": 1115,
                        "content": "<p>It represents a Lambda function definition</p>",
                        "isValid": false
                    },
                    {
                        "id": 1116,
                        "content": "<p>It represents an intrinsic function</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 274,
            "attributes": {
                "createdAt": "2023-09-07T08:39:26.272Z",
                "updatedAt": "2023-09-07T08:39:26.272Z",
                "content": "<p>A SaaS company runs a HealthCare web application that is used worldwide by users. There have been requests by mobile developers to expose public APIs for the application-specific functionality. You decide to make the APIs available to mobile developers as product offerings.</p>\n\n<p>Which of the following options will allow you to do that?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use API Gateway Usage Plans</strong></p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>A usage plan specifies who can access one or more deployed API stages and methodsâ€”and also how much and how fast they can access them. The plan uses API keys to identify API clients and meters access to the associated API stages for each key.</p>\n\n<p>You can configure usage plans and API keys to allow customers to access selected APIs at agreed-upon request rates and quotas that meet their business requirements and budget constraints.</p>\n\n<p>Overview of API Gateway Usage Plans and API keys:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q18-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Billing Usage Plans</strong> - AWS Billing and Cost Management is the service that you use to pay your AWS bill, monitor your usage, and analyze and control your costs. There is no such thing as AWS Billing Usage Plans. You cannot use AWS Billing to set up public APIs for the application.</p>\n\n<p><strong>Use CloudFront Usage Plans</strong> - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. There is no such thing as CloudFront Usage Plans. You cannot use CloudFront to set up public APIs for the application.</p>\n\n<p><strong>Use AWS Lambda Custom Authorizers</strong> - Lambda is a separate service than Gateway API, therefore, it cannot be used to determine the API usage limits.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html</a></p>\n",
                "options": [
                    {
                        "id": 1117,
                        "content": "<p>Use AWS Lambda Custom Authorizers</p>",
                        "isValid": false
                    },
                    {
                        "id": 1118,
                        "content": "<p>Use AWS Billing Usage Plans</p>",
                        "isValid": false
                    },
                    {
                        "id": 1119,
                        "content": "<p>Use API Gateway Usage Plans</p>",
                        "isValid": true
                    },
                    {
                        "id": 1120,
                        "content": "<p>Use CloudFront Usage Plans</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 275,
            "attributes": {
                "createdAt": "2023-09-07T08:39:26.353Z",
                "updatedAt": "2023-09-07T08:39:26.353Z",
                "content": "<p>The manager at an IT company wants to set up member access to user-specific folders in an Amazon S3 bucket - <code>bucket-a</code>. So, user x can only access files in his folder - <code>bucket-a/user/user-x/</code> and user y can only access files in her folder - <code>bucket-a/user/user-y/</code> and so on.</p>\n\n<p>As a Developer Associate, which of the following IAM constructs would you recommend so that the policy snippet can be made generic for all team members and the manager does not need to create separate IAM policy for each team member?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>IAM policy variables</strong></p>\n\n<p>Instead of creating individual policies for each user, you can use policy variables and create a single policy that applies to multiple users (a group policy). Policy variables act as placeholders. When you make a request to AWS, the placeholder is replaced by a value from the request when the policy is evaluated.</p>\n\n<p>As an example, the following policy gives each of the users in the group full programmatic access to a user-specific object (their own \"home directory\") in Amazon S3.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q40-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>IAM policy principal</strong> - You can use the Principal element in a policy to specify the principal that is allowed or denied access to a resource (In IAM, a principal is a person or application that can make a request for an action or operation on an AWS resource. The principal is authenticated as the AWS account root user or an IAM entity to make requests to AWS). You cannot use the Principal element in an IAM identity-based policy. You can use it in the trust policies for IAM roles and in resource-based policies.</p>\n\n<p><strong>IAM policy condition</strong> - The Condition element (or Condition block) lets you specify conditions for when a policy is in effect, like so - <code>\"Condition\" : { \"StringEquals\" : { \"aws:username\" : \"johndoe\" }}</code>. This can not be used to address the requirements of the given use-case.</p>\n\n<p><strong>IAM policy resource</strong> - The Resource element specifies the object or objects that the statement covers. You specify a resource using an ARN. This can not be used to address the requirements of the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/\">https://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/</a></p>\n",
                "options": [
                    {
                        "id": 1121,
                        "content": "<p>IAM policy condition</p>",
                        "isValid": false
                    },
                    {
                        "id": 1122,
                        "content": "<p>IAM policy resource</p>",
                        "isValid": false
                    },
                    {
                        "id": 1123,
                        "content": "<p>IAM policy variables</p>",
                        "isValid": true
                    },
                    {
                        "id": 1124,
                        "content": "<p>IAM policy principal</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 276,
            "attributes": {
                "createdAt": "2023-09-07T08:39:26.426Z",
                "updatedAt": "2023-09-07T08:39:26.426Z",
                "content": "<p>Which of the following best describes how KMS Encryption works?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>KMS stores the CMK, and receives data from the clients, which it encrypts and sends back</strong></p>\n\n<p>A customer master key (CMK) is a logical representation of a master key. The CMK includes metadata, such as the key ID, creation date, description, and key state. The CMK also contains the key material used to encrypt and decrypt data. You can generate CMKs in KMS, in an AWS CloudHSM cluster, or import them from your key management infrastructure.</p>\n\n<p>AWS KMS supports symmetric and asymmetric CMKs. A symmetric CMK represents a 256-bit key that is used for encryption and decryption. An asymmetric CMK represents an RSA key pair that is used for encryption and decryption or signing and verification (but not both), or an elliptic curve (ECC) key pair that is used for signing and verification.</p>\n\n<p>AWS KMS supports three types of CMKs: customer-managed CMKs, AWS managed CMKs, and AWS owned CMKs.</p>\n\n<p>Overview of Customer master keys (CMKs):\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q14-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>KMS receives CMK from the client at every encrypt call, and encrypts the data with that</strong> - You can import your own CMK (Customer Master Key) but it is done once and then you can encrypt/decrypt as needed.</p>\n\n<p><strong>KMS sends the CMK to the client, which performs the encryption and then deletes the CMK</strong> - KMS does not send CMK to the client, KMS itself encrypts, and then decrypts the data.</p>\n\n<p><strong>KMS generates a new CMK for each Encrypt call and encrypts the data with it</strong> - KMS does not generate a new key each time but you can have KMS rotate the keys for you. Best practices discourage extensive reuse of encryption keys so it is good practice to generate new keys.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html</a></p>\n",
                "options": [
                    {
                        "id": 1125,
                        "content": "<p>KMS receives CMK from the client at every Encrypt call, and encrypts the data with that</p>",
                        "isValid": false
                    },
                    {
                        "id": 1126,
                        "content": "<p>KMS stores the CMK, and receives data from the clients, which it encrypts and sends back</p>",
                        "isValid": true
                    },
                    {
                        "id": 1127,
                        "content": "<p>KMS generates a new CMK for each Encrypt call and encrypts the data with it</p>",
                        "isValid": false
                    },
                    {
                        "id": 1128,
                        "content": "<p>KMS sends the CMK to the client, which performs the encryption and then deletes the CMK</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 277,
            "attributes": {
                "createdAt": "2023-09-07T08:39:26.506Z",
                "updatedAt": "2023-09-07T08:39:26.506Z",
                "content": "<p>An organization has hosted its EC2 instances in two AZs. AZ1 has two instances and AZ2 has 8 instances. The Elastic Load Balancer managing the instances in the two AZs has cross-zone load balancing enabled in its configuration.</p>\n\n<p>What percentage traffic will each of the instances in AZ1 receive?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>A load balancer accepts incoming traffic from clients and routes requests to its registered targets (such as EC2 instances) in one or more Availability Zones.</p>\n\n<p>The nodes for a load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. With Application Load Balancers, cross-zone load balancing is always enabled.</p>\n\n<p><strong>10</strong> - When cross-zone load balancing is enabled, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route its 50% of the client traffic to all 10 targets (present in both AZs).</p>\n\n<p>Cross-Zone Load Balancing Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q30-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>25</strong> - If cross-zone load balancing is disabled, each of the two targets in AZ1 will receive 25% of the traffic. Because the load balancer is only able to send to the targets registered in AZ1 (AZ2 instances are not accessible for load balancer on AZ1)</p>\n\n<p><strong>20</strong> - Invalid option, given only as a distractor.</p>\n\n<p><strong>15</strong> - Invalid option, given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n",
                "options": [
                    {
                        "id": 1129,
                        "content": "<p>20</p>",
                        "isValid": false
                    },
                    {
                        "id": 1130,
                        "content": "<p>15</p>",
                        "isValid": false
                    },
                    {
                        "id": 1131,
                        "content": "<p>25</p>",
                        "isValid": false
                    },
                    {
                        "id": 1132,
                        "content": "<p>10</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 278,
            "attributes": {
                "createdAt": "2023-09-07T08:39:26.581Z",
                "updatedAt": "2023-09-07T08:39:26.581Z",
                "content": "<p>The development team has just configured and attached the IAM policy needed to access AWS Billing and Cost Management for all users under the Finance department. But, the users are unable to see AWS Billing and Cost Management service in the AWS console.</p>\n\n<p>What could be the reason for this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>You need to activate IAM user access to the Billing and Cost Management console for all the users who need access</strong> - By default, IAM users do not have access to the AWS Billing and Cost Management console. You or your account administrator must grant users access. You can do this by activating IAM user access to the Billing and Cost Management console and attaching an IAM policy to your users. Then, you need to activate IAM user access for IAM policies to take effect. You only need to activate IAM user access once.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The users might have another policy that restricts them from accessing the Billing information</strong> - This is an incorrect option, as deduced from the given use-case.</p>\n\n<p><strong>Only root user has access to AWS Billing and Cost Management console</strong> - This is an incorrect statement. AWS Billing and Cost Management access can be provided to any user through user activation and policies, as discussed above.</p>\n\n<p><strong>IAM user should be created under AWS Billing and Cost Management and not under the AWS account to have access to Billing console</strong> - IAM is a feature of your AWS account. All IAM users are created and managed from a single place, irrespective of the services they wish to you.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/control-access-billing.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/control-access-billing.html</a></p>\n",
                "options": [
                    {
                        "id": 1133,
                        "content": "<p>Only root user has access to AWS Billing and Cost Management console</p>",
                        "isValid": false
                    },
                    {
                        "id": 1134,
                        "content": "<p>The users might have another policy that restricts them from accessing the Billing information</p>",
                        "isValid": false
                    },
                    {
                        "id": 1135,
                        "content": "<p>IAM user should be created under AWS Billing and Cost Management and not under AWS account to have access to Billing console</p>",
                        "isValid": false
                    },
                    {
                        "id": 1136,
                        "content": "<p>You need to activate IAM user access to the Billing and Cost Management console for all the users who need access</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 279,
            "attributes": {
                "createdAt": "2023-09-07T08:39:26.665Z",
                "updatedAt": "2023-09-07T08:39:26.665Z",
                "content": "<p>An E-commerce business, has its applications built on a fleet of Amazon EC2 instances, spread across various Regions and AZs. The technical team has suggested using Elastic Load Balancers for better architectural design.</p>\n\n<p>What characteristics of an Elastic Load Balancer make it a winning choice? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p>A load balancer accepts incoming traffic from clients and routes requests to its registered targets (such as EC2 instances) in one or more Availability Zones. The load balancer also monitors the health of its registered targets and ensures that it routes traffic only to healthy targets. When the load balancer detects an unhealthy target, it stops routing traffic to that target. It then resumes routing traffic to that target when it detects that the target is healthy again.</p>\n\n<p>Elastic Load Balancing supports three types of load balancers:</p>\n\n<p>Application Load Balancers</p>\n\n<p>Network Load Balancers</p>\n\n<p>Classic Load Balancers</p>\n\n<p><strong>Separate public traffic from private traffic</strong> - The nodes of an internet-facing load balancer have public IP addresses. Load balancers route requests to your targets using private IP addresses. Therefore, your targets do not need public IP addresses to receive requests from users over the internet.</p>\n\n<p><strong>Build a highly available system</strong> - Elastic Load Balancing provides fault tolerance for your applications by automatically balancing traffic across targets â€“ Amazon EC2 instances, containers, IP addresses, and Lambda functions â€“ in multiple Availability Zones while ensuring only healthy targets receive traffic.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Load Balancer communicates with the underlying EC2 instances using their public IPs</strong> - This is an incorrect statement. The Load Balancer communicates with the underlying EC2 instances using their private IPs.</p>\n\n<p><strong>Improve vertical scalability of the system</strong> - This is an incorrect statement. Elastic Load Balancers can connect with Auto Scaling groups to provide horizontal scaling.</p>\n\n<p><strong>Deploy EC2 instances across multiple AWS Regions</strong> - A Load Balancer can target EC2 instances only within an AWS Region.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticloadbalancing/\">https://aws.amazon.com/elasticloadbalancing/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n",
                "options": [
                    {
                        "id": 1137,
                        "content": "<p>The Load Balancer communicates with the underlying EC2 instances using their public IPs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1138,
                        "content": "<p>Build a highly available system</p>",
                        "isValid": true
                    },
                    {
                        "id": 1139,
                        "content": "<p>Improve vertical scalability of the system</p>",
                        "isValid": false
                    },
                    {
                        "id": 1140,
                        "content": "<p>Deploy EC2 instances across multiple AWS Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 1141,
                        "content": "<p>Separate public traffic from private traffic</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 280,
            "attributes": {
                "createdAt": "2023-09-07T08:39:26.747Z",
                "updatedAt": "2023-09-07T08:39:26.747Z",
                "content": "<p>An IT company is configuring Auto Scaling for its Amazon EC2 instances spread across different AZs and Regions.</p>\n\n<p>Which of the following scenarios are NOT correct about EC2 Auto Scaling? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Auto Scaling groups that span across multiple Regions need to be enabled for all the Regions specified</strong> - This is not valid for Auto Scaling groups. Auto Scaling groups cannot span across multiple Regions.</p>\n\n<p><strong>An Auto Scaling group can contain EC2 instances in only one Availability Zone of a Region</strong> - This is not valid for Auto Scaling groups. An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region.</p>\n\n<p>Amazon EC2 Auto Scaling Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q27-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region</strong> - This is a valid statement. Auto Scaling groups can span across the availability Zones of a Region.</p>\n\n<p><strong>Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group</strong> -  When one Availability Zone becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected Availability Zone. When the unhealthy Availability Zone returns to a healthy state, Auto Scaling automatically redistributes the application instances evenly across all of the designated Availability Zones.</p>\n\n<p><strong>For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets</strong> - For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets. Customers can select the subnets for your EC2 instances when you create or update the Auto Scaling group.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p>\n",
                "options": [
                    {
                        "id": 1142,
                        "content": "<p>An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region</p>",
                        "isValid": false
                    },
                    {
                        "id": 1143,
                        "content": "<p>Auto Scaling groups that span across multiple Regions need to be enabled for all the Regions specified</p>",
                        "isValid": true
                    },
                    {
                        "id": 1144,
                        "content": "<p>Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group</p>",
                        "isValid": false
                    },
                    {
                        "id": 1145,
                        "content": "<p>An Auto Scaling group can contain EC2 instances in only one Availability Zone of a Region</p>",
                        "isValid": true
                    },
                    {
                        "id": 1146,
                        "content": "<p>For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 281,
            "attributes": {
                "createdAt": "2023-09-07T08:39:26.825Z",
                "updatedAt": "2023-09-07T08:39:26.825Z",
                "content": "<p>You're a developer working on a large scale order processing application. After developing the features, you commit your code to AWS CodeCommit and begin building the project with AWS CodeBuild before it gets deployed to the server. The build is taking too long and the error points to an issue resolving dependencies from a third-party. You would like to prevent a build running this long in the future for similar underlying reasons.</p>\n\n<p>Which of the following options represents the best solution to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable CodeBuild timeouts</strong></p>\n\n<p>A build represents a set of actions performed by AWS CodeBuild to create output artifacts (for example, a JAR file) based on a set of input artifacts (for example, a collection of Java class files).</p>\n\n<p>The following rules apply when you run multiple builds:</p>\n\n<p>When possible, builds run concurrently. The maximum number of concurrently running builds can vary.</p>\n\n<p>Builds are queued if the number of concurrently running builds reaches its limit. The maximum number of builds in a queue is five times the concurrent build limit.</p>\n\n<p>A build in a queue that does not start after the number of minutes specified in its time out value is removed from the queue. The default timeout value is eight hours. You can override the build queue timeout with a value between five minutes and eight hours when you run your build.</p>\n\n<p>By setting the timeout configuration, the build process will automatically terminate post the expiry of the configured timeout.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Lambda cannot be used to impact the code build process.</p>\n\n<p><strong>Use AWS CloudWatch Events</strong> - Amazon CloudWatch allows you to monitor AWS cloud resources and the applications you run on AWS. Metrics are provided automatically for a number of AWS products and services. CloudWatch is good for monitoring and viewing logs. CloudWatch cannot be used to impact the code build process.</p>\n\n<p><strong>Use VPC Flow Logs</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC but not for code compiling configuration. VPC Flow Logs cannot be used to impact the code build process.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/builds-working.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/builds-working.html</a></p>\n",
                "options": [
                    {
                        "id": 1147,
                        "content": "<p>Use AWS CloudWatch Events</p>",
                        "isValid": false
                    },
                    {
                        "id": 1148,
                        "content": "<p>Enable CodeBuild timeouts</p>",
                        "isValid": true
                    },
                    {
                        "id": 1149,
                        "content": "<p>Use VPC Flow Logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1150,
                        "content": "<p>Use AWS Lambda</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 282,
            "attributes": {
                "createdAt": "2023-09-07T08:39:26.903Z",
                "updatedAt": "2023-09-07T08:39:26.903Z",
                "content": "<p>A company has built its technology stack on AWS serverless architecture for managing all its business functions. To expedite development for a new business requirement, the company is looking at using pre-built serverless applications.</p>\n\n<p>Which AWS service represents the easiest solution to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Serverless Application Repository (SAR)</strong> - The AWS Serverless Application Repository is a managed repository for serverless applications. It enables teams, organizations, and individual developers to store and share reusable applications, and easily assemble and deploy serverless architectures in powerful new ways. Using the Serverless Application Repository, you don't need to clone, build, package, or publish source code to AWS before deploying it. Instead, you can use pre-built applications from the Serverless Application Repository in your serverless architectures, helping you and your teams reduce duplicated work, ensure organizational best practices, and get to market faster. Integration with AWS Identity and Access Management (IAM) provides resource-level control of each application, enabling you to publicly share applications with everyone or privately share them with specific AWS accounts.</p>\n\n<p>Deploying applications using SAR:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q6-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/serverless/serverlessrepo/\">https://aws.amazon.com/serverless/serverlessrepo/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Marketplace</strong> - The AWS Marketplace enables qualified partners to market and sell their software to AWS Customers. AWS Marketplace is an online software store that helps customers find, buy, and immediately start using the software and services that run on AWS. AWS Marketplace is designed for Independent Software Vendors (ISVs), Value-Added Resellers (VARs), and Systems Integrators (SIs) who have software products they want to offer to customers in the cloud.</p>\n\n<p><strong>AWS AppSync</strong> - AWS AppSync is a fully managed service that makes it easy to develop GraphQL APIs by handling the heavy lifting of securely connecting to data sources like AWS DynamoDB, Lambda, and more. Organizations choose to build APIs with GraphQL because it helps them develop applications faster, by giving front-end developers the ability to query multiple databases, microservices, and APIs with a single GraphQL endpoint.</p>\n\n<p><strong>AWS Service Catalog</strong> - AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS. These IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier application architectures. AWS Service Catalog allows you to centrally manage deployed IT services and your applications, resources, and metadata. This helps you achieve consistent governance and meet your compliance requirements while enabling users to quickly deploy only the approved IT services they need.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/serverless/serverlessrepo/\">https://aws.amazon.com/serverless/serverlessrepo/</a></p>\n",
                "options": [
                    {
                        "id": 1151,
                        "content": "<p>AWS Marketplace</p>",
                        "isValid": false
                    },
                    {
                        "id": 1152,
                        "content": "<p>AWS Service Catalog</p>",
                        "isValid": false
                    },
                    {
                        "id": 1153,
                        "content": "<p>AWS AppSync</p>",
                        "isValid": false
                    },
                    {
                        "id": 1154,
                        "content": "<p>AWS Serverless Application Repository (SAR)</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 283,
            "attributes": {
                "createdAt": "2023-09-07T08:39:26.983Z",
                "updatedAt": "2023-09-07T08:39:26.983Z",
                "content": "<p>As an AWS Certified Developer Associate, you have been asked to create an AWS Elastic Beanstalk environment to handle deployment for an application that has high traffic and high availability needs. You need to deploy the new version using Beanstalk while making sure that performance and availability are not affected.</p>\n\n<p>Which of the following is the MOST optimal way to do this while keeping the solution cost-effective?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS Elastic Beanstalk offers several deployment policies and settings. Choosing the right deployment policy for your application is a tradeoff based on a few considerations and depends on your business needs.</p>\n\n<p><strong>Deploy using 'Rolling with additional batch' deployment policy</strong> - With this method, Elastic Beanstalk launches an extra batch of instances, then performs a rolling deployment. Launching the extra batch takes time, and ensures that the same bandwidth is retained throughout the deployment. This policy also avoids any reduced availability, although at a cost of an even longer deployment time compared to the Rolling method. Finally, this option is suitable if you must maintain the same bandwidth throughout the deployment.</p>\n\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q61-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy using 'Immutable' deployment policy</strong> - A slower deployment method, that ensures your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. With this method, Elastic Beanstalk performs an immutable update to deploy your application. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks.</p>\n\n<p><strong>Deploy using 'All at once' deployment policy</strong> - This is the quickest deployment method. Suitable if you can accept a short loss of service, and if quick deployments are important to you. With this method, Elastic Beanstalk deploys the new application version to each instance. Then, the web proxy or application server might need to restart. As a result, your application might be unavailable to users (or have low availability) for a short time.</p>\n\n<p><strong>Deploy using 'Rolling' deployment policy</strong> - With this method, your application is deployed to your environment one batch of instances at a time. Most bandwidth is retained throughout the deployment. Avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. Suitable if you can't accept any period of completely lost service. The use case states that the application has high traffic and high availability requirements, so full capacity must be maintained during deployments, hence rolling with additional batch deployment is a better fit than the rolling deployment.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n",
                "options": [
                    {
                        "id": 1155,
                        "content": "<p>Deploy using 'Rolling with additional batch' deployment policy</p>",
                        "isValid": true
                    },
                    {
                        "id": 1156,
                        "content": "<p>Deploy using 'Immutable' deployment policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 1157,
                        "content": "<p>Deploy using 'All at once' deployment policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 1158,
                        "content": "<p>Deploy using 'Rolling' deployment policy</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 284,
            "attributes": {
                "createdAt": "2023-09-07T08:39:27.059Z",
                "updatedAt": "2023-09-07T08:39:27.059Z",
                "content": "<p>You are a developer working on AWS Lambda functions that are invoked via REST API's using Amazon API Gateway. Currently, when a GET request is invoked by the consumer, the entire data-set returned by the Lambda function is visible. Your team lead asked you to format the data response.</p>\n\n<p>Which feature of the API Gateway can be used to solve this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use API Gateway Mapping Templates</strong> - In API Gateway, an API's method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, vice versa is also possible. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.</p>\n\n<p>Suppose we have an API for managing fruit and vegetable inventory in the produce department of a supermarket. When a manager queries the backend for the current inventory, the server sends back the following response payload:</p>\n\n<pre><code>{\n  \"department\": \"produce\",\n  \"categories\": [\n    \"fruit\",\n    \"vegetables\"\n  ],\n  \"bins\": [\n    {\n      \"category\": \"fruit\",\n      \"type\": \"apples\",\n      \"price\": 1.99,\n      \"unit\": \"pound\",\n      \"quantity\": 232\n    },\n    {\n      \"category\": \"fruit\",\n      \"type\": \"bananas\",\n      \"price\": 0.19,\n      \"unit\": \"each\",\n      \"quantity\": 112\n    },\n    {\n      \"category\": \"vegetables\",\n      \"type\": \"carrots\",\n      \"price\": 1.29,\n      \"unit\": \"bag\",\n      \"quantity\": 57\n    }\n  ]\n}\n</code></pre>\n\n<p>When the backend returns the query results shown above, the manager of the produce department might be interested in reading them, as follows:</p>\n\n<pre><code>{\n  \"choices\": [\n    {\n      \"kind\": \"apples\",\n      \"suggestedPrice\": \"1.99 per pound\",\n      \"available\": 232\n    },\n    {\n      \"kind\": \"bananas\",\n      \"suggestedPrice\": \"0.19 per each\",\n      \"available\": 112\n    },\n    {\n      \"kind\": \"carrots\",\n      \"suggestedPrice\": \"1.29 per bag\",\n      \"available\": 57\n    }\n  ]\n}\n</code></pre>\n\n<p>To enable this, we need to provide API Gateway with a mapping template to translate the data from the backend format like so:</p>\n\n<pre><code>#set($inputRoot = $input.path('$'))\n{\n  \"choices\": [\n#foreach($elem in $inputRoot.bins)\n    {\n      \"kind\": \"$elem.type\",\n      \"suggestedPrice\": \"$elem.price per $elem.unit\",\n      \"available\": $elem.quantity\n    }#if($foreach.hasNext),#end\n\n#end\n  ]\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy an interceptor shell script</strong> - This option has been added as a distractor.</p>\n\n<p><strong>Use an API Gateway stage variable</strong> - Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates. This feature is not useful for the current use case.</p>\n\n<p><strong>Use a Lambda custom interceptor</strong> - This is a made-up option. Lambda cannot intercept the response for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a></p>\n",
                "options": [
                    {
                        "id": 1159,
                        "content": "<p>Use API Gateway Mapping Templates</p>",
                        "isValid": true
                    },
                    {
                        "id": 1160,
                        "content": "<p>Use an API Gateway stage variable</p>",
                        "isValid": false
                    },
                    {
                        "id": 1161,
                        "content": "<p>Deploy an interceptor shell script</p>",
                        "isValid": false
                    },
                    {
                        "id": 1162,
                        "content": "<p>Use a Lambda custom interceptor</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 285,
            "attributes": {
                "createdAt": "2023-09-07T08:39:27.141Z",
                "updatedAt": "2023-09-07T08:39:27.141Z",
                "content": "<p>An organization has offices across multiple locations and the technology team has configured an Application Load Balancer across targets in multiple Availability Zones. The team wants to analyze the incoming requests for latencies and the client's IP address patterns.</p>\n\n<p>Which feature of the Load Balancer will help collect the required information?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>ALB access logs</strong> - Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues. Access logging is an optional feature of Elastic Load Balancing that is disabled by default.</p>\n\n<p>Access logs for your Application Load Balancer:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q29-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudTrail logs</strong> - Elastic Load Balancing is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Elastic Load Balancing. CloudTrail captures all API calls for Elastic Load Balancing as events. You can use AWS CloudTrail to capture detailed information about the calls made to the Elastic Load Balancing API and store them as log files in Amazon S3. You can use these CloudTrail logs to determine which API calls were made, the source IP address where the API call came from, who made the call, when the call was made, and so on.</p>\n\n<p><strong>CloudWatch metrics</strong> - Elastic Load Balancing publishes data points to Amazon CloudWatch for your load balancers and your targets. CloudWatch enables you to retrieve statistics about those data points as an ordered set of time-series data, known as metrics. You can use metrics to verify that your system is performing as expected. This is the right feature if you wish to track a certain metric.</p>\n\n<p><strong>ALB request tracing</strong> - You can use request tracing to track HTTP requests. The load balancer adds a header with a trace identifier to each request it receives. Request tracing will not help you to analyze latency specific data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n",
                "options": [
                    {
                        "id": 1163,
                        "content": "<p>ALB access logs</p>",
                        "isValid": true
                    },
                    {
                        "id": 1164,
                        "content": "<p>ALB request tracing</p>",
                        "isValid": false
                    },
                    {
                        "id": 1165,
                        "content": "<p>CloudTrail logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1166,
                        "content": "<p>CloudWatch metrics</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 286,
            "attributes": {
                "createdAt": "2023-09-07T08:39:27.223Z",
                "updatedAt": "2023-09-07T08:39:27.223Z",
                "content": "<p>You are running workloads on AWS and have embedded RDS database connection strings within each web server hosting your applications. After failing a security audit, you are looking at a different approach to store your secrets securely and automatically rotate the database credentials.</p>\n\n<p>Which AWS service can you use to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Secrets Manager</strong></p>\n\n<p>AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.</p>\n\n<p>Benefits of Secrets Manager:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q25-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSM Parameter Store</strong> - AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. SSM Parameter Store cannot be used to automatically rotate the database credentials.</p>\n\n<p><strong>Systems Manager</strong> - AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources. Systems Manager cannot be used to store your secrets securely and automatically rotate the database credentials.</p>\n\n<p><strong>KMS</strong> - AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. KMS cannot be used to store your secrets securely and automatically rotate the database credentials.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html</a></p>\n",
                "options": [
                    {
                        "id": 1167,
                        "content": "<p>Systems Manager</p>",
                        "isValid": false
                    },
                    {
                        "id": 1168,
                        "content": "<p>Secrets Manager</p>",
                        "isValid": true
                    },
                    {
                        "id": 1169,
                        "content": "<p>KMS</p>",
                        "isValid": false
                    },
                    {
                        "id": 1170,
                        "content": "<p>SSM Parameter Store</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 287,
            "attributes": {
                "createdAt": "2023-09-07T08:39:27.315Z",
                "updatedAt": "2023-09-07T08:39:27.315Z",
                "content": "<p>A data analytics company processes Internet-of-Things (IoT) data using Amazon Kinesis. The development team has noticed that the IoT data feed into Kinesis experiences periodic spikes. The PutRecords API call occasionally fails and the logs show that the failed call returns the response shown below:</p>\n\n<pre><code>HTTP/1.1 200 OK\nx-amzn-RequestId: &lt;RequestId&gt;\nContent-Type: application/x-amz-json-1.1\nContent-Length: &lt;PayloadSizeBytes&gt;\nDate: &lt;Date&gt;\n{\n    \"FailedRecordCount\": 2,\n    \"Records\": [\n        {\n            \"SequenceNumber\": \"49543463076548007577105092703039560359975228518395012686\",\n            \"ShardId\": \"shardId-000000000000\"\n        },\n        {\n            \"ErrorCode\": \"ProvisionedThroughputExceededException\",\n            \"ErrorMessage\": \"Rate exceeded for shard shardId-000000000001 in stream exampleStreamName under account 111111111111.\"\n        },\n        {\n            \"ErrorCode\": \"InternalFailure\",\n            \"ErrorMessage\": \"Internal service failure.\"\n        }\n    ]\n}\n</code></pre>\n\n<p>As an AWS Certified Developer Associate, which of the following options would you recommend to address this use case? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use an error retry and exponential backoff mechanism</strong></p>\n\n<p><strong>Decrease the frequency or size of your requests</strong></p>\n\n<p>You can use PutRecords API call to write multiple data records into a Kinesis data stream in a single call. Each PutRecords request can support up to 500 records. Each record in the request can be as large as 1 MiB, up to a limit of 5 MiB for the entire request, including partition keys. Each shard can support writes up to 1,000 records per second, up to a maximum data write of 1 MiB per second.</p>\n\n<p>The response Records array includes both successfully and unsuccessfully processed records. Kinesis Data Streams attempts to process all records in each PutRecords request. A single record failure does not stop the processing of subsequent records. As a result, PutRecords doesn't guarantee the ordering of records. An unsuccessfully processed record includes ErrorCode and ErrorMessage values. ErrorCode reflects the type of error and can be one of the following values: <code>ProvisionedThroughputExceededException</code> or <code>InternalFailure</code>. <code>ProvisionedThroughputExceededException</code> indicates that the request rate for the stream is too high, or the requested data is too large for the available throughput. Reduce the frequency or size of your requests.</p>\n\n<p>To address the given use case, you can apply these best practices:</p>\n\n<p>Reshard your stream to increase the number of shards in the stream.</p>\n\n<p>Reduce the frequency or size of your requests.</p>\n\n<p>Distribute read and write operations as evenly as possible across all of the shards in Data Streams.</p>\n\n<p>Use an error retry and exponential backoff mechanism.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Merge the shards to decrease the number of shards in the stream</strong></p>\n\n<p><strong>Increase the frequency or size of your requests</strong></p>\n\n<p>These two options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p><strong>Decrease the number of KCL consumers</strong> - This option has been added as a distractor. The number of KCL consumers is irrelevant for the given use case since the <code>ProvisionedThroughputExceededException</code> is due to the PutRecords API call being used by the producers.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-readprovisionedthroughputexceeded/\">https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-readprovisionedthroughputexceeded/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html\">https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html</a></p>\n",
                "options": [
                    {
                        "id": 1171,
                        "content": "<p>Merge the shards to decrease the number of shards in the stream</p>",
                        "isValid": false
                    },
                    {
                        "id": 1172,
                        "content": "<p>Use an error retry and exponential backoff mechanism</p>",
                        "isValid": true
                    },
                    {
                        "id": 1173,
                        "content": "<p>Decrease the number of KCL consumers</p>",
                        "isValid": false
                    },
                    {
                        "id": 1174,
                        "content": "<p>Decrease the frequency or size of your requests</p>",
                        "isValid": true
                    },
                    {
                        "id": 1175,
                        "content": "<p>Increase the frequency or size of your requests</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 288,
            "attributes": {
                "createdAt": "2023-09-07T08:39:27.394Z",
                "updatedAt": "2023-09-07T08:39:27.394Z",
                "content": "<p>A media company has created a video streaming application and it would like their Brazilian users to be served by the company's Brazilian servers. Other users around the globe should not be able to access the servers through DNS queries.</p>\n\n<p>Which Route 53 routing policy meets this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Geolocation</strong></p>\n\n<p>Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict distribution of content to only the locations in which you have distribution rights</p>\n\n<p>You can create a default record that handles both queries from IP addresses that aren't mapped to any location and queries that come from locations that you haven't created geolocation records for. If you don't create a default record, Route 53 returns a \"no answer\" response for queries from those locations.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q38-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q38-i2.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Failover</strong> - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy.</p>\n\n<p><strong>Latency</strong> - If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency.</p>\n\n<p><strong>Weighted</strong> - Use this policy to route traffic to multiple resources in proportions that you specify.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p>\n",
                "options": [
                    {
                        "id": 1176,
                        "content": "<p>Failover</p>",
                        "isValid": false
                    },
                    {
                        "id": 1177,
                        "content": "<p>Geolocation</p>",
                        "isValid": true
                    },
                    {
                        "id": 1178,
                        "content": "<p>Weighted</p>",
                        "isValid": false
                    },
                    {
                        "id": 1179,
                        "content": "<p>Latency</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 289,
            "attributes": {
                "createdAt": "2023-09-07T08:39:27.479Z",
                "updatedAt": "2023-09-07T08:39:27.479Z",
                "content": "<p>A global e-commerce company wants to perform geographic load testing of its order processing API. The company must deploy resources to multiple AWS Regions to support the load testing of the API.</p>\n\n<p>How can the company address these requirements without additional application code?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up an AWS CloudFormation template that defines the load test resources. Leverage the AWS CLI create-stack-set command to create a stack set in the desired Regions</strong></p>\n\n<p>AWS CloudFormation StackSets extends the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified AWS Regions.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/images/AWSCloudFormation/latest/UserGuide/images/stack_set_conceptual_sv.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p>\n\n<p>A stack set lets you create stacks in AWS accounts across regions by using a single CloudFormation template. A stack set's CloudFormation template defines all the resources in each stack. As you create the stack set, specify the template to use, in addition to any parameters and capabilities that the template requires.</p>\n\n<p>After you've defined a stack set, you can create, update, or delete stacks in the target accounts and AWS Regions you specify.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q16-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an AWS CloudFormation template that defines the load test resources. Develop region-specific Lambda functions to create a stack from the AWS CloudFormation template in each Region when the respective function is invoked</strong> - If you do not use a stack set, then you need to define the CloudFormation templates in each region as well as develop lambda functions in each region to create a stack from the corresponding CloudFormation template. This is unnecessary bloat that can be avoided by simply using the CloudFormation StackSets.</p>\n\n<p><strong>Set up an AWS Cloud Development Kit (CDK) ToolKit that defines the load test resources. Leverage the CDK CLI to create a stack from the template in each Region</strong> - AWS CDK is a framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation. The CDK Toolkit again poses regional limitations and is not the right fit for the given use case.</p>\n\n<p><strong>Set up an AWS Organizations template that defines the load test resources across the organization. Leverage the AWS CLI create-stack-set command to create a stack set in the desired Regions</strong> - This option acts as a distractor. AWS Organizations cannot be used to create templates for provisioning AWS infrastructure. AWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p>\n",
                "options": [
                    {
                        "id": 1180,
                        "content": "<p>Set up an AWS Cloud Development Kit (CDK) ToolKit that defines the load test resources. Leverage the CDK CLI to create a stack from the template in each Region</p>",
                        "isValid": false
                    },
                    {
                        "id": 1181,
                        "content": "<p>Set up an AWS CloudFormation template that defines the load test resources. Leverage the AWS CLI create-stack-set command to create a stack set in the desired Regions</p>",
                        "isValid": true
                    },
                    {
                        "id": 1182,
                        "content": "<p>Set up an AWS Organizations template that defines the load test resources across the organization. Leverage the AWS CLI create-stack-set command to create a stack set in the desired Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 1183,
                        "content": "<p>Set up an AWS CloudFormation template that defines the load test resources. Develop region-specific Lambda functions to create a stack from the AWS CloudFormation template in each Region when the respective function is invoked</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 290,
            "attributes": {
                "createdAt": "2023-09-07T08:39:27.609Z",
                "updatedAt": "2023-09-07T08:39:27.609Z",
                "content": "<p>CodeCommit is a managed version control service that hosts private Git repositories in the AWS cloud.</p>\n\n<p>Which of the following credential types is NOT supported by IAM for CodeCommit?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>IAM username and password</strong> - IAM username and password credentials cannot be used to access CodeCommit.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Git credentials</strong> - These are IAM -generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.</p>\n\n<p><strong>SSH Keys</strong> - Are locally generated public-private key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.</p>\n\n<p><strong>AWS access keys</strong> - You can use these keys with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html</a></p>\n",
                "options": [
                    {
                        "id": 1184,
                        "content": "<p>AWS Access Keys</p>",
                        "isValid": false
                    },
                    {
                        "id": 1185,
                        "content": "<p>SSH Keys</p>",
                        "isValid": false
                    },
                    {
                        "id": 1186,
                        "content": "<p>Git credentials</p>",
                        "isValid": false
                    },
                    {
                        "id": 1187,
                        "content": "<p>IAM username and password</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 291,
            "attributes": {
                "createdAt": "2023-09-07T08:39:27.759Z",
                "updatedAt": "2023-09-07T08:39:27.759Z",
                "content": "<p>You are creating a Cloud Formation template to deploy your CMS application running on an EC2 instance within your AWS account. Since the application will be deployed across multiple regions, you need to create a map of all the possible values for the base AMI.</p>\n\n<p>How will you invoke the <code>!FindInMap</code> function to fulfill this use case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong><code>!FindInMap [ MapName, TopLevelKey, SecondLevelKey ]</code></strong> - The intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that is declared in the Mappings section. YAML Syntax for the full function name:  Fn::FindInMap: [ MapName, TopLevelKey, SecondLevelKey ]</p>\n\n<p>Short form of the above syntax is : !FindInMap [ MapName, TopLevelKey, SecondLevelKey ]</p>\n\n<p>Where,</p>\n\n<p>MapName - Is the logical name of a mapping declared in the Mappings section that contains the keys and values.\nTopLevelKey - The top-level key name. Its value is a list of key-value pairs.\nSecondLevelKey - The second-level key name, which is set to one of the keys from the list assigned to TopLevelKey.</p>\n\n<p>Consider the following YAML template:</p>\n\n<pre><code>Mappings:\n  RegionMap:\n    us-east-1:\n      HVM64: \"ami-0ff8a91507f77f867\"\n      HVMG2: \"ami-0a584ac55a7631c0c\"\n    us-west-1:\n      HVM64: \"ami-0bdb828fd58c52235\"\n      HVMG2: \"ami-066ee5fd4a9ef77f1\"\n    eu-west-1:\n      HVM64: \"ami-047bb4163c506cd98\"\n      HVMG2: \"ami-31c2f645\"\n    ap-southeast-1:\n      HVM64: \"ami-08569b978cc4dfa10\"\n      HVMG2: \"ami-0be9df32ae9f92309\"\n    ap-northeast-1:\n      HVM64: \"ami-06cd52961ce9f0d85\"\n      HVMG2: \"ami-053cdd503598e4a9d\"\nResources:\n  myEC2Instance:\n    Type: \"AWS::EC2::Instance\"\n    Properties:\n      ImageId: !FindInMap\n        - RegionMap\n        - !Ref 'AWS::Region'\n        - HVM64\n      InstanceType: m1.small\n</code></pre>\n\n<p>The example template contains an AWS::EC2::Instance resource whose ImageId property is set by the FindInMap function.</p>\n\n<p>MapName is set to the map of interest, \"RegionMap\" in this example. TopLevelKey is set to the region where the stack is created, which is determined by using the \"AWS::Region\" pseudo parameter. SecondLevelKey is set to the desired architecture, \"HVM64\" for this example.</p>\n\n<p>FindInMap returns the AMI assigned to FindInMap. For a HVM64 instance in us-east-1, FindInMap would return \"ami-0ff8a91507f77f867\".</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>!FindInMap [ MapName, TopLevelKey ]</code></strong></p>\n\n<p><strong><code>!FindInMap [ MapName ]</code></strong></p>\n\n<p><strong><code>!FindInMap [ MapName, TopLevelKey, SecondLevelKey, ThirdLevelKey ]</code></strong></p>\n\n<p>These three options contradict the explanation provided above, hence these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-findinmap.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-findinmap.html</a></p>\n",
                "options": [
                    {
                        "id": 1188,
                        "content": "<p><code>!FindInMap [ MapName, TopLevelKey ]</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 1189,
                        "content": "<p><code>!FindInMap [ MapName, TopLevelKey, SecondLevelKey, ThirdLevelKey ]</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 1190,
                        "content": "<p><code>!FindInMap [ MapName ]</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 1191,
                        "content": "<p><code>!FindInMap [ MapName, TopLevelKey, SecondLevelKey ]</code></p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 292,
            "attributes": {
                "createdAt": "2023-09-07T08:39:27.893Z",
                "updatedAt": "2023-09-07T08:39:27.893Z",
                "content": "<p>A Developer has been entrusted with the job of securing certain S3 buckets that are shared by a large team of users. Last time, a bucket policy was changed, the bucket was erroneously available for everyone, outside the organization too.</p>\n\n<p>Which feature/service will help the developer identify similar security issues with minimum effort?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>IAM Access Analyzer</strong> - AWS IAM Access Analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, that are shared with an external entity. This lets you identify unintended access to your resources and data, which is a security risk.</p>\n\n<p>You can set the scope for the analyzer to an organization or an AWS account. This is your zone of trust. The analyzer scans all of the supported resources within your zone of trust. When Access Analyzer finds a policy that allows access to a resource from outside of your zone of trust, it generates an active finding.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Access Advisor feature on IAM console</strong> - To help identify the unused roles, IAM reports the last-used timestamp that represents when a role was last used to make an AWS request. Your security team can use this information to identify, analyze, and then confidently remove unused roles. This helps improve the security posture of your AWS environments. This does not provide information about non-IAM entities such as S3, hence it's not a correct choice here.</p>\n\n<p><strong>S3 Object Lock</strong> - S3 Object Lock enables you to store objects using a \"Write Once Read Many\" (WORM) model. S3 Object Lock can help prevent accidental or inappropriate deletion of data, it is not the right choice for the current scenario.</p>\n\n<p><strong>S3 Analytics</strong> - By using Amazon S3 analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. You cannot use S3 Analytics to identify unintended access to your S3 resources.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html</a></p>\n",
                "options": [
                    {
                        "id": 1192,
                        "content": "<p>IAM Access Analyzer</p>",
                        "isValid": true
                    },
                    {
                        "id": 1193,
                        "content": "<p>S3 Object Lock</p>",
                        "isValid": false
                    },
                    {
                        "id": 1194,
                        "content": "<p>Access Advisor feature on IAM console</p>",
                        "isValid": false
                    },
                    {
                        "id": 1195,
                        "content": "<p>S3 Analytics</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 293,
            "attributes": {
                "createdAt": "2023-09-07T08:39:27.997Z",
                "updatedAt": "2023-09-07T08:39:27.997Z",
                "content": "<p>An e-commerce company has developed an API that is hosted on Amazon ECS. Variable traffic spikes on the application are causing order processing to take too long. The application processes orders using Amazon SQS queues. The <code>ApproximateNumberOfMessagesVisible</code> metric spikes at very high values throughout the day which triggers the CloudWatch alarm. Other ECS metrics for the API containers are well within limits.</p>\n\n<p>As a Developer Associate, which of the following will you recommend for improving performance while keeping costs low? </p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use backlog per instance metric with target tracking scaling policy</strong> - If you use a target tracking scaling policy based on a custom Amazon SQS queue metric, dynamic scaling can adjust to the demand curve of your application more effectively.</p>\n\n<p>The issue with using a CloudWatch Amazon SQS metric like <code>ApproximateNumberOfMessagesVisible</code> for target tracking is that the number of messages in the queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue. That's because the number of messages in your SQS queue does not solely define the number of instances needed. The number of instances in your Auto Scaling group can be driven by multiple factors, including how long it takes to process a message and the acceptable amount of latency (queue delay).</p>\n\n<p>The solution is to use a backlog per instance metric with the target value being the acceptable backlog per instance to maintain. You can calculate these numbers as follows:</p>\n\n<p>Backlog per instance: To calculate your backlog per instance, start with the ApproximateNumberOfMessages queue attribute to determine the length of the SQS queue (number of messages available for retrieval from the queue). Divide that number by the fleet's running capacity, which for an Auto Scaling group is the number of instances in the InService state, to get the backlog per instance.</p>\n\n<p>Acceptable backlog per instance: To calculate your target value, first determine what your application can accept in terms of latency. Then, take the acceptable latency value and divide it by the average time that an EC2 instance takes to process a message.</p>\n\n<p>To illustrate with an example, let's say that the current ApproximateNumberOfMessages is 1500 and the fleet's running capacity is 10. If the average processing time is 0.1 seconds for each message and the longest acceptable latency is 10 seconds, then the acceptable backlog per instance is 10 / 0.1, which equals 100. This means that 100 is the target value for your target tracking policy. If the backlog per instance is currently at 150 (1500 / 10), your fleet scales out, and it scales out by five instances to maintain proportion to the target value.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Docker swarm</strong> - A Docker swarm is a container orchestration tool, meaning that it allows the user to manage multiple containers deployed across multiple host machines. A swarm consists of multiple Docker hosts which run in swarm mode and act as managers (to manage membership and delegation) and workers (which run swarm services).</p>\n\n<p><strong>Use ECS service scheduler</strong> - Amazon ECS provides a service scheduler (for long-running tasks and applications), the ability to run tasks manually (for batch jobs or single run tasks), with Amazon ECS placing tasks on your cluster for you. You can specify task placement strategies and constraints that allow you to run tasks in the configuration you choose, such as spread out across Availability Zones. It is also possible to integrate with custom or third-party schedulers.</p>\n\n<p><strong>Use ECS step scaling policy</strong> - Although Amazon ECS Service Auto Scaling supports using Application Auto Scaling step scaling policies, AWS recommends using target tracking scaling policies instead. For example, if you want to scale your service when CPU utilization falls below or rises above a certain level, create a target tracking scaling policy based on the CPU utilization metric provided by Amazon ECS.</p>\n\n<p>With step scaling policies, you create and manage the CloudWatch alarms that trigger the scaling process. If the target tracking alarms don't work for your use case, you can use step scaling. You can also use target tracking scaling with step scaling for an advanced scaling policy configuration. For example, you can configure a more aggressive response when utilization reaches a certain level.</p>\n\n<p>Step Scaling scales your cluster on various lengths of steps based on different ranges of thresholds. Target tracking on the other hand intelligently picks the smart lengths needed for the given configuration.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p>\n",
                "options": [
                    {
                        "id": 1196,
                        "content": "<p>Use ECS step scaling policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 1197,
                        "content": "<p>Use ECS service scheduler</p>",
                        "isValid": false
                    },
                    {
                        "id": 1198,
                        "content": "<p>Use Docker swarm</p>",
                        "isValid": false
                    },
                    {
                        "id": 1199,
                        "content": "<p>Use backlog per instance metric with target tracking scaling policy</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 294,
            "attributes": {
                "createdAt": "2023-09-07T08:39:28.084Z",
                "updatedAt": "2023-09-07T08:39:28.084Z",
                "content": "<p>A multi-national company has just moved to AWS Cloud and it has configured forecast-based AWS Budgets alerts for cost management. However, no alerts have been received even though the account and the budgets have been created almost three weeks ago.</p>\n\n<p>What could be the issue with the AWS Budgets configuration?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS Budgets lets customers set custom budgets and receive alerts if their costs or usage exceed (or are forecasted to exceed) their budgeted amount.</p>\n\n<p><strong>AWS requires approximately 5 weeks of usage data to generate budget forecasts</strong> - AWS requires approximately 5 weeks of usage data to generate budget forecasts. If you set a budget to alert based on a forecasted amount, this budget alert isn't triggered until you have enough historical usage information.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Budget forecast has been created from an account that does not have enough privileges</strong> - This is an incorrect statement. If the user account does not have enough privileges, the user will not be able to create the budget at all.</p>\n\n<p><strong>Amazon CloudWatch could be down and hence alerts are not being sent</strong> - Amazon CloudWatch is fully managed by AWS, this option has been added as a distractor.</p>\n\n<p><strong>Account has to be part of AWS Organizations to receive AWS Budget alerts</strong> - This is an incorrect statement. Stand-alone accounts too can create budgets and being part of an Organization is not mandatory to use AWS Budgets.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-best-practices.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-best-practices.html</a></p>\n",
                "options": [
                    {
                        "id": 1200,
                        "content": "<p>Budget forecast has been created from an account that does not have enough privileges</p>",
                        "isValid": false
                    },
                    {
                        "id": 1201,
                        "content": "<p>Amazon CloudWatch could be down and hence alerts are not being sent</p>",
                        "isValid": false
                    },
                    {
                        "id": 1202,
                        "content": "<p>Account has to be part of AWS Organizations to receive AWS Budgets alerts</p>",
                        "isValid": false
                    },
                    {
                        "id": 1203,
                        "content": "<p>AWS requires approximately 5 weeks of usage data to generate budget forecasts</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 295,
            "attributes": {
                "createdAt": "2023-09-07T08:39:28.165Z",
                "updatedAt": "2023-09-07T08:39:28.165Z",
                "content": "<p>A firm runs its technology operations on a fleet of Amazon EC2 instances. The firm needs a certain software to be available on the instances to support their daily workflows. The developer team has been told to use the user data feature of EC2 instances.</p>\n\n<p>Which of the following are true about the user data EC2 configuration? ( Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p>User Data is generally used to perform common automated configuration tasks and even run scripts after the instance starts. When you launch an instance in Amazon EC2, you can pass two types of user data - shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text or as a file.</p>\n\n<p><strong>By default, scripts entered as user data are executed with root user privileges</strong> - Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script. Any files you create will be owned by root; if you need non-root users to have file access, you should modify the permissions accordingly in the script.</p>\n\n<p><strong>By default, user data runs only during the boot cycle when you first launch an instance</strong> - By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>By default, user data is executed every time an EC2 instance is re-started</strong> - As discussed above, this is not a default configuration of the system. But, can be achieved by explicitly configuring the instance.</p>\n\n<p><strong>When an instance is running, you can update user data by using root user credentials</strong> - You can't change the user data if the instance is running (even by using root user credentials), but you can view it.</p>\n\n<p><strong>By default, scripts entered as user data do not have root user privileges for executing</strong> - Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html</a></p>\n",
                "options": [
                    {
                        "id": 1204,
                        "content": "<p>By default, user data runs only during the boot cycle when you first launch an instance</p>",
                        "isValid": true
                    },
                    {
                        "id": 1205,
                        "content": "<p>By default, user data is executed every time an EC2 instance is re-started</p>",
                        "isValid": false
                    },
                    {
                        "id": 1206,
                        "content": "<p>When an instance is running, you can update user data by using root user credentials</p>",
                        "isValid": false
                    },
                    {
                        "id": 1207,
                        "content": "<p>By default, scripts entered as user data do not have root user privileges for executing</p>",
                        "isValid": false
                    },
                    {
                        "id": 1208,
                        "content": "<p>By default, scripts entered as user data are executed with root user privileges</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 296,
            "attributes": {
                "createdAt": "2023-09-07T08:39:28.250Z",
                "updatedAt": "2023-09-07T08:39:28.250Z",
                "content": "<p>You have chosen AWS Elastic Beanstalk to upload your application code and allow it to handle details such as provisioning resources and monitoring.</p>\n\n<p>When creating configuration files for AWS Elastic Beanstalk which naming convention should you follow?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong><code>.ebextensions/&lt;mysettings&gt;.config</code></strong> : You can add AWS Elastic Beanstalk configuration files (<code>.ebextensions</code>) to your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML or JSON formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q64-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>.ebextensions_&lt;mysettings&gt;.config</code></strong></p>\n\n<p><strong><code>.config/&lt;mysettings&gt;.ebextensions</code></strong></p>\n\n<p><strong><code>.config_&lt;mysettings&gt;.ebextensions</code></strong></p>\n\n<p>These three options contradict the explanation provided earlier. So these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n",
                "options": [
                    {
                        "id": 1209,
                        "content": "<p><code>.ebextensions/&lt;mysettings&gt;.config</code></p>",
                        "isValid": true
                    },
                    {
                        "id": 1210,
                        "content": "<p><code>.ebextensions_&lt;mysettings&gt;.config</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 1211,
                        "content": "<p><code>.config/&lt;mysettings&gt;.ebextensions</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 1212,
                        "content": "<p><code>.config_&lt;mysettings&gt;.ebextensions</code></p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 297,
            "attributes": {
                "createdAt": "2023-09-07T08:39:28.327Z",
                "updatedAt": "2023-09-07T08:39:28.327Z",
                "content": "<p>You are a developer for a web application written in .NET which uses the AWS SDK. You need to implement an authentication mechanism that returns a JWT (JSON Web Token).</p>\n\n<p>Which AWS service will help you with token handling and management?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>\"Cognito User Pools\"</p>\n\n<p>After successful authentication, Amazon Cognito returns user pool tokens to your app. You can use the tokens to grant your users access to your own server-side resources, or to the Amazon API Gateway.</p>\n\n<p>Amazon Cognito user pools implement ID, access, and refresh tokens as defined by the OpenID Connect (OIDC) open standard.</p>\n\n<p>The ID token is a JSON Web Token (JWT) that contains claims about the identity of the authenticated user such as name, email, and phone_number. You can use this identity information inside your application. The ID token can also be used to authenticate users against your resource servers or server applications.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q53-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"API Gateway\" - If you are processing tokens server-side and using other programming languages not supported in AWS it may be a good choice. Other than that, go with a service already providing the functionality.</p>\n\n<p>\"Cognito Identity Pools\" - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.</p>\n\n<p>\"Cognito Sync\" - Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q53-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html</a></p>\n",
                "options": [
                    {
                        "id": 1213,
                        "content": "<p>API Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 1214,
                        "content": "<p>Cognito User Pools</p>",
                        "isValid": true
                    },
                    {
                        "id": 1215,
                        "content": "<p>Cognito Identity Pools</p>",
                        "isValid": false
                    },
                    {
                        "id": 1216,
                        "content": "<p>Cognito Sync</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 298,
            "attributes": {
                "createdAt": "2023-09-07T08:39:28.402Z",
                "updatedAt": "2023-09-07T08:39:28.402Z",
                "content": "<p>You are a developer in a manufacturing company that has several servers on-site. The company decides to move new development to the cloud using serverless technology. You decide to use the AWS Serverless Application Model (AWS SAM) and work with an AWS SAM template file to represent your serverless architecture.</p>\n\n<p>Which of the following is NOT a valid serverless resource type?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS::Serverless::UserPool</strong></p>\n\n<p>The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML.</p>\n\n<p>SAM supports the following resource types:</p>\n\n<p>AWS::Serverless::Api</p>\n\n<p>AWS::Serverless::Application</p>\n\n<p>AWS::Serverless::Function</p>\n\n<p>AWS::Serverless::HttpApi</p>\n\n<p>AWS::Serverless::LayerVersion</p>\n\n<p>AWS::Serverless::SimpleTable</p>\n\n<p>AWS::Serverless::StateMachine</p>\n\n<p>UserPool applies to the Cognito service which is used for authentication for mobile app and web. There is no resource named UserPool in the Serverless Application Model.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS::Serverless::Function</strong> - This resource creates a Lambda function, IAM execution role, and event source mappings that trigger the function.</p>\n\n<p><strong>AWS::Serverless::Api</strong> - This creates a collection of Amazon API Gateway resources and methods that can be invoked through HTTPS endpoints. It is useful for advanced use cases where you want full control and flexibility when you configure your APIs.</p>\n\n<p><strong>AWS::Serverless::SimpleTable</strong> - This creates a DynamoDB table with a single attribute primary key. It is useful when data only needs to be accessed via a primary key.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-resources-and-properties.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-resources-and-properties.html</a></p>\n",
                "options": [
                    {
                        "id": 1217,
                        "content": "<p>AWS::Serverless::Function</p>",
                        "isValid": false
                    },
                    {
                        "id": 1218,
                        "content": "<p>AWS::Serverless::UserPool</p>",
                        "isValid": true
                    },
                    {
                        "id": 1219,
                        "content": "<p>AWS::Serverless::Api</p>",
                        "isValid": false
                    },
                    {
                        "id": 1220,
                        "content": "<p>AWS::Serverless::SimpleTable</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 299,
            "attributes": {
                "createdAt": "2023-09-07T08:39:28.475Z",
                "updatedAt": "2023-09-07T08:39:28.475Z",
                "content": "<p>A developer is testing Amazon Simple Queue Service (SQS) queues in a development environment. The queue along with all its contents has to be deleted after testing.</p>\n\n<p>Which SQS API should be used for this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>DeleteQueue</strong> - Deletes the queue specified by the QueueUrl, regardless of the queue's contents.  When you delete a queue, any messages in the queue are no longer available.</p>\n\n<p>When you delete a queue, the deletion process takes up to 60 seconds. Requests you send involving that queue during the 60 seconds might succeed. For example, a SendMessage request might succeed, but after 60 seconds the queue and the message you sent no longer exist.</p>\n\n<p>When you delete a queue, you must wait at least 60 seconds before creating a queue with the same name.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>PurgeQueue</strong> - Deletes the messages in a queue specified by the QueueURL parameter. When you use the PurgeQueue action, you can't retrieve any messages deleted from a queue. The queue however remains.</p>\n\n<p><strong>RemoveQueue</strong> - This is an invalid option, given only as a distractor.</p>\n\n<p><strong>RemovePermission</strong> - Revokes any permissions in the queue policy that matches the specified Label parameter.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_RemovePermission.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_RemovePermission.html</a></p>\n",
                "options": [
                    {
                        "id": 1221,
                        "content": "<p>RemoveQueue</p>",
                        "isValid": false
                    },
                    {
                        "id": 1222,
                        "content": "<p>PurgeQueue</p>",
                        "isValid": false
                    },
                    {
                        "id": 1223,
                        "content": "<p>RemovePermission</p>",
                        "isValid": false
                    },
                    {
                        "id": 1224,
                        "content": "<p>DeleteQueue</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 300,
            "attributes": {
                "createdAt": "2023-09-07T08:39:28.549Z",
                "updatedAt": "2023-09-07T08:39:28.549Z",
                "content": "<p>A development team wants to build an application using serverless architecture. The team plans to use AWS Lambda functions extensively to achieve this goal. The developers of the team work on different programming languages like Python, .NET and Javascript. The team wants to model the cloud infrastructure using any of these programming languages.</p>\n\n<p>Which AWS service/tool should the team use for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Cloud Development Kit (CDK)</strong> - The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources using familiar programming languages.</p>\n\n<p>Provisioning cloud applications can be a challenging process that requires you to perform manual actions, write custom scripts, maintain templates, or learn domain-specific languages. AWS CDK uses the familiarity and expressive power of programming languages such as JavaScript/TypeScript, Python, Java, and .NET for modeling your applications. It provides you with high-level components called constructs that preconfigure cloud resources with proven defaults, so you can build cloud applications without needing to be an expert. AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. It also enables you to compose and share your own custom constructs that incorporate your organization's requirements, helping you start new projects faster.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS CloudFormation</strong> - When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection. But, CloudFormation by itself is not sufficient for the current use case.</p>\n\n<p><strong>AWS Serverless Application Model (SAM)</strong> - The AWS Serverless Application Repository is a managed repository for serverless applications. It enables teams, organizations, and individual developers to store and share reusable applications, and easily assemble and deploy serverless architectures in powerful new ways. Using the Serverless Application Repository, you don't need to clone, build, package, or publish source code to AWS before deploying it.</p>\n\n<p>AWS Serverless Application Model and AWS CDK both abstract AWS infrastructure as code making it easier for you to define your cloud infrastructure. If you prefer defining your serverless infrastructure in concise declarative templates, SAM is the better fit. If you want to define your AWS infrastructure in a familiar programming language, as is the requirement in the current use case, AWS CDK is the right fit.</p>\n\n<p><strong>AWS CodeDeploy</strong> - AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. CodeDeploy can be used with AWS CDK for deployments.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/cdk/faqs/\">https://aws.amazon.com/cdk/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 1225,
                        "content": "<p>AWS CloudFormation</p>",
                        "isValid": false
                    },
                    {
                        "id": 1226,
                        "content": "<p>AWS CodeDeploy</p>",
                        "isValid": false
                    },
                    {
                        "id": 1227,
                        "content": "<p>AWS Serverless Application Model (SAM)</p>",
                        "isValid": false
                    },
                    {
                        "id": 1228,
                        "content": "<p>AWS Cloud Development Kit (CDK)</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 301,
            "attributes": {
                "createdAt": "2023-09-07T08:39:28.623Z",
                "updatedAt": "2023-09-07T08:39:28.623Z",
                "content": "<p>A startup with newly created AWS account is testing different EC2 instances. They have used Burstable performance instance - T2.micro - for 35 seconds and stopped the instance.</p>\n\n<p>At the end of the month, what is the instance usage duration that the company is charged for?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Burstable performance instances, which are T3, T3a, and T2 instances, are designed to provide a baseline level of CPU performance with the ability to burst to a higher level when required by your workload. Burstable performance instances are the only instance types that use credits for CPU usage.</p>\n\n<p><strong>0 seconds</strong> - AWS states that, if your AWS account is less than 12 months old, you can use a t2.micro instance for free within certain usage limits.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>35 seconds</strong></p>\n\n<p><strong>60 seconds</strong></p>\n\n<p><strong>30 seconds</strong></p>\n\n<p>These three options contradict the explanation provided earlier, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html</a></p>\n",
                "options": [
                    {
                        "id": 1229,
                        "content": "<p>35 seconds</p>",
                        "isValid": false
                    },
                    {
                        "id": 1230,
                        "content": "<p>0 seconds</p>",
                        "isValid": true
                    },
                    {
                        "id": 1231,
                        "content": "<p>30 seconds</p>",
                        "isValid": false
                    },
                    {
                        "id": 1232,
                        "content": "<p>60 seconds</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 302,
            "attributes": {
                "createdAt": "2023-09-07T08:39:28.709Z",
                "updatedAt": "2023-09-07T08:39:28.709Z",
                "content": "<p>Amazon Simple Queue Service (SQS) has a set of APIs for various actions supported by the service.</p>\n\n<p>As a developer associate, which of the following would you identify as correct regarding the <code>CreateQueue</code> API? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>You can't change the queue type after you create it</strong> - You can't change the queue type after you create it and you can't convert an existing standard queue into a FIFO queue. You must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.</p>\n\n<p><strong>The visibility timeout value for the queue is in seconds, which defaults to 30 seconds</strong> - The visibility timeout for the queue is in seconds. Valid values are: An integer from 0 to 43,200 (12 hours), the Default value is 30.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The dead-letter queue of a FIFO queue must also be a FIFO queue. Whereas, the dead-letter queue of a standard queue can be a standard queue or a FIFO queue</strong> - The dead-letter queue of a FIFO queue must also be a FIFO queue. Similarly, the dead-letter queue of a standard queue must also be a standard queue.</p>\n\n<p><strong>The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using <code>MessageRetentionPeriod</code> attribute</strong> - The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using <code>DelaySeconds</code> attribute. <code>MessageRetentionPeriod</code> attribute controls the length of time, in seconds, for which Amazon SQS retains a message.</p>\n\n<p><strong>Queue tags are case insensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag</strong> - Queue tags are case-sensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag. To be able to tag a queue on creation, you must have the <code>sqs:CreateQueue</code> and <code>sqs:TagQueue</code> permissions.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_CreateQueue.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_CreateQueue.html</a></p>\n",
                "options": [
                    {
                        "id": 1233,
                        "content": "<p>The dead-letter queue of a FIFO queue must also be a FIFO queue. Whereas, the dead-letter queue of a standard queue can be a standard queue or a FIFO queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 1234,
                        "content": "<p>The visibility timeout value for the queue is in seconds, which defaults to 30 seconds</p>",
                        "isValid": true
                    },
                    {
                        "id": 1235,
                        "content": "<p>Queue tags are case insensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag</p>",
                        "isValid": false
                    },
                    {
                        "id": 1236,
                        "content": "<p>The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using <code>MessageRetentionPeriod</code> attribute</p>",
                        "isValid": false
                    },
                    {
                        "id": 1237,
                        "content": "<p>You can't change the queue type after you create it</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 303,
            "attributes": {
                "createdAt": "2023-09-07T08:39:28.785Z",
                "updatedAt": "2023-09-07T08:39:28.785Z",
                "content": "<p>Your company has stored all application secrets in SSM Parameter Store. The audit team has requested to get a report to better understand when and who has issued API calls against SSM Parameter Store.</p>\n\n<p>Which of the following options can be used to produce your report?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS CloudTrail to get a record of actions taken by a user</strong></p>\n\n<p>AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data.</p>\n\n<p>AWS CloudTrail provides a record of actions taken by a user, role, or an AWS service in Systems Manager. Using the information collected by AWS CloudTrail, you can determine the request that was made to Systems Manager, the IP address from which the request was made, who made the request, when it was made, and additional details.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use SSM Parameter Store List feature to get a record of actions taken by a user</strong> - This option has been added as a distractor.</p>\n\n<p><strong>Use SSM Parameter Store Access Logs in CloudWatch Logs to get a record of actions taken by a user</strong> - CloudWatch Logs can be integrated but that will not help determine who issued API calls.</p>\n\n<p><strong>Use SSM Parameter Store Access Logs in S3 to get a record of actions taken by a user</strong> - S3 Access Logs can be integrated but that will not help determine who issued API calls.</p>\n",
                "options": [
                    {
                        "id": 1238,
                        "content": "<p>Use AWS CloudTrail to get a record of actions taken by a user</p>",
                        "isValid": true
                    },
                    {
                        "id": 1239,
                        "content": "<p>Use SSM Parameter Store Access Logs in CloudWatch Logs to get a record of actions taken by a user</p>",
                        "isValid": false
                    },
                    {
                        "id": 1240,
                        "content": "<p>Use SSM Parameter Store List feature to get a record of actions taken by a user</p>",
                        "isValid": false
                    },
                    {
                        "id": 1241,
                        "content": "<p>Use SSM Parameter Store Access Logs in S3 to get a record of actions taken by a user</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 304,
            "attributes": {
                "createdAt": "2023-09-07T08:39:28.867Z",
                "updatedAt": "2023-09-07T08:39:28.867Z",
                "content": "<p>A company wants to improve the performance of its popular API service that offers unauthenticated read access to daily updated statistical information via Amazon API Gateway and AWS Lambda.</p>\n\n<p>What measures can the company take?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable API caching in API Gateway</strong></p>\n\n<p>API Gateway provides a few strategies for optimizing your API to improve responsiveness, like response caching and payload compression. You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up usage plans and API keys in API Gateway</strong> - After you create, test, and deploy your APIs, you can use API Gateway usage plans to make them available as product offerings for your customers. You can configure usage plans and API keys to allow customers to access selected APIs, and begin throttling requests to those APIs based on defined limits and quotas. These can be set at the API, or API method level. This option is incorrect as usage plans and API keys cannot be used to improve the responsiveness of the API.</p>\n\n<p><strong>Configure API Gateway to use Elasticache for Memcached</strong> - This option has been added as a distractor. Elasticache for Memcached cannot be used with API Gateway to improve the responsiveness of the API for the given use case. You should note that Elasticache for Memcached is a downstream service in the request flow.</p>\n\n<p><strong>Configure API Gateway to use Gateway VPC Endpoint</strong> - This option has been added as a distractor. Gateway endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC. Gateway endpoints do not enable AWS PrivateLink.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html</a></p>\n",
                "options": [
                    {
                        "id": 1242,
                        "content": "<p>Set up usage plans and API keys in API Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 1243,
                        "content": "<p>Configure API Gateway to use Elasticache for Memcached</p>",
                        "isValid": false
                    },
                    {
                        "id": 1244,
                        "content": "<p>Enable API caching in API Gateway</p>",
                        "isValid": true
                    },
                    {
                        "id": 1245,
                        "content": "<p>Configure API Gateway to use Gateway VPC Endpoint</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 305,
            "attributes": {
                "createdAt": "2023-09-07T08:39:28.944Z",
                "updatedAt": "2023-09-07T08:39:28.944Z",
                "content": "<p>As a developer, you are working on creating an application using AWS Cloud Development Kit (CDK).</p>\n\n<p>Which of the following represents the correct order of steps to be followed for creating an app using AWS CDK?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create the app from a template provided by AWS CDK -&gt; Add code to the app to create resources within stacks -&gt; Build the app (optional) -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account</strong></p>\n\n<p>The standard AWS CDK development workflow is similar to the workflow you're already familiar as a developer. There are a few extra steps:</p>\n\n<ol>\n<li><p>Create the app from a template provided by AWS CDK - Each AWS CDK app should be in its own directory, with its own local module dependencies. Create a new directory for your app. Now initialize the app using the <code>cdk init</code> command, specifying the desired template (\"app\") and programming language. The <code>cdk init</code> command creates a number of files and folders inside the created home directory to help you organize the source code for your AWS CDK app.</p></li>\n<li><p>Add code to the app to create resources within stacks - Add custom code as is needed for your application.</p></li>\n<li><p>Build the app (optional) - In most programming environments, after making changes to your code, you'd build (compile) it. This isn't strictly necessary with the AWS CDKâ€”the Toolkit does it for you so you can't forget. But you can still build manually whenever you want to catch syntax and type errors.</p></li>\n<li><p>Synthesize one or more stacks in the app to create an AWS CloudFormation template - Synthesize one or more stacks in the app to create an AWS CloudFormation template. The synthesis step catches logical errors in defining your AWS resources. If your app contains more than one stack, you'd need to specify which stack(s) to synthesize.</p></li>\n<li><p>Deploy one or more stacks to your AWS account - It is optional (though good practice) to synthesize before deploying. The AWS CDK synthesizes your stack before each deployment. If your code has security implications, you'll see a summary of these and need to confirm them before deployment proceeds. <code>cdk deploy</code> is used to deploy the stack using CloudFormation templates. This command displays progress information as your stack is deployed. When it's done, the command prompt reappears.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create the app from a template provided by AWS CloudFormation -&gt; Add code to the app to create resources within stacks -&gt; Build the app (optional) -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account</strong></p>\n\n<p><strong>Create the app from a template provided by AWS CloudFormation -&gt; Add code to the app to create resources within stacks -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account -&gt; Build the app</strong></p>\n\n<p>For both these options, you cannot use AWS CloudFormation to create the app. So these options are incorrect.</p>\n\n<p><strong>Create the app from a template provided by AWS CDK -&gt; Add code to the app to create resources within stacks -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account -&gt; Build the app</strong> - You cannot have the build step after deployment. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cdk/latest/guide/hello_world.html\">https://docs.aws.amazon.com/cdk/latest/guide/hello_world.html</a></p>\n",
                "options": [
                    {
                        "id": 1246,
                        "content": "<p>Create the app from a template provided by AWS CDK -&gt; Add code to the app to create resources within stacks -&gt; Build the app (optional) -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account</p>",
                        "isValid": true
                    },
                    {
                        "id": 1247,
                        "content": "<p>Create the app from a template provided by AWS CloudFormation -&gt; Add code to the app to create resources within stacks -&gt; Build the app (optional) -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account</p>",
                        "isValid": false
                    },
                    {
                        "id": 1248,
                        "content": "<p>Create the app from a template provided by AWS CDK -&gt; Add code to the app to create resources within stacks -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account -&gt; Build the app</p>",
                        "isValid": false
                    },
                    {
                        "id": 1249,
                        "content": "<p>Create the app from a template provided by AWS CloudFormation -&gt; Add code to the app to create resources within stacks -&gt; Synthesize one or more stacks in the app -&gt; Deploy stack(s) to your AWS account -&gt; Build the app</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 306,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.028Z",
                "updatedAt": "2023-09-07T08:39:29.028Z",
                "content": "<p>A development team at a social media company uses AWS Lambda for its serverless stack on AWS Cloud. For a new deployment, the Team Lead wants to send only a certain portion of the traffic to the new Lambda version. In case the deployment goes wrong, the solution should also support the ability to roll back to a previous version of the Lambda function, with MIMINUM downtime for the application.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up the application to use an alias that points to the current version. Deploy the new version of the code and configure the alias to send 10% of the users to this new version. If the deployment goes wrong, reset the alias to point all traffic to the current version</strong></p>\n\n<p>You can use versions to manage the deployment of your AWS Lambda functions. For example, you can publish a new version of a function for beta testing without affecting users of the stable production version. You can change the function code and settings only on the unpublished version of a function. When you publish a version, the code and most of the settings are locked to ensure a consistent experience for users of that version.</p>\n\n<p>You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. You can use routing configuration on an alias to send a portion of traffic to a Lambda function version. For example, you can reduce the risk of deploying a new version by configuring the alias to send most of the traffic to the existing version, and only a small percentage of traffic to the new version.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q13-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q13-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up the application to use an alias that points to the current version. Deploy the new version of the code and configure alias to send all users to this new version. If the deployment goes wrong, reset the alias to point to the current version</strong> - In this case, the application uses an alias to send all traffic to the new version which does not meet the requirement of sending only a certain portion of the traffic to the new Lambda version. In addition, if the deployment goes wrong, the application would see a downtime. Hence this option is incorrect.</p>\n\n<p><strong>Set up the application to directly deploy the new Lambda version. If the deployment goes wrong, reset the application back to the current version using the version number in the ARN</strong> - In this case, the application sends all traffic to the new version which does not meet the requirement of sending only a certain portion of the traffic to the new Lambda version. In addition, if the deployment goes wrong, the application would see a downtime. Hence this option is incorrect.</p>\n\n<p><strong>Set up the application to have multiple alias of the Lambda function. Deploy the new version of the code. Configure a new alias that points to the current alias of the Lambda function for handling 10% of the traffic. If the deployment goes wrong, reset the new alias to point all traffic to the most recent working alias of the Lambda function</strong> - This option has been added as a distractor. The alias for a Lambda function can only point to a Lambda function version. It cannot point to another alias.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html</a></p>\n",
                "options": [
                    {
                        "id": 1250,
                        "content": "<p>Set up the application to directly deploy the new Lambda version. If the deployment goes wrong, reset the application back to the current version using the version number in the ARN</p>",
                        "isValid": false
                    },
                    {
                        "id": 1251,
                        "content": "<p>Set up the application to use an alias that points to the current version. Deploy the new version of the code and configure the alias to send 10% of the users to this new version. If the deployment goes wrong, reset the alias to point all traffic to the current version</p>",
                        "isValid": true
                    },
                    {
                        "id": 1252,
                        "content": "<p>Set up the application to use an alias that points to the current version. Deploy the new version of the code and configure alias to send all users to this new version. If the deployment goes wrong, reset the alias to point to the current version</p>",
                        "isValid": false
                    },
                    {
                        "id": 1253,
                        "content": "<p>Set up the application to have multiple alias of the Lambda function. Deploy the new version of the code. Configure a new alias that points to the current alias of the Lambda function for handling 10% of the traffic. If the deployment goes wrong, reset the new alias to point all traffic to the most recent working alias of the Lambda function</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 307,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.100Z",
                "updatedAt": "2023-09-07T08:39:29.100Z",
                "content": "<p>A development team wants to deploy an AWS Lambda function that requires significant CPU utilization.</p>\n\n<p>As a Developer Associate, which of the following would you suggest for reducing the average runtime of the function?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the function with its memory allocation set to the maximum amount</strong> - Lambda allocates CPU power in proportion to the amount of memory configured. Memory is the amount of memory available to your Lambda function at runtime. You can increase or decrease the memory and CPU power allocated to your function using the Memory (MB) setting. To configure the memory for your function, set a value between 128 MB and 10,240 MB in 1-MB increments. At 1,769 MB, a function has the equivalent of one vCPU (one vCPU-second of credits per second).</p>\n\n<p>Configuring function memory from AWS console:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q47-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the function into multiple AWS Regions</strong> - Deploying the Lambda function to multiple AWS Regions does not increase the compute capacity or CPU utilization capacity of Lambda. So, this option is irrelevant.</p>\n\n<p><strong>Deploy the function using Lambda layers</strong> - A Lambda layer is a .zip file archive that can contain additional code or data. A layer can contain libraries, a custom runtime, data, or configuration files. Layers promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic. Layers do not increase the computational capacity of Lambda.</p>\n\n<p><strong>Deploy the function with its CPU allocation set to the maximum amount</strong> - This statement is given as a distractor. <code>CPU allocation</code> is an invalid parameter. As discussed above, the CPU is allocated in proportion to the memory allocated to the function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html</a></p>\n",
                "options": [
                    {
                        "id": 1254,
                        "content": "<p>Deploy the function with its CPU allocation set to the maximum amount</p>",
                        "isValid": false
                    },
                    {
                        "id": 1255,
                        "content": "<p>Deploy the function into multiple AWS Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 1256,
                        "content": "<p>Deploy the function using Lambda layers</p>",
                        "isValid": false
                    },
                    {
                        "id": 1257,
                        "content": "<p>Deploy the function with its memory allocation set to the maximum amount</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 308,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.174Z",
                "updatedAt": "2023-09-07T08:39:29.174Z",
                "content": "<p>ECS Fargate container tasks are usually spread across Availability Zones (AZs) and the underlying workloads need persistent cross-AZ shared access to the data volumes configured for the container tasks.</p>\n\n<p>Which of the following solutions is the best choice for these workloads?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EFS volumes</strong> - EFS volumes provide a simple, scalable, and persistent file storage for use with your Amazon ECS tasks. With Amazon EFS, storage capacity is elastic, growing and shrinking automatically as you add and remove files. Your applications can have the storage they need, when they need it. Amazon EFS volumes are supported for tasks hosted on Fargate or Amazon EC2 instances.</p>\n\n<p>You can use Amazon EFS file systems with Amazon ECS to export file system data across your fleet of container instances. That way, your tasks have access to the same persistent storage, no matter the instance on which they land. However, you must configure your container instance AMI to mount the Amazon EFS file system before the Docker daemon starts. Also, your task definitions must reference volume mounts on the container instance to use the file system.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Docker volumes</strong> -  A Docker-managed volume that is created under /var/lib/docker/volumes on the host Amazon EC2 instance. Docker volume drivers (also referred to as plugins) are used to integrate the volumes with external storage systems, such as Amazon EBS. The built-in local volume driver or a third-party volume driver can be used. Docker volumes are only supported when running tasks on Amazon EC2 instances.</p>\n\n<p><strong>Bind mounts</strong> - A file or directory on the host, such as an Amazon EC2 instance or AWS Fargate, is mounted into a container. Bind mount host volumes are supported for tasks hosted on Fargate or Amazon EC2 instances. Bind mounts provide temporary storage, and hence these are a wrong choice for this use case.</p>\n\n<p><strong>AWS Storage Gateway volumes</strong> - This is an incorrect choice, given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_data_volumes.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_data_volumes.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/containers/amazon-ecs-availability-best-practices/\">https://aws.amazon.com/blogs/containers/amazon-ecs-availability-best-practices/</a></p>\n",
                "options": [
                    {
                        "id": 1258,
                        "content": "<p>Amazon EFS volumes</p>",
                        "isValid": true
                    },
                    {
                        "id": 1259,
                        "content": "<p>Docker volumes</p>",
                        "isValid": false
                    },
                    {
                        "id": 1260,
                        "content": "<p>Bind mounts</p>",
                        "isValid": false
                    },
                    {
                        "id": 1261,
                        "content": "<p>AWS Gateway Storage volumes</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 309,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.247Z",
                "updatedAt": "2023-09-07T08:39:29.247Z",
                "content": "<p>After a test deployment in ElasticBeanstalk environment, a developer noticed that all accumulated Amazon EC2 burst balances were lost.</p>\n\n<p>Which of the following options can lead to this behavior?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The deployment was either run with immutable updates or in traffic splitting mode</strong> - Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments.</p>\n\n<p>Traffic-splitting deployments let you perform canary testing as part of your application deployment. In a traffic-splitting deployment, Elastic Beanstalk launches a full set of new instances just like during an immutable deployment. It then forwards a specified percentage of incoming client traffic to the new application version for a specified evaluation period.</p>\n\n<p>Some policies replace all instances during the deployment or update. This causes all accumulated Amazon EC2 burst balances to be lost. It happens in the following cases:</p>\n\n<ol>\n<li>Managed platform updates with instance replacement enabled</li>\n<li>Immutable updates</li>\n<li>Deployments with immutable updates or traffic splitting enabled</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances</strong> - With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. Rolling deployments do not result in loss of EC2 burst balances.</p>\n\n<p><strong>The deployment was run as a All-at-once deployment, flushing all the accumulated EC2 burst balances</strong> - The traditional All-at-once deployment, wherein all the instances are updated simultaneously, does not result in loss of EC2 burst balances.</p>\n\n<p><strong>When a canary deployment fails, it resets the EC2 burst balances to zero</strong> - This is incorrect and given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n",
                "options": [
                    {
                        "id": 1262,
                        "content": "<p>The deployment was either run with immutable updates or in traffic splitting mode</p>",
                        "isValid": true
                    },
                    {
                        "id": 1263,
                        "content": "<p>The deployment was run as a All-at-once deployment, flushing all the accumulated EC2 burst balances</p>",
                        "isValid": false
                    },
                    {
                        "id": 1264,
                        "content": "<p>The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances</p>",
                        "isValid": false
                    },
                    {
                        "id": 1265,
                        "content": "<p>When a canary deployment fails, it resets the EC2 burst balances to zero</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 310,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.321Z",
                "updatedAt": "2023-09-07T08:39:29.321Z",
                "content": "<p>A developer has been asked to create a web application to be deployed on EC2 instances. The developer just wants to focus on writing application code without worrying about server provisioning, configuration and deployment.</p>\n\n<p>As a Developer Associate, which AWS service would you recommend for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Elastic Beanstalk</strong></p>\n\n<p>AWS Elastic Beanstalk provides an environment to easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for you to manage the lifecycle of your applications.</p>\n\n<p>AWS Elastic Beanstalk lets you manage all of the resources that run your application as environments where each environment runs only a single application version at a time. When an environment is being created, Elastic Beanstalk provisions all the required resources needed to run the application version. You don't need to worry about server provisioning, configuration, and deployment as that's taken care of by Beanstalk.</p>\n\n<p>Benefits of Elastic Beanstalk:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q33-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudFormation</strong> - AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion. With CloudFormation, you still need to create a template to specify the type of resources you need, hence this option is not correct.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p><strong>CodeDeploy</strong> - AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. It can deploy an application to an instance but it cannot provision the instance.</p>\n\n<p><strong>Serverless Application Model</strong> - The AWS Serverless Application Model (AWS SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. As the web application needs to be deployed on EC2 instances, so this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n",
                "options": [
                    {
                        "id": 1266,
                        "content": "<p>CloudFormation</p>",
                        "isValid": false
                    },
                    {
                        "id": 1267,
                        "content": "<p>Serverless Application Model</p>",
                        "isValid": false
                    },
                    {
                        "id": 1268,
                        "content": "<p>Elastic Beanstalk</p>",
                        "isValid": true
                    },
                    {
                        "id": 1269,
                        "content": "<p>CodeDeploy</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 311,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.389Z",
                "updatedAt": "2023-09-07T08:39:29.389Z",
                "content": "<p>You are storing bids information on your betting application and you would like to automatically expire DynamoDB table data after one week.</p>\n\n<p>What should you use?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use TTL</strong></p>\n\n<p>Time To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database. TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use DynamoDB Streams</strong> - These help you get a changelog of your DynamoDB table but won't help you delete expired data. Note that data expired using a TTL will appear as an event in your DynamoDB streams.</p>\n\n<p><strong>Use DAX</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement: from milliseconds to microseconds: even at millions of requests per second. This is a caching technology for your DynamoDB tables.</p>\n\n<p><strong>Use a Lambda function</strong> - This could work but would require setting up indexes, queries, or scans to work, as well as trigger them often enough using a CloudWatch Events. This band-aid solution would never be as good as using the TTL feature in DynamoDB.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html</a></p>\n",
                "options": [
                    {
                        "id": 1270,
                        "content": "<p>Use DAX</p>",
                        "isValid": false
                    },
                    {
                        "id": 1271,
                        "content": "<p>Use TTL</p>",
                        "isValid": true
                    },
                    {
                        "id": 1272,
                        "content": "<p>Use DynamoDB Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 1273,
                        "content": "<p>Use a Lambda function</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 312,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.465Z",
                "updatedAt": "2023-09-07T08:39:29.465Z",
                "content": "<p>You have deployed a Java application to an EC2 instance where it uses the X-Ray SDK. When testing from your personal computer, the application sends data to X-Ray but when the application runs from within EC2, the application fails to send data to X-Ray.</p>\n\n<p>Which of the following does <strong>NOT</strong> help with debugging the issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>X-Ray sampling</strong></p>\n\n<p>By customizing sampling rules, you can control the amount of data that you record, and modify sampling behavior on the fly without modifying or redeploying your code. Sampling rules tell the X-Ray SDK how many requests to record for a set of criteria. X-Ray SDK applies a sampling algorithm to determine which requests get traced however because our application is failing to send data to X-Ray it does not help in determining the cause of failure.</p>\n\n<p>X-Ray Overview:\n<img src=\"https://docs.aws.amazon.com/xray/latest/devguide/images/architecture-dataflow.png\">\nvia - <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html\">https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EC2 X-Ray Daemon</strong> - The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon logs could help with figuring out the problem.</p>\n\n<p><strong>EC2 Instance Role</strong> - The X-Ray daemon uses the AWS SDK to upload trace data to X-Ray, and it needs AWS credentials with permission to do that. On Amazon EC2, the daemon uses the instance's instance profile role automatically. Eliminates API permission issues (in case the role doesn't have IAM permissions to write data to the X-Ray service)</p>\n\n<p><strong>CloudTrail</strong> - With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - â€œWho made an API call to modify this resource?â€. CloudTrail provides event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You can check CloudTrail to see if any API call is being denied on X-Ray.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html\">https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html</a></p>\n",
                "options": [
                    {
                        "id": 1274,
                        "content": "<p>X-Ray sampling</p>",
                        "isValid": true
                    },
                    {
                        "id": 1275,
                        "content": "<p>EC2 X-Ray Daemon</p>",
                        "isValid": false
                    },
                    {
                        "id": 1276,
                        "content": "<p>EC2 Instance Role</p>",
                        "isValid": false
                    },
                    {
                        "id": 1277,
                        "content": "<p>CloudTrail</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 313,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.535Z",
                "updatedAt": "2023-09-07T08:39:29.535Z",
                "content": "<p>A developer has an application that stores data in an Amazon S3 bucket. The application uses an HTTP API to store and retrieve objects. When the PutObject API operation adds objects to the S3 bucket the developer must encrypt these objects at rest by using server-side encryption with Amazon S3-managed keys (SSE-S3).</p>\n\n<p>Which solution will guarantee that any upload request without the mandated encryption is not processed?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Invoke the PutObject API operation and set the <code>x-amz-server-side-encryption</code> header as <code>AES256</code>. Use an S3 bucket policy to deny permission to upload an object unless the request has this header</strong></p>\n\n<p>SSE-S3 server-side encryption protects data at rest. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available to encrypt your data, 256-bit Advanced Encryption Standard (AES-256).</p>\n\n<p>You can use the following bucket policy to deny permissions to upload an object unless the request includes the x-amz-server-side-encryption header to request server-side encryption using SSE-S3:</p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"PutObjectPolicy\",\n  \"Statement\": [\n    {\n      \"Sid\": \"DenyIncorrectEncryptionHeader\",\n      \"Effect\": \"Deny\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:PutObject\",\n      \"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\n      \"Condition\": {\n        \"StringNotEquals\": {\n          \"s3:x-amz-server-side-encryption\": \"AES256\"\n        }\n      }\n    },\n    {\n      \"Sid\": \"DenyUnencryptedObjectUploads\",\n      \"Effect\": \"Deny\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:PutObject\",\n      \"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\n      \"Condition\": {\n        \"Null\": {\n          \"s3:x-amz-server-side-encryption\": \"true\"\n        }\n      }\n    }\n  ]\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Invoke the PutObject API operation and set the <code>x-amz-server-side-encryption</code> header as <code>aws:kms</code>. Use an S3 bucket policy to deny permission to upload an object unless the request has this header</strong> - As mentioned above, you need to use <code>AES256</code> rather than <code>aws:kms</code> for the given use case. <code>aws:kms</code> is used when you want to use server-side encryption with AWS KMS (SSE-KMS).</p>\n\n<p><strong>Invoke the PutObject API operation and set the <code>x-amz-server-side-encryption</code> header as <code>sse:s3</code>. Use an S3 bucket policy to deny permission to upload an object unless the request has this header</strong> - This is a made-up option as the <code>x-amz-server-side-encryption</code> header has no such value as <code>sse:s3</code>.</p>\n\n<p><strong>Set the encryption key for SSE-S3 in the HTTP header of every request. Use an S3 bucket policy to deny permission to upload an object unless the request has this header</strong> - This option has been added as a distractor. For SSE-S3, Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a key that it rotates regularly. The encryption key for SSE-S3 encryption key cannot be accessed.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html\">https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p>\n",
                "options": [
                    {
                        "id": 1278,
                        "content": "<p>Invoke the PutObject API operation and set the <code>x-amz-server-side-encryption</code> header as <code>aws:kms</code>. Use an S3 bucket policy to deny permission to upload an object unless the request has this header</p>",
                        "isValid": false
                    },
                    {
                        "id": 1279,
                        "content": "<p>Set the encryption key for SSE-S3 in the HTTP header of every request. Use an S3 bucket policy to deny permission to upload an object unless the request has this header</p>",
                        "isValid": false
                    },
                    {
                        "id": 1280,
                        "content": "<p>Invoke the PutObject API operation and set the <code>x-amz-server-side-encryption</code> header as <code>sse:s3</code>. Use an S3 bucket policy to deny permission to upload an object unless the request has this header</p>",
                        "isValid": false
                    },
                    {
                        "id": 1281,
                        "content": "<p>Invoke the PutObject API operation and set the <code>x-amz-server-side-encryption</code> header as <code>AES256</code>. Use an S3 bucket policy to deny permission to upload an object unless the request has this header</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 314,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.616Z",
                "updatedAt": "2023-09-07T08:39:29.616Z",
                "content": "<p>You have created an Elastic Load Balancer that has marked all the EC2 instances in the target group as unhealthy. Surprisingly, when you enter the IP address of the EC2 instances in your web browser, you can access your website.</p>\n\n<p>What could be the reason your instances are being marked as unhealthy? (Select two)</p>",
                "answerExplanation": "<p>Correct options</p>\n\n<p><strong>The security group of the EC2 instance does not allow for traffic from the security group of the Application Load Balancer</strong></p>\n\n<p><strong>The route for the health check is misconfigured</strong></p>\n\n<p>You must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port. Whenever you add a listener to your load balancer or update the health check port for a target group used by the load balancer to route requests, you must verify that the security groups associated with the load balancer allow traffic on the new port in both directions.</p>\n\n<p>Application Load Balancer Configuration for Security Groups and Health Check Routes:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q28-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EBS volumes have been improperly mounted</strong> - You can access the website using the IP address which means there is no issue with the EBS volumes. So this option is not correct.</p>\n\n<p><strong>Your web-app has a runtime that is not supported by the Application Load Balancer</strong> - There is no connection between a web app and the application load balancer. This option has been added as a distractor.</p>\n\n<p><strong>You need to attach Elastic IP to the EC2 instances</strong> - This option is a distractor as Elastic IPs do not need to be assigned to EC2 instances while using an Application Load Balancer.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 1282,
                        "content": "<p>The security group of the EC2 instance does not allow for traffic from the security group of the Application Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 1283,
                        "content": "<p>You need to attach Elastic IP to the EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 1284,
                        "content": "<p>The route for the health check is misconfigured</p>",
                        "isValid": true
                    },
                    {
                        "id": 1285,
                        "content": "<p>The EBS volumes have been improperly mounted</p>",
                        "isValid": false
                    },
                    {
                        "id": 1286,
                        "content": "<p>Your web-app has a runtime that is not supported by the Application Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 315,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.688Z",
                "updatedAt": "2023-09-07T08:39:29.688Z",
                "content": "<p>A developer is configuring a bucket policy that denies upload object permission to any requests that do not include the <code>x-amz-server-side-encryption</code> header requesting server-side encryption with SSE-KMS for an Amazon S3 bucket  - <code>examplebucket</code>.</p>\n\n<p>Which of the following policies is the right fit for the given requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"<em>\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/</em>\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:kms\"\n            }\n         }\n      }\n   ]\n}</strong> - This bucket policy denies upload object (s3:PutObject) permission if the request does not include the x-amz-server-side-encryption header requesting server-side encryption with SSE-KMS. To ensure that a particular AWS KMS CMK be used to encrypt the objects in a bucket, you can use the <code>s3:x-amz-server-side-encryption-aws-kms-key-id</code> condition key. To specify the AWS KMS CMK, you must use a key Amazon Resource Name (ARN) that is in the \"arn:aws:kms:region:acct-id:key/key-id\" format.</p>\n\n<p>When you upload an object, you can specify the AWS KMS CMK using the <code>x-amz-server-side-encryption-aws-kms-key-id</code> header. If the header is not present in the request, Amazon S3 assumes the AWS-managed CMK.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"<em>\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/</em>\",\n         \"Condition\":{\n            \"StringEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:kms\"\n            }\n         }\n      }\n   ]\n}</strong> - The condition is incorrect in this policy. The condition should use StringNotEquals.</p>\n\n<p><strong>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"<em>\",\n         \"Action\":\"s3:GetObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/</em>\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:AES256\"\n            }\n         }\n      }\n   ]\n}</strong> - AES256 is used for Amazon S3-managed encryption keys (SSE-S3). Amazon S3 server-side encryption uses one of the strongest block ciphers available to encrypt your data, 256-bit Advanced Encryption Standard (AES-256).</p>\n\n<p><strong>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"<em>\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/</em>\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"false\"\n            }\n         }\n      }\n   ]\n}</strong> - The condition is incorrect in this policy. The condition should use <code>\"s3:x-amz-server-side-encryption\":\"aws:kms\"</code>.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p>\n",
                "options": [
                    {
                        "id": 1287,
                        "content": "<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"*\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/*\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:kms\"\n            }\n         }\n      }\n   ]\n}\n</code></pre>",
                        "isValid": true
                    },
                    {
                        "id": 1288,
                        "content": "<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"*\",\n         \"Action\":\"s3:GetObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/*\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:AES256\"\n            }\n         }\n      }\n   ]\n}\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 1289,
                        "content": "<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"*\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/*\",\n         \"Condition\":{\n            \"StringEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:kms\"\n            }\n         }\n      }\n   ]\n}\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 1290,
                        "content": "<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"*\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::examplebucket/*\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"false\"\n            }\n         }\n      }\n   ]\n}\n</code></pre>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 316,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.760Z",
                "updatedAt": "2023-09-07T08:39:29.760Z",
                "content": "<p>A multi-national company has multiple business units with each unit having its own AWS account. The development team at the company would like to debug and trace data across accounts and visualize it in a centralized account.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>X-Ray</strong></p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your applicationâ€™s underlying components.</p>\n\n<p>You can use X-Ray to collect data across AWS Accounts. The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>VPC Flow Logs</strong>: VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data is used to analyze network traces and helps with network security. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. You cannot use VPC Flow Logs to debug and trace data across accounts.</p>\n\n<p><strong>CloudWatch Events</strong>: Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. These help to trigger notifications based on changes happening in AWS services. You cannot use CloudWatch Events to debug and trace data across accounts.</p>\n\n<p><strong>CloudTrail</strong>: With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - â€œWho made an API call to modify this resource?â€. CloudTrail provides event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You cannot use CloudTrail to debug and trace data across accounts.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n",
                "options": [
                    {
                        "id": 1291,
                        "content": "<p>CloudTrail</p>",
                        "isValid": false
                    },
                    {
                        "id": 1292,
                        "content": "<p>CloudWatch Events</p>",
                        "isValid": false
                    },
                    {
                        "id": 1293,
                        "content": "<p>X-Ray</p>",
                        "isValid": true
                    },
                    {
                        "id": 1294,
                        "content": "<p>VPC Flow Logs</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 317,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.837Z",
                "updatedAt": "2023-09-07T08:39:29.837Z",
                "content": "<p>A development team lead is responsible for managing access for her IAM principals. At the start of the cycle, she has granted excess privileges to users to keep them motivated for trying new things. She now wants to ensure that the team has only the minimum permissions required to finish their work.</p>\n\n<p>Which of the following will help her identify unused IAM roles and remove them without disrupting any service?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Access Advisor feature on IAM console</strong>- To help identify the unused roles, IAM reports the last-used timestamp that represents when a role was last used to make an AWS request. Your security team can use this information to identify, analyze, and then confidently remove unused roles. This helps improve the security posture of your AWS environments. Additionally, by removing unused roles, you can simplify your monitoring and auditing efforts by focusing only on roles that are in use.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Trusted Advisor</strong> - AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices on cost optimization, security, fault tolerance, service limits, and performance improvement.</p>\n\n<p><strong>IAM Access Analyzer</strong> - AWS IAM Access Analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, that are shared with an external entity. This lets you identify unintended access to your resources and data, which is a security risk.</p>\n\n<p><strong>Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor-view-data.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor-view-data.html</a></p>\n",
                "options": [
                    {
                        "id": 1295,
                        "content": "<p>Access Advisor feature on IAM console</p>",
                        "isValid": true
                    },
                    {
                        "id": 1296,
                        "content": "<p>Amazon Inspector</p>",
                        "isValid": false
                    },
                    {
                        "id": 1297,
                        "content": "<p>IAM Access Analyzer</p>",
                        "isValid": false
                    },
                    {
                        "id": 1298,
                        "content": "<p>AWS Trusted Advisor</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 318,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.915Z",
                "updatedAt": "2023-09-07T08:39:29.915Z",
                "content": "<p>A retail company is migrating its on-premises database to Amazon RDS for PostgreSQL. The company has read-heavy workloads. The development team at the company is looking at refactoring the code to achieve optimum read performance for SQL queries.</p>\n\n<p>Which solution will address this requirement with the least current as well as future development effort?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up Amazon RDS with one or more read replicas. Refactor the application code so that the queries use the endpoint for the read replicas</strong></p>\n\n<p>Amazon RDS uses the PostgreSQL DB engine's built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica. You can reduce the load on your primary DB instance by routing read queries from your applications to the read replica. Using read replicas, you can elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the given use case, you can achieve optimum read performance for SQL queries by using the read-replica endpoint for the read-heavy workload.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/images/AmazonRDS/latest/UserGuide/images/read-replica.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Elasticache for Redis to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Redis endpoint</strong></p>\n\n<p><strong>Configure Elasticache for Memcached to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Memcached endpoint</strong></p>\n\n<p>Both Redis and Memcached are popular, open-source, in-memory data stores (also known as in-memory caches). These are not relational databases and cannot be used to run SQL queries. So, both these options are incorrect.</p>\n\n<p><strong>Set up Amazon RDS in the multi-AZ configuration with a single standby instance. Refactor the application code so that the queries use the standby instance endpoint</strong> - In an Amazon RDS Multi-AZ deployment with a single standby instance, Amazon RDS automatically creates a primary database (DB) instance and synchronously replicates the data to an instance in a different AZ. When it detects a failure, Amazon RDS automatically fails over to a standby instance without manual intervention. You cannot route the read queries from an application to the standby instance of a multi-AZ RDS database as it's not accessible for the read traffic in the single standby instance configuration.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/requests-rds-read-replicas/\">https://aws.amazon.com/premiumsupport/knowledge-center/requests-rds-read-replicas/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/readable-standby-instances-in-amazon-rds-multi-az-deployments-a-new-high-availability-option/\">https://aws.amazon.com/blogs/database/readable-standby-instances-in-amazon-rds-multi-az-deployments-a-new-high-availability-option/</a></p>\n",
                "options": [
                    {
                        "id": 1299,
                        "content": "<p>Set up Amazon RDS with one or more read replicas. Refactor the application code so that the queries use the endpoint for the read replicas</p>",
                        "isValid": true
                    },
                    {
                        "id": 1300,
                        "content": "<p>Configure Elasticache for Redis to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Redis endpoint</p>",
                        "isValid": false
                    },
                    {
                        "id": 1301,
                        "content": "<p>Configure Elasticache for Memcached to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Memcached endpoint</p>",
                        "isValid": false
                    },
                    {
                        "id": 1302,
                        "content": "<p>Set up Amazon RDS in the multi-AZ configuration with a single standby instance. Refactor the application code so that the queries use the standby instance endpoint</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 319,
            "attributes": {
                "createdAt": "2023-09-07T08:39:29.989Z",
                "updatedAt": "2023-09-07T08:39:29.989Z",
                "content": "<p>A gaming company wants to store information about all the games that the company has released. Each game has a name, version number, and category (such as sports, puzzles, strategy, etc). The game information also can include additional properties about the supported platforms and technical specifications. This additional information is inconsistent across games.</p>\n\n<p>You have been hired as an AWS Certified Developer Associate to build a solution that addresses the following use cases:</p>\n\n<p>For a given name and version number, get all details about the game that has that name and version number.</p>\n\n<p>For a given name, get all details about all games that have that name.</p>\n\n<p>For a given category, get all details about all games in that category.</p>\n\n<p>What will you recommend as the most efficient solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key</strong></p>\n\n<p>When you create a DynamoDB table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key.\nYou can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesn't require that you use indexes, but they give your applications more flexibility when querying your data.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q32-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html</a></p>\n\n<p>You should note that a global secondary index (GSI) contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The Global secondary indexes allow you to perform queries on attributes that are not part of the table's primary key.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q32-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q32-i3.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon DynamoDB table with a primary key that consists of the category as the partition key and the version number as the sort key. Create a global secondary index that has the name as the partition key</strong> - The DynamoDB table for this option has the primary key and GSI that do not solve for the condition - \"For a given name and version number, get all details about the game that has that name and version number\". This option does not allow for efficient querying of a specific game by its name and version number as you need multiple queries which would be less efficient than the single query allowed by the correct option.</p>\n\n<p><strong>Set up an Amazon RDS MySQL instance having a <code>games</code> table that contains columns for name, version number, and category. Configure the name column as the primary key</strong> - This option is not the right fit as it does not allow you to efficiently query on the version number and category columns.</p>\n\n<p><strong>Permanently store the name, version number, and category information about the games in an Amazon Elasticache for Memcached instance</strong> - You cannot use Elasticache for Memcached to permanently store values meant to be persisted in a database (relational or NoSQL). Elasticache is a caching layer. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/primary-key-dynamodb-table/\">https://aws.amazon.com/premiumsupport/knowledge-center/primary-key-dynamodb-table/</a></p>\n",
                "options": [
                    {
                        "id": 1303,
                        "content": "<p>Set up an Amazon DynamoDB table with a primary key that consists of the category as the partition key and the version number as the sort key. Create a global secondary index that has the name as the partition key</p>",
                        "isValid": false
                    },
                    {
                        "id": 1304,
                        "content": "<p>Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key</p>",
                        "isValid": true
                    },
                    {
                        "id": 1305,
                        "content": "<p>Set up an Amazon RDS MySQL instance having a <code>games</code> table that contains columns for name, version number, and category. Configure the name column as the primary key</p>",
                        "isValid": false
                    },
                    {
                        "id": 1306,
                        "content": "<p>Permanently store the name, version number, and category information about the games in an Amazon Elasticache for Memcached instance</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 320,
            "attributes": {
                "createdAt": "2023-09-07T08:39:30.062Z",
                "updatedAt": "2023-09-07T08:39:30.062Z",
                "content": "<p>As part of his development work, an AWS Certified Developer Associate is creating policies and attaching them to IAM identities. After creating necessary Identity-based policies, he is now creating Resource-based policies.</p>\n\n<p>Which is the only resource-based policy that the IAM service supports?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions.\nResource-based policies are JSON policy documents that you attach to a resource such as an Amazon S3 bucket. These policies grant the specified principal permission to perform specific actions on that resource and define under what conditions this applies.</p>\n\n<p><strong>Trust policy</strong> - Trust policies define which principal entities (accounts, users, roles, and federated users) can assume the role. An IAM role is both an identity and a resource that supports resource-based policies. For this reason, you must attach both a trust policy and an identity-based policy to an IAM role. The IAM service supports only one type of resource-based policy called a role trust policy, which is attached to an IAM role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Organizations Service Control Policies (SCP)</strong> - If you enable all features of AWS organization, then you can apply service control policies (SCPs) to any or all of your accounts. SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). The SCP limits permissions for entities in member accounts, including each AWS account root user. An explicit deny in any of these policies overrides the allow.</p>\n\n<p><strong>Access control list (ACL)</strong> - Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account. Amazon S3, AWS WAF, and Amazon VPC are examples of services that support ACLs.</p>\n\n<p><strong>Permissions boundary</strong> - AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resource-based\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resource-based</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>\n",
                "options": [
                    {
                        "id": 1307,
                        "content": "<p>Access control list (ACL)</p>",
                        "isValid": false
                    },
                    {
                        "id": 1308,
                        "content": "<p>Trust policy</p>",
                        "isValid": true
                    },
                    {
                        "id": 1309,
                        "content": "<p>AWS Organizations Service Control Policies (SCP)</p>",
                        "isValid": false
                    },
                    {
                        "id": 1310,
                        "content": "<p>Permissions boundary</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 321,
            "attributes": {
                "createdAt": "2023-09-07T08:39:30.139Z",
                "updatedAt": "2023-09-07T08:39:30.139Z",
                "content": "<p>A company wants to provide beta access to some developers on its development team for a new version of the company's Amazon API Gateway REST API, without causing any disturbance to the existing customers who are using the API via a frontend UI and Amazon Cognito authentication. The new version has new endpoints and backward-incompatible interface changes, and the company's development team is responsible for its maintenance.</p>\n\n<p>Which of the following will satisfy these requirements in the MOST operationally efficient manner?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a development stage on the API Gateway API and then have the developers point the endpoints to the development stage</strong></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-basic-concept.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-basic-concept.html</a></p>\n\n<p>A stage is a logical reference to a lifecycle state of your API (for example, 'dev', 'prod', 'beta', 'v2'). API stages are identified by API ID and stage name. You use a stage to manage and optimize a particular deployment. For example, you can configure stage settings to enable caching, customize request throttling, configure logging, define stage variables, or attach a canary release for testing. After the initial deployment, you can add more stages and associate them with existing deployments. You can use the API Gateway console to create a new stage, or you can choose an existing stage while deploying an API. In general, you can add a new stage to an API deployment before redeploying the API.</p>\n\n<p>For the given use case, you can configure a development stage for your API Gateway API and then integrate it with the new version of the backend functionality that has new endpoints and backward-incompatible interface changes. The customers can continue to use the existing API.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a canary release deployment on the API Gateway API and then have the developers point to the relevant deployment by referencing the stage variable in the endpoint</strong> - An API deployment is a point-in-time snapshot of your API Gateway API. To be available for clients to use, the deployment must be associated with one or more API stages. Canary release is a software development strategy in which a new version of an API (as well as other software) is deployed for testing purposes, and the base version remains deployed as a production release for normal operations on the same stage. In a canary release deployment, total API traffic is separated at random into a production release and a canary release with a pre-configured ratio. Typically, the canary release receives a small percentage of API traffic and the production release takes up the rest. The updated API features are only visible to API traffic through the canary. You can adjust the canary traffic percentage to optimize test coverage or performance. By keeping canary traffic small and the selection random, most users are not adversely affected at any time by potential bugs in the new version, and no single user is adversely affected all the time.</p>\n\n<p>This option is incorrect for the given use case as some of the customers would also access the new version of the API.</p>\n\n<p><strong>Create new API keys on the API Gateway API and then have the developers point the endpoints by passing the new API keys</strong> - AN API key is an alphanumeric string that API Gateway uses to identify an app developer who uses your REST or WebSocket API. This option is a distractor as you cannot selectively provide access to the new version just based on API keys.</p>\n\n<p><strong>Create a new API Gateway API that points to the new API application code and then have the developers point the endpoints to the new API</strong> - This is an overkill for the given requirement as there is no need to create a completely new API just to provide some developers early access to the beta version.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-basic-concept.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-basic-concept.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p>\n",
                "options": [
                    {
                        "id": 1311,
                        "content": "<p>Create new API keys on the API Gateway API and then have the developers point the endpoints by passing the new API keys</p>",
                        "isValid": false
                    },
                    {
                        "id": 1312,
                        "content": "<p>Create a new API Gateway API that points to the new API application code and then have the developers point the endpoints to the new API</p>",
                        "isValid": false
                    },
                    {
                        "id": 1313,
                        "content": "<p>Configure a canary release deployment on the API Gateway API and then have the developers point to the relevant deployment by referencing the stage variable in the endpoint</p>",
                        "isValid": false
                    },
                    {
                        "id": 1314,
                        "content": "<p>Create a development stage on the API Gateway API and then have the developers point the endpoints to the development stage</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 322,
            "attributes": {
                "createdAt": "2023-09-07T08:39:30.216Z",
                "updatedAt": "2023-09-07T08:39:30.216Z",
                "content": "<p>A cybersecurity firm wants to run their applications on single-tenant hardware to meet security guidelines.</p>\n\n<p>Which of the following is the MOST cost-effective way of isolating their Amazon EC2 instances to a single tenant?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Dedicated Instances</strong> - Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.</p>\n\n<p>A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.</p>\n\n<p>Differences between Dedicated Hosts and Dedicated Instances:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Spot Instances</strong> -  A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price.  Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option.</p>\n\n<p><strong>Dedicated Hosts</strong> - An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement.</p>\n\n<p><strong>On-Demand Instances</strong> - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycleâ€”you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements.</p>\n\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q21-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html</a></p>\n",
                "options": [
                    {
                        "id": 1315,
                        "content": "<p>On-Demand Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 1316,
                        "content": "<p>Spot Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 1317,
                        "content": "<p>Dedicated Hosts</p>",
                        "isValid": false
                    },
                    {
                        "id": 1318,
                        "content": "<p>Dedicated Instances</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 323,
            "attributes": {
                "createdAt": "2023-09-07T08:39:30.291Z",
                "updatedAt": "2023-09-07T08:39:30.291Z",
                "content": "<p>A developer has been asked to create an application that can be deployed across a fleet of EC2 instances. The configuration must allow for full control over the deployment steps using the blue-green deployment.</p>\n\n<p>Which service will help you achieve that?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>CodeDeploy</strong></p>\n\n<p>AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions. AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.</p>\n\n<p>The blue/green deployment type uses the blue/green deployment model controlled by CodeDeploy. This deployment type enables you to verify a new deployment of service before sending production traffic to it.</p>\n\n<p>CodeDeploy offers lot of control over deployment steps. Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q58-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/about-aws/whats-new/2017/01/aws-codedeploy-introduces-blue-green-deployments/\">https://aws.amazon.com/about-aws/whats-new/2017/01/aws-codedeploy-introduces-blue-green-deployments/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. It cannot be used to deploy applications.</p>\n\n<p><strong>Elastic Beanstalk</strong> - AWS Elastic Beanstalk offers hooks but not as much control as CodeDeploy. Because AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly.</p>\n\n<p><strong>CodePipeline</strong> - CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change. CodePipeline by itself cannot deploy applications.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n",
                "options": [
                    {
                        "id": 1319,
                        "content": "<p>CodeDeploy</p>",
                        "isValid": true
                    },
                    {
                        "id": 1320,
                        "content": "<p>CodePipeline</p>",
                        "isValid": false
                    },
                    {
                        "id": 1321,
                        "content": "<p>CodeBuild</p>",
                        "isValid": false
                    },
                    {
                        "id": 1322,
                        "content": "<p>Elastic Beanstalk</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 324,
            "attributes": {
                "createdAt": "2023-09-07T08:39:30.363Z",
                "updatedAt": "2023-09-07T08:39:30.363Z",
                "content": "<p>When running a Rolling deployment in Elastic Beanstalk environment, only two batches completed the deployment successfully, while rest of the batches failed to deploy the updated version. Following this, the development team terminated the instances from the failed deployment.</p>\n\n<p>What will be the status of these failed instances post termination?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Elastic Beanstalk will replace them with instances running the application version from the most recent successful deployment</strong></p>\n\n<p>When processing a batch, Elastic Beanstalk detaches all instances in the batch from the load balancer, deploys the new application version, and then reattaches the instances. If you enable connection draining, Elastic Beanstalk drains existing connections from the Amazon EC2 instances in each batch before beginning the deployment.</p>\n\n<p>If a deployment fails after one or more batches completed successfully, the completed batches run the new version of your application while any pending batches continue to run the old version. You can identify the version running on the instances in your environment on the health page in the console. This page displays the deployment ID of the most recent deployment that was executed on each instance in your environment. If you terminate instances from the failed deployment, Elastic Beanstalk replaces them with instances running the application version from the most recent successful deployment.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Elastic Beanstalk will not replace the failed instances</strong></p>\n\n<p><strong>Elastic Beanstalk will replace the failed instances with instances running the application version from the oldest successful deployment</strong></p>\n\n<p><strong>Elastic Beanstalk will replace the failed instances after the application version to be installed is manually chosen from AWS Console</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n",
                "options": [
                    {
                        "id": 1323,
                        "content": "<p>Elastic Beanstalk will replace the failed instances with instances running the application version from the most recent successful deployment</p>",
                        "isValid": true
                    },
                    {
                        "id": 1324,
                        "content": "<p>Elastic Beanstalk will not replace the failed instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 1325,
                        "content": "<p>Elastic Beanstalk will replace the failed instances after the application version to be installed is manually chosen from AWS Console</p>",
                        "isValid": false
                    },
                    {
                        "id": 1326,
                        "content": "<p>Elastic Beanstalk will replace the failed instances with instances running the application version from the oldest successful deployment</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 325,
            "attributes": {
                "createdAt": "2023-09-07T08:39:30.440Z",
                "updatedAt": "2023-09-07T08:39:30.440Z",
                "content": "<p>A company is creating a gaming application that will be deployed on mobile devices. The application will send data to a Lambda function-based RESTful API. The application will assign each API request a unique identifier. The volume of API requests from the application can randomly vary at any given time of day. During request throttling, the application might need to retry requests. The API must be able to address duplicate requests without inconsistencies or data loss.</p>\n\n<p>Which of the following would you recommend to handle these requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Persist the unique identifier for each request in a DynamoDB table. Change the Lambda function to check the table for the identifier before processing the request</strong></p>\n\n<p>DynamoDB is a fully managed, serverless, key-value NoSQL database designed to run high-performance applications at any scale. DynamoDB offers built-in security, continuous backups, automated multi-Region replication, in-memory caching, and data import and export tools. On-demand backup and restore allows you to create full backups of your DynamoDB. Point-in-time recovery (PITR) helps protect your DynamoDB tables from accidental write or delete operations. PITR provides continuous backups of your DynamoDB table data, and you can restore that table to any point in time up to the second during the preceding 35 days.</p>\n\n<p>These features ensure that there is no data loss for the application, thereby meeting a key requirement for the given use case. The solution should also be able to address any duplicate requests without inconsistencies, so the Lambda function should be changed to inspect the table for the given identifier and process the request only if the identifier is unique.</p>\n\n<p>DynamoDB Overview:\n<img src=\"https://d1.awsstatic.com/product-page-diagram_Amazon-DynamoDBa.1f8742c44147f1aed11719df4a14ccdb0b13d9a3.png\">\nvia - <a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n\n<p>DynamoDB</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Persist the unique identifier for each request in an ElastiCache for Memcached cache. Change the Lambda function to check the cache for the identifier before processing the request</strong> -  Memcached is designed for simplicity and it does not offer any snapshot or replication features. This can lead to data loss for applications. Therefore, this option is not the right fit for the given use case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q22-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p><strong>Persist the unique identifier for each request in an RDS MySQL table. Change the Lambda function to check the table for the identifier before processing the request</strong> - DynamoDB is a better fit than RDS MySQL to handle massive traffic spikes for write requests. DynamoDB is a key-value and document database that supports tables of virtually any size with horizontal scaling. DynamoDB scales to more than 10 trillion requests per day and with tables that have more than ten million read and write requests per second and petabytes of data storage. DynamoDB can be used to build applications that need consistent single-digit millisecond performance. MySQL RDS can be scaled vertically, however, it cannot match the performance benefits offered by DynamoDB for the given use case.</p>\n\n<p><strong>Persist the unique identifier for each request in a DynamoDB table. Change the Lambda function to send a client error response when the function receives a duplicate request</strong> - The solution should be able to address any duplicates without any inconsistencies. If Lambda sends a client error response upon receiving a duplicate request, it represents an inconsistent response. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n",
                "options": [
                    {
                        "id": 1327,
                        "content": "<p>Persist the unique identifier for each request in an ElastiCache for Memcached cache. Change the Lambda function to check the cache for the identifier before processing the request</p>",
                        "isValid": false
                    },
                    {
                        "id": 1328,
                        "content": "<p>Persist the unique identifier for each request in a DynamoDB table. Change the Lambda function to send a client error response when the function receives a duplicate request</p>",
                        "isValid": false
                    },
                    {
                        "id": 1329,
                        "content": "<p>Persist the unique identifier for each request in a DynamoDB table. Change the Lambda function to check the table for the identifier before processing the request</p>",
                        "isValid": true
                    },
                    {
                        "id": 1330,
                        "content": "<p>Persist the unique identifier for each request in an RDS MySQL table. Change the Lambda function to check the table for the identifier before processing the request</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 326,
            "attributes": {
                "createdAt": "2023-09-07T08:39:30.516Z",
                "updatedAt": "2023-09-07T08:39:30.516Z",
                "content": "<p>The Technical Lead of your team has reviewed a CloudFormation YAML template written by a new recruit and specified that an invalid section has been added to the template.</p>\n\n<p>Which of the following represents an invalid section of the CloudFormation template?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Templates include several major sections. The Resources section is the only required section.</p>\n\n<p>Sample CloudFormation YAML template:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q60-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p>\n\n<p><strong>'Dependencies' section of the template</strong> - As you can see, there is no section called 'Dependencies' in the template. Although dependencies can be mentioned, there is no section itself for dependencies.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>'Conditions' section of the template</strong> - This optional section includes conditions that control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update. For example, you could conditionally create a resource that depends on whether the stack is for a production or test environment.</p>\n\n<p><strong>'Resources' section of the template</strong> - This is the only required section and specifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket. You can refer to resources in the Resources and Outputs sections of the template.</p>\n\n<p><strong>'Parameters' section of the template</strong> - This optional section is helpful in passing Values to your template at runtime (when you create or update a stack). You can refer to parameters from the Resources and Outputs sections of the template.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p>\n",
                "options": [
                    {
                        "id": 1331,
                        "content": "<p>'Dependencies' section of the template</p>",
                        "isValid": true
                    },
                    {
                        "id": 1332,
                        "content": "<p>'Parameters' section of the template</p>",
                        "isValid": false
                    },
                    {
                        "id": 1333,
                        "content": "<p>'Conditions' section of the template</p>",
                        "isValid": false
                    },
                    {
                        "id": 1334,
                        "content": "<p>'Resources' section of the template</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 327,
            "attributes": {
                "createdAt": "2023-09-07T08:39:30.596Z",
                "updatedAt": "2023-09-07T08:39:30.596Z",
                "content": "<p>As an AWS Certified Developer Associate, you have configured the AWS CLI on your workstation. Your default region is us-east-1 and your IAM user has permissions to operate commands on services such as EC2, S3 and RDS in any region. You would like to execute a command to stop an EC2 instance in the us-east-2 region.</p>\n\n<p>What of the following is the MOST optimal solution to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use the --region parameter</strong>: If the region parameter is not set, then the CLI command is executed against the default AWS region.</p>\n\n<p>You can also review all general options for AWS CLI:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q62-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cli/latest/topic/config-vars.html#general-options\">https://docs.aws.amazon.com/cli/latest/topic/config-vars.html#general-options</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You need to override the default region by using aws configure</strong> - This is not the most optimal way as you will have to change it again to reset the default region.</p>\n\n<p><strong>You should create a new IAM user just for that other region</strong> - This is not the most optimal way as you would need to manage two IAM user profiles.</p>\n\n<p><strong>Use boto3 dependency injection</strong> - With the CLI you do not use boto3. This option is a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/topic/config-vars.html#general-options\">https://docs.aws.amazon.com/cli/latest/topic/config-vars.html#general-options</a></p>\n",
                "options": [
                    {
                        "id": 1335,
                        "content": "<p>You should create a new IAM user just for that other region</p>",
                        "isValid": false
                    },
                    {
                        "id": 1336,
                        "content": "<p>Use the --region parameter</p>",
                        "isValid": true
                    },
                    {
                        "id": 1337,
                        "content": "<p>Use boto3 dependency injection</p>",
                        "isValid": false
                    },
                    {
                        "id": 1338,
                        "content": "<p>You need to override the default region by using aws configure</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 328,
            "attributes": {
                "createdAt": "2023-09-07T08:39:30.670Z",
                "updatedAt": "2023-09-07T08:39:30.670Z",
                "content": "<p>A Developer is configuring Amazon EC2 Auto Scaling group to scale dynamically.</p>\n\n<p>Which metric below is NOT part of Target Tracking Scaling Policy?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>ApproximateNumberOfMessagesVisible</strong> - This is a CloudWatch Amazon SQS queue metric. The number of messages in a queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue. Hence, this metric does not work for target tracking.</p>\n\n<p>Incorrect options:</p>\n\n<p>With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value.</p>\n\n<p>It is important to note that a target tracking scaling policy assumes that it should scale out your Auto Scaling group when the specified metric is above the target value. You cannot use a target tracking scaling policy to scale out your Auto Scaling group when the specified metric is below the target value.</p>\n\n<p><strong>ASGAverageCPUUtilization</strong> - This is a predefined metric for target tracking scaling policy. This represents the Average CPU utilization of the Auto Scaling group.</p>\n\n<p><strong>ASGAverageNetworkOut</strong> - This is a predefined metric for target tracking scaling policy. This represents the Average number of bytes sent out on all network interfaces by the Auto Scaling group.</p>\n\n<p><strong>ALBRequestCountPerTarget</strong> - This is a predefined metric for target tracking scaling policy. This represents the Number of requests completed per target in an Application Load Balancer target group.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n",
                "options": [
                    {
                        "id": 1339,
                        "content": "<p>ALBRequestCountPerTarget</p>",
                        "isValid": false
                    },
                    {
                        "id": 1340,
                        "content": "<p>ASGAverageNetworkOut</p>",
                        "isValid": false
                    },
                    {
                        "id": 1341,
                        "content": "<p>ApproximateNumberOfMessagesVisible</p>",
                        "isValid": true
                    },
                    {
                        "id": 1342,
                        "content": "<p>ASGAverageCPUUtilization</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 329,
            "attributes": {
                "createdAt": "2023-09-07T08:39:30.742Z",
                "updatedAt": "2023-09-07T08:39:30.742Z",
                "content": "<p>A company has created an Amazon S3 bucket that holds customer data. The team lead has just enabled access logging to this bucket. The bucket size has grown substantially after starting access logging. Since no new files have been added to the bucket, the perplexed team lead is looking for an answer.</p>\n\n<p>Which of the following reasons explains this behavior?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>S3 access logging is pointing to the same bucket and is responsible for the substantial growth of bucket size</strong> - When your source bucket and target bucket are the same bucket, additional logs are created for the logs that are written to the bucket. The extra logs about logs might make it harder to find the log that you are looking for. This configuration would drastically increase the size of the S3 bucket.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q57-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-server-access-logs-same-bucket/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-server-access-logs-same-bucket/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Erroneous Bucket policies for batch uploads can sometimes be responsible for the exponential growth of S3 Bucket size</strong> - This is an incorrect statement. A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. A bucket policy, for batch processes or normal processes, will not increase the size of the bucket or the objects in it.</p>\n\n<p><strong>A DDOS attack on your S3 bucket can potentially blow up the size of data in the bucket if the bucket security is compromised during the attack</strong> - This is an incorrect statement. AWS handles DDoS attacks on all of its managed services. However, a DDoS attack will not increase the size of the bucket.</p>\n\n<p><strong>Object Encryption has been enabled and each object is stored twice as part of this configuration</strong> - Encryption does not increase a bucket's size, that too, on daily basis, as if the case in the current scenario</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/set-permissions.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/set-permissions.html</a></p>\n",
                "options": [
                    {
                        "id": 1343,
                        "content": "<p>Object Encryption has been enabled and each object is stored twice as part of this configuration</p>",
                        "isValid": false
                    },
                    {
                        "id": 1344,
                        "content": "<p>Erroneous Bucket policies for batch uploads can sometimes be responsible for the exponential growth of S3 Bucket size</p>",
                        "isValid": false
                    },
                    {
                        "id": 1345,
                        "content": "<p>S3 access logging is pointing to the same bucket and is responsible for the substantial growth of bucket size</p>",
                        "isValid": true
                    },
                    {
                        "id": 1346,
                        "content": "<p>A DDoS attack on your S3 bucket can potentially blow up the size of data in the bucket if the bucket security is compromised during the attack</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 330,
            "attributes": {
                "createdAt": "2023-09-07T08:39:30.812Z",
                "updatedAt": "2023-09-07T08:39:30.812Z",
                "content": "<p>After a code review, a developer has been asked to make his publicly accessible S3 buckets private, and enable access to objects with a time-bound constraint.</p>\n\n<p>Which of the following options will address the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Share pre-signed URLs with resources that need access</strong> - All objects by default are private, with the object owner having permission to access the objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The pre-signed URLs are valid only for the specified duration.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Bucket policy to block the unintended access</strong> - A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. Bucket policy can be used to block off unintended access, but it's not possible to provide time-based access, as is the case in the current use case.</p>\n\n<p><strong>Use Routing policies to re-route unintended access</strong> - There is no such facility directly available with Amazon S3.</p>\n\n<p><strong>It is not possible to implement time constraints on Amazon S3 Bucket access</strong> - This is an incorrect statement. As explained above, it is possible to give time-bound access permissions on S3 buckets and objects.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html</a></p>\n",
                "options": [
                    {
                        "id": 1347,
                        "content": "<p>It is not possible to implement time constraints on Amazon S3 Bucket access</p>",
                        "isValid": false
                    },
                    {
                        "id": 1348,
                        "content": "<p>Use Routing policies to re-route unintended access</p>",
                        "isValid": false
                    },
                    {
                        "id": 1349,
                        "content": "<p>Share pre-signed URLs with resources that need access</p>",
                        "isValid": true
                    },
                    {
                        "id": 1350,
                        "content": "<p>Use Bucket policy to block the unintended access</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 331,
            "attributes": {
                "createdAt": "2023-09-07T08:39:30.892Z",
                "updatedAt": "2023-09-07T08:39:30.892Z",
                "content": "<p>A developer wants to package the code and dependencies for the application-specific Lambda functions as container images to be hosted on Amazon Elastic Container Registry (ECR).</p>\n\n<p>Which of the following options are correct for the given requirement? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>To deploy a container image to Lambda, the container image must implement the Lambda Runtime API</strong> - To deploy a container image to Lambda, the container image must implement the Lambda Runtime API. The AWS open-source runtime interface clients implement the API. You can add a runtime interface client to your preferred base image to make it compatible with Lambda.</p>\n\n<p><strong>You must create the Lambda function from the same account as the container registry in Amazon ECR</strong> - You can package your Lambda function code and dependencies as a container image, using tools such as the Docker CLI. You can then upload the image to your container registry hosted on Amazon Elastic Container Registry (Amazon ECR). Note that you must create the Lambda function from the same account as the container registry in Amazon ECR.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Lambda supports both Windows and Linux-based container images</strong> - Lambda currently supports only Linux-based container images.</p>\n\n<p><strong>You can test the containers locally using the Lambda Runtime API</strong> - You can test the containers locally using the Lambda Runtime Interface Emulator.</p>\n\n<p><strong>You can deploy Lambda function as a container image, with a maximum size of 15 GB</strong> - You can deploy Lambda function as container image with the maximum size of 10GB.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/images-create.html\">https://docs.aws.amazon.com/lambda/latest/dg/images-create.html</a></p>\n",
                "options": [
                    {
                        "id": 1351,
                        "content": "<p>Lambda supports both Windows and Linux-based container images</p>",
                        "isValid": false
                    },
                    {
                        "id": 1352,
                        "content": "<p>You must create the Lambda function from the same account as the container registry in Amazon ECR</p>",
                        "isValid": true
                    },
                    {
                        "id": 1353,
                        "content": "<p>You can deploy Lambda function as a container image, with a maximum size of 15 GB</p>",
                        "isValid": false
                    },
                    {
                        "id": 1354,
                        "content": "<p>You can test the containers locally using the Lambda Runtime API</p>",
                        "isValid": false
                    },
                    {
                        "id": 1355,
                        "content": "<p>To deploy a container image to Lambda, the container image must implement the Lambda Runtime API</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 332,
            "attributes": {
                "createdAt": "2023-09-07T08:39:30.961Z",
                "updatedAt": "2023-09-07T08:39:30.961Z",
                "content": "<p>A company needs a version control system for their fast development lifecycle with incremental changes, version control, and support to existing Git tools.</p>\n\n<p>Which AWS service will meet these requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS CodeCommit</strong> - AWS CodeCommit is a fully-managed Source Control service that hosts secure Git-based repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem. AWS CodeCommit helps you collaborate on code with teammates via pull requests, branching and merging. AWS CodeCommit keeps your repositories close to your build, staging, and production environments in the AWS cloud. You can transfer incremental changes instead of the entire application.\nAWS CodeCommit supports all Git commands and works with your existing Git tools. You can keep using your preferred development environment plugins, continuous integration/continuous delivery systems, and graphical clients with CodeCommit.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Versioned S3 Bucket</strong> - AWS CodeCommit is designed for collaborative software development. It manages batches of changes across multiple files, offers parallel branching, and includes version differencing (\"diffing\"). In comparison, Amazon S3 versioning supports recovering past versions of individual files but doesnâ€™t support tracking batched changes that span multiple files or other features needed for collaborative software development.</p>\n\n<p><strong>AWS CodePipeline</strong> - AWS CodePipeline is a fully managed \"continuous delivery\" service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define.</p>\n\n<p><strong>AWS CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you donâ€™t need to provision, manage, and scale your own build servers. CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codecommit/\">https://aws.amazon.com/codecommit/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/\">https://aws.amazon.com/codepipeline/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codebuild/\">https://aws.amazon.com/codebuild/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n",
                "options": [
                    {
                        "id": 1356,
                        "content": "<p>AWS CodeCommit</p>",
                        "isValid": true
                    },
                    {
                        "id": 1357,
                        "content": "<p>AWS CodeBuild</p>",
                        "isValid": false
                    },
                    {
                        "id": 1358,
                        "content": "<p>Amazon Versioned S3 Bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 1359,
                        "content": "<p>AWS CodePipeline</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 333,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.030Z",
                "updatedAt": "2023-09-07T08:39:31.030Z",
                "content": "<p>An Auto Scaling group has a maximum capacity of 3, a current capacity of 2, and a scaling policy that adds 3 instances.</p>\n\n<p>When executing this scaling policy, what is the expected outcome?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>A scaling policy instructs Amazon EC2 Auto Scaling to track a specific CloudWatch metric, and it defines what action to take when the associated CloudWatch alarm is in ALARM.</p>\n\n<p>When a scaling policy is executed, if the capacity calculation produces a number outside of the minimum and maximum size range of the group, Amazon EC2 Auto Scaling ensures that the new capacity never goes outside of the minimum and maximum size limits.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a></p>\n\n<p><strong>Amazon EC2 Auto Scaling adds only 1 instance to the group</strong></p>\n\n<p>For the given use-case, Amazon EC2 Auto Scaling adds only 1 instance to the group to prevent the group from exceeding its maximum size.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon EC2 Auto Scaling adds 3 instances to the group</strong> - This is an incorrect statement. Auto Scaling ensures that the new capacity never goes outside of the minimum and maximum size limits.</p>\n\n<p><strong>Amazon EC2 Auto Scaling adds 3 instances to the group and scales down 2 of those instances eventually</strong> - This is an incorrect statement. Adding the instances initially and immediately downsizing them is impractical.</p>\n\n<p><strong>Amazon EC2 Auto Scaling does not add any instances to the group, but suggests changing the scaling policy to add one instance</strong> - This option has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a></p>\n",
                "options": [
                    {
                        "id": 1360,
                        "content": "<p>Amazon EC2 Auto Scaling adds 3 instances to the group</p>",
                        "isValid": false
                    },
                    {
                        "id": 1361,
                        "content": "<p>Amazon EC2 Auto Scaling adds 3 instances to the group and scales down 2 of those instances eventually</p>",
                        "isValid": false
                    },
                    {
                        "id": 1362,
                        "content": "<p>Amazon EC2 Auto Scaling adds only 1 instance to the group</p>",
                        "isValid": true
                    },
                    {
                        "id": 1363,
                        "content": "<p>Amazon EC2 Auto Scaling does not add any instances to the group, but suggests changing the scaling policy to add one instance</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 334,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.099Z",
                "updatedAt": "2023-09-07T08:39:31.099Z",
                "content": "<p>A serverless application built on AWS processes customer orders 24/7 using an AWS Lambda function and communicates with an external vendor's HTTP API for payment processing. The development team wants to notify the support team in near real-time using an existing Amazon Simple Notification Service (Amazon SNS) topic, but only when the external API error rate exceeds 5% of the total transactions processed in an hour.</p>\n\n<p>As an AWS Certified Developer Associate, which option will you suggest as the most efficient solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure and push high-resolution custom metrics to CloudWatch that record the failures of the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate</strong></p>\n\n<p>You can publish your own metrics, known as custom metrics, to CloudWatch using the AWS CLI or an API.</p>\n\n<p>Each metric is one of the following:</p>\n\n<p>Standard resolution, with data having a one-minute granularity</p>\n\n<p>High resolution, with data at a granularity of one second</p>\n\n<p>Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p>\n\n<p>High-resolution metrics can give you more immediate insight into your application's sub-minute activity. Keep in mind that every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a high-resolution metric can lead to higher charges.</p>\n\n<p>You can create metric and composite alarms in Amazon CloudWatch. For the given use case, you can set up a CloudWatch metric alarm that watches the custom metric that captures the API errors and then triggers the alarm when the API error rate exceeds the 5% threshold. The alarm then sends a notification via the existing SNS topic.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure CloudWatch metrics with detailed monitoring for the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate</strong> - CloudWatch provides two categories of monitoring: basic monitoring and detailed monitoring. Detailed monitoring options differ based on the services that offer it. For example, Amazon EC2 detailed monitoring provides more frequent metrics, published at one-minute intervals, instead of the five-minute intervals used in Amazon EC2 basic monitoring. Detailed monitoring is offered by only some services. As explained above, you need to use custom metrics to capture data for the external payment processing API calls since detailed monitoring for the standard CloudWatch metrics cannot be used for this scenario.</p>\n\n<p><strong>Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Logs Insights to query the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Logs Insights on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate</strong> - CloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you more efficiently and effectively respond to operational issues. This option is not the right fit for the given use case since Lambda cannot monitor the output of the CloudWatch Logs Insights on a real-time basis since it is being invoked on a schedule. Also, it is not an efficient solution since Lambda will need significant custom code to parse and compute the external API error rate from the CloudWatch Logs Insights data.</p>\n\n<p><strong>Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Metric Filter to look at the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Metric Filter on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate</strong> - You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. This option is not the best fit for the given use case since Lambda cannot monitor the output of the CloudWatch Metric Filter on a real-time basis since it is being invoked on a schedule. Also, it is not an efficient solution since Lambda will need significant custom code to parse and compute the external API error rate from the CloudWatch Metric Filter data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-push-custom-metrics/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-push-custom-metrics/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n",
                "options": [
                    {
                        "id": 1364,
                        "content": "<p>Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Metric Filter to look at the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Metric Filter on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate</p>",
                        "isValid": false
                    },
                    {
                        "id": 1365,
                        "content": "<p>Configure CloudWatch metrics with detailed monitoring for the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate</p>",
                        "isValid": false
                    },
                    {
                        "id": 1366,
                        "content": "<p>Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Logs Insights to query the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Logs Insights on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate</p>",
                        "isValid": false
                    },
                    {
                        "id": 1367,
                        "content": "<p>Configure and push high-resolution custom metrics to CloudWatch that record the failures of the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 335,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.172Z",
                "updatedAt": "2023-09-07T08:39:31.172Z",
                "content": "<p>While defining a business workflow as state machine on AWS Step Functions, a developer has configured several states.</p>\n\n<p>Which of the following would you identify as the state that represents a single unit of work performed by a state machine?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<pre><code>\"HelloWorld\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:HelloFunction\",\n  \"Next\": \"AfterHelloWorldState\",\n  \"Comment\": \"Run the HelloWorld Lambda function\"\n}\n</code></pre>\n\n<p>A Task state (\"Type\": \"Task\") represents a single unit of work performed by a state machine.</p>\n\n<p>All work in your state machine is done by tasks. A task performs work by using an activity or an AWS Lambda function, or by passing parameters to the API actions of other services.</p>\n\n<p>AWS Step Functions can invoke Lambda functions directly from a task state. A Lambda function is a cloud-native task that runs on AWS Lambda. You can write Lambda functions in a variety of programming languages, using the AWS Management Console or by uploading code to Lambda.</p>\n\n<p>Incorrect options:</p>\n\n<pre><code>\"wait_until\" : {\n  \"Type\": \"Wait\",\n  \"Timestamp\": \"2016-03-14T01:59:00Z\",\n  \"Next\": \"NextState\"\n}\n</code></pre>\n\n<ul>\n<li>A Wait state (\"Type\": \"Wait\") delays the state machine from continuing for a specified time.</li>\n</ul>\n\n<pre><code>\"No-op\": {\n  \"Type\": \"Task\",\n  \"Result\": {\n    \"x-datum\": 0.381018,\n    \"y-datum\": 622.2269926397355\n  },\n  \"ResultPath\": \"$.coords\",\n  \"Next\": \"End\"\n}\n</code></pre>\n\n<ul>\n<li><code>Resource</code> field is a required parameter for <code>Task</code> state. This definition is not of a <code>Task</code> but of type <code>Pass</code>.</li>\n</ul>\n\n<pre><code>\"FailState\": {\n  \"Type\": \"Fail\",\n  \"Cause\": \"Invalid response.\",\n  \"Error\": \"ErrorA\"\n}\n</code></pre>\n\n<ul>\n<li>A Fail state (\"Type\": \"Fail\") stops the execution of the state machine and marks it as a failure unless it is caught by a Catch block. Because Fail states always exit the state machine, they have no Next field and don't require an End field.</li>\n</ul>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html\">https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html</a></p>\n",
                "options": [
                    {
                        "id": 1368,
                        "content": "<pre><code>\"No-op\": {\n  \"Type\": \"Task\",\n  \"Result\": {\n    \"x-datum\": 0.381018,\n    \"y-datum\": 622.2269926397355\n  },\n  \"ResultPath\": \"$.coords\",\n  \"Next\": \"End\"\n}\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 1369,
                        "content": "<pre><code>\"FailState\": {\n  \"Type\": \"Fail\",\n  \"Cause\": \"Invalid response.\",\n  \"Error\": \"ErrorA\"\n}\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 1370,
                        "content": "<pre><code>\"wait_until\" : {\n  \"Type\": \"Wait\",\n  \"Timestamp\": \"2016-03-14T01:59:00Z\",\n  \"Next\": \"NextState\"\n}\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 1371,
                        "content": "<pre><code>\"HelloWorld\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:HelloFunction\",\n  \"Next\": \"AfterHelloWorldState\",\n  \"Comment\": \"Run the HelloWorld Lambda function\"\n}\n</code></pre>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 336,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.240Z",
                "updatedAt": "2023-09-07T08:39:31.240Z",
                "content": "<p>A company has a cloud system in AWS with components that send and receive messages using SQS queues. While reviewing the system you see that it processes a lot of information and would like to be aware of any limits of the system.</p>\n\n<p>Which of the following represents the maximum number of messages that can be stored in an SQS queue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>\"no limit\": There are no message limits for storing in SQS, but 'in-flight messages' do have limits. Make sure to delete messages after you have processed them. There can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue).</p>\n\n<p>Incorrect options:</p>\n\n<p>\"10000\"</p>\n\n<p>\"100000\"</p>\n\n<p>\"10000000\"</p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-limits.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-limits.html</a></p>\n",
                "options": [
                    {
                        "id": 1372,
                        "content": "<p>10000</p>",
                        "isValid": false
                    },
                    {
                        "id": 1373,
                        "content": "<p>10000000</p>",
                        "isValid": false
                    },
                    {
                        "id": 1374,
                        "content": "<p>100000</p>",
                        "isValid": false
                    },
                    {
                        "id": 1375,
                        "content": "<p>no limit</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 337,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.315Z",
                "updatedAt": "2023-09-07T08:39:31.315Z",
                "content": "<p>The development team at an analytics company is using SQS queues for decoupling the various components of application architecture. As the consumers need additional time to process SQS messages, the development team wants to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to the development team?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use delay queues to postpone the delivery of new messages to the queue for a few seconds</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Delay queues let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use FIFO queues to postpone the delivery of new messages to the queue for a few seconds</strong> - SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. You cannot use FIFO queues to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p><strong>Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds</strong> - Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. You cannot use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p><strong>Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds</strong> - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a></p>\n",
                "options": [
                    {
                        "id": 1376,
                        "content": "<p>Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds</p>",
                        "isValid": false
                    },
                    {
                        "id": 1377,
                        "content": "<p>Use FIFO queues to postpone the delivery of new messages to the queue for a few seconds</p>",
                        "isValid": false
                    },
                    {
                        "id": 1378,
                        "content": "<p>Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds</p>",
                        "isValid": false
                    },
                    {
                        "id": 1379,
                        "content": "<p>Use delay queues to postpone the delivery of new messages to the queue for a few seconds</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 338,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.389Z",
                "updatedAt": "2023-09-07T08:39:31.389Z",
                "content": "<p>A junior developer has been asked to configure access to an Amazon EC2 instance hosting a web application. The developer has configured a new security group to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules. A custom Network Access Control List (NACL) connected with the instance's subnet is configured to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules.</p>\n\n<p>Which of the following solutions would you suggest if the EC2 instance needs to accept and respond to requests from the internet?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port range</strong></p>\n\n<p>Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic. By default, each custom Network ACL denies all inbound and outbound traffic until you add rules.</p>\n\n<p>To enable the connection to a service running on an instance, the associated network ACL must allow both:\n1. Inbound traffic on the port that the service is listening on\n2. Outbound traffic to ephemeral ports</p>\n\n<p>When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port.</p>\n\n<p>The designated ephemeral port becomes the destination port for return traffic from the service. Outbound traffic to the ephemeral port must be allowed in the network ACL.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The configuration is complete on the EC2 instance for accepting and responding to requests</strong> - As explained above, this is an incorrect statement.</p>\n\n<p><strong>An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port</strong> - Security groups are stateful. Therefore you don't need a rule that allows responses to inbound traffic.</p>\n\n<p><em>Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway</em>* - Security Groups are stateful. Hence, return traffic is automatically allowed, so there is no need to configure an outbound rule on the security group.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/\">https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports</a></p>\n",
                "options": [
                    {
                        "id": 1380,
                        "content": "<p>Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 1381,
                        "content": "<p>The configuration is complete on the EC2 instance for accepting and responding to requests</p>",
                        "isValid": false
                    },
                    {
                        "id": 1382,
                        "content": "<p>An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port range</p>",
                        "isValid": true
                    },
                    {
                        "id": 1383,
                        "content": "<p>An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 339,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.456Z",
                "updatedAt": "2023-09-07T08:39:31.456Z",
                "content": "<p>You are a development team lead setting permissions for other IAM users with limited permissions. On the AWS Management Console, you created a dev group where new developers will be added, and on your workstation, you configured a developer profile. You would like to test that this user cannot terminate instances.</p>\n\n<p>Which of the following options would you execute?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use the AWS CLI --dry-run option</strong>: The --dry-run option checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation, otherwise, it is UnauthorizedOperation.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the AWS CLI --test option</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p><strong>Retrieve the policy using the EC2 metadata service and use the IAM policy simulator</strong> - EC2 metadata service is used to retrieve dynamic information such as instance-id, local-hostname, public-hostname. This cannot be used to check whether you have the required permissions for the action.</p>\n\n<p><strong>Using the CLI, create a dummy EC2 and delete it using another CLI call</strong> - That would not work as the current EC2 may have permissions that the dummy instance does not have. If permissions were the same it can work but it's not as elegant as using the dry-run option.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html\">https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/ec2/terminate-instances.html\">https://docs.aws.amazon.com/cli/latest/reference/ec2/terminate-instances.html</a></p>\n",
                "options": [
                    {
                        "id": 1384,
                        "content": "<p>Retrieve the policy using the EC2 metadata service and use the IAM policy simulator</p>",
                        "isValid": false
                    },
                    {
                        "id": 1385,
                        "content": "<p>Use the AWS CLI --dry-run option</p>",
                        "isValid": true
                    },
                    {
                        "id": 1386,
                        "content": "<p>Use the AWS CLI --test option</p>",
                        "isValid": false
                    },
                    {
                        "id": 1387,
                        "content": "<p>Using the CLI, create a dummy EC2 and delete it using another CLI call</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 340,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.526Z",
                "updatedAt": "2023-09-07T08:39:31.526Z",
                "content": "<p>A company uses Amazon Simple Email Service (SES) to cost-effectively send susbscription emails to the customers. Intermittently, the SES service throws the error: <code>Throttling â€“ Maximum sending rate exceeded</code>.</p>\n\n<p>As a developer associate, which of the following would you recommend to fix this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again</strong> - A â€œThrottling â€“ Maximum sending rate exceededâ€ error is retriable. This error is different than other errors returned by Amazon SES. A request rejected with a â€œThrottlingâ€ error can be retried at a later time and is likely to succeed.</p>\n\n<p>Retries are â€œselfish.â€ In other words, when a client retries, it spends more of the server's time to get a higher chance of success. Where failures are rare or transient, that's not a problem. This is because the overall number of retried requests is small, and the tradeoff of increasing apparent availability works well. When failures are caused by overload, retries that increase load can make matters significantly worse. They can even delay recovery by keeping the load high long after the original issue is resolved.</p>\n\n<p>The preferred solution is to use a backoff. Instead of retrying immediately and aggressively, the client waits some amount of time between tries. The most common pattern is an exponential backoff, where the wait time is increased exponentially after every attempt.</p>\n\n<p>A variety of factors can affect your send rate, e.g. message size, network performance or Amazon SES availability. The advantage of the exponential backoff approach is that your application will self-tune and it will call Amazon SES at close to the maximum allowed rate.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Timeout mechanism for each request made to the SES service</strong> - Requests are configured to timeout if they do not complete successfully in a given time. This helps free up the database, application and any other resource that could potentially keep on waiting to eventually succeed. But, if errors are caused by load, retries can be ineffective if all clients retry at the same time. Throttling error signifies that load is high on SES and it does not make sense to keep retrying.</p>\n\n<p><strong>Raise a service request with Amazon to increase the throttling limit for the SES API</strong> - If throttling error is persistent, then it indicates a high load on the system consistently and increasing the throttling limit will be the right solution for the problem. But, the error is only intermittent here, signifying that decreasing the rate of requests will handle the error.</p>\n\n<p><strong>Implement retry mechanism for all 4xx errors to avoid throttling error</strong> - 4xx status codes indicate that there was a problem with the client request. Common client request errors include providing invalid credentials and omitting required parameters. When you get a 4xx error, you need to correct the problem and resubmit a properly formed client request. Throttling is a server error and not a client error, hence retry on 4xx errors does not make sense here.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/\">https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/messaging-and-targeting/how-to-handle-a-throttling-maximum-sending-rate-exceeded-error/\">https://aws.amazon.com/blogs/messaging-and-targeting/how-to-handle-a-throttling-maximum-sending-rate-exceeded-error/</a></p>\n",
                "options": [
                    {
                        "id": 1388,
                        "content": "<p>Raise a service request with Amazon to increase the throttling limit for the SES API</p>",
                        "isValid": false
                    },
                    {
                        "id": 1389,
                        "content": "<p>Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again</p>",
                        "isValid": true
                    },
                    {
                        "id": 1390,
                        "content": "<p>Configure Timeout mechanism for each request made to the SES service</p>",
                        "isValid": false
                    },
                    {
                        "id": 1391,
                        "content": "<p>Implement retry mechanism for all 4xx errors to avoid throttling error</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 341,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.598Z",
                "updatedAt": "2023-09-07T08:39:31.598Z",
                "content": "<p>A developer working with EC2 Windows instance has installed Kinesis Agent for Windows to stream JSON-formatted log files to Amazon Simple Storage Service (S3) via Amazon Kinesis Data Firehose. The developer wants to understand the sink type capabilities of Kinesis Firehose.</p>\n\n<p>Which of the following sink types is NOT supported by Kinesis Firehose.</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon Elasticsearch Service (Amazon ES), and Splunk. With Kinesis Data Firehose, you don't need to write applications or manage resources. You configure your data producers to send data to Kinesis Data Firehose, and it automatically delivers the data to the destination that you specified.</p>\n\n<p><strong>Amazon ElastiCache with Amazon S3 as backup</strong> - Amazon ElastiCache is a fully managed in-memory data store, compatible with Redis or Memcached. ElastiCache is NOT a supported destination for Amazon Kinesis Data Firehose.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Elasticsearch Service (Amazon ES) with optionally backing up data to Amazon S3</strong> - Amazon ES is a supported destination type for Kinesis Firehose. Streaming data is delivered to your Amazon ES cluster, and can optionally be backed up to your S3 bucket concurrently.</p>\n\n<p>Data Flow for ES:\n<img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-es.png\">\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n\n<p><strong>Amazon Simple Storage Service (Amazon S3) as a direct Firehose destination</strong> - For Amazon S3 destinations, streaming data is delivered to your S3 bucket. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket.</p>\n\n<p>Data Flow for S3:\n<img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-s3.png\">\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n\n<p><strong>Amazon Redshift with Amazon S3</strong> - For Amazon Redshift destinations, streaming data is delivered to your S3 bucket first. Kinesis Data Firehose then issues an Amazon Redshift COPY command to load data from your S3 bucket to your Amazon Redshift cluster. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket.</p>\n\n<p>Data Flow for Redshift:\n<img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-rs.png\">\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n",
                "options": [
                    {
                        "id": 1392,
                        "content": "<p>Amazon ElastiCache with Amazon S3 as backup</p>",
                        "isValid": true
                    },
                    {
                        "id": 1393,
                        "content": "<p>Amazon Redshift with Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 1394,
                        "content": "<p>Amazon Elasticsearch Service (Amazon ES) with optionally backing up data to Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 1395,
                        "content": "<p>Amazon Simple Storage Service (Amazon S3) as a direct Firehose destination</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 342,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.671Z",
                "updatedAt": "2023-09-07T08:39:31.671Z",
                "content": "<p>As an AWS Certified Developer Associate, you have been hired to work with the development team at a company to create a REST API using the serverless architecture.</p>\n\n<p>Which of the following solutions will you choose to move the company to the serverless architecture paradigm?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>API Gateway exposing Lambda Functionality</strong></p>\n\n<p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\">\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p>API Gateway can expose Lambda functionality through RESTful APIs. Both are serverless options offered by AWS and hence the right choice for this scenario, considering all the functionality they offer.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Fargate with Lambda at the front</strong> - Lambda cannot directly handle RESTful API requests. You can invoke a Lambda function over HTTPS by defining a custom RESTful API using Amazon API Gateway. So, Fargate with Lambda as the front-facing service is a wrong combination, though both Fargate and Lambda are serverless.</p>\n\n<p><strong>Public-facing Application Load Balancer with ECS on Amazon EC2</strong> - ECS on Amazon EC2 does not come under serverless and hence cannot be considered for this use case.</p>\n\n<p><strong>Route 53 with EC2 as backend</strong> - Amazon EC2 is not a serverless service and hence cannot be considered for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/serverless/\">https://aws.amazon.com/serverless/</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n",
                "options": [
                    {
                        "id": 1396,
                        "content": "<p>Fargate with Lambda at the front</p>",
                        "isValid": false
                    },
                    {
                        "id": 1397,
                        "content": "<p>Route 53 with EC2 as backend</p>",
                        "isValid": false
                    },
                    {
                        "id": 1398,
                        "content": "<p>API Gateway exposing Lambda Functionality</p>",
                        "isValid": true
                    },
                    {
                        "id": 1399,
                        "content": "<p>Public-facing Application Load Balancer with ECS on Amazon EC2</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 343,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.743Z",
                "updatedAt": "2023-09-07T08:39:31.743Z",
                "content": "<p>A development team is building a game where players can buy items with virtual coins. For every virtual coin bought by a user, both the players table as well as the items table in DynamodDB need to be updated simultaneously using an all-or-nothing operation.</p>\n\n<p>As a developer associate, how will you implement this functionality?</p>",
                "answerExplanation": "<p>Correct option:\n<strong>Use <code>TransactWriteItems</code> API of DynamoDB Transactions</strong></p>\n\n<p>With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing <code>TransactWriteItems</code> or <code>TransactGetItems</code> operation.</p>\n\n<p><code>TransactWriteItems</code> is a synchronous and idempotent write operation that groups up to 25 write actions in a single all-or-nothing operation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same Region. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions are completed atomically so that either all of them succeed or none of them succeeds.</p>\n\n<p>You can optionally include a client token when you make a TransactWriteItems call to ensure that the request is idempotent. Making your transactions idempotent helps prevent application errors if the same operation is submitted multiple times due to a connection time-out or other connectivity issue.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use <code>BatchWriteItem</code> API to update multiple tables simultaneously</strong> - A <code>TransactWriteItems</code> operation differs from a <code>BatchWriteItem</code> operation in that all the actions it contains must be completed successfully, or no changes are made at all. With a <code>BatchWriteItem</code> operation, it is possible that only some of the actions in the batch succeed while the others do not.</p>\n\n<p><strong>Capture the transactions in the players table using DynamoDB streams and then sync with the items table</strong></p>\n\n<p><strong>Capture the transactions in the items table using DynamoDB streams and then sync with the players table</strong></p>\n\n<p>Many applications benefit from capturing changes to items stored in a DynamoDB table, at the point in time when such changes occur. DynamoDB supports streaming of item-level change data capture records in near-real-time. You can build applications that consume these streams and take action based on the contents. DynamoDB streams cannot be used to capture transactions in DynamoDB, therefore both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html#transaction-apis-txwriteitems\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html#transaction-apis-txwriteitems</a></p>\n",
                "options": [
                    {
                        "id": 1400,
                        "content": "<p>Use <code>TransactWriteItems</code> API of DynamoDB Transactions</p>",
                        "isValid": true
                    },
                    {
                        "id": 1401,
                        "content": "<p>Capture the transactions in the items table using DynamoDB streams and then sync with the players table</p>",
                        "isValid": false
                    },
                    {
                        "id": 1402,
                        "content": "<p>Capture the transactions in the players table using DynamoDB streams and then sync with the items table</p>",
                        "isValid": false
                    },
                    {
                        "id": 1403,
                        "content": "<p>Use <code>BatchWriteItem</code> API to update multiple tables simultaneously</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 344,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.812Z",
                "updatedAt": "2023-09-07T08:39:31.812Z",
                "content": "<p>You are a developer working on a web application written in Java and would like to use AWS Elastic Beanstalk for deployment because it would handle deployment, capacity provisioning, load balancing, auto-scaling, and application health monitoring. In the past, you connected to your provisioned instances through SSH to issue configuration commands. Now, you would like a configuration mechanism that automatically applies settings for you.</p>\n\n<p>Which of the following options would help do this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Include config files in .ebextensions/ at the root of your source code</strong></p>\n\n<p>The option_settings section of a configuration file defines values for configuration options. Configuration options let you configure your Elastic Beanstalk environment, the AWS resources in it, and the software that runs your application. Configuration files are only one of several ways to set configuration options.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q27-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy a CloudFormation wrapper</strong> - This is a made-up option. This has been added as a distractor.</p>\n\n<p><strong>Use SSM parameter store as an input to your Elastic Beanstalk Configurations</strong> - SSM parameter is still not supported for Elastic Beanstalk. So this option is incorrect.</p>\n\n<p><strong>Use an AWS Lambda hook</strong> - Lambda functions are not the best-fit to trigger these configuration changes as it would involve significant development effort.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions-optionsettings.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions-optionsettings.html</a></p>\n",
                "options": [
                    {
                        "id": 1404,
                        "content": "<p>Deploy a CloudFormation wrapper</p>",
                        "isValid": false
                    },
                    {
                        "id": 1405,
                        "content": "<p>Use SSM parameter store as an input to your Elastic Beanstalk Configurations</p>",
                        "isValid": false
                    },
                    {
                        "id": 1406,
                        "content": "<p>Use an AWS Lambda hook</p>",
                        "isValid": false
                    },
                    {
                        "id": 1407,
                        "content": "<p>Include config files in .ebextensions/ at the root of your source code</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 345,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.885Z",
                "updatedAt": "2023-09-07T08:39:31.885Z",
                "content": "<p>The technology team at an investment bank uses DynamoDB to facilitate high-frequency trading where multiple trades can try and update an item at the same time.</p>\n\n<p>Which of the following actions would make sure that only the last updated value of any item is used in the application?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use ConsistentRead = true while doing GetItem operation for any item</strong></p>\n\n<p>DynamoDB supports eventually consistent and strongly consistent reads.</p>\n\n<p>Eventually Consistent Reads</p>\n\n<p>When you read data from a DynamoDB table, the response might not reflect the results of a recently completed write operation. The response might include some stale data. If you repeat your read request after a short time, the response should return the latest data.</p>\n\n<p>Strongly Consistent Reads</p>\n\n<p>When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful.</p>\n\n<p>DynamoDB uses eventually consistent reads by default. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. As per the given use-case, to make sure that only the last updated value of any item is used in the application, you should use strongly consistent reads by setting ConsistentRead = true for GetItem operation.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q9-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use ConsistentRead = true while doing UpdateItem operation for any item</strong></p>\n\n<p><strong>Use ConsistentRead = true while doing PutItem operation for any item</strong></p>\n\n<p><strong>Use ConsistentRead = false while doing PutItem operation for any item</strong></p>\n\n<p>As mentioned in the explanation above, strongly consistent reads apply only while using the read operations (such as GetItem, Query, and Scan). So these three options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html</a></p>\n",
                "options": [
                    {
                        "id": 1408,
                        "content": "<p>Use ConsistentRead = false while doing PutItem operation for any item</p>",
                        "isValid": false
                    },
                    {
                        "id": 1409,
                        "content": "<p>Use ConsistentRead = true while doing UpdateItem operation for any item</p>",
                        "isValid": false
                    },
                    {
                        "id": 1410,
                        "content": "<p>Use ConsistentRead = true while doing PutItem operation for any item</p>",
                        "isValid": false
                    },
                    {
                        "id": 1411,
                        "content": "<p>Use ConsistentRead = true while doing GetItem operation for any item</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 346,
            "attributes": {
                "createdAt": "2023-09-07T08:39:31.957Z",
                "updatedAt": "2023-09-07T08:39:31.957Z",
                "content": "<p>A pharmaceutical company uses Amazon EC2 instances for application hosting and Amazon CloudFront for content delivery. A new research paper with critical findings has to be shared with a research team that is spread across the world.</p>\n\n<p>Which of the following represents the most optimal solution to address this requirement without compromising the security of the content?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use CloudFront signed URL feature to control access to the file</strong></p>\n\n<p>A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content.</p>\n\n<p>Here's an overview of how you configure CloudFront for signed URLs and how CloudFront responds when a user uses a signed URL to request a file:</p>\n\n<ol>\n<li><p>In your CloudFront distribution, specify one or more trusted key groups, which contain the public keys that CloudFront can use to verify the URL signature. You use the corresponding private keys to sign the URLs.</p></li>\n<li><p>Develop your application to determine whether a user should have access to your content and to create signed URLs for the files or parts of your application that you want to restrict access to.</p></li>\n<li><p>A user requests a file for which you want to require signed URLs. Your application verifies that the user is entitled to access the file: they've signed in, they've paid for access to the content, or they've met some other requirement for access.</p></li>\n<li><p>Your application creates and returns a signed URL to the user. The signed URL allows the user to download or stream the content.</p></li>\n</ol>\n\n<p>This step is automatic; the user usually doesn't have to do anything additional to access the content. For example, if a user is accessing your content in a web browser, your application returns the signed URL to the browser. The browser immediately uses the signed URL to access the file in the CloudFront edge cache without any intervention from the user.</p>\n\n<ol>\n<li>CloudFront uses the public key to validate the signature and confirm that the URL hasn't been tampered with. If the signature is invalid, the request is rejected. If the request meets the requirements in the policy statement, CloudFront does the standard operations: determines whether the file is already in the edge cache, forwards the request to the origin if necessary, and returns the file to the user.</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudFront signed cookies feature to control access to the file</strong> - CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website. Our requirement has only one file that needs to be shared and hence signed URL is the optimal solution.</p>\n\n<p>Signed URLs take precedence over signed cookies. If you use both signed URLs and signed cookies to control access to the same files and a viewer uses a signed URL to request a file, CloudFront determines whether to return the file to the viewer based only on the signed URL.</p>\n\n<p><strong>Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront</strong> - AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to CloudFront, and lets you control access to your content. Based on conditions that you specify, such as the values of query strings or the IP addresses that requests originate from, CloudFront responds to requests either with the requested content or with an HTTP status code 403 (Forbidden). A firewall is optimal for broader use cases than restricted access to a single file.</p>\n\n<p><strong>Using CloudFront's Field-Level Encryption to help protect sensitive data</strong> - CloudFront's field-level encryption further encrypts sensitive data in an HTTPS form using field-specific encryption keys (which you supply) before a POST request is forwarded to your origin. This ensures that sensitive data can only be decrypted and viewed by certain components or services in your application stack. This feature is not useful for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/12/introducing-field-level-encryption-on-amazon-cloudfront/\">https://aws.amazon.com/about-aws/whats-new/2017/12/introducing-field-level-encryption-on-amazon-cloudfront/</a></p>\n",
                "options": [
                    {
                        "id": 1412,
                        "content": "<p>Use CloudFront signed URL feature to control access to the file</p>",
                        "isValid": true
                    },
                    {
                        "id": 1413,
                        "content": "<p>Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront</p>",
                        "isValid": false
                    },
                    {
                        "id": 1414,
                        "content": "<p>Use CloudFront signed cookies feature to control access to the file</p>",
                        "isValid": false
                    },
                    {
                        "id": 1415,
                        "content": "<p>Using CloudFront's Field-Level Encryption to help protect sensitive data</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 347,
            "attributes": {
                "createdAt": "2023-09-07T08:39:32.024Z",
                "updatedAt": "2023-09-07T08:39:32.024Z",
                "content": "<p>An e-commerce company uses AWS CloudFormation to implement Infrastructure as Code for the entire organization. Maintaining resources as stacks with CloudFormation has greatly reduced the management effort needed to manage and maintain the resources. However, a few teams have been complaining of failing stack updates owing to out-of-band fixes running on the stack resources.</p>\n\n<p>Which of the following is the best solution that can help in keeping the CloudFormation stack and its resources in sync with each other?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Drift Detection feature of CloudFormation</strong></p>\n\n<p>Drift detection enables you to detect whether a stack's actual configuration differs, or has drifted, from its expected configuration. Use CloudFormation to detect drift on an entire stack, or individual resources within the stack. A resource is considered to have drifted if any of its actual property values differ from the expected property values. This includes if the property or resource has been deleted. A stack is considered to have drifted if one or more of its resources have drifted.</p>\n\n<p>To determine whether a resource has drifted, CloudFormation determines the expected resource property values, as defined in the stack template and any values specified as template parameters. CloudFormation then compares those expected values with the actual values of those resource properties as they currently exist in the stack. A resource is considered to have drifted if one or more of its properties have been deleted, or had their value changed.</p>\n\n<p>You can then take corrective action so that your stack resources are again in sync with their definitions in the stack template, such as updating the drifted resources directly so that they agree with their template definition. Resolving drift helps to ensure configuration consistency and successful stack operations.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudFormation in Elastic Beanstalk environment to reduce direct changes to CloudFormation resources</strong> - Elastic Beanstalk environment provides full access to the resources created. So, it is possible to edit the resources and hence does not solve the issue mentioned for the given use case.</p>\n\n<p><strong>Use Tag feature of CloudFormation to monitor the changes happening on specific resources</strong> - Tags help you identify and categorize the resources created as part of CloudFormation template. This feature is not helpful for the given use case.</p>\n\n<p><strong>Use Change Sets feature of CloudFormation</strong> - When you need to update a stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. Change sets allow you to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. Change sets are not useful for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/detect-drift-stack.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/detect-drift-stack.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n",
                "options": [
                    {
                        "id": 1416,
                        "content": "<p>Use Tag feature of CloudFormation to monitor the changes happening on specific resources</p>",
                        "isValid": false
                    },
                    {
                        "id": 1417,
                        "content": "<p>Use Drift Detection feature of CloudFormation</p>",
                        "isValid": true
                    },
                    {
                        "id": 1418,
                        "content": "<p>Use CloudFormation in Elastic Beanstalk environment to reduce direct changes to CloudFormation resources</p>",
                        "isValid": false
                    },
                    {
                        "id": 1419,
                        "content": "<p>Use Change Sets feature of CloudFormation</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 348,
            "attributes": {
                "createdAt": "2023-09-07T08:39:32.105Z",
                "updatedAt": "2023-09-07T08:39:32.105Z",
                "content": "<p>A company runs its flagship application on a fleet of Amazon EC2 instances. After misplacing a couple of private keys from the SSH key pairs, they have decided to re-use their SSH key pairs for the different instances across AWS Regions.</p>\n\n<p>As a Developer Associate, which of the following would you recommend to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Generate a public SSH key from a private SSH key. Then, import the key into each of your AWS Regions</strong></p>\n\n<p>Here is the correct way of reusing SSH keys in your AWS Regions:</p>\n\n<ol>\n<li><p>Generate a public SSH key (.pub) file from the private SSH key (.pem) file.</p></li>\n<li><p>Set the AWS Region you wish to import to.</p></li>\n<li><p>Import the public SSH key into the new Region.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>It is not possible to reuse SSH key pairs across AWS Regions</strong> - As explained above, it is possible to reuse with manual import.</p>\n\n<p><strong>Store the public and private SSH key pair in AWS Trusted Advisor and access it across AWS Regions</strong> - AWS Trusted Advisor is an application that draws upon best practices learned from AWS' aggregated operational history of serving hundreds of thousands of AWS customers. Trusted Advisor inspects your AWS environment and makes recommendations for saving money, improving system performance, or closing security gaps. It does not store key pair credentials.</p>\n\n<p><strong>Encrypt the private SSH key and store it in the S3 bucket to be accessed from any AWS Region</strong> - Storing private key to Amazon S3 is possible. But, this will not make the key accessible for all AWS Regions, as is the need in the current use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html</a></p>\n",
                "options": [
                    {
                        "id": 1420,
                        "content": "<p>It is not possible to reuse SSH key pairs across AWS Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 1421,
                        "content": "<p>Encrypt the private SSH key and store it in the S3 bucket to be accessed from any AWS Region</p>",
                        "isValid": false
                    },
                    {
                        "id": 1422,
                        "content": "<p>Store the public and private SSH key pair in AWS Trusted Advisor and access it across AWS Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 1423,
                        "content": "<p>Generate a public SSH key from a private SSH key. Then, import the key into each of your AWS Regions</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 349,
            "attributes": {
                "createdAt": "2023-09-07T08:39:32.195Z",
                "updatedAt": "2023-09-07T08:39:32.195Z",
                "content": "<p>As an AWS certified developer associate, you are working on an AWS CloudFormation template that will create resources for a company's cloud infrastructure. Your template is composed of three stacks which are Stack-A, Stack-B, and Stack-C. Stack-A will provision a VPC, a security group, and subnets for public web applications that will be referenced in Stack-B and Stack-C.</p>\n\n<p>After running the stacks you decide to delete them, in which order should you do it?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p><strong>Stack B, then Stack C, then Stack A</strong></p>\n\n<p>All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you must delete Stack B as well as Stack C, before you delete Stack A.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Stack A, then Stack B, then Stack C</strong> - All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you cannot delete Stack A first because that's being referenced in the other Stacks.</p>\n\n<p><strong>Stack A, Stack C then Stack B</strong> - All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you cannot delete Stack A first because that's being referenced in the other Stacks.</p>\n\n<p><strong>Stack C then Stack A then Stack B</strong> - Stack C is fine but you should delete Stack B before Stack A because all of the imports must be removed before you can delete the exporting stack or modify the output value.</p>\n\n<p>Reference:\n<a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html</a></p>\n",
                "options": [
                    {
                        "id": 1424,
                        "content": "<p>Stack C then Stack A then Stack B</p>",
                        "isValid": false
                    },
                    {
                        "id": 1425,
                        "content": "<p>Stack B, then Stack C, then Stack A</p>",
                        "isValid": true
                    },
                    {
                        "id": 1426,
                        "content": "<p>Stack A, then Stack B, then Stack C</p>",
                        "isValid": false
                    },
                    {
                        "id": 1427,
                        "content": "<p>Stack A, Stack C then Stack B</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 350,
            "attributes": {
                "createdAt": "2023-09-07T08:39:32.276Z",
                "updatedAt": "2023-09-07T08:39:32.276Z",
                "content": "<p>Consider an application that enables users to store their mobile phone images in the cloud and supports tens of thousands of users. The application should utilize an Amazon API Gateway REST API that leverages AWS Lambda functions for photo processing while storing photo details in Amazon DynamoDB. The application should allow users to create an account, upload images, and retrieve previously uploaded images, with images ranging in size from 500 KB to 5 MB.</p>\n\n<p>How will you design the application with the least operational overhead?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</strong></p>\n\n<p>A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and SAML identity providers. Whether your users sign in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK).</p>\n\n<p>User pools provide:</p>\n\n<p>Sign-up and sign-in services.</p>\n\n<p>A built-in, customizable web UI to sign in users.</p>\n\n<p>Social sign-in with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as sign-in with SAML identity providers from your user pool.</p>\n\n<p>User directory management and user profiles.</p>\n\n<p>Security features such as multi-factor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.</p>\n\n<p>Customized workflows and user migration through AWS Lambda triggers.</p>\n\n<p>To use an Amazon Cognito user pool with your Amazon API Gateway API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header.</p>\n\n<p>For the given use case, you can use a Cognito user pool to manage user accounts and configure an Amazon Cognito user pool authorizer in API Gateway to control access to the API. You should use a Lambda function to store the actual images on S3 and the image metadata on DynamoDB. Finally, you can get the images using the Lambda function that leverages the metadata stored in DynamoDB.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Cognito identity pools to manage user accounts and set up an Amazon Cognito identity pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</strong></p>\n\n<p><strong>Use Cognito identity pools to create an IAM user for each user of the application during the sign-up process. Leverage IAM authentication in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</strong></p>\n\n<p>Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. You cannot use identity pools to manage users or to create IAM users. So both of these options are incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q29-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/\">https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/</a></p>\n\n<p><strong>Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images as well as the image metadata in a DynamoDB table. Have the Lambda function retrieve previously uploaded images from DynamoDB</strong> - You cannot use DynamoDB to store images as the maximum allowed item size is 400KB and the images range in size from 500KB to 5MB. You should also note that storing images on DynamoDB is an anti-pattern. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/\">https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/</a></p>\n",
                "options": [
                    {
                        "id": 1428,
                        "content": "<p>Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</p>",
                        "isValid": true
                    },
                    {
                        "id": 1429,
                        "content": "<p>Use Cognito identity pools to create an IAM user for each user of the application during the sign-up process. Leverage IAM authentication in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</p>",
                        "isValid": false
                    },
                    {
                        "id": 1430,
                        "content": "<p>Use Cognito identity pools to manage user accounts and set up an Amazon Cognito identity pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</p>",
                        "isValid": false
                    },
                    {
                        "id": 1431,
                        "content": "<p>Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images as well as the image metadata in a DynamoDB table. Have the Lambda function retrieve previously uploaded images from DynamoDB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 351,
            "attributes": {
                "createdAt": "2023-09-07T08:39:32.381Z",
                "updatedAt": "2023-09-07T08:39:32.381Z",
                "content": "<p>A developer needs to automate software package deployment to both Amazon EC2 instances and virtual servers running on-premises, as part of continuous integration and delivery that the business has adopted.</p>\n\n<p>Which AWS service should he use to accomplish this task?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Continuous integration is a DevOps software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run.</p>\n\n<p>Continuous delivery is a software development practice where code changes are automatically prepared for a release to production. A pillar of modern application development, continuous delivery expands upon continuous integration by deploying all code changes to a testing environment and/or a production environment after the build stage.</p>\n\n<p><strong>AWS CodeDeploy</strong> - AWS CodeDeploy is a fully managed \"deployment\" service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. This is the right choice for the current use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS CodePipeline</strong> - AWS CodePipeline is a fully managed \"continuous delivery\" service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define. This enables you to rapidly and reliably deliver features and updates. Whereas CodeDeploy is a deployment service, CodePipeline is a continuous delivery service. For our current scenario, CodeDeploy is the correct choice.</p>\n\n<p><strong>AWS CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you donâ€™t need to provision, manage, and scale your own build servers. CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.</p>\n\n<p><strong>AWS Elastic Beanstalk</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codedeploy/\">https://aws.amazon.com/codedeploy/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/\">https://aws.amazon.com/codepipeline/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codebuild/\">https://aws.amazon.com/codebuild/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n\n<p><a href=\"https://aws.amazon.com/devops/continuous-delivery/\">https://aws.amazon.com/devops/continuous-delivery/</a></p>\n\n<p><a href=\"https://aws.amazon.com/devops/continuous-integration/\">https://aws.amazon.com/devops/continuous-integration/</a></p>\n",
                "options": [
                    {
                        "id": 1432,
                        "content": "<p>AWS CodeDeploy</p>",
                        "isValid": true
                    },
                    {
                        "id": 1433,
                        "content": "<p>AWS CodePipeline</p>",
                        "isValid": false
                    },
                    {
                        "id": 1434,
                        "content": "<p>AWS Elastic Beanstalk</p>",
                        "isValid": false
                    },
                    {
                        "id": 1435,
                        "content": "<p>AWS CodeBuild</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 352,
            "attributes": {
                "createdAt": "2023-09-07T08:39:32.460Z",
                "updatedAt": "2023-09-07T08:39:32.460Z",
                "content": "<p>A developer is defining the signers that can create signed URLs for their Amazon CloudFront distributions.</p>\n\n<p>Which of the following statements should the developer consider while defining the signers? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>When you create a signer, the public key is with CloudFront and private key is used to sign a portion of URL</strong> - Each signer that you use to create CloudFront signed URLs or signed cookies must have a publicâ€“private key pair. The signer uses its private key to sign the URL or cookies, and CloudFront uses the public key to verify the signature.</p>\n\n<p>When you create signed URLs or signed cookies, you use the private key from the signerâ€™s key pair to sign a portion of the URL or the cookie. When someone requests a restricted file, CloudFront compares the signature in the URL or cookie with the unsigned URL or cookie, to verify that it hasnâ€™t been tampered with. CloudFront also verifies that the URL or cookie is valid, meaning, for example, that the expiration date and time havenâ€™t passed.</p>\n\n<p><strong>When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account</strong> - When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account.</p>\n\n<p>Whereas, with CloudFront key groups, you can associate a higher number of public keys with your CloudFront distribution, giving you more flexibility in how you use and manage the public keys. By default, you can associate up to four key groups with a single distribution, and you can have up to five public keys in a key group.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can also use AWS Identity and Access Management (IAM) permissions policies to restrict what the root user can do with CloudFront key pairs</strong> - When you use the AWS account root user to manage CloudFront key pairs, you canâ€™t restrict what the root user can do or the conditions in which it can do them. You canâ€™t apply IAM permissions policies to the root user, which is one reason why AWS best practices recommend against using the root user.</p>\n\n<p><strong>CloudFront key pairs can be created with any account that has administrative permissions and full access to CloudFront resources</strong> - CloudFront key pairs can only be created using the root user account and hence is not a best practice to create CloudFront key pairs as signers.</p>\n\n<p><strong>Both the signers (trusted key groups and CloudFront key pairs) can be managed using the CloudFront APIs</strong> - With CloudFront key groups, you can manage public keys, key groups, and trusted signers using the CloudFront API. You can use the API to automate key creation and key rotation. When you use the AWS root user, you have to use the AWS Management Console to manage CloudFront key pairs, so you canâ€™t automate the process.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html</a></p>\n",
                "options": [
                    {
                        "id": 1436,
                        "content": "<p>When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account</p>",
                        "isValid": true
                    },
                    {
                        "id": 1437,
                        "content": "<p>You can also use AWS Identity and Access Management (IAM) permissions policies to restrict what the root user can do with CloudFront key pairs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1438,
                        "content": "<p>CloudFront key pairs can be created with any account that has administrative permissions and full access to CloudFront resources</p>",
                        "isValid": false
                    },
                    {
                        "id": 1439,
                        "content": "<p>When you create a signer, the public key is with CloudFront and private key is used to sign a portion of URL</p>",
                        "isValid": true
                    },
                    {
                        "id": 1440,
                        "content": "<p>Both the signers (trusted key groups and CloudFront key pairs) can be managed using the CloudFront APIs</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 353,
            "attributes": {
                "createdAt": "2023-09-07T08:39:32.535Z",
                "updatedAt": "2023-09-07T08:39:32.535Z",
                "content": "<p>An application running on EC2 instances processes messages from an SQS queue. However, sometimes the messages are not processed and they end up in errors.  These messages need to be isolated for further processing and troubleshooting.</p>\n\n<p>Which of the following options will help achieve this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Implement a Dead-Letter Queue</strong> - Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.\nAmazon SQS does not create the dead-letter queue automatically. You must first create the queue before using it as a dead-letter queue.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the VisibilityTimeout</strong> - When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Immediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. Increasing visibility timeout will not help in troubleshooting the messages running into error or isolating them from the rest. Hence this is an incorrect option for the current use case.</p>\n\n<p><strong>Use DeleteMessage</strong> - Deletes the specified message from the specified queue. This will not help understand the reason for error or isolate messages ending with the error.</p>\n\n<p><strong>Reduce the VisibilityTimeout</strong> - As explained above, VisibilityTimeout makes sure that the message is not read by any other consumer while it is being processed by one consumer. By reducing the VisibilityTimeout, more consumers will receive the same failed message. Hence, this is an incorrect option for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n",
                "options": [
                    {
                        "id": 1441,
                        "content": "<p>Reduce the VisibilityTimeout</p>",
                        "isValid": false
                    },
                    {
                        "id": 1442,
                        "content": "<p>Increase the VisibilityTimeout</p>",
                        "isValid": false
                    },
                    {
                        "id": 1443,
                        "content": "<p>Implement a Dead-Letter Queue</p>",
                        "isValid": true
                    },
                    {
                        "id": 1444,
                        "content": "<p>Use DeleteMessage</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 354,
            "attributes": {
                "createdAt": "2023-09-07T08:39:32.610Z",
                "updatedAt": "2023-09-07T08:39:32.610Z",
                "content": "<p>A social gaming application supports the transfer of gift vouchers between users. When a user hits a certain milestone on the leaderboard, they earn a gift voucher that can be redeemed or transferred to another user. The development team wants to ensure that this transfer is captured in the database such that the records for both users are either written successfully with the new gift vouchers or the status quo is maintained.</p>\n\n<p>Which of the following solutions represent the best-fit options to meet the requirements for the given use-case? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use the DynamoDB transactional read and write APIs on the table items as a single, all-or-nothing operation</strong></p>\n\n<p>You can use DynamoDB transactions to make coordinated all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, helping you to maintain data correctness in your applications.</p>\n\n<p>DynamoDB Transactions Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a></p>\n\n<p><strong>Complete both operations on RDS MySQL in a single transaction block</strong></p>\n\n<p>Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database with support for transactions in the cloud. A relational database is a collection of data items with pre-defined relationships between them. RDS supports the most demanding database applications. You can choose between two SSD-backed storage options: one optimized for high-performance Online Transaction Processing (OLTP) applications, and the other for cost-effective general-purpose use.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q8-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/relational-database/\">https://aws.amazon.com/relational-database/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Perform DynamoDB read and write operations with ConsistentRead parameter set to true</strong> - DynamoDB uses eventually consistent reads unless you specify otherwise. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. Read consistency does not facilitate DynamoDB transactions and this option has been added as a distractor.</p>\n\n<p><strong>Complete both operations on Amazon RedShift in a single transaction block</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. It cannot be used to manage database transactions.</p>\n\n<p><strong>Use the Amazon Athena transactional read and write APIs on the table items as a single, all-or-nothing operation</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. It cannot be used to manage database transactions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/relational-database/\">https://aws.amazon.com/relational-database/</a></p>\n",
                "options": [
                    {
                        "id": 1445,
                        "content": "<p>Complete both operations on Amazon RedShift in a single transaction block</p>",
                        "isValid": false
                    },
                    {
                        "id": 1446,
                        "content": "<p>Perform DynamoDB read and write operations with ConsistentRead parameter set to true</p>",
                        "isValid": false
                    },
                    {
                        "id": 1447,
                        "content": "<p>Use the Amazon Athena transactional read and write APIs on the table items as a single, all-or-nothing operation</p>",
                        "isValid": false
                    },
                    {
                        "id": 1448,
                        "content": "<p>Complete both operations on RDS MySQL in a single transaction block</p>",
                        "isValid": true
                    },
                    {
                        "id": 1449,
                        "content": "<p>Use the DynamoDB transactional read and write APIs on the table items as a single, all-or-nothing operation</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 355,
            "attributes": {
                "createdAt": "2023-09-07T08:39:32.689Z",
                "updatedAt": "2023-09-07T08:39:32.689Z",
                "content": "<p>The development team at a HealthCare company has deployed EC2 instances in AWS Account A. These instances need to access patient data with Personally Identifiable Information (PII) on multiple S3 buckets in another AWS Account B.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM role with S3 access in Account B and set Account A as a trusted entity. Create another role (instance profile) in Account A and attach it to the EC2 instances in Account A and add an inline policy to this role to assume the role from Account B</strong></p>\n\n<p>You can give EC2 instances in one account (\"account A\") permissions to assume a role from another account (\"account B\") to access resources such as S3 buckets. You need to create an IAM role in Account B and set Account A as a trusted entity. Then attach a policy to this IAM role such that it delegates access to Amazon S3 like so -</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::awsexamplebucket1\",\n                \"arn:aws:s3:::awsexamplebucket1/*\",\n                \"arn:aws:s3:::awsexamplebucket2\",\n                \"arn:aws:s3:::awsexamplebucket2/*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Then you can create another role (instance profile) in Account A and attach it to the EC2 instances in Account A and add an inline policy to this role to assume the role from Account B like so -</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": \"arn:aws:iam::AccountB_ID:role/ROLENAME\"\n        }\n    ]\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM role (instance profile) in Account A and set Account B as a trusted entity. Attach this role to the EC2 instances in Account A and add an inline policy to this role to access S3 data from Account B</strong> - This option contradicts the explanation provided earlier in the explanation, hence this option is incorrect.</p>\n\n<p><strong>Copy the underlying AMI for the EC2 instances from Account A into Account B. Launch EC2 instances in Account B using this AMI and then access the PII data on Amazon S3 in Account B</strong> - Copying the AMI is a distractor as this does not solve the use-case outlined in the problem statement.</p>\n\n<p><strong>Add a bucket policy to all the Amazon S3 buckets in Account B to allow access from EC2 instances in Account A</strong> - Just adding a bucket policy in Account B is not enough, as you also need to create an IAM policy in Account A to access S3 objects in Account B.</p>\n\n<p>Please review this reference material for a deep-dive on cross-account access to objects that are in Amazon S3 buckets -\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/\">https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-instance-access-bucket/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-instance-access-bucket/</a></p>\n",
                "options": [
                    {
                        "id": 1450,
                        "content": "<p>Add a bucket policy to all the Amazon S3 buckets in Account B to allow access from EC2 instances in Account A</p>",
                        "isValid": false
                    },
                    {
                        "id": 1451,
                        "content": "<p>Create an IAM role with S3 access in Account B and set Account A as a trusted entity. Create another role (instance profile) in Account A and attach it to the EC2 instances in Account A and add an inline policy to this role to assume the role from Account B</p>",
                        "isValid": true
                    },
                    {
                        "id": 1452,
                        "content": "<p>Create an IAM role (instance profile) in Account A and set Account B as a trusted entity. Attach this role to the EC2 instances in Account A and add an inline policy to this role to access S3 data from Account B</p>",
                        "isValid": false
                    },
                    {
                        "id": 1453,
                        "content": "<p>Copy the underlying AMI for the EC2 instances from Account A into Account B. Launch EC2 instances in Account B using this AMI and then access the PII data on Amazon S3 in Account B</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 356,
            "attributes": {
                "createdAt": "2023-09-07T08:39:32.767Z",
                "updatedAt": "2023-09-07T08:39:32.767Z",
                "content": "<p>A company wants to share information with a third party via an HTTP API endpoint managed by the third party. The company has the necessary API key to access the endpoint and the integration of the API key with the company's application code must not impact the application's performance.</p>\n\n<p>What is the most secure approach?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Keep the API credentials in AWS Secrets Manager and use the credentials to make the API call by fetching the API credentials at runtime by using the AWS SDK</strong></p>\n\n<p>Secrets Manager enables you to replace hardcoded credentials in your code, including passwords, with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can't be compromised by someone examining your code, because the secret no longer exists in the code. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a specified schedule. This enables you to replace long-term secrets with short-term ones, significantly reducing the risk of compromise.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q38-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a></p>\n\n<p>In the past, when you created a custom application to retrieve information from a database, you typically embedded the credentials, the secret, for accessing the database directly in the application. When the time came to rotate the credentials, you had to do more than just create new credentials. You had to invest time to update the application to use the new credentials. Then you distributed the updated application. If you had multiple applications with shared credentials and you missed updating one of them, the application failed. Because of this risk, many customers choose not to regularly rotate credentials, which effectively substitutes one risk for another. You can also use caching with Secrets Manager to significantly improve the availability and latency of applications.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Keep the API credentials in an encrypted table in MySQL RDS and use the credentials to make the API call by fetching the API credentials from RDS at runtime by using the AWS SDK</strong></p>\n\n<p><strong>Keep the API credentials in an encrypted file in S3 and use the credentials to make the API call by fetching the API credentials from S3 at runtime by using the AWS SDK</strong></p>\n\n<p><strong>Keep the API credentials in a local code variable and use the local code variable at runtime to make the API call</strong></p>\n\n<p>It is considered a security bad practice to keep sensitive access credentials in code, database, or a flat file on a file system or object storage. Therefore, all three options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/improve-availability-and-latency-of-applications-by-using-aws-secret-managers-python-client-side-caching-library/\">https://aws.amazon.com/blogs/security/improve-availability-and-latency-of-applications-by-using-aws-secret-managers-python-client-side-caching-library/</a></p>\n",
                "options": [
                    {
                        "id": 1454,
                        "content": "<p>Keep the API credentials in an encrypted file in S3 and use the credentials to make the API call by fetching the API credentials from S3 at runtime by using the AWS SDK</p>",
                        "isValid": false
                    },
                    {
                        "id": 1455,
                        "content": "<p>Keep the API credentials in a local code variable and use the local code variable at runtime to make the API call</p>",
                        "isValid": false
                    },
                    {
                        "id": 1456,
                        "content": "<p>Keep the API credentials in an encrypted table in MySQL RDS and use the credentials to make the API call by fetching the API credentials from RDS at runtime by using the AWS SDK</p>",
                        "isValid": false
                    },
                    {
                        "id": 1457,
                        "content": "<p>Keep the API credentials in AWS Secrets Manager and use the credentials to make the API call by fetching the API credentials at runtime by using the AWS SDK</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 357,
            "attributes": {
                "createdAt": "2023-09-07T08:39:32.837Z",
                "updatedAt": "2023-09-07T08:39:32.837Z",
                "content": "<p>The development team at a multi-national retail company wants to support trusted third-party authenticated users from the supplier organizations to create and update records in specific DynamoDB tables in the company's AWS account.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito Identity pools to enable trusted third-party authenticated users to access DynamoDB</strong></p>\n\n<p>Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. Amazon Cognito identity pools support the following identity providers:</p>\n\n<p>Public providers: Login with Amazon (Identity Pools), Facebook (Identity Pools), Google (Identity Pools), Sign in with Apple (Identity Pools).</p>\n\n<p>Amazon Cognito User Pools</p>\n\n<p>Open ID Connect Providers (Identity Pools)</p>\n\n<p>SAML Identity Providers (Identity Pools)</p>\n\n<p>Developer Authenticated Identities (Identity Pools)</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Cognito User pools to enable trusted third-party authenticated users to access DynamoDB</strong> - A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Cognito User Pools cannot be used to obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB.</p>\n\n<p><strong>Create a new IAM user in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM user credentials to access DynamoDB</strong></p>\n\n<p><strong>Create a new IAM group in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM group credentials to access DynamoDB</strong></p>\n\n<p>Both these options involve setting up IAM resources such as IAM users or IAM groups just to provide access to DynamoDB tables. As the users are already trusted third-party authenticated users, Cognito Identity Pool can address this use-case in an elegant way.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n",
                "options": [
                    {
                        "id": 1458,
                        "content": "<p>Create a new IAM user in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM user credentials to access DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 1459,
                        "content": "<p>Use Cognito User pools to enable trusted third-party authenticated users to access DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 1460,
                        "content": "<p>Use Cognito Identity pools to enable trusted third-party authenticated users to access DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 1461,
                        "content": "<p>Create a new IAM group in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM group credentials to access DynamoDB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 358,
            "attributes": {
                "createdAt": "2023-09-07T08:39:32.912Z",
                "updatedAt": "2023-09-07T08:39:32.912Z",
                "content": "<p>A developer in your company was just promoted to Team Lead and will be in charge of code deployment on EC2 instances via AWS CodeCommit and AWS CodeDeploy. Per the new requirements, the deployment process should be able to change permissions for deployed files as well as verify the deployment success.</p>\n\n<p>Which of the following actions should the new Developer take?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Define an <code>appspec.yml</code> file in the root directory</strong>: An AppSpec file must be a YAML-formatted file named appspec.yml and it must be placed in the root of the directory structure of an application's source code.</p>\n\n<p>The AppSpec file is used to:</p>\n\n<p>Map the source files in your application revision to their destinations on the instance.</p>\n\n<p>Specify custom permissions for deployed files.</p>\n\n<p>Specify scripts to be run on each instance at various stages of the deployment process.</p>\n\n<p>During deployment, the CodeDeploy agent looks up the name of the current event in the hooks section of the AppSpec file. If the event is not found, the CodeDeploy agent moves on to the next step. If the event is found, the CodeDeploy agent retrieves the list of scripts to execute. The scripts are run sequentially, in the order in which they appear in the file. The status of each script is logged in the CodeDeploy agent log file on the instance.</p>\n\n<p>If a script runs successfully, it returns an exit code of 0 (zero). If the CodeDeploy agent installed on the operating system doesn't match what's listed in the AppSpec file, the deployment fails.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Define a <code>buildspec.yml</code> file in the root directory</strong> - This is a file used by AWS CodeBuild to run a build. This is not relevant to the given use case.</p>\n\n<p><strong>Define a <code>buildspec.yml</code> file in the codebuild/ directory</strong> - This is a file used by AWS CodeBuild to run a build. This is not relevant to the given use case.</p>\n\n<p><strong>Define an <code>appspec.yml</code> file in the codebuild/ directory</strong> - This file is for AWS CodeDeploy and must be placed in the root of the directory structure of an application's source code.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/application-specification-files.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/application-specification-files.html</a></p>\n",
                "options": [
                    {
                        "id": 1462,
                        "content": "<p>Define a <code>buildspec.yml</code> file in the codebuild/ directory</p>",
                        "isValid": false
                    },
                    {
                        "id": 1463,
                        "content": "<p>Define a <code>buildspec.yml</code> file in the root directory</p>",
                        "isValid": false
                    },
                    {
                        "id": 1464,
                        "content": "<p>Define an <code>appspec.yml</code> file in the root directory</p>",
                        "isValid": true
                    },
                    {
                        "id": 1465,
                        "content": "<p>Define an <code>appspec.yml</code> file in the codebuild/ directory</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 359,
            "attributes": {
                "createdAt": "2023-09-07T08:39:32.984Z",
                "updatedAt": "2023-09-07T08:39:32.984Z",
                "content": "<p>A developer with access to the AWS Management Console terminated an instance in the us-east-1a availability zone. The attached EBS volume remained and is now available for attachment to other instances. Your colleague launches a new Linux EC2 instance in the us-east-1e availability zone and is attempting to attach the EBS volume. Your colleague informs you that it is not possible and need your help.</p>\n\n<p>Which of the following explanations would you provide to them?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>EBS volumes are AZ locked</strong></p>\n\n<p>An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. For current-generation volumes attached to current-generation instance types, you can dynamically increase size, modify the provisioned IOPS capacity, and change volume type on live production volumes.</p>\n\n<p>When you create an EBS volume, it is automatically replicated within its Availability Zone to prevent data loss due to the failure of any single hardware component. You can attach an EBS volume to an EC2 instance in the same Availability Zone.</p>\n\n<p>![EBS Volume Overview]https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q62-i1.jpg)\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EBS volumes are region locked</strong> - It's confined to an Availability Zone and not by region.</p>\n\n<p><strong>The required IAM permissions are missing</strong> - This is a possibility as well but if permissions are not an issue then you are still confined to an availability zone.</p>\n\n<p><strong>The EBS volume is encrypted</strong> - This doesn't affect the ability to attach an EBS volume.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html</a></p>\n",
                "options": [
                    {
                        "id": 1466,
                        "content": "<p>EBS volumes are AZ locked</p>",
                        "isValid": true
                    },
                    {
                        "id": 1467,
                        "content": "<p>The EBS volume is encrypted</p>",
                        "isValid": false
                    },
                    {
                        "id": 1468,
                        "content": "<p>The required IAM permissions are missing</p>",
                        "isValid": false
                    },
                    {
                        "id": 1469,
                        "content": "<p>EBS volumes are region locked</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 360,
            "attributes": {
                "createdAt": "2023-09-07T08:39:33.063Z",
                "updatedAt": "2023-09-07T08:39:33.063Z",
                "content": "<p>As a Team Lead, you are expected to generate a report of the code builds for every week to report internally and to the client. This report consists of the number of code builds performed for a week, the percentage success and failure, and overall time spent on these builds by the team members. You also need to retrieve the CodeBuild logs for failed builds and analyze them in Athena.</p>\n\n<p>Which of the following options will help achieve this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable S3 and CloudWatch Logs integration</strong> - AWS CodeBuild monitors functions on your behalf and reports metrics through Amazon CloudWatch. These metrics include the number of total builds, failed builds, successful builds, and the duration of builds. You can monitor your builds at two levels: Project level, AWS account level. You can export log data from your log groups to an Amazon S3 bucket and use this data in custom processing and analysis, or to load onto other systems.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudWatch Events</strong> - You can integrate CloudWatch Events with CodeBuild. However, we are looking at storing and running queries on logs, so Cloudwatch logs with S3 integration makes sense for this context.o</p>\n\n<p><strong>Use AWS Lambda integration</strong> - Lambda is a good choice to use boto3 library to read logs programmatically. But, CloudWatch and S3 integration is already built-in and is an optimized way of managing the given use-case.</p>\n\n<p><strong>Use AWS CloudTrail and deliver logs to S3</strong> - AWS CodeBuild is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in CodeBuild. CloudTrail captures all API calls for CodeBuild as events, including calls from the CodeBuild console and from code calls to the CodeBuild APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an S3 bucket, including events for CodeBuild. This is an important feature for monitoring a service but isn't a good fit for the current scenario.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-metrics.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-metrics.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/getting-started-input-bucket-console.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/getting-started-input-bucket-console.html</a></p>\n",
                "options": [
                    {
                        "id": 1470,
                        "content": "<p>Use CloudWatch Events</p>",
                        "isValid": false
                    },
                    {
                        "id": 1471,
                        "content": "<p>Use AWS Lambda integration</p>",
                        "isValid": false
                    },
                    {
                        "id": 1472,
                        "content": "<p>Enable S3 and CloudWatch Logs integration</p>",
                        "isValid": true
                    },
                    {
                        "id": 1473,
                        "content": "<p>Use AWS CloudTrail and deliver logs to S3</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 361,
            "attributes": {
                "createdAt": "2023-09-07T08:39:33.145Z",
                "updatedAt": "2023-09-07T08:39:33.145Z",
                "content": "<p>Your team lead has asked you to learn AWS CloudFormation to create a collection of related AWS resources and provision them in an orderly fashion. You decide to provide AWS-specific parameter types to catch invalid values.</p>\n\n<p>When specifying parameters which of the following is not a valid Parameter type?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p>Parameter types enable CloudFormation to validate inputs earlier in the stack creation process.</p>\n\n<p>CloudFormation currently supports the following parameter types:</p>\n\n<pre><code>String â€“ A literal string\nNumber â€“ An integer or float\nList&lt;Number&gt; â€“ An array of integers or floats\nCommaDelimitedList â€“ An array of literal strings that are separated by commas\nAWS::EC2::KeyPair::KeyName â€“ An Amazon EC2 key pair name\nAWS::EC2::SecurityGroup::Id â€“ A security group ID\nAWS::EC2::Subnet::Id â€“ A subnet ID\nAWS::EC2::VPC::Id â€“ A VPC ID\nList&lt;AWS::EC2::VPC::Id&gt; â€“ An array of VPC IDs\nList&lt;AWS::EC2::SecurityGroup::Id&gt; â€“ An array of security group IDs\nList&lt;AWS::EC2::Subnet::Id&gt; â€“ An array of subnet IDs\n</code></pre>\n\n<p><strong>DependentParameter</strong></p>\n\n<p>In CloudFormation, parameters are all independent and cannot depend on each other. Therefore, this is an invalid parameter type.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>String</strong></p>\n\n<p><strong>CommaDelimitedList</strong></p>\n\n<p><strong>AWS::EC2::KeyPair::KeyName</strong></p>\n\n<p>As mentioned in the explanation above, these are valid parameter types.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/using-the-new-cloudformation-parameter-types/\">https://aws.amazon.com/blogs/devops/using-the-new-cloudformation-parameter-types/</a></p>\n",
                "options": [
                    {
                        "id": 1474,
                        "content": "<p>String</p>",
                        "isValid": false
                    },
                    {
                        "id": 1475,
                        "content": "<p>AWS::EC2::KeyPair::KeyName</p>",
                        "isValid": false
                    },
                    {
                        "id": 1476,
                        "content": "<p>CommaDelimitedList</p>",
                        "isValid": false
                    },
                    {
                        "id": 1477,
                        "content": "<p>DependentParameter</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 362,
            "attributes": {
                "createdAt": "2023-09-07T08:39:33.211Z",
                "updatedAt": "2023-09-07T08:39:33.211Z",
                "content": "<p>A business has purchased one m4.xlarge Reserved Instance but it has used three m4.xlarge instances concurrently for an hour.</p>\n\n<p>As a Developer, explain how the instances are charged?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>All Reserved Instances provide you with a discount compared to On-Demand pricing.</p>\n\n<p><strong>One instance is charged at one hour of Reserved Instance usage and the other two instances are charged at two hours of On-Demand usage</strong></p>\n\n<p>A Reserved Instance billing benefit can apply to a maximum of 3600 seconds (one hour) of instance usage per clock-hour. You can run multiple instances concurrently, but can only receive the benefit of the Reserved Instance discount for a total of 3600 seconds per clock-hour; instance usage that exceeds 3600 seconds in a clock-hour is billed at the On-Demand rate.</p>\n\n<p>Please review this note on the EC2 Reserved Instance types:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html</a></p>\n\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q34-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>All instances are charged at one hour of Reserved Instance usage</strong> - This is incorrect.</p>\n\n<p><strong>All instances are charged at one hour of On-Demand Instance usage</strong> - This is incorrect.</p>\n\n<p><strong>One instance is charged at one hour of On-Demand usage and the other two instances are charged at two hours of Reserved Instance usage</strong> - This is incorrect. If multiple eligible instances are running concurrently, the Reserved Instance billing benefit is applied to all the instances at the same time up to a maximum of 3600 seconds in a clock-hour; thereafter, On-Demand rates apply.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts-reserved-instances-application.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts-reserved-instances-application.html</a></p>\n",
                "options": [
                    {
                        "id": 1478,
                        "content": "<p>One instance is charged at one hour of Reserved Instance usage and the other two instances are charged at two hours of On-Demand usage</p>",
                        "isValid": true
                    },
                    {
                        "id": 1479,
                        "content": "<p>One instance is charged at one hour of On-Demand usage and the other two instances are charged at two hours of Reserved Instance usage</p>",
                        "isValid": false
                    },
                    {
                        "id": 1480,
                        "content": "<p>All instances are charged at one hour of Reserved Instance usage</p>",
                        "isValid": false
                    },
                    {
                        "id": 1481,
                        "content": "<p>All instances are charged at one hour of On-Demand Instance usage</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 363,
            "attributes": {
                "createdAt": "2023-09-07T08:39:33.284Z",
                "updatedAt": "2023-09-07T08:39:33.284Z",
                "content": "<p>A company is using a Border Gateway Protocol (BGP) based AWS VPN connection to connect from its on-premises data center to Amazon EC2 instances in the companyâ€™s account. The development team can access an EC2 instance in subnet A but is unable to access an EC2 instance in subnet B in the same VPC.</p>\n\n<p>Which logs can be used to verify whether the traffic is reaching subnet B?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>VPC Flow Logs</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. After you've created a flow log, you can retrieve and view its data in the chosen destination.</p>\n\n<p>You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored.</p>\n\n<p>Flow log data for a monitored network interface is recorded as flow log records, which are log events consisting of fields that describe the traffic flow.</p>\n\n<p>To create a flow log, you specify:</p>\n\n<ol>\n<li><p>The resource for which to create the flow log</p></li>\n<li><p>The type of traffic to capture (accepted traffic, rejected traffic, or all traffic)</p></li>\n<li><p>The destinations to which you want to publish the flow log data</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>VPN logs</strong></p>\n\n<p><strong>Subnet logs</strong></p>\n\n<p><strong>BGP logs</strong></p>\n\n<p>These three options are incorrect and have been added as distractors.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html</a></p>\n",
                "options": [
                    {
                        "id": 1482,
                        "content": "<p>Subnet logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1483,
                        "content": "<p>VPC Flow Logs</p>",
                        "isValid": true
                    },
                    {
                        "id": 1484,
                        "content": "<p>BGP logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1485,
                        "content": "<p>VPN logs</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 364,
            "attributes": {
                "createdAt": "2023-09-07T08:39:33.352Z",
                "updatedAt": "2023-09-07T08:39:33.352Z",
                "content": "<p>The app development team at a social gaming mobile app wants to simplify the user sign up process for the app. The team is looking for a fully managed scalable solution for user management in anticipation of the rapid growth that the app foresees.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest so that it requires the LEAST amount of development effort?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito User pools to facilitate sign up and user management for the mobile app</strong></p>\n\n<p>Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps. Your users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, Google or Apple.</p>\n\n<p>A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.</p>\n\n<p>Cognito is fully managed by AWS and works out of the box so it meets the requirements for the given use-case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Cognito Identity pools to facilitate sign up and user management for the mobile app</strong> - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q2-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p><strong>Create a custom solution with EC2 and DynamoDB to facilitate sign up and user management for the mobile app</strong></p>\n\n<p><strong>Create a custom solution with Lambda and DynamoDB to facilitate sign up and user management for the mobile app</strong></p>\n\n<p>As the problem statement mentions that the solution needs to be fully managed and should require the least amount of development effort, so you cannot use EC2 or Lambda functions with DynamoDB to create a custom solution.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n",
                "options": [
                    {
                        "id": 1486,
                        "content": "<p>Create a custom solution using Lambda and DynamoDB to facilitate sign up and user management for the mobile app</p>",
                        "isValid": false
                    },
                    {
                        "id": 1487,
                        "content": "<p>Use Cognito Identity pools to facilitate sign up and user management for the mobile app</p>",
                        "isValid": false
                    },
                    {
                        "id": 1488,
                        "content": "<p>Create a custom solution using EC2 and DynamoDB to facilitate sign up and user management for the mobile app</p>",
                        "isValid": false
                    },
                    {
                        "id": 1489,
                        "content": "<p>Use Cognito User pools to facilitate sign up and user management for the mobile app</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 365,
            "attributes": {
                "createdAt": "2023-09-07T08:39:33.428Z",
                "updatedAt": "2023-09-07T08:39:33.428Z",
                "content": "<p>As a Senior Developer, you are tasked with creating several API Gateway powered APIs along with your team of developers. The developers are working on the API in the development environment, but they find the changes made to the APIs are not reflected when the API is called.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Redeploy the API to an existing stage or to a new stage</strong></p>\n\n<p>After creating your API, you must deploy it to make it callable by your users. To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. Every time you update an API, you must redeploy the API to an existing stage or to a new stage. Updating an API includes modifying routes, methods, integrations, authorizers, and anything else other than stage settings.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Developers need IAM permissions on API execution component of API Gateway</strong> - Access control access to Amazon API Gateway APIs is done with IAM permissions. To call a deployed API or to refresh the API caching, you must grant the API caller permissions to perform required IAM actions supported by the API execution component of API Gateway. In the current scenario, developers do not need permissions on \"execution components\" but on \"management components\" of API Gateway that help them to create, deploy, and manage an API. Hence, this statement is an incorrect option.</p>\n\n<p><strong>Enable Lambda authorizer to access API</strong> - A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. So, this feature too helps in access control, but in the current scenario its the developers and not the users who are facing the issue. So, this statement is an incorrect option.</p>\n\n<p><strong>Use Stage Variables for development state of API</strong> - Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates. Stage variables are not connected to the scenario described in the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a></p>\n",
                "options": [
                    {
                        "id": 1490,
                        "content": "<p>Redeploy the API to an existing stage or to a new stage</p>",
                        "isValid": true
                    },
                    {
                        "id": 1491,
                        "content": "<p>Developers need IAM permissions on API execution component of API Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 1492,
                        "content": "<p>Enable Lambda authorizer to access API</p>",
                        "isValid": false
                    },
                    {
                        "id": 1493,
                        "content": "<p>Use Stage Variables for development state of API</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 366,
            "attributes": {
                "createdAt": "2023-09-07T08:39:33.500Z",
                "updatedAt": "2023-09-07T08:39:33.500Z",
                "content": "<p>You have launched several AWS Lambda functions written in Java. A new requirement was given that over 1MB of data should be passed to the functions and should be encrypted and decrypted at runtime.</p>\n\n<p>Which of the following methods is suitable to address the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Envelope Encryption and reference the data as file within the code</strong></p>\n\n<p>While AWS KMS does support sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency.</p>\n\n<p>AWS Lambda environment variables can have a maximum size of 4 KB. Additionally, the direct 'Encrypt' API of KMS also has an upper limit of 4 KB for the data payload. To encrypt 1 MB, you need to use the Encryption SDK and pack the encrypted file with the lambda function.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use KMS direct encryption and store as file</strong> - You can only encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information, so this option is not correct for the given use-case.</p>\n\n<p><strong>Use Envelope Encryption and store as an environment variable</strong> - Environment variables must not exceed 4 KB, so this option is not correct for the given use-case.</p>\n\n<p><strong>Use KMS Encryption and store as an environment variable</strong> - You can encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information. Lambda Environment variables must not exceed 4 KB. So this option is not correct for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html\">https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kms/faqs/\">https://aws.amazon.com/kms/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 1494,
                        "content": "<p>Use Envelope Encryption and store as environment variable</p>",
                        "isValid": false
                    },
                    {
                        "id": 1495,
                        "content": "<p>Use KMS Encryption and store as environment variable</p>",
                        "isValid": false
                    },
                    {
                        "id": 1496,
                        "content": "<p>Use Envelope Encryption and reference the data as file within the code</p>",
                        "isValid": true
                    },
                    {
                        "id": 1497,
                        "content": "<p>Use KMS direct encryption and store as file</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 367,
            "attributes": {
                "createdAt": "2023-09-07T08:39:33.571Z",
                "updatedAt": "2023-09-07T08:39:33.571Z",
                "content": "<p>A diagnostic lab stores its data on DynamoDB. The lab wants to backup a particular DynamoDB table data on Amazon S3, so it can download the S3 backup locally for some operational use.</p>\n\n<p>Which of the following options is NOT feasible?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use the DynamoDB on-demand backup capability to write to Amazon S3 and download locally</strong> - This option is not feasible for the given use-case. DynamoDB has two built-in backup methods (On-demand, Point-in-time recovery) that write to Amazon S3, but you will not have access to the S3 buckets that are used for these backups.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q58-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/\">https://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Data Pipeline to export your table to an S3 bucket in the account of your choice and download locally</strong> - This is the easiest method. This method is used when you want to make a one-time backup using the lowest amount of AWS resources possible. Data Pipeline uses Amazon EMR to create the backup, and the scripting is done for you. You don't have to learn Apache Hive or Apache Spark to accomplish this task.</p>\n\n<p><strong>Use Hive with Amazon EMR to export your data to an S3 bucket and download locally</strong> - Use Hive to export data to an S3 bucket. Or, use the open-source emr-dynamodb-connector to manage your own custom backup method in Spark or Hive. These methods are the best practice to use if you're an active Amazon EMR user and are comfortable with Hive or Spark. These methods offer more control than the Data Pipeline method.</p>\n\n<p><strong>Use AWS Glue to copy your table to Amazon S3 and download locally</strong> - Use AWS Glue to copy your table to Amazon S3. This is the best practice to use if you want automated, continuous backups that you can also use in another service, such as Amazon Athena.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/EMR_Hive_Commands.html#EMR_Hive_Commands_exporting\">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/EMR_Hive_Commands.html#EMR_Hive_Commands_exporting</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/how-to-export-an-amazon-dynamodb-table-to-amazon-s3-using-aws-step-functions-and-aws-glue/\">https://aws.amazon.com/blogs/big-data/how-to-export-an-amazon-dynamodb-table-to-amazon-s3-using-aws-step-functions-and-aws-glue/</a></p>\n",
                "options": [
                    {
                        "id": 1498,
                        "content": "<p>Use Hive with Amazon EMR to export your data to an S3 bucket and download locally</p>",
                        "isValid": false
                    },
                    {
                        "id": 1499,
                        "content": "<p>Use AWS Data Pipeline to export your table to an S3 bucket in the account of your choice and download locally</p>",
                        "isValid": false
                    },
                    {
                        "id": 1500,
                        "content": "<p>Use AWS Glue to copy your table to Amazon S3 and download locally</p>",
                        "isValid": false
                    },
                    {
                        "id": 1501,
                        "content": "<p>Use the DynamoDB on-demand backup capability to write to Amazon S3 and download locally</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 368,
            "attributes": {
                "createdAt": "2023-09-07T08:39:33.648Z",
                "updatedAt": "2023-09-07T08:39:33.648Z",
                "content": "<p>A CRM application is hosted on Amazon EC2 instances with the database tier using DynamoDB. The customers have raised privacy and security concerns regarding sending and receiving data across the public internet.</p>\n\n<p>As a developer associate, which of the following would you suggest as an optimal solution for providing communication between EC2 instances and DynamoDB without using the public internet?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure VPC endpoints for DynamoDB that will provide required internal access without using public internet</strong></p>\n\n<p>When you create a VPC endpoint for DynamoDB, any requests to a DynamoDB endpoint within the Region (for example, dynamodb.us-west-2.amazonaws.com) are routed to a private DynamoDB endpoint within the Amazon network. You don't need to modify your applications running on EC2 instances in your VPC. The endpoint name remains the same, but the route to DynamoDB stays entirely within the Amazon network, and does not access the public internet. You use endpoint policies to control access to DynamoDB. Traffic between your VPC and the AWS service does not leave the Amazon network.</p>\n\n<p>Using Amazon VPC Endpoints to Access DynamoDB:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The firm can use a virtual private network (VPN) to route all DynamoDB network traffic through their own corporate network infrastructure</strong> - You can address the requested security concerns by using a virtual private network (VPN) to route all DynamoDB network traffic through your own corporate network infrastructure. However, this approach can introduce bandwidth and availability challenges and hence is not an optimal solution here.</p>\n\n<p><strong>Create a NAT Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</strong> - You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. NAT Gateway is not useful here since the instance and DynamoDB are present in AWS network and do not need NAT Gateway for communicating with each other.</p>\n\n<p><strong>Create an Internet Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</strong> - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. Using an Internet Gateway would imply that the EC2 instances are connecting to DynamoDB using the public internet. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/Carrier_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/Carrier_Gateway.html</a></p>\n",
                "options": [
                    {
                        "id": 1502,
                        "content": "<p>Create an Internet Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 1503,
                        "content": "<p>The firm can use a virtual private network (VPN) to route all DynamoDB network traffic through their own corporate network infrastructure</p>",
                        "isValid": false
                    },
                    {
                        "id": 1504,
                        "content": "<p>Configure VPC endpoints for DynamoDB that will provide required internal access without using public internet</p>",
                        "isValid": true
                    },
                    {
                        "id": 1505,
                        "content": "<p>Create a NAT Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 369,
            "attributes": {
                "createdAt": "2023-09-07T08:39:33.719Z",
                "updatedAt": "2023-09-07T08:39:33.719Z",
                "content": "<p>As a senior architect, you are responsible for the development, support, maintenance, and implementation of all database applications written using NoSQL technology. A new project demands a throughput requirement of 10 strongly consistent reads per second of 6KB in size each.</p>\n\n<p>How many read capacity units will you need when configuring your DynamoDB table?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Before proceeding with the calculations, please review the following:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q44-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q44-i2.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p><strong>20</strong></p>\n\n<p>One read capacity unit represents one strongly consistent read per second for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units.</p>\n\n<p>1) Item Size / 4KB, rounding to the nearest whole number.</p>\n\n<p>So, in the above case, 6KB / 4 KB = 1.5 or 2 read capacity units.</p>\n\n<p>2) 1 read capacity unit per item (since strongly consistent read) Ã— No of reads per second</p>\n\n<p>So, in the above case, 2 x 10 = 20 read capacity units.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>60</strong></p>\n\n<p><strong>30</strong></p>\n\n<p><strong>10</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n",
                "options": [
                    {
                        "id": 1506,
                        "content": "<p>20</p>",
                        "isValid": true
                    },
                    {
                        "id": 1507,
                        "content": "<p>30</p>",
                        "isValid": false
                    },
                    {
                        "id": 1508,
                        "content": "<p>10</p>",
                        "isValid": false
                    },
                    {
                        "id": 1509,
                        "content": "<p>60</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 370,
            "attributes": {
                "createdAt": "2023-09-07T08:39:33.790Z",
                "updatedAt": "2023-09-07T08:39:33.790Z",
                "content": "<p>You create an Auto Scaling group to work with an Application Load Balancer. The scaling group is configured with a minimum size value of 5, a maximum value of 20, and the desired capacity value of 10. One of the 10 EC2 instances has been reported as unhealthy.</p>\n\n<p>Which of the following actions will take place?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The ASG will terminate the EC2 Instance</strong></p>\n\n<p>To maintain the same number of instances, Amazon EC2 Auto Scaling performs a periodic health check on running instances within an Auto Scaling group. When it finds that an instance is unhealthy, it terminates that instance and launches a new one. Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The ASG will detach the EC2 instance from the group, and leave it running</strong> - The goal of the auto-scaling group is to get rid of the bad instance and replace it</p>\n\n<p><strong>The ASG will keep the instance running and re-start the application</strong> - The ASG does not have control of your application</p>\n\n<p><strong>The ASG will format the root EBS drive on the EC2 instance and run the User Data again</strong> - This will not happen, the ASG cannot assume the format of your EBS drive, and User Data only runs once at instance first boot.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html#replace-unhealthy-instance\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html#replace-unhealthy-instance</a></p>\n",
                "options": [
                    {
                        "id": 1510,
                        "content": "<p>The ASG will terminate the EC2 Instance</p>",
                        "isValid": true
                    },
                    {
                        "id": 1511,
                        "content": "<p>The ASG will detach the EC2 instance from the group, and leave it running</p>",
                        "isValid": false
                    },
                    {
                        "id": 1512,
                        "content": "<p>The ASG will keep the instance running and re-start the application</p>",
                        "isValid": false
                    },
                    {
                        "id": 1513,
                        "content": "<p>The ASG will format the root EBS drive on the EC2 instance and run the User Data again</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 371,
            "attributes": {
                "createdAt": "2023-09-07T08:39:33.865Z",
                "updatedAt": "2023-09-07T08:39:33.865Z",
                "content": "<p>A development team is working on an AWS Lambda function that accesses DynamoDB. The Lambda function must do an upsert, that is, it must retrieve an item and update some of its attributes or create the item if it does not exist.</p>\n\n<p>Which of the following represents the solution with MINIMUM IAM permissions that can be used for the Lambda function to achieve this functionality?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>dynamodb:UpdateItem, dynamodb:GetItem</strong> - With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing TransactWriteItems or TransactGetItems operation.</p>\n\n<p>You can use AWS Identity and Access Management (IAM) to restrict the actions that transactional operations can perform in Amazon DynamoDB. Permissions for Put, Update, Delete, and Get actions are governed by the permissions used for the underlying PutItem, UpdateItem, DeleteItem, and GetItem operations. For the ConditionCheck action, you can use the <code>dynamodb:ConditionCheck</code> permission in IAM policies.</p>\n\n<p><code>UpdateItem</code> action of DynamoDB APIs, edits an existing item's attributes or adds a new item to the table if it does not already exist. You can put, delete, or add attribute values. You can also perform a conditional update on an existing item (insert a new attribute name-value pair if it doesn't exist, or replace an existing name-value pair if it has certain expected attribute values).</p>\n\n<p>There is no need to inlcude the <code>dynamodb:PutItem</code> action for the given use-case.</p>\n\n<p>So, the IAM policy must include permissions to get and update the item in the DynamoDB table.</p>\n\n<p>Actions defined by DynamoDB:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q48-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html\">https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>dynamodb:AddItem, dynamodb:GetItem</strong></p>\n\n<p><strong>dynamodb:GetRecords, dynamodb:PutItem, dynamodb:UpdateTable</strong></p>\n\n<p><strong>dynamodb:UpdateItem, dynamodb:GetItem, dynamodb:PutItem</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html\">https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html</a></p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/transaction-apis-iam.html\">https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/transaction-apis-iam.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html\">https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html</a></p>\n",
                "options": [
                    {
                        "id": 1514,
                        "content": "<p>dynamodb:UpdateItem, dynamodb:GetItem, dynamodb:PutItem</p>",
                        "isValid": false
                    },
                    {
                        "id": 1515,
                        "content": "<p>dynamodb:UpdateItem, dynamodb:GetItem</p>",
                        "isValid": true
                    },
                    {
                        "id": 1516,
                        "content": "<p>dynamodb:GetRecords, dynamodb:PutItem, dynamodb:UpdateTable</p>",
                        "isValid": false
                    },
                    {
                        "id": 1517,
                        "content": "<p>dynamodb:AddItem, dynamodb:GetItem</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 372,
            "attributes": {
                "createdAt": "2023-09-07T08:39:33.949Z",
                "updatedAt": "2023-09-07T08:39:33.949Z",
                "content": "<p>A pharmaceutical company runs their database workloads on Provisioned IOPS SSD (io1) volumes.</p>\n\n<p>As a Developer Associate, which of the following options would you identify as an <strong>INVALID</strong> configuration for io1 EBS volume types?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>200 GiB size volume with 15000 IOPS</strong> - This is an invalid configuration. The maximum ratio of provisioned IOPS to requested volume size (in GiB) is 50:1. So, for a 200 GiB volume size, max IOPS possible is 200*50 = 10000 IOPS.</p>\n\n<p>Overview of Provisioned IOPS SSD (io1) volumes:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q36-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>Provisioned IOPS SSD (io1) volumes allow you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time. An io1 volume can range in size from 4 GiB to 16 TiB. The maximum ratio of provisioned IOPS to the requested volume size (in GiB) is 50:1. For example, a 100 GiB volume can be provisioned with up to 5,000 IOPS.</p>\n\n<p><strong>200 GiB size volume with 2000 IOPS</strong> - As explained above, up to 10000 IOPS is a valid configuration for the given use-case.</p>\n\n<p><strong>200 GiB size volume with 10000 IOPS</strong> - As explained above, up to 10000 IOPS is a valid configuration for the given use-case.</p>\n\n<p><strong>200 GiB size volume with 5000 IOPS</strong> - As explained above, up to 10000 IOPS is a valid configuration for the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n",
                "options": [
                    {
                        "id": 1518,
                        "content": "<p>200 GiB size volume with 5000 IOPS</p>",
                        "isValid": false
                    },
                    {
                        "id": 1519,
                        "content": "<p>200 GiB size volume with 15000 IOPS</p>",
                        "isValid": true
                    },
                    {
                        "id": 1520,
                        "content": "<p>200 GiB size volume with 10000 IOPS</p>",
                        "isValid": false
                    },
                    {
                        "id": 1521,
                        "content": "<p>200 GiB size volume with 2000 IOPS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 373,
            "attributes": {
                "createdAt": "2023-09-07T08:39:34.028Z",
                "updatedAt": "2023-09-07T08:39:34.028Z",
                "content": "<p>A startup has been experimenting with DynamoDB in its new test environment. The development team has discovered that some of the write operations have been overwriting existing items that have the specified primary key. This has messed up their data, leading to data discrepancies.</p>\n\n<p>Which DynamoDB write option should be selected to prevent this kind of overwriting?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Conditional writes</strong> - DynamoDB optionally supports conditional writes for write operations (PutItem, UpdateItem, DeleteItem). A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error.</p>\n\n<p>For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item. This is the right choice for the current scenario.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Batch writes</strong> - Bath operations (read and write) help reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Applications benefit from this parallelism without having to manage concurrency or threading. But, this is of no use in the current scenario of overwriting changes.</p>\n\n<p><strong>Atomic Counters</strong> - Atomic Counters is a numeric attribute that is incremented, unconditionally, without interfering with other write requests. You might use an atomic counter to track the number of visitors to a website. This functionality is not useful for the current scenario.</p>\n\n<p><strong>Use Scan operation</strong> - A Scan operation in Amazon DynamoDB reads every item in a table or a secondary index. By default, a Scan operation returns all of the data attributes for every item in the table or index. This is given as a distractor and not related to DynamoDB item updates.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate</a></p>\n",
                "options": [
                    {
                        "id": 1522,
                        "content": "<p>Atomic Counters</p>",
                        "isValid": false
                    },
                    {
                        "id": 1523,
                        "content": "<p>Use Scan operation</p>",
                        "isValid": false
                    },
                    {
                        "id": 1524,
                        "content": "<p>Batch writes</p>",
                        "isValid": false
                    },
                    {
                        "id": 1525,
                        "content": "<p>Conditional writes</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 374,
            "attributes": {
                "createdAt": "2023-09-07T08:39:34.105Z",
                "updatedAt": "2023-09-07T08:39:34.105Z",
                "content": "<p>A media publishing company is using Amazon EC2 instances for running their business-critical applications. Their IT team is looking at reserving capacity apart from savings plans for the critical instances.</p>\n\n<p>As a Developer Associate, which of the following reserved instance types you would select to provide capacity reservations?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>When you purchase a Reserved Instance for a specific Availability Zone, it's referred to as a Zonal Reserved Instance. Zonal Reserved Instances provide capacity reservations as well as discounts.</p>\n\n<p><strong>Zonal Reserved Instances</strong> - A zonal Reserved Instance provides a capacity reservation in the specified Availability Zone. Capacity Reservations enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration. This gives you the ability to create and manage Capacity Reservations independently from the billing discounts offered by Savings Plans or regional Reserved Instances.</p>\n\n<p>Regional and Zonal Reserved Instances:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html</a></p>\n\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q31-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Regional Reserved Instances</strong> - When you purchase a Reserved Instance for a Region, it's referred to as a regional Reserved Instance. A regional Reserved Instance does not provide a capacity reservation.</p>\n\n<p><strong>Both Regional Reserved Instances and Zonal Reserved Instances</strong> - As discussed above, only Zonal Reserved Instances provide capacity reservation.</p>\n\n<p><strong>Neither Regional Reserved Instances nor Zonal Reserved Instances</strong> - As discussed above, Zonal Reserved Instances provide capacity reservation.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a></p>\n",
                "options": [
                    {
                        "id": 1526,
                        "content": "<p>Neither Regional Reserved Instances nor Zonal Reserved Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 1527,
                        "content": "<p>Zonal Reserved Instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 1528,
                        "content": "<p>Both Regional Reserved Instances and Zonal Reserved Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 1529,
                        "content": "<p>Regional Reserved Instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 375,
            "attributes": {
                "createdAt": "2023-09-07T08:39:34.184Z",
                "updatedAt": "2023-09-07T08:39:34.184Z",
                "content": "<p>The development team at an e-commerce company completed the last deployment for their application at a reduced capacity because of the deployment policy. The application took a performance hit because of the traffic spike due to an on-going sale.</p>\n\n<p>Which of the following represents the BEST deployment option for the upcoming application version such that it maintains at least the FULL capacity of the application and MINIMAL impact of failed deployment?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the new application version using 'Immutable' deployment policy</strong></p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>How Elastic BeanStalk Works:\n<img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/clearbox-flow-00.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p>\n\n<p>The 'Immutable' deployment policy ensures that your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks. In case of deployment failure, the new instances are terminated, so the impact is minimal.</p>\n\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q7-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the new application version using 'All at once' deployment policy</strong> - Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time. Also in case of deployment failure, the application sees a downtime, so this option is not correct.</p>\n\n<p><strong>Deploy the new application version using 'Rolling' deployment policy</strong> - This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. However in case of deployment failure, the rollback process is via manual redeploy, so it's not as quick as the Immutable deployment.</p>\n\n<p><strong>Deploy the new application version using 'Rolling with additional batch' deployment policy</strong> - This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. However in case of deployment failure, the rollback process is via manual redeploy, so it's not as quick as the Immutable deployment.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n",
                "options": [
                    {
                        "id": 1530,
                        "content": "<p>Deploy the new application version using 'All at once' deployment policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 1531,
                        "content": "<p>Deploy the new application version using 'Rolling' deployment policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 1532,
                        "content": "<p>Deploy the new application version using 'Immutable' deployment policy</p>",
                        "isValid": true
                    },
                    {
                        "id": 1533,
                        "content": "<p>Deploy the new application version using 'Rolling with additional batch' deployment policy</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 376,
            "attributes": {
                "createdAt": "2023-09-07T08:39:34.262Z",
                "updatedAt": "2023-09-07T08:39:34.262Z",
                "content": "<p>Other than the <code>Resources</code> section, which of the following sections in a Serverless Application Model (SAM) Template is mandatory?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Transform</strong></p>\n\n<p>The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications on AWS.</p>\n\n<p>A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda functionâ€”it can include additional resources such as APIs, databases, and event source mappings.</p>\n\n<p>Serverless Application Model (SAM) Templates include several major sections. Transform and Resources are the only required sections.</p>\n\n<p>Please review this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Parameters</strong></p>\n\n<p><strong>Mappings</strong></p>\n\n<p><strong>Globals</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these options are not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html</a></p>\n",
                "options": [
                    {
                        "id": 1534,
                        "content": "<p>Parameters</p>",
                        "isValid": false
                    },
                    {
                        "id": 1535,
                        "content": "<p>Transform</p>",
                        "isValid": true
                    },
                    {
                        "id": 1536,
                        "content": "<p>Globals</p>",
                        "isValid": false
                    },
                    {
                        "id": 1537,
                        "content": "<p>Mappings</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 377,
            "attributes": {
                "createdAt": "2023-09-07T08:39:34.335Z",
                "updatedAt": "2023-09-07T08:39:34.335Z",
                "content": "<p>A company uses AWS CodeDeploy to deploy applications from GitHub to EC2 instances running Amazon Linux. The deployment process uses a file called appspec.yml for specifying deployment hooks. A final lifecycle event should be specified to verify the deployment success.</p>\n\n<p>Which of the following hook events should be used to verify the success of the deployment?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.</p>\n\n<p>An EC2/On-Premises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q56-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order</a></p>\n\n<p><strong>ValidateService</strong>: ValidateService is the last deployment lifecycle event. It is used to verify the deployment was completed successfully.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AfterInstall</strong> - You can use this deployment lifecycle event for tasks such as configuring your application or changing file permissions</p>\n\n<p><strong>ApplicationStart</strong> - You typically use this deployment lifecycle event to restart services that were stopped during ApplicationStop</p>\n\n<p><strong>AllowTraffic</strong> - During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. This event is reserved for the AWS CodeDeploy agent and cannot be used to run scripts</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order</a></p>\n",
                "options": [
                    {
                        "id": 1538,
                        "content": "<p>AllowTraffic</p>",
                        "isValid": false
                    },
                    {
                        "id": 1539,
                        "content": "<p>ValidateService</p>",
                        "isValid": true
                    },
                    {
                        "id": 1540,
                        "content": "<p>AfterInstall</p>",
                        "isValid": false
                    },
                    {
                        "id": 1541,
                        "content": "<p>ApplicationStart</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 378,
            "attributes": {
                "createdAt": "2023-09-07T08:39:34.429Z",
                "updatedAt": "2023-09-07T08:39:34.429Z",
                "content": "<p>You have created a continuous delivery service model with automated steps using AWS CodePipeline. Your pipeline uses your code, maintained in a CodeCommit repository, AWS CodeBuild, and AWS Elastic Beanstalk to automatically deploy your code every time there is a code change. However, the deployment to Elastic Beanstalk is taking a very long time due to resolving dependencies on all of your 100 target EC2 instances.</p>\n\n<p>Which of the following actions should you take to improve performance with limited code changes?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Bundle the dependencies in the source code during the build stage of CodeBuild</strong></p>\n\n<p>AWS CodeBuild is a fully managed build service. There are no servers to provision and scale, or software to install, configure, and operate.</p>\n\n<p>A typical application build process includes phases like preparing the environment, updating the configuration, downloading dependencies, running unit tests, and finally, packaging the built artifact.</p>\n\n<p>Downloading dependencies is a critical phase in the build process. These dependent files can range in size from a few KBs to multiple MBs. Because most of the dependent files do not change frequently between builds, you can noticeably reduce your build time by caching dependencies.</p>\n\n<p>This will allow the code bundle to be deployed to Elastic Beanstalk to have both the dependencies and the code, hence speeding up the deployment time to Elastic Beanstalk</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Bundle the dependencies in the source code in CodeCommit</strong> - This is not the best practice and could make the CodeCommit repository huge.</p>\n\n<p><strong>Store the dependencies in S3, to be used while deploying to Beanstalk</strong> - This option acts as a distractor. S3 can be used as a storage location for your source code, logs, and other artifacts that are created when you use Elastic Beanstalk. Dependencies are used during the process of building code, not while deploying to Beanstalk.</p>\n\n<p><strong>Create a custom platform for Elastic Beanstalk</strong> - This is a more advanced feature that requires code changes, so does not fit the use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/\">https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/</a></p>\n",
                "options": [
                    {
                        "id": 1542,
                        "content": "<p>Create a custom platform for Elastic Beanstalk</p>",
                        "isValid": false
                    },
                    {
                        "id": 1543,
                        "content": "<p>Bundle the dependencies in the source code during the build stage of CodeBuild</p>",
                        "isValid": true
                    },
                    {
                        "id": 1544,
                        "content": "<p>Store the dependencies in S3, to be used while deploying to Beanstalk</p>",
                        "isValid": false
                    },
                    {
                        "id": 1545,
                        "content": "<p>Bundle the dependencies in the source code in CodeCommit</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 379,
            "attributes": {
                "createdAt": "2023-09-07T08:39:34.497Z",
                "updatedAt": "2023-09-07T08:39:34.497Z",
                "content": "<p>What steps can a developer take to optimize the performance of a CPU-bound AWS Lambda function and ensure fast response time?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Increase the function's memory</strong></p>\n\n<p>Memory is the principal lever available to Lambda developers for controlling the performance of a function. You can configure the amount of memory allocated to a Lambda function, between 128 MB and 10,240 MB. The Lambda console defaults new functions to the smallest setting and many developers also choose 128 MB for their functions.</p>\n\n<p>The amount of memory also determines the amount of virtual CPU available to a function. Adding more memory proportionally increases the amount of CPU, increasing the overall computational power available. If a function is CPU-, network- or memory-bound, then changing the memory setting can dramatically improve its performance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the function's provisioned concurrency</strong></p>\n\n<p><strong>Increase the function's reserved concurrency</strong></p>\n\n<p>In Lambda, concurrency is the number of requests your function can handle at the same time. There are two types of concurrency controls available:</p>\n\n<p>Reserved concurrency â€“ Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can use that concurrency. There is no charge for configuring reserved concurrency for a function.</p>\n\n<p>Provisioned concurrency â€“ Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations. Note that configuring provisioned concurrency incurs charges to your AWS account.</p>\n\n<p>Neither reserved concurrency nor provisioned concurrency has any impact on the CPU available to a function, so both these options are incorrect</p>\n\n<p><strong>Increase the function's CPU</strong> - This is a distractor as you cannot directly increase the CPU available to a function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/operatorguide/computing-power.html\">https://docs.aws.amazon.com/lambda/latest/operatorguide/computing-power.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n",
                "options": [
                    {
                        "id": 1546,
                        "content": "<p>Increase the function's CPU</p>",
                        "isValid": false
                    },
                    {
                        "id": 1547,
                        "content": "<p>Increase the function's provisioned concurrency</p>",
                        "isValid": false
                    },
                    {
                        "id": 1548,
                        "content": "<p>Increase the function's timeout</p>",
                        "isValid": false
                    },
                    {
                        "id": 1549,
                        "content": "<p>Increase the function's memory</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 380,
            "attributes": {
                "createdAt": "2023-09-07T08:39:34.581Z",
                "updatedAt": "2023-09-07T08:39:34.581Z",
                "content": "<p>While troubleshooting, a developer realized that the Amazon EC2 instance is unable to connect to the Internet using the Internet Gateway.</p>\n\n<p>Which conditions should be met for Internet connectivity to be established? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic</strong> - The network access control lists (ACLs) that are associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivity</p>\n\n<p><strong>The route table in the instanceâ€™s subnet should have a route to an Internet Gateway</strong> - A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instanceâ€™s subnet should have a route defined to the Internet Gateway.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The instance's subnet is not associated with any route table</strong> - This is an incorrect statement. A subnet is implicitly associated with the main route table if it is not explicitly associated with a particular route table. So, a subnet is always associated with some route table.</p>\n\n<p><strong>The instance's subnet is associated with multiple route tables with conflicting configurations</strong> - This is an incorrect statement. A subnet can only be associated with one route table at a time.</p>\n\n<p><strong>The subnet has been configured to be Public and has no access to internet</strong> - This is an incorrect statement. Public subnets have access to the internet via Internet Gateway.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html</a></p>\n",
                "options": [
                    {
                        "id": 1550,
                        "content": "<p>The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic</p>",
                        "isValid": true
                    },
                    {
                        "id": 1551,
                        "content": "<p>The instance's subnet is associated with multiple route tables with conflicting configurations</p>",
                        "isValid": false
                    },
                    {
                        "id": 1552,
                        "content": "<p>The subnet has been configured to be Public and has no access to the internet</p>",
                        "isValid": false
                    },
                    {
                        "id": 1553,
                        "content": "<p>The instance's subnet is not associated with any route table</p>",
                        "isValid": false
                    },
                    {
                        "id": 1554,
                        "content": "<p>The route table in the instanceâ€™s subnet should have a route to an Internet Gateway</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 381,
            "attributes": {
                "createdAt": "2023-09-07T08:39:34.653Z",
                "updatedAt": "2023-09-07T08:39:34.653Z",
                "content": "<p>A developer is looking at establishing access control for an API that connects to a Lambda function downstream.</p>\n\n<p>Which of the following represents a mechanism that CANNOT be used for authenticating with the API Gateway?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p><strong>AWS Security Token Service (STS)</strong> - AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). However, it is not supported by API Gateway.</p>\n\n<p>API Gateway supports the following mechanisms for authentication and authorization:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q55-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Standard AWS IAM roles and policies</strong> - Standard AWS IAM roles and policies offer flexible and robust access controls that can be applied to an entire API or individual methods. IAM roles and policies can be used for controlling who can create and manage your APIs, as well as who can invoke them.</p>\n\n<p><strong>Lambda Authorizer</strong> - Lambda authorizers are Lambda functions that control access to REST API methods using bearer token authenticationâ€”as well as information described by headers, paths, query strings, stage variables, or context variables request parameters. Lambda authorizers are used to control who can invoke REST API methods.</p>\n\n<p><strong>Cognito User Pools</strong> - Amazon Cognito user pools let you create customizable authentication and authorization solutions for your REST APIs. Amazon Cognito user pools are used to control who can invoke REST API methods.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html\">https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html</a></p>\n",
                "options": [
                    {
                        "id": 1555,
                        "content": "<p>Cognito User Pools</p>",
                        "isValid": false
                    },
                    {
                        "id": 1556,
                        "content": "<p>AWS Security Token Service (STS)</p>",
                        "isValid": true
                    },
                    {
                        "id": 1557,
                        "content": "<p>Standard AWS IAM roles and policies</p>",
                        "isValid": false
                    },
                    {
                        "id": 1558,
                        "content": "<p>Lambda Authorizer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 382,
            "attributes": {
                "createdAt": "2023-09-07T08:39:34.727Z",
                "updatedAt": "2023-09-07T08:39:34.727Z",
                "content": "<p>As a Developer, you are given a document written in YAML that represents the architecture of a serverless application. The first line of the document contains <code>Transform: 'AWS::Serverless-2016-10-31'</code>.</p>\n\n<p>What does the <code>Transform</code> section in the document represent?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. Templates include several major sections. The \"Resources\" section is the only required section. The optional \"Transform\" section specifies one or more macros that AWS CloudFormation uses to process your template.</p>\n\n<p><strong>Presence of <code>Transform</code> section indicates it is a Serverless Application Model (SAM) template</strong> - The AWS::Serverless transform, which is a macro hosted by AWS CloudFormation, takes an entire template written in the AWS Serverless Application Model (AWS SAM) syntax and transforms and expands it into a compliant AWS CloudFormation template. So, the presence of the <code>Transform</code> section indicates, the document is a SAM template.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>It represents a Lambda function definition</strong> - Lambda function is created using \"AWS::Lambda::Function\" resource and has no connection to <code>Transform</code> section.</p>\n\n<p><strong>It represents an intrinsic function</strong> - Intrinsic Functions in templates are used to assign values to properties that are not available until runtime. They usually start with <code>Fn::</code> or <code>!</code>. Example: <code>!Sub</code> or <code>Fn::Sub</code>.</p>\n\n<p><strong>Presence of 'Transform' section indicates it is a CloudFormation Parameter</strong> - CloudFormation parameters are part of <code>Parameters</code> block of the template, similar to below code:</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p>\n",
                "options": [
                    {
                        "id": 1559,
                        "content": "<p>Presence of <code>Transform</code> section indicates it is a Serverless Application Model (SAM) template</p>",
                        "isValid": true
                    },
                    {
                        "id": 1560,
                        "content": "<p>Presence of <code>Transform</code> section indicates it is a CloudFormation Parameter</p>",
                        "isValid": false
                    },
                    {
                        "id": 1561,
                        "content": "<p>It represents a Lambda function definition</p>",
                        "isValid": false
                    },
                    {
                        "id": 1562,
                        "content": "<p>It represents an intrinsic function</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 383,
            "attributes": {
                "createdAt": "2023-09-07T08:39:34.799Z",
                "updatedAt": "2023-09-07T08:39:34.799Z",
                "content": "<p>A business has their test environment built on Amazon EC2 configured on General purpose SSD volume.</p>\n\n<p>At which gp2 volume size will their test environment hit the max IOPS?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>The performance of gp2 volumes is tied to volume size, which determines the baseline performance level of the volume and how quickly it accumulates I/O credits; larger volumes have higher baseline performance levels and accumulate I/O credits faster.</p>\n\n<p><strong>5.3 TiB</strong> - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods of time. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size.</p>\n\n<p>Maximum IOPS vs Volume Size for General Purpose SSD (gp2) volumes:\n<img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/gp2_iops_1.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>10.6 TiB</strong> - As explained above, this is an incorrect option.</p>\n\n<p><strong>16 TiB</strong> - As explained above, this is an incorrect option.</p>\n\n<p><strong>2.7 TiB</strong> - As explained above, this is an incorrect option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n",
                "options": [
                    {
                        "id": 1563,
                        "content": "<p>16 TiB</p>",
                        "isValid": false
                    },
                    {
                        "id": 1564,
                        "content": "<p>2.7 TiB</p>",
                        "isValid": false
                    },
                    {
                        "id": 1565,
                        "content": "<p>10.6 TiB</p>",
                        "isValid": false
                    },
                    {
                        "id": 1566,
                        "content": "<p>5.3 TiB</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 384,
            "attributes": {
                "createdAt": "2023-09-07T08:39:34.887Z",
                "updatedAt": "2023-09-07T08:39:34.887Z",
                "content": "<p>A data analytics company is processing real-time Internet-of-Things (IoT) data via Kinesis Producer Library (KPL) and sending the data to a Kinesis Data Streams driven application. The application has halted data processing because of a ProvisionedThroughputExceeded exception.</p>\n\n<p>Which of the following actions would help in addressing this issue? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure the data producer to retry with an exponential backoff</strong></p>\n\n<p><strong>Increase the number of shards within your data streams to provide enough capacity</strong></p>\n\n<p>Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources.</p>\n\n<p>How Kinesis Data Streams Work\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p>The capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception.</p>\n\n<p>If this is due to a temporary rise of the data streamâ€™s input data rate, retry (with exponential backoff) by the data producer will eventually lead to the completion of the requests.</p>\n\n<p>If this is due to a sustained rise of the data streamâ€™s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Kinesis Agent instead of Kinesis Producer Library (KPL) for sending data to Kinesis Data Streams</strong> - Kinesis Agent works with data producers. Using Kinesis Agent instead of KPL will not help as the constraint is the capacity limit of the Kinesis Data Stream.</p>\n\n<p><strong>Use Amazon SQS instead of Kinesis Data Streams</strong> - This is a distractor as using SQS will not help address the ProvisionedThroughputExceeded exception for the Kinesis Data Stream. This option does not address the issues in the use-case.</p>\n\n<p><strong>Use Kinesis enhanced fan-out for Kinesis Data Streams</strong> - You should use enhanced fan-out if you have, or expect to have, multiple consumers retrieving data from a stream in parallel. Therefore, using enhanced fan-out will not help address the ProvisionedThroughputExceeded exception as the constraint is the capacity limit of the Kinesis Data Stream.</p>\n\n<p>Please review this note for more details on enhanced fan-out for Kinesis Data Streams:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q6-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 1567,
                        "content": "<p>Increase the number of shards within your data streams to provide enough capacity</p>",
                        "isValid": true
                    },
                    {
                        "id": 1568,
                        "content": "<p>Use Amazon Kinesis Agent instead of Kinesis Producer Library (KPL) for sending data to Kinesis Data Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 1569,
                        "content": "<p>Use Amazon SQS instead of Kinesis Data Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 1570,
                        "content": "<p>Use Kinesis enhanced fan-out for Kinesis Data Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 1571,
                        "content": "<p>Configure the data producer to retry with an exponential backoff</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 385,
            "attributes": {
                "createdAt": "2023-09-07T08:39:34.965Z",
                "updatedAt": "2023-09-07T08:39:34.965Z",
                "content": "<p>An Accounting firm extensively uses Amazon EBS volumes for persistent storage of application data of Amazon EC2 instances. The volumes are encrypted to protect the critical data of the clients. As part of managing the security credentials, the project manager has come across a policy snippet that looks like the following:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Allow for use of this Key\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111122223333:role/UserRole\"\n            },\n            \"Action\": [\n                \"kms:GenerateDataKeyWithoutPlaintext\",\n                \"kms:Decrypt\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"Allow for EC2 Use\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111122223333:role/UserRole\"\n            },\n            \"Action\": [\n                \"kms:CreateGrant\",\n                \"kms:ListGrants\",\n                \"kms:RevokeGrant\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                \"kms:ViaService\": \"ec2.us-west-2.amazonaws.com\"\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>Which of the following options are correct regarding the policy?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The first statement provides a specified IAM principal the ability to generate a data key and decrypt that data key from the CMK when necessary</strong> - To create and use an encrypted Amazon Elastic Block Store (EBS) volume, you need permissions to use Amazon EBS. The key policy associated with the CMK would need to include these. The above policy is an example of one such policy.</p>\n\n<p>In this CMK policy, the first statement provides a specified IAM principal the ability to generate a data key and decrypt that data key from the CMK when necessary. These two APIs are necessary to encrypt the EBS volume while itâ€™s attached to an Amazon Elastic Compute Cloud (EC2) instance.</p>\n\n<p>The second statement in this policy provides the specified IAM principal the ability to create, list, and revoke grants for Amazon EC2. Grants are used to delegate a subset of permissions to AWS services, or other principals, so that they can use your keys on your behalf. In this case, the condition policy explicitly ensures that only Amazon EC2 can use the grants. Amazon EC2 will use them to re-attach an encrypted EBS volume back to an instance if the volume gets detached due to a planned or unplanned outage. These events will be recorded within AWS CloudTrail when, and if, they do occur for your\nauditing.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The first statement provides the security group the ability to generate a data key and decrypt that data key from the CMK when necessary</strong></p>\n\n<p><strong>The second statement in this policy provides the security group (mentioned in the first statement of the policy), the ability to create, list, and revoke grants for Amazon EC2</strong></p>\n\n<p><strong>The second statement in the policy mentions that all the resources stated in the first statement can take the specified role which will provide the ability to create, list, and revoke grants for Amazon EC2</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://d0.awsstatic.com/whitepapers/aws-kms-best-practices.pdf\">https://d0.awsstatic.com/whitepapers/aws-kms-best-practices.pdf</a></p>\n",
                "options": [
                    {
                        "id": 1572,
                        "content": "<p>The first statement provides a specified IAM principal the ability to generate a data key and decrypt that data key from the CMK when necessary</p>",
                        "isValid": true
                    },
                    {
                        "id": 1573,
                        "content": "<p>The second statement in this policy provides the security group (mentioned in first statement of the policy), the ability to create, list, and revoke grants for Amazon EC2</p>",
                        "isValid": false
                    },
                    {
                        "id": 1574,
                        "content": "<p>The first statement provides the security group the ability to generate a data key and decrypt that data key from the CMK when necessary</p>",
                        "isValid": false
                    },
                    {
                        "id": 1575,
                        "content": "<p>The second statement in the policy mentions that all the resources stated in the first statement can take the specified role which will provide the ability to create, list, and revoke grants for Amazon EC2</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 386,
            "attributes": {
                "createdAt": "2023-09-07T08:39:35.040Z",
                "updatedAt": "2023-09-07T08:39:35.040Z",
                "content": "<p>A company wants to automate its order fulfillment and inventory tracking workflow. Starting from order creation to updating inventory to shipment, the entire process has to be tracked, managed and updated automatically.</p>\n\n<p>Which of the following would you recommend as the most optimal solution for this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Step Functions to coordinate and manage the components of order management and inventory tracking workflow</strong></p>\n\n<p>AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that maintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by your business logic.</p>\n\n<p>AWS Step Functions enables you to implement a business process as a series of steps that make up a workflow. The individual steps in the workflow can invoke a Lambda function or a container that has some business logic, update a database such as DynamoDB or publish a message to a queue once that step or the entire workflow completes execution.</p>\n\n<p>Benefits of Step Functions:</p>\n\n<p>Build and update apps quickly: AWS Step Functions lets you build visual workflows that enable the fast translation of business requirements into technical requirements. You can build applications in a matter of minutes, and when needs change, you can swap or reorganize components without customizing any code.</p>\n\n<p>Improve resiliency: AWS Step Functions manages state, checkpoints and restarts for you to make sure that your application executes in order and as expected. Built-in try/catch, retry and rollback capabilities deal with errors and exceptions automatically.</p>\n\n<p>Write less code: AWS Step Functions manages the logic of your application for you and implements basic primitives such as branching, parallel execution, and timeouts. This removes extra code that may be repeated in your microservices and functions.</p>\n\n<p>How Step Functions work:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q19-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Simple Queue Service (Amazon SQS) queue to pass information from order management to inventory tracking workflow</strong> - You should consider AWS Step Functions when you need to coordinate service components in the development of highly scalable and auditable applications. You should consider using Amazon Simple Queue Service (Amazon SQS), when you need a reliable, highly scalable, hosted queue for sending, storing, and receiving messages between services. Step Functions keeps track of all tasks and events in an application. Amazon SQS requires you to implement your own application-level tracking, especially if your application uses multiple queues.</p>\n\n<p><strong>Configure Amazon EventBridge to track the flow of work from order management to inventory tracking systems</strong> - Both Amazon EventBridge and Amazon SNS can be used to develop event-driven applications, and your choice will depend on your specific needs. Amazon EventBridge is recommended when you want to build an application that reacts to events from SaaS applications and/or AWS services. Amazon EventBridge is the only event-based service that integrates directly with third-party SaaS partners.</p>\n\n<p><strong>Use Amazon SNS to develop event-driven applications that can share information</strong> - Amazon SNS is recommended when you want to build an application that reacts to high throughput or low latency messages published by other applications or microservices (as Amazon SNS provides nearly unlimited throughput), or for applications that need very high fan-out (thousands or millions of endpoints).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/faqs/\">https://aws.amazon.com/step-functions/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/eventbridge/faqs/\">https://aws.amazon.com/eventbridge/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 1576,
                        "content": "<p>Use AWS Step Functions to coordinate and manage the components of order management and inventory tracking workflow</p>",
                        "isValid": true
                    },
                    {
                        "id": 1577,
                        "content": "<p>Use Amazon Simple Queue Service (Amazon SQS) queue to pass information from order management to inventory tracking workflow</p>",
                        "isValid": false
                    },
                    {
                        "id": 1578,
                        "content": "<p>Use Amazon SNS to develop event-driven applications that can share information</p>",
                        "isValid": false
                    },
                    {
                        "id": 1579,
                        "content": "<p>Configure Amazon EventBridge to track the flow of work from order management to inventory tracking systems</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 387,
            "attributes": {
                "createdAt": "2023-09-07T08:39:35.128Z",
                "updatedAt": "2023-09-07T08:39:35.128Z",
                "content": "<p>Recently in your organization, the AWS X-Ray SDK was bundled into each Lambda function to record outgoing calls for tracing purposes. When your team leader goes to the X-Ray service in the AWS Management Console to get an overview of the information collected, they discover that no data is available.</p>\n\n<p>What is the most likely reason for this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your applicationâ€™s underlying components.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p><strong>Fix the IAM Role</strong></p>\n\n<p>Create an IAM role with write permissions and assign it to the resources running your application. You can use AWS Identity and Access Management (IAM) to grant X-Ray permissions to users and compute resources in your account. This should be one of the first places you start by checking that your permissions are properly configured before exploring other troubleshooting options.</p>\n\n<p>Here is an example of X-Ray Read-Only permissions via an IAM policy:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"xray:GetSamplingRules\",\n                \"xray:GetSamplingTargets\",\n                \"xray:GetSamplingStatisticSummaries\",\n                \"xray:BatchGetTraces\",\n                \"xray:GetServiceGraph\",\n                \"xray:GetTraceGraph\",\n                \"xray:GetTraceSummaries\",\n                \"xray:GetGroups\",\n                \"xray:GetGroup\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Another example of write permissions for using X-Ray via an IAM policy:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"xray:PutTraceSegments\",\n                \"xray:PutTelemetryRecords\",\n                \"xray:GetSamplingRules\",\n                \"xray:GetSamplingTargets\",\n                \"xray:GetSamplingStatisticSummaries\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable X-Ray sampling</strong> - If permissions are not configured correctly sampling will not work, so this option is not correct.</p>\n\n<p><strong>X-Ray only works with AWS Lambda aliases</strong> - This is not true, aliases are pointers to specific Lambda function versions. To use the X-Ray SDK on Lambda, bundle it with your function code each time you create a new version.</p>\n\n<p><strong>Change the security group rules</strong> - You grant permissions to your Lambda function to access other resources using an IAM role and not via security groups.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/security_iam_troubleshoot.html\">https://docs.aws.amazon.com/xray/latest/devguide/security_iam_troubleshoot.html</a></p>\n",
                "options": [
                    {
                        "id": 1580,
                        "content": "<p>Fix the IAM Role</p>",
                        "isValid": true
                    },
                    {
                        "id": 1581,
                        "content": "<p>Enable X-Ray sampling</p>",
                        "isValid": false
                    },
                    {
                        "id": 1582,
                        "content": "<p>Change the security group rules</p>",
                        "isValid": false
                    },
                    {
                        "id": 1583,
                        "content": "<p>X-Ray only works with AWS Lambda aliases</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 388,
            "attributes": {
                "createdAt": "2023-09-07T08:39:35.202Z",
                "updatedAt": "2023-09-07T08:39:35.202Z",
                "content": "<p>Your company has embraced cloud-native microservices architectures. New applications must be dockerized and stored in a registry service offered by AWS. The architecture should support dynamic port mapping and support multiple tasks from a single service on the same container instance. All services should run on the same EC2 instance.</p>\n\n<p>Which of the following options offers the best-fit solution for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Application Load Balancer + ECS</strong></p>\n\n<p>Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control over your infrastructure, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.</p>\n\n<p><img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png\">\nvia - <a href=\"https://aws.amazon.com/ecs/\">https://aws.amazon.com/ecs/</a></p>\n\n<p>An Application load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/images/component_architecture.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p>\n\n<p>When you deploy your services using Amazon Elastic Container Service (Amazon ECS), you can use dynamic port mapping to support multiple tasks from a single service on the same container instance. Amazon ECS manages updates to your services by automatically registering and deregistering containers with your target group using the instance ID and port for each container.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q24-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Classic Load Balancer + Beanstalk</strong> - The Classic Load Balancer doesn't allow you to run multiple copies of a task on the same instance. Instead, with the Classic Load Balancer, you must statically map port numbers on a container instance. So this option is ruled out.</p>\n\n<p><strong>Application Load Balancer + Beanstalk</strong> - You can create docker environments that support multiple containers per Amazon EC2 instance with a multi-container Docker platform for Elastic Beanstalk. However, ECS gives you finer control.</p>\n\n<p><strong>Classic Load Balancer + ECS</strong> - The Classic Load Balancer doesn't allow you to run multiple copies of a task in the same instance. Instead, with the Classic Load Balancer, you must statically map port numbers on a container instance. So this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/tutorial-target-ecs-containers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/tutorial-target-ecs-containers.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html</a></p>\n",
                "options": [
                    {
                        "id": 1584,
                        "content": "<p>Classic Load Balancer + Beanstalk</p>",
                        "isValid": false
                    },
                    {
                        "id": 1585,
                        "content": "<p>Classic Load Balancer + ECS</p>",
                        "isValid": false
                    },
                    {
                        "id": 1586,
                        "content": "<p>Application Load Balancer + ECS</p>",
                        "isValid": true
                    },
                    {
                        "id": 1587,
                        "content": "<p>Application Load Balancer + Beanstalk</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 389,
            "attributes": {
                "createdAt": "2023-09-07T08:39:35.275Z",
                "updatedAt": "2023-09-07T08:39:35.275Z",
                "content": "<p>A university has created a student portal that is accessible through a smartphone app and web application. The smartphone app is available in both Android and IOS and the web application works on most major browsers. Students will be able to do group study online and create forum questions. All changes made via smartphone devices should be available even when offline and should synchronize with other devices.</p>\n\n<p>Which of the following AWS services will meet these requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Cognito Sync</strong></p>\n\n<p>Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Cognito Identity Pools</strong> - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.</p>\n\n<p><strong>Cognito User Pools</strong> - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q40-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p><strong>Beanstalk</strong> - With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>How Elastic BeanStalk Works:\n<img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/clearbox-flow-00.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html</a></p>\n",
                "options": [
                    {
                        "id": 1588,
                        "content": "<p>BeanStalk</p>",
                        "isValid": false
                    },
                    {
                        "id": 1589,
                        "content": "<p>Cognito Sync</p>",
                        "isValid": true
                    },
                    {
                        "id": 1590,
                        "content": "<p>Cognito User Pools</p>",
                        "isValid": false
                    },
                    {
                        "id": 1591,
                        "content": "<p>Cognito Identity Pools</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 390,
            "attributes": {
                "createdAt": "2023-09-07T08:39:35.347Z",
                "updatedAt": "2023-09-07T08:39:35.347Z",
                "content": "<p>The development team at a retail company is gearing up for the upcoming Thanksgiving sale and wants to make sure that the application's serverless backend running via Lambda functions does not hit latency bottlenecks as a result of the traffic spike.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure Application Auto Scaling to manage Lambda provisioned concurrency on a schedule</strong></p>\n\n<p>Concurrency is the number of requests that a Lambda function is serving at any given time. If a Lambda function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.</p>\n\n<p>Due to a spike in traffic, when Lambda functions scale, this causes the portion of requests that are served by new instances to have higher latency than the rest. To enable your function to scale without fluctuations in latency, use provisioned concurrency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency.</p>\n\n<p>You can configure Application Auto Scaling to manage provisioned concurrency on a schedule or based on utilization. Use scheduled scaling to increase provisioned concurrency in anticipation of peak traffic. To increase provisioned concurrency automatically as needed, use the Application Auto Scaling API to register a target and create a scaling policy.</p>\n\n<p>Please see this note for more details on provisioned concurrency:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule</strong> - To ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency. When a function has reserved concurrency, no other function can use that concurrency. More importantly, reserved concurrency also limits the maximum concurrency for the function, and applies to the function as a whole, including versions and aliases.</p>\n\n<p>You cannot configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule.</p>\n\n<p><strong>Add an Application Load Balancer in front of the Lambda functions</strong> - This is a distractor as just adding the Application Load Balancer will not help in scaling the Lambda functions to address the surge in traffic.</p>\n\n<p><strong>No need to make any special provisions as Lambda is automatically scalable because of its serverless nature</strong> - It's true that Lambda is serverless, however, due to the surge in traffic the Lambda functions can still hit the concurrency limits. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</a></p>\n",
                "options": [
                    {
                        "id": 1592,
                        "content": "<p>Add an Application Load Balancer in front of the Lambda functions</p>",
                        "isValid": false
                    },
                    {
                        "id": 1593,
                        "content": "<p>Configure Application Auto Scaling to manage Lambda provisioned concurrency on a schedule</p>",
                        "isValid": true
                    },
                    {
                        "id": 1594,
                        "content": "<p>No need to make any special provisions as Lambda is automatically scalable because of its serverless nature</p>",
                        "isValid": false
                    },
                    {
                        "id": 1595,
                        "content": "<p>Configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 391,
            "attributes": {
                "createdAt": "2023-09-07T08:39:35.427Z",
                "updatedAt": "2023-09-07T08:39:35.427Z",
                "content": "<p>An e-commerce company manages a microservices application that receives orders from various partners through a customized API for each partner exposed via Amazon API Gateway. The orders are processed by a shared Lambda function.</p>\n\n<p>How can the company notify each partner regarding the status of their respective orders in the most efficient manner, without affecting other partners' orders? Also, the solution should be scalable to accommodate new partners with minimal code changes required.</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up an SNS topic and subscribe each partner to the SNS topic. Modify the Lambda function to publish messages with specific attributes to the SNS topic and apply the appropriate filter policy to the topic subscriptions</strong></p>\n\n<p>An Amazon SNS topic is a logical access point that acts as a communication channel. A topic lets you group multiple endpoints (such as AWS Lambda, Amazon SQS, HTTP/S, or an email address). For example, to broadcast the messages of a message-producer system (such as, an e-commerce website) working with multiple other services that require its messages (for example, checkout and fulfillment systems), you can create a topic for your producer system.</p>\n\n<p>By default, an Amazon SNS topic subscriber receives every message that's published to the topic. To receive only a subset of the messages, a subscriber must assign a filter policy to the topic subscription. A filter policy is a JSON object containing properties that define which messages the subscriber receives. Amazon SNS supports policies that act on the message attributes or the message body, according to the filter policy scope that you set for the subscription. Filter policies for the message body assume that the message payload is a well-formed JSON object.</p>\n\n<p>For the given use case, you can change the Lambda function to publish messages with specific attributes to the single SNS topic and apply the appropriate filter policy to the topic subscriptions for each of the partners. This is also easily scalable for new partners since only the filter policy needs to be set up for the new partner.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a separate SNS topic for each partner. Modify the Lambda function to publish messages for each partner to the partner's SNS topic</strong></p>\n\n<p><strong>Set up a separate SNS topic for each partner and subscribe each partner to the respective SNS topic. Modify the Lambda function to publish messages with specific attributes to the partner's SNS topic and apply the appropriate filter policy to the topic subscriptions</strong></p>\n\n<p>Both of these options represent an inefficient solution as there is no need to segregate each partner's updates into a separate SNS topic. A single SNS topic with distinct filter policies is sufficient.</p>\n\n<p><strong>Set up a separate Lambda function for each partner. Set up an SNS topic and subscribe each partner to the SNS topic. Modify each partner's Lambda function to publish messages with specific attributes to the SNS topic and apply the appropriate filter policy to the topic subscriptions</strong> - This is again an inefficient solution as there is no need to create a separate Lambda function for each partner as just a shared Lambda function is sufficient to process the orders and send an update to the single SNS topic with distinct filter policies.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-create-topic.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-create-topic.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html</a></p>\n",
                "options": [
                    {
                        "id": 1596,
                        "content": "<p>Set up a separate Lambda function for each partner. Set up an SNS topic and subscribe each partner to the SNS topic. Modify each partner's Lambda function to publish messages with specific attributes to the SNS topic and apply the appropriate filter policy to the topic subscriptions</p>",
                        "isValid": false
                    },
                    {
                        "id": 1597,
                        "content": "<p>Set up an SNS topic and subscribe each partner to the SNS topic. Modify the Lambda function to publish messages with specific attributes to the SNS topic and apply the appropriate filter policy to the topic subscriptions</p>",
                        "isValid": true
                    },
                    {
                        "id": 1598,
                        "content": "<p>Set up a separate SNS topic for each partner. Modify the Lambda function to publish messages for each partner to the partner's SNS topic</p>",
                        "isValid": false
                    },
                    {
                        "id": 1599,
                        "content": "<p>Set up a separate SNS topic for each partner and subscribe each partner to the respective SNS topic. Modify the Lambda function to publish messages with specific attributes to the partner's SNS topic and apply the appropriate filter policy to the topic subscriptions</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 392,
            "attributes": {
                "createdAt": "2023-09-07T08:39:35.501Z",
                "updatedAt": "2023-09-07T08:39:35.501Z",
                "content": "<p>A developer wants to securely store and retrieve various types of variables, such as remote API authentication information, API URL, and related credentials across different environments of an application deployed on Amazon Elastic Container Service (Amazon ECS).</p>\n\n<p>What would be the best approach that needs minimal modifications in the application code?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure the application to fetch the variables and credentials from AWS Systems Manager Parameter Store by leveraging hierarchical unique paths in Parameter Store for each variable in each environment</strong></p>\n\n<p>Parameter Stores is a capability of AWS Systems Manager that provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter.</p>\n\n<p>Managing dozens or hundreds of parameters as a flat list is time-consuming and prone to errors. It can also be difficult to identify the correct parameter for a task. This means you might accidentally use the wrong parameter, or you might create multiple parameters that use the same configuration data.</p>\n\n<p>You can use parameter hierarchies to help you organize and manage parameters. A hierarchy is a parameter name that includes a path that you define by using forward slashes (/).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q30-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-hierarchies.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-hierarchies.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the application to fetch the variables from AWS KMS by storing the API URL and credentials as unique keys in KMS for each environment</strong> - AWS KMS lets you create, manage, and control cryptographic keys across your applications and AWS services. KMS is not a key-value service that can be used for the given use case.</p>\n\n<p><strong>Configure the application to fetch the variables from an encrypted file that is stored with the application by storing the API URL and credentials in unique files for each environment</strong> - It is not considered a security best practice to store sensitive data and credentials in an encrypted file with the application. So this option is incorrect.</p>\n\n<p><strong>Configure the application to fetch the variables from each of the deployed environments by defining the authentication information and API URL in the ECS task definition as unique names during the deployment process</strong> - ECS task definition can be thought of as a blueprint for your application. Task definitions specify various parameters for your application. Examples of task definition parameters are which containers to use, which launch type to use, which ports should be opened for your application, and what data volumes should be used with the containers in the task. The specific parameters available for the task definition depend on which launch type you are using. The task definition is a text file, in JSON format, that describes one or more containers, up to a maximum of ten, that form your application. A task is the instantiation of a task definition within a cluster. After you create a task definition for your application within Amazon ECS, you can specify the number of tasks to run on your cluster.</p>\n\n<p>AWS recommends storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters. Environment variables specified in the task definition are readable by all users and roles that are allowed the DescribeTaskDefinition action for the task definition. So this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q30-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-hierarchies.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-hierarchies.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kms/\">https://aws.amazon.com/kms/</a></p>\n\n<p><a href=\"https://ecsworkshop.com/introduction/ecs_basics/task_definition/\">https://ecsworkshop.com/introduction/ecs_basics/task_definition/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html</a></p>\n",
                "options": [
                    {
                        "id": 1600,
                        "content": "<p>Configure the application to fetch the variables from an encrypted file that is stored with the application by storing the API URL and credentials in unique files for each environment</p>",
                        "isValid": false
                    },
                    {
                        "id": 1601,
                        "content": "<p>Configure the application to fetch the variables from AWS KMS by storing the API URL and credentials as unique keys in KMS for each environment</p>",
                        "isValid": false
                    },
                    {
                        "id": 1602,
                        "content": "<p>Configure the application to fetch the variables and credentials from AWS Systems Manager Parameter Store by leveraging hierarchical unique paths in Parameter Store for each variable in each environment</p>",
                        "isValid": true
                    },
                    {
                        "id": 1603,
                        "content": "<p>Configure the application to fetch the variables from each of the deployed environments by defining the authentication information and API URL in the ECS task definition as unique names during the deployment process</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 393,
            "attributes": {
                "createdAt": "2023-09-07T08:39:35.579Z",
                "updatedAt": "2023-09-07T08:39:35.579Z",
                "content": "<p>A company uses Amazon RDS as its database. For improved user experience, it has been decided that a highly reliable fully-managed caching layer has to be configured in front of RDS.</p>\n\n<p>Which of the following is the right choice, keeping in mind that cache content regeneration is a costly activity?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Implement Amazon ElastiCache Redis in Cluster-Mode</strong> - One can leverage ElastiCache for Redis with cluster mode enabled to enhance reliability and availability with little change to your existing workload. Cluster mode comes with the primary benefit of horizontal scaling of your Redis cluster, with almost zero impact on the performance of the cluster.</p>\n\n<p>When building production workloads, you should consider using a configuration with replication, unless you can easily recreate your data. Enabling Cluster-Mode provides a number of additional benefits in scaling your cluster. In short, it allows you to scale in or out the number of shards (horizontal scaling) versus scaling up or down the node type (vertical scaling). This means that Cluster-Mode can scale to very large amounts of storage (potentially 100s of terabytes) across up to 90 shards, whereas a single node can only store as much data in memory as the instance type has capacity for.</p>\n\n<p>Redis Cluster config:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q14-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/\">https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Install Redis on an Amazon EC2 instance</strong> - It is possible to install Redis directly onto Amazon EC2 instance. But, unlike ElastiCache for Redis, which is a managed service, you will need to maintain and manage your Redis installation.</p>\n\n<p><strong>Implement Amazon ElastiCache Memcached</strong> - Redis and Memcached are popular, open-source, in-memory data stores. Although they are both easy to use and offer high performance, there are important differences to consider when choosing an engine. Memcached is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases. Redis offers snapshots facility, replication, and supports transactions, which Memcached cannot and hence ElastiCache Redis is the right choice for our use case.</p>\n\n<p><strong>Migrate the database to Amazon Redshift</strong> - Amazon Redshift belongs to \"Big Data as a Service\" cloud facility, while Redis can be primarily classified under \"In-Memory Databases\". \"Data Warehousing\" is the primary reason why developers consider Amazon Redshift over the competitors, whereas \"Performance\" is the key factor in picking Redis.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/\">https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n",
                "options": [
                    {
                        "id": 1604,
                        "content": "<p>Implement Amazon ElastiCache Redis in Cluster Mode</p>",
                        "isValid": true
                    },
                    {
                        "id": 1605,
                        "content": "<p>Implement Amazon ElastiCache Memcached</p>",
                        "isValid": false
                    },
                    {
                        "id": 1606,
                        "content": "<p>Migrate the database to Amazon Redshift</p>",
                        "isValid": false
                    },
                    {
                        "id": 1607,
                        "content": "<p>Install Redis on an Amazon EC2 instance</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 394,
            "attributes": {
                "createdAt": "2023-09-07T08:39:35.649Z",
                "updatedAt": "2023-09-07T08:39:35.649Z",
                "content": "<p>A large firm stores its static data assets on Amazon S3 buckets. Each service line of the firm has its own AWS account. For a business use case, the Finance department needs to give access to their S3 bucket's data to the Human Resources department.</p>\n\n<p>Which of the below options is NOT feasible for cross-account access of S3 bucket objects?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use IAM roles and resource-based policies delegate access across accounts within different partitions via programmatic access only</strong> - This statement is incorrect and hence the right choice for this question. IAM roles and resource-based policies delegate access across accounts only within a single partition. For example, assume that you have an account in US West (N. California) in the standard <code>aws</code> partition. You also have an account in China (Beijing) in the <code>aws-cn</code> partition. You can't use an Amazon S3 resource-based policy in your account in China (Beijing) to allow access for users in your standard AWS account.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Resource-based policies and AWS Identity and Access Management (IAM) policies for programmatic-only access to S3 bucket objects</strong> - Use bucket policies to manage cross-account control and audit the S3 object's permissions. If you apply a bucket policy at the bucket level, you can define who can access (Principal element), which objects they can access (Resource element), and how they can access (Action element). Applying a bucket policy at the bucket level allows you to define granular access to different objects inside the bucket by using multiple policies to control access. You can also review the bucket policy to see who can access objects in an S3 bucket.</p>\n\n<p><strong>Use Access Control List (ACL) and IAM policies for programmatic-only access to S3 bucket objects</strong> - Use object ACLs to manage permissions only for specific scenarios and only if ACLs meet your needs better than IAM and S3 bucket policies. Amazon S3 ACLs allow users to define only the following permissions sets: READ, WRITE, READ_ACP, WRITE_ACP, and FULL_CONTROL. You can use only an AWS account or one of the predefined Amazon S3 groups as a grantee for the Amazon S3 ACL.</p>\n\n<p><strong>Use Cross-account IAM roles for programmatic and console access to S3 bucket objects</strong> - Not all AWS services support resource-based policies. This means that you can use cross-account IAM roles to centralize permission management when providing cross-account access to multiple services. Using cross-account IAM roles simplifies provisioning cross-account access to S3 objects that are stored in multiple S3 buckets, removing the need to manage multiple policies for S3 buckets. This method allows cross-account access to objects that are owned or uploaded by another AWS account or AWS services. If you don't use cross-account IAM roles, the object ACL must be modified.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroughs-managing-access-example3.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroughs-managing-access-example3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/\">https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/</a></p>\n",
                "options": [
                    {
                        "id": 1608,
                        "content": "<p>Use Resource-based policies and AWS Identity and Access Management (IAM) policies for programmatic-only access to S3 bucket objects</p>",
                        "isValid": false
                    },
                    {
                        "id": 1609,
                        "content": "<p>Use Access Control List (ACL) and IAM policies for programmatic-only access to S3 bucket objects</p>",
                        "isValid": false
                    },
                    {
                        "id": 1610,
                        "content": "<p>Use IAM roles and resource-based policies delegate access across accounts within different partitions via programmatic access only</p>",
                        "isValid": true
                    },
                    {
                        "id": 1611,
                        "content": "<p>Use Cross-account IAM roles for programmatic and console access to S3 bucket objects</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 395,
            "attributes": {
                "createdAt": "2023-09-07T08:39:35.734Z",
                "updatedAt": "2023-09-07T08:39:35.734Z",
                "content": "<p>A new member of your team is working on creating Dead Letter Queue (DLQ) for AWS Lambda functions.</p>\n\n<p>As a Developer Associate, can you help him identify the use cases, wherein AWS Lambda will add a message into a DLQ after being processed? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The Lambda function invocation is asynchronous</strong> - When an asynchronous invocation event exceeds the maximum age or fails all retry attempts, Lambda discards it. Or sends it to dead-letter queue if you have configured one.</p>\n\n<p><strong>The event fails all processing attempt</strong> - A dead-letter queue acts the same as an on-failure destination in that it is used when an event fails all processing attempts or expires without being processed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Lambda function invocation is synchronous</strong> - When you invoke a function synchronously, Lambda runs the function and waits for a response. Queues are generally used with asynchronous invocations since queues implement the decoupling feature of various connected systems. It does not make sense to use queues when the calling code will wait on it for a response.</p>\n\n<p><strong>The event has been processed successfully</strong> - A successfully processed event is not sent to the dead-letter queue.</p>\n\n<p><strong>The event processing failed only once but succeeded thereafter</strong> - A successfully processed event is not sent to the dead-letter queue.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html</a></p>\n",
                "options": [
                    {
                        "id": 1612,
                        "content": "<p>The Lambda function invocation failed only once but succeeded thereafter</p>",
                        "isValid": false
                    },
                    {
                        "id": 1613,
                        "content": "<p>The Lambda function invocation is synchronous</p>",
                        "isValid": false
                    },
                    {
                        "id": 1614,
                        "content": "<p>The Lambda function invocation is asynchronous</p>",
                        "isValid": true
                    },
                    {
                        "id": 1615,
                        "content": "<p>The event fails all processing attempts</p>",
                        "isValid": true
                    },
                    {
                        "id": 1616,
                        "content": "<p>The event has been processed successfully</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 396,
            "attributes": {
                "createdAt": "2023-09-07T08:39:35.808Z",
                "updatedAt": "2023-09-07T08:39:35.808Z",
                "content": "<p>You are running a cloud file storage website with an Internet-facing Application Load Balancer, which routes requests from users over the internet to 10 registered Amazon EC2 instances. Users are complaining that your website always asks them to re-authenticate when they switch pages. You are puzzled because this behavior is not seen in your local machine or dev environment.</p>\n\n<p>What could be the reason?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The Load Balancer does not have stickiness enabled</strong> - Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies.</p>\n\n<p>When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, the load balancer detects the cookie and routes the request to the same target.</p>\n\n<p>More info here:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q58-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Application Load Balancer is in slow-start mode, which gives ALB a little more time to read and write session data</strong> - This is an invalid statement. The load balancer serves as a single point of contact for clients and distributes incoming traffic across its healthy registered targets. By default, a target starts to receive its full share of requests as soon as it is registered with a target group and passes an initial health check. Using slow start mode gives targets time to warm up before the load balancer sends them a full share of requests. This does not help in session management.</p>\n\n<p><strong>The EC2 instances are logging out the users because the instances never have access to the client IPs because of the Load Balancer</strong> - This is an incorrect statement. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to the server. If needed, the server can read IP addresses from this data.</p>\n\n<p><strong>The Load Balancer does not have TLS enabled</strong> - To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. The load balancer uses a server certificate to terminate the front-end connection and then decrypt requests from clients before sending them to the targets. This does not help in session management.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#slow-start-mode\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#slow-start-mode</a></p>\n",
                "options": [
                    {
                        "id": 1617,
                        "content": "<p>The Load Balancer does not have TLS enabled</p>",
                        "isValid": false
                    },
                    {
                        "id": 1618,
                        "content": "<p>Application Load Balancer is in slow-start mode, which gives ALB a little more time to read and write session data</p>",
                        "isValid": false
                    },
                    {
                        "id": 1619,
                        "content": "<p>The Load Balancer does not have stickiness enabled</p>",
                        "isValid": true
                    },
                    {
                        "id": 1620,
                        "content": "<p>The EC2 instances are logging out the users because the instances never have access to the client IPs because of the Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 397,
            "attributes": {
                "createdAt": "2023-09-07T08:39:35.882Z",
                "updatedAt": "2023-09-07T08:39:35.882Z",
                "content": "<p>A developer is working on an AWS Lambda function that reads data from Amazon S3 objects and writes the data to an Amazon DynamoDB table. Although the function triggers successfully from an S3 event notification upon object creation, it encounters a failure while attempting to write data to the DynamoDB table.</p>\n\n<p>What is the probable reason for the failure?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The Lambda function does not have IAM permissions to write to DynamoDB</strong></p>\n\n<p>You need to use an identity-based policy that allows read and write access to a specific Amazon DynamoDB table. To use this policy, attach the policy to a Lambda service role. A service role is a role that you create in your account to allow a service to perform actions on your behalf. That service role must include AWS Lambda as the principal in the trust policy. The role is then used to grant a Lambda function access to a DynamoDB table. By using an IAM policy and role to control access, you donâ€™t need to embed credentials in code and can tightly control which services the Lambda function can access.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Lambda function's provisioned concurrency limit has been exceeded</strong></p>\n\n<p><strong>The Lambda function's reserved concurrency limit has been exceeded</strong></p>\n\n<p>Reserved concurrency â€“ Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can use that concurrency. There is no charge for configuring reserved concurrency for a function.</p>\n\n<p>Provisioned concurrency â€“ Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations. Note that configuring provisioned concurrency incurs charges to your AWS account.</p>\n\n<p>Neither reserved concurrency nor provisioned concurrency has any relevance to the given use case. Both options have been added as distractors.</p>\n\n<p><strong>DynamoDB table does not have a Gateway VPC Endpoint, which is required by the Lambda function for a successful write</strong> - Gateway endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC. Gateway endpoints do not enable AWS PrivateLink. This option acts as a distractor since the Lambda function is not provisioned within a VPC by default, so there is no need of a Gateway VPC Endpoint to access DynamoDB.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_lambda-access-dynamodb.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_lambda-access-dynamodb.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-create-an-aws-iam-policy-to-grant-aws-lambda-access-to-an-amazon-dynamodb-table/\">https://aws.amazon.com/blogs/security/how-to-create-an-aws-iam-policy-to-grant-aws-lambda-access-to-an-amazon-dynamodb-table/</a></p>\n",
                "options": [
                    {
                        "id": 1621,
                        "content": "<p>The Lambda function does not have IAM permissions to write to DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 1622,
                        "content": "<p>The Lambda function's provisioned concurrency limit has been exceeded</p>",
                        "isValid": false
                    },
                    {
                        "id": 1623,
                        "content": "<p>The Lambda function's reserved concurrency limit has been exceeded</p>",
                        "isValid": false
                    },
                    {
                        "id": 1624,
                        "content": "<p>DynamoDB table does not have a Gateway VPC Endpoint, which is required by the Lambda function for a successful write</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 398,
            "attributes": {
                "createdAt": "2023-09-07T08:39:35.963Z",
                "updatedAt": "2023-09-07T08:39:35.963Z",
                "content": "<p>Your application is deployed automatically using AWS Elastic Beanstalk. Your YAML configuration files are stored in the folder .ebextensions and new files are added or updated often. The DevOps team does not want to re-deploy the application every time there are configuration changes, instead, they would rather manage configuration externally, securely, and have it load dynamically into the application at runtime.</p>\n\n<p>What option allows you to do this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use SSM Parameter Store</strong></p>\n\n<p>AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. For the given use-case, as the DevOps team does not want to re-deploy the application every time there are configuration changes, so they can use the SSM Parameter Store to store the configuration externally.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Environment variables</strong> - Environment variables provide another way to specify configuration options and credentials, and can be useful for scripting or temporarily setting a named profile as the default. Your application is not running AWS CLI. Since the use-case requires the configuration to be stored securely, so using Environment variables is ruled out, as these are not encrypted at rest and these are visible in clear text in the AWS Console as well as in the response of some actions of the Elastic BeanStalk API.</p>\n\n<p><strong>Use Stage Variables</strong> - You can use stage variables for managing multiple release stages for API Gateway, this is not what you are looking for here.</p>\n\n<p><strong>Use S3</strong> - S3 offers the same benefit as the SSM Parameter Store where there are no servers to manage. With S3 you have to set encryption and choose other security options and there are more chances of misconfiguring security if you share your S3 bucket with other objects. You would have to create a custom setup to come close to the parameter store. Use Parameter Store and let AWS handle the rest.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html</a></p>\n",
                "options": [
                    {
                        "id": 1625,
                        "content": "<p>Use Environment variables</p>",
                        "isValid": false
                    },
                    {
                        "id": 1626,
                        "content": "<p>Use SSM Parameter Store</p>",
                        "isValid": true
                    },
                    {
                        "id": 1627,
                        "content": "<p>Use S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 1628,
                        "content": "<p>Use Stage Variables</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 399,
            "attributes": {
                "createdAt": "2023-09-07T08:39:36.040Z",
                "updatedAt": "2023-09-07T08:39:36.040Z",
                "content": "<p>AWS CloudFormation helps model and provision all the cloud infrastructure resources needed for your business.</p>\n\n<p>Which of the following services rely on CloudFormation to provision resources (Select two)?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Elastic Beanstalk</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. Elastic Beanstalk uses AWS CloudFormation to launch the resources in your environment and propagate configuration changes.</p>\n\n<p><strong>AWS Serverless Application Model (AWS SAM)</strong> - You use the AWS SAM specification to define your serverless application. AWS SAM templates are an extension of AWS CloudFormation templates, with some additional components that make them easier to work with. AWS SAM needs CloudFormation templates as a basis for its configuration.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Hence, Lamda does not need CloudFormation to run its services.</p>\n\n<p><strong>AWS Autoscaling</strong> - AWS Auto Scaling monitors your applications and automatically adjusts the capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, itâ€™s easy to setup application scaling for multiple resources across multiple services in minutes. Auto Scaling used CloudFormation but is not a mandatory requirement.</p>\n\n<p><strong>AWS CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you donâ€™t need to provision, manage, and scale your own build servers. AWS CodePipeline uses AWS CloudFormation as a deployment action but is not a mandatory service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n",
                "options": [
                    {
                        "id": 1629,
                        "content": "<p>AWS CodeBuild</p>",
                        "isValid": false
                    },
                    {
                        "id": 1630,
                        "content": "<p>AWS Autoscaling</p>",
                        "isValid": false
                    },
                    {
                        "id": 1631,
                        "content": "<p>AWS Serverless Application Model (AWS SAM)</p>",
                        "isValid": true
                    },
                    {
                        "id": 1632,
                        "content": "<p>AWS Elastic Beanstalk</p>",
                        "isValid": true
                    },
                    {
                        "id": 1633,
                        "content": "<p>AWS Lambda</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 400,
            "attributes": {
                "createdAt": "2023-09-07T08:39:36.110Z",
                "updatedAt": "2023-09-07T08:39:36.110Z",
                "content": "<p>A financial services company wants to ensure that the customer data is always kept encrypted on Amazon S3 but wants an AWS managed solution that allows full control to create, rotate and remove the encryption keys.</p>\n\n<p>As a Developer Associate, which of the following would you recommend to address the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)</strong></p>\n\n<p>You have the following options for protecting data at rest in Amazon S3:</p>\n\n<p>Server-Side Encryption â€“ Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.</p>\n\n<p>Client-Side Encryption â€“ Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>When you use server-side encryption with AWS KMS (SSE-KMS), you can use the default AWS managed CMK, or you can specify a customer-managed CMK that you have already created.</p>\n\n<p>Creating your own customer-managed CMK gives you more flexibility and control over the CMK. For example, you can create, rotate, and disable customer-managed CMKs. You can also define access controls and audit the customer-managed CMKs that you use to protect your data.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q7-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, AWS encrypts the key itself with a master key that it regularly rotates. So this option is incorrect for the given use-case.</p>\n\n<p><strong>Server-Side Encryption with Customer-Provided Keys (SSE-C)</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you will need to create the encryption keys as well as manage the corresponding process to rotate and remove the encryption keys. Amazon S3 manages the data encryption, as it writes to disks, as well as the data decryption when you access your objects. So this option is incorrect for the given use-case.</p>\n\n<p><strong>Server-Side Encryption with Secrets Manager</strong> - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. You cannot combine Server-Side Encryption with Secrets Manager to create, rotate, or disable the encryption keys.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n",
                "options": [
                    {
                        "id": 1634,
                        "content": "<p>Server-Side Encryption with Secrets Manager</p>",
                        "isValid": false
                    },
                    {
                        "id": 1635,
                        "content": "<p>Server-Side Encryption with Customer-Provided Keys (SSE-C)</p>",
                        "isValid": false
                    },
                    {
                        "id": 1636,
                        "content": "<p>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)</p>",
                        "isValid": true
                    },
                    {
                        "id": 1637,
                        "content": "<p>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 401,
            "attributes": {
                "createdAt": "2023-09-07T08:39:36.186Z",
                "updatedAt": "2023-09-07T08:39:36.186Z",
                "content": "<p>As part of their on-boarding, the employees at an IT company need to upload their profile photos in a private S3 bucket. The company wants to build an in-house web application hosted on an EC2 instance that should display the profile photos in a secure way when the employees mark their attendance.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>\"Save the S3 key for each user's profile photo in a DynamoDB table and use a lambda function to dynamically generate a pre-signed URL. Reference this URL for display via the web application\"</p>\n\n<p>On Amazon S3, all objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects.</p>\n\n<p>You can also use an IAM instance profile to create a pre-signed URL. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The pre-signed URLs are valid only for the specified duration. So for the given use-case, the object key can be retrieved from the DynamoDB table, and then the application can generate the pre-signed URL using the IAM instance profile.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Make the S3 bucket public so that the application can reference the image URL for display\" - Making the S3 bucket public would violate the security and privacy requirements for the use-case, so this option is incorrect.</p>\n\n<p>\"Keep each user's profile image encoded in base64 format in a DynamoDB table and reference it from the application for display\"</p>\n\n<p>\"Keep each user's profile image encoded in base64 format in an RDS table and reference it from the application for display\"</p>\n\n<p>It's a bad practice to keep the raw image data in the database itself. Also, it would not be possible to create a secure access URL for the image without a significant development effort. Hence both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n",
                "options": [
                    {
                        "id": 1638,
                        "content": "<p>Save the S3 key for each user's profile photo in a DynamoDB table and use a lambda function to dynamically generate a pre-signed URL. Reference this URL for display via the web application</p>",
                        "isValid": true
                    },
                    {
                        "id": 1639,
                        "content": "<p>Make the S3 bucket public so that the application can reference the image URL for display</p>",
                        "isValid": false
                    },
                    {
                        "id": 1640,
                        "content": "<p>Keep each user's profile image encoded in base64 format in a DynamoDB table and reference it from the application for display</p>",
                        "isValid": false
                    },
                    {
                        "id": 1641,
                        "content": "<p>Keep each user's profile image encoded in base64 format in an RDS table and reference it from the application for display</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 402,
            "attributes": {
                "createdAt": "2023-09-07T08:39:36.264Z",
                "updatedAt": "2023-09-07T08:39:36.264Z",
                "content": "<p>A banking application needs to send real-time alerts and notifications based on any updates from the backend services. The company wants to avoid implementing complex polling mechanisms for these notifications.</p>\n\n<p>Which of the following types of APIs supported by the Amazon API Gateway is the right fit?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>WebSocket APIs</strong></p>\n\n<p>In a WebSocket API, the client and the server can both send messages to each other at any time. Backend servers can easily push data to connected users and devices, avoiding the need to implement complex polling mechanisms.</p>\n\n<p>For example, you could build a serverless application using an API Gateway WebSocket API and AWS Lambda to send and receive messages to and from individual users or groups of users in a chat room. Or you could invoke backend services such as AWS Lambda, Amazon Kinesis, or an HTTP endpoint based on message content.</p>\n\n<p>You can use API Gateway WebSocket APIs to build secure, real-time communication applications without having to provision or manage any servers to manage connections or large-scale data exchanges. Targeted use cases include real-time applications such as the following:</p>\n\n<ol>\n<li>Chat applications</li>\n<li>Real-time dashboards such as stock tickers</li>\n<li>Real-time alerts and notifications</li>\n</ol>\n\n<p>API Gateway provides WebSocket API management functionality such as the following:</p>\n\n<ol>\n<li>Monitoring and throttling of connections and messages</li>\n<li>Using AWS X-Ray to trace messages as they travel through the APIs to backend services</li>\n<li>Easy integration with HTTP/HTTPS endpoints</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>REST or HTTP APIs</strong></p>\n\n<p><strong>REST APIs</strong> - An API Gateway REST API is made up of resources and methods. A resource is a logical entity that an app can access through a resource path. A method corresponds to a REST API request that is submitted by the user of your API and the response returned to the user.</p>\n\n<p>For example, /incomes could be the path of a resource representing the income of the app user. A resource can have one or more operations that are defined by appropriate HTTP verbs such as GET, POST, PUT, PATCH, and DELETE. A combination of a resource path and an operation identifies a method of the API. For example, a POST /incomes method could add an income earned by the caller, and a GET /expenses method could query the reported expenses incurred by the caller.</p>\n\n<p><strong>HTTP APIs</strong> - HTTP APIs enable you to create RESTful APIs with lower latency and lower cost than REST APIs. You can use HTTP APIs to send requests to AWS Lambda functions or to any publicly routable HTTP endpoint.</p>\n\n<p>For example, you can create an HTTP API that integrates with a Lambda function on the backend. When a client calls your API, API Gateway sends the request to the Lambda function and returns the function's response to the client.</p>\n\n<p>Server push mechanism is not possible in REST and HTTP APIs.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.html</a></p>\n",
                "options": [
                    {
                        "id": 1642,
                        "content": "<p>WebSocket APIs</p>",
                        "isValid": true
                    },
                    {
                        "id": 1643,
                        "content": "<p>HTTP APIs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1644,
                        "content": "<p>REST or HTTP APIs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1645,
                        "content": "<p>REST APIs</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 403,
            "attributes": {
                "createdAt": "2023-09-07T08:39:36.333Z",
                "updatedAt": "2023-09-07T08:39:36.333Z",
                "content": "<p>You have an Auto Scaling group configured to a minimum capacity of 1 and a maximum capacity of 5, designed to launch EC2 instances across 3 Availability Zones. During a low utilization period, an entire Availability Zone went down and your application experienced downtime.</p>\n\n<p>What can you do to ensure that your application remains highly available?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Increase the minimum instance capacity of the Auto Scaling Group to 2</strong> -</p>\n\n<p>You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity.</p>\n\n<p>Since a minimum capacity of 1 was defined, an instance was launched in only one AZ. This AZ went down, taking the application with it. If the minimum capacity is set to 2. As per Auto Scale AZ configuration, it would have launched 2 instances- one in each AZ, making the architecture disaster-proof and hence highly available.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q65-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the scaling metric of auto-scaling policy to network bytes</strong> - With target tracking scaling policies, you select a scaling metric and set a target value. You can use predefined customized metrics. Setting the metric to network bytes will not help in this context since the instances have to be spread across different AZs for high availability. The optimized way of doing it, is by defining minimum and maximum instance capacities, as discussed above.</p>\n\n<p><strong>Configure ASG fast failover</strong> - This is a made-up option, given as a distractor.</p>\n\n<p><strong>Enable RDS Multi-AZ</strong> - This configuration will make your database highly available. But for the current scenario, you will need to have more than 1 instance in separate availability zones to keep the application highly available.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n",
                "options": [
                    {
                        "id": 1646,
                        "content": "<p>Increase the minimum instance capacity of the Auto Scaling Group to 2</p>",
                        "isValid": true
                    },
                    {
                        "id": 1647,
                        "content": "<p>Enable RDS Multi-AZ</p>",
                        "isValid": false
                    },
                    {
                        "id": 1648,
                        "content": "<p>Configure ASG fast failover</p>",
                        "isValid": false
                    },
                    {
                        "id": 1649,
                        "content": "<p>Change the scaling metric of auto-scaling policy to network bytes</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 404,
            "attributes": {
                "createdAt": "2023-09-07T08:39:36.405Z",
                "updatedAt": "2023-09-07T08:39:36.405Z",
                "content": "<p>You have migrated an on-premise SQL Server database to an Amazon Relational Database Service (RDS) database attached to a VPC inside a private subnet. Also, the related Java application, hosted on-premise, has been moved to an Amazon Lambda function.</p>\n\n<p>Which of the following should you implement to connect AWS Lambda function to its RDS instance?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS</strong> - You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your lambda function to the VPC to access private resources during execution. When you connect a function to a VPC, Lambda creates an elastic network interface for each combination of the security group and subnet in your function's VPC configuration. This is the right way of giving RDS access to Lambda.</p>\n\n<p>Lambda VPC Config:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Lambda layers to connect to the internet and RDS separately</strong> - You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. Layers will not help in configuring access to RDS instance and hence is an incorrect choice.</p>\n\n<p><strong>Configure lambda to connect to the public subnet that will give internet access and use the Security Group to access RDS inside the private subnet</strong> - This is an incorrect statement. Connecting a Lambda function to a public subnet does not give it internet access or a public IP address. To grant internet access to your function, its associated VPC must have a NAT gateway (or NAT instance) in a public subnet.</p>\n\n<p><strong>Use Environment variables to pass in the RDS connection string</strong> - You can use environment variables to store secrets securely and adjust your function's behavior without updating code. You can use environment variables to exchange data with RDS, but you will still need access to RDS, which is not possible with just environment variables.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/\">https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/</a></p>\n",
                "options": [
                    {
                        "id": 1650,
                        "content": "<p>Configure lambda to connect to the public subnet that will give internet access and use Security Group to access RDS inside the private subnet</p>",
                        "isValid": false
                    },
                    {
                        "id": 1651,
                        "content": "<p>Use Lambda layers to connect to the internet and RDS separately</p>",
                        "isValid": false
                    },
                    {
                        "id": 1652,
                        "content": "<p>Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS</p>",
                        "isValid": true
                    },
                    {
                        "id": 1653,
                        "content": "<p>Use Environment variables to pass in the RDS connection string</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 405,
            "attributes": {
                "createdAt": "2023-09-07T08:39:36.485Z",
                "updatedAt": "2023-09-07T08:39:36.485Z",
                "content": "<p>You have a three-tier web application consisting of a web layer using AngularJS, an application layer using an AWS API Gateway and a data layer in an Amazon Relational Database Service (RDS) database. Your web application allows visitors to look up popular movies from the past. The company is looking at reducing the number of calls made to endpoint and improve latency to the API.</p>\n\n<p>What can you do to improve performance?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable API Gateway Caching</strong> - You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Mapping Templates</strong> - A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be easy to ready. Mapping Templates do not help in latency issues of the APIs.</p>\n\n<p><strong>Use Stage Variables</strong> - Stage variables act like environment variables and can be used to change the behavior of your API Gateway methods for each deployment stage; for example, making it possible to reach a different back end depending on which stage the API is running on. Stage variables do not help in latency issues.</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to stream incoming data and reduce the burden on Gateway APIs</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p>\n",
                "options": [
                    {
                        "id": 1654,
                        "content": "<p>Enable API Gateway Caching</p>",
                        "isValid": true
                    },
                    {
                        "id": 1655,
                        "content": "<p>Use Amazon Kinesis Data Streams to stream incoming data and reduce the burden on Gateway APIs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1656,
                        "content": "<p>Use Mapping Templates</p>",
                        "isValid": false
                    },
                    {
                        "id": 1657,
                        "content": "<p>Use Stage Variables</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 406,
            "attributes": {
                "createdAt": "2023-09-07T08:39:36.557Z",
                "updatedAt": "2023-09-07T08:39:36.557Z",
                "content": "<p>An application runs on an EC2 instance and processes orders on a nightly basis. This EC2 instance needs to access the orders that are stored in S3.</p>\n\n<p>How would you recommend the EC2 instance access the orders securely?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use an IAM role</strong></p>\n\n<p>IAM roles have been incorporated so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.</p>\n\n<p>Amazon EC2 uses an instance profile as a container for an IAM role. When you create an IAM role using the IAM console, the console creates an instance profile automatically and gives it the same name as the role to which it corresponds.</p>\n\n<p>This is the most secure option as the role assigned to EC2 can be used to access S3 without storing any credentials onto the EC2 instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM programmatic user and store the access key and secret access key on the EC2 <code>~/.aws/credentials</code> file.</strong> - While this would work, this is highly insecure as anyone gaining access to the EC2 instance would be able to steal the credentials stored in that file.</p>\n\n<p><strong>Use EC2 User Data</strong> - EC2 User Data is used to run bootstrap scripts at an instance's first launch. This option is not the right fit for the given use-case.</p>\n\n<p><strong>Create an S3 bucket policy that authorizes public access</strong> - While this would work, it would allow anyone to access your S3 bucket files. So this option is ruled out.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p>\n",
                "options": [
                    {
                        "id": 1658,
                        "content": "<p>Use an IAM role</p>",
                        "isValid": true
                    },
                    {
                        "id": 1659,
                        "content": "<p>Create an S3 bucket policy that authorises public access</p>",
                        "isValid": false
                    },
                    {
                        "id": 1660,
                        "content": "<p>Use EC2 User Data</p>",
                        "isValid": false
                    },
                    {
                        "id": 1661,
                        "content": "<p>Create an IAM programmatic user and store the access key and secret access key on the EC2 <code>~/.aws/credentials</code> file.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 407,
            "attributes": {
                "createdAt": "2023-09-07T08:39:36.649Z",
                "updatedAt": "2023-09-07T08:39:36.649Z",
                "content": "<p>A leading financial services company offers data aggregation services for Wall Street trading firms. The company bills its clients based on per unit of clickstream data provided to the clients. As the company operates in a regulated industry, it needs to have the same ordered clickstream data available for auditing within a window of 7 days.</p>\n\n<p>As a Developer Associate, which of the following AWS services do you think provides the ability to run the billing process and auditing process on the given clickstream data in the same order?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).\nAmazon Kinesis Data Streams is recommended when you need the ability to consume records in the same order a few hours later.</p>\n\n<p>For example, you have a billing application and an audit application that runs a few hours behind the billing application. By default, records of a stream are accessible for up to 24 hours from the time they are added to the stream. You can raise this limit to a maximum of 365 days. For the given use-case, Amazon Kinesis Data Streams can be configured to store data for up to 7 days and you can run the audit application up to 7 days behind the billing application.</p>\n\n<p>KDS provides the ability to consume records in the same order a few hours later\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q6-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards youâ€™re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect.</p>\n\n<p><strong>AWS Kinesis Data Analytics</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect.</p>\n\n<p><strong>Amazon SQS</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers in the same order a few hours later, therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 1662,
                        "content": "<p>AWS Kinesis Data Firehose</p>",
                        "isValid": false
                    },
                    {
                        "id": 1663,
                        "content": "<p>Amazon SQS</p>",
                        "isValid": false
                    },
                    {
                        "id": 1664,
                        "content": "<p>AWS Kinesis Data Analytics</p>",
                        "isValid": false
                    },
                    {
                        "id": 1665,
                        "content": "<p>AWS Kinesis Data Streams</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 408,
            "attributes": {
                "createdAt": "2023-09-07T08:39:36.738Z",
                "updatedAt": "2023-09-07T08:39:36.738Z",
                "content": "<p>A telecommunications company that provides internet service for mobile device users maintains over 100 c4.large instances in the us-east-1 region. The EC2 instances run complex algorithms. The manager would like to track CPU utilization of the EC2 instances as frequently as every 10 seconds.</p>\n\n<p>Which of the following represents the BEST solution for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a high-resolution custom metric and push the data using a script triggered every 10 seconds</strong></p>\n\n<p>Using high-resolution custom metric, your applications can publish metrics to CloudWatch with 1-second resolution. You can watch the metrics scroll across your screen seconds after they are published and you can set up high-resolution CloudWatch Alarms that evaluate as frequently as every 10 seconds. You can alert with High-Resolution Alarms, as frequently as 10-second periods. High-Resolution Alarms allow you to react and take actions faster and support the same actions available today with standard 1-minute alarms.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q11-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/\">https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable EC2 detailed monitoring</strong> - As part of basic monitoring, Amazon EC2 sends metric data to CloudWatch in 5-minute periods. To send metric data for your instance to CloudWatch in 1-minute periods, you can enable detailed monitoring on the instance, however, this comes at an additional cost.</p>\n\n<p><strong>Simply get it from the CloudWatch Metrics</strong> - You can get data from metrics. The basic monitoring data is available automatically in a 5-minute interval and detailed monitoring data is available in a 1-minute interval.</p>\n\n<p><strong>Open a support ticket with AWS</strong> - This option has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/\">https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/</a></p>\n",
                "options": [
                    {
                        "id": 1666,
                        "content": "<p>Simply get it from the CloudWatch Metrics</p>",
                        "isValid": false
                    },
                    {
                        "id": 1667,
                        "content": "<p>Create a high-resolution custom metric and push the data using a script triggered every 10 seconds</p>",
                        "isValid": true
                    },
                    {
                        "id": 1668,
                        "content": "<p>Open a support ticket with AWS</p>",
                        "isValid": false
                    },
                    {
                        "id": 1669,
                        "content": "<p>Enable EC2 detailed monitoring</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 409,
            "attributes": {
                "createdAt": "2023-09-07T08:39:36.809Z",
                "updatedAt": "2023-09-07T08:39:36.809Z",
                "content": "<p>A development team has deployed a REST API in Amazon API Gateway to two different stages - a test stage and a prod stage. The test stage is used as a test build and the prod stage as a stable build. After the updates have passed the test, the team wishes to promote the test stage to the prod stage.</p>\n\n<p>Which of the following represents the optimal solution for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Update stage variable value from the stage name of test to that of prod</strong></p>\n\n<p>After creating your API, you must deploy it to make it callable by your users. To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. They're included in the URL that you use to invoke the API. Each stage is a named reference to a deployment of the API and is made available for client applications to call.</p>\n\n<p>Stages enable robust version control of your API. In our current use-case, after the updates pass the test, you can promote the test stage to the prod stage. The promotion can be done by redeploying the API to the prod stage or updating a stage variable value from the stage name of test to that of prod.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the API without choosing a stage. This way, the working deployment will be updated in all stages</strong> - An API can only be deployed to a stage. Hence, it is not possible to deploy an API without choosing a stage.</p>\n\n<p><em>Delete the existing prod stage. Create a new stage with the same name (prod) and deploy the tested version on this stage</em>* - This is possible, but not an optimal way of deploying a change. Also, as prod refers to real production system, this option will result in downtime.</p>\n\n<p><strong>API performance is optimized in a different way for prod environments. Hence, promoting test to prod is not correct. The promotion should be done by redeploying the API to the prod stage</strong> - For each stage, you can optimize API performance by adjusting the default account-level request throttling limits and enabling API caching. And these settings can be changed/updated at any time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html</a></p>\n",
                "options": [
                    {
                        "id": 1670,
                        "content": "<p>Delete the existing prod stage. Create a new stage with the same name (prod) and deploy the tested version on this stage</p>",
                        "isValid": false
                    },
                    {
                        "id": 1671,
                        "content": "<p>Update stage variable value from the stage name of test to that of prod</p>",
                        "isValid": true
                    },
                    {
                        "id": 1672,
                        "content": "<p>Deploy the API without choosing a stage. This way, the working deployment will be updated in all stages</p>",
                        "isValid": false
                    },
                    {
                        "id": 1673,
                        "content": "<p>API performance is optimized in a different way for prod environments. Hence, promoting test to prod is not correct. The promotion should be done by redeploying the API to the prod stage</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 410,
            "attributes": {
                "createdAt": "2023-09-07T08:39:36.886Z",
                "updatedAt": "2023-09-07T08:39:36.886Z",
                "content": "<p>An e-commerce company has an order processing workflow with several tasks to be done in parallel as well as decision steps to be evaluated for successful processing of the order. All the tasks are implemented via Lambda functions.</p>\n\n<p>Which of the following is the BEST solution to meet these business requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Step Functions state machines to orchestrate the workflow</strong></p>\n\n<p>AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly.</p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>The following are key features of AWS Step Functions:</p>\n\n<p>Step Functions are based on the concepts of tasks and state machines. You define state machines using the JSON-based Amazon States Language. A state machine is defined by the states it contains and the relationships between them. States are elements in your state machine. Individual states can make decisions based on their input, perform actions, and pass output to other states. In this way, a state machine can orchestrate workflows.</p>\n\n<p>Please see this note for a simple example of a State Machine:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html\">https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Step Functions activities to orchestrate the workflow</strong> - In AWS Step Functions, activities are a way to associate code running somewhere (known as an activity worker) with a specific task in a state machine. When a Step Function reaches an activity task state, the workflow waits for an activity worker to poll for a task. For example, an activity worker can be an application running on an Amazon EC2 instance or an AWS Lambda function. AWS Step Functions activities cannot orchestrate a workflow.</p>\n\n<p><strong>Use AWS Glue to orchestrate the workflow</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue cannot orchestrate a workflow.</p>\n\n<p><strong>Use AWS Batch to orchestrate the workflow</strong> - AWS Batch runs batch computing jobs on the AWS Cloud. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. AWS Batch cannot orchestrate a workflow.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html\">https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html</a></p>\n",
                "options": [
                    {
                        "id": 1674,
                        "content": "<p>Use AWS Batch to orchestrate the workflow</p>",
                        "isValid": false
                    },
                    {
                        "id": 1675,
                        "content": "<p>Use AWS Glue to orchestrate the workflow</p>",
                        "isValid": false
                    },
                    {
                        "id": 1676,
                        "content": "<p>Use AWS Step Functions state machines to orchestrate the workflow</p>",
                        "isValid": true
                    },
                    {
                        "id": 1677,
                        "content": "<p>Use AWS Step Functions activities to orchestrate the workflow</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 411,
            "attributes": {
                "createdAt": "2023-09-07T08:39:37.000Z",
                "updatedAt": "2023-09-07T08:39:37.000Z",
                "content": "<p>You team maintains a public API Gateway that is accessed by clients from another domain. Usage has been consistent for the last few months but recently it has more than doubled. As a result, your costs have gone up and would like to prevent other unauthorized domains from accessing your API.</p>\n\n<p>Which of the following actions should you take?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Restrict access by using CORS</strong> - Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. When your API's resources receive requests from a domain other than the API's own domain and you want to restrict servicing these requests, you must disable cross-origin resource sharing (CORS) for selected methods on the resource.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Account-level throttling</strong> - To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API. By default, API Gateway limits the steady-state request rate to 10,000 requests per second (rps). It limits the burst (that is, the maximum bucket size) to 5,000 requests across all APIs within an AWS account. This is Account-level throttling. As you see, this is about limit on the number of requests and is not a suitable answer for the current scenario.</p>\n\n<p><strong>Use Mapping Templates</strong> - A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be easy to ready. Mapping Templates have nothing to do with access and are not useful for the current scenario.</p>\n\n<p><strong>Assign a Security Group to your API Gateway</strong> - API Gateway does not use security groups but uses resource policies, which are JSON policy documents that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. You can restrict IP address using this, the downside being, an IP address can be changed by the accessing user. So, this is not an optimal solution for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-protect.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-protect.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html</a></p>\n",
                "options": [
                    {
                        "id": 1678,
                        "content": "<p>Use Mapping Templates</p>",
                        "isValid": false
                    },
                    {
                        "id": 1679,
                        "content": "<p>Restrict access by using CORS</p>",
                        "isValid": true
                    },
                    {
                        "id": 1680,
                        "content": "<p>Assign a Security Group to your API Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 1681,
                        "content": "<p>Use Account-level throttling</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 412,
            "attributes": {
                "createdAt": "2023-09-07T08:39:37.068Z",
                "updatedAt": "2023-09-07T08:39:37.068Z",
                "content": "<p>A development team uses shared Amazon S3 buckets to upload files. Due to this shared access, objects in S3 buckets have different owners making it difficult to manage the objects.</p>\n\n<p>As a developer associate, which of the following would you suggest to automatically make the S3 bucket owner, also the owner of all objects in the bucket, irrespective of the AWS account used for uploading the objects?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use S3 Object Ownership to default bucket owner to be the owner of all objects in the bucket</strong></p>\n\n<p>S3 Object Ownership is an Amazon S3 bucket setting that you can use to control ownership of new objects that are uploaded to your buckets. By default, when other AWS accounts upload objects to your bucket, the objects remain owned by the uploading account. With S3 Object Ownership, any new objects that are written by other accounts with the bucket-owner-full-control canned access control list (ACL) automatically become owned by the bucket owner, who then has full control of the objects.</p>\n\n<p>S3 Object Ownership has two settings:\n1. Object writer â€“ The uploading account will own the object.\n2. Bucket owner preferred â€“ The bucket owner will own the object if the object is uploaded with the <code>bucket-owner-full-control</code> canned ACL. Without this setting and canned ACL, the object is uploaded and remains owned by the uploading account.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use S3 CORS to make the S3 bucket owner, the owner of all objects in the bucket</strong> - Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.</p>\n\n<p><strong>Use S3 Access Analyzer to identify the owners of all objects and change the ownership to the bucket owner</strong> - Access Analyzer for S3 helps review all buckets that have bucket access control lists (ACLs), bucket policies, or access point policies that grant public or shared access. Access Analyzer for S3 alerts you to buckets that are configured to allow access to anyone on the internet or other AWS accounts, including AWS accounts outside of your organization.</p>\n\n<p><strong>Use Bucket Access Control Lists (ACLs) to control access on S3 bucket and then define its owner</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. A bucket ACLs allow you to control access at bucket level.</p>\n\n<p>None of the above features are useful for the current scenario and hence are incorrect options.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html</a></p>\n",
                "options": [
                    {
                        "id": 1682,
                        "content": "<p>Use Bucket Access Control Lists (ACLs) to control access on S3 bucket and then define its owner</p>",
                        "isValid": false
                    },
                    {
                        "id": 1683,
                        "content": "<p>Use S3 CORS to make the S3 bucket owner, the owner of all objects in the bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 1684,
                        "content": "<p>Use S3 Access Analyzer to identify the owners of all objects and change the ownership to the bucket owner</p>",
                        "isValid": false
                    },
                    {
                        "id": 1685,
                        "content": "<p>Use S3 Object Ownership to default bucket owner to be the owner of all objects in the bucket</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 413,
            "attributes": {
                "createdAt": "2023-09-07T08:39:37.149Z",
                "updatedAt": "2023-09-07T08:39:37.149Z",
                "content": "<p>A company follows collaborative development practices. The engineering manager wants to isolate the development effort by setting up simulations of API components owned by various development teams.</p>\n\n<p>Which API integration type is best suited for this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>MOCK</strong></p>\n\n<p>This type of integration lets API Gateway return a response without sending the request further to the backend. This is useful for API testing because it can be used to test the integration setup without incurring charges for using the backend and to enable collaborative development of an API.</p>\n\n<p>In collaborative development, a team can isolate their development effort by setting up simulations of API components owned by other teams by using the MOCK integrations. It is also used to return CORS-related headers to ensure that the API method permits CORS access. In fact, the API Gateway console integrates the OPTIONS method to support CORS with a mock integration.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS_PROXY</strong> - This type of integration lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup. This integration relies on direct interactions between the client and the integrated Lambda function.</p>\n\n<p><strong>HTTP_PROXY</strong> - The HTTP proxy integration allows a client to access the backend HTTP endpoints with a streamlined integration setup on single API method. You do not set the integration request or the integration response. API Gateway passes the incoming request from the client to the HTTP endpoint and passes the outgoing response from the HTTP endpoint to the client.</p>\n\n<p><strong>HTTP</strong> - This type of integration lets an API expose HTTP endpoints in the backend. With the HTTP integration, you must configure both the integration request and integration response. You must set up necessary data mappings from the method request to the integration request, and from the integration response to the method response.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html</a></p>\n",
                "options": [
                    {
                        "id": 1686,
                        "content": "<p>HTTP_PROXY</p>",
                        "isValid": false
                    },
                    {
                        "id": 1687,
                        "content": "<p>AWS_PROXY</p>",
                        "isValid": false
                    },
                    {
                        "id": 1688,
                        "content": "<p>HTTP</p>",
                        "isValid": false
                    },
                    {
                        "id": 1689,
                        "content": "<p>MOCK</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 414,
            "attributes": {
                "createdAt": "2023-09-07T08:39:37.222Z",
                "updatedAt": "2023-09-07T08:39:37.222Z",
                "content": "<p>You are creating a mobile application that needs access to the AWS API Gateway. Users will need to register first before they can access your API and you would like the user management to be fully managed.</p>\n\n<p>Which authentication option should you use for your API Gateway layer?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito User Pools</strong> - As an alternative to using IAM roles and policies or Lambda authorizers, you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway.\nTo use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Lambda Authorizer</strong>- A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity. This won't be a fully managed user management solution but it would allow you to check for access at the AWS API Gateway level.</p>\n\n<p><strong>Use IAM permissions with sigv4</strong> - Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. For security, most requests to AWS must be signed with an access key, which consists of an access key ID and secret access key. These two keys are commonly referred to as your security credentials. But, we cannot possibly create an IAM user for every visitor of the site, so this is where social identity providers come in to help.</p>\n\n<p><strong>Use API Gateway User Pools</strong> - This is a made-up option.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html\">https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html</a></p>\n",
                "options": [
                    {
                        "id": 1690,
                        "content": "<p>Use Lambda Authorizer</p>",
                        "isValid": false
                    },
                    {
                        "id": 1691,
                        "content": "<p>Use Cognito User Pools</p>",
                        "isValid": true
                    },
                    {
                        "id": 1692,
                        "content": "<p>Use IAM permissions with sigv4</p>",
                        "isValid": false
                    },
                    {
                        "id": 1693,
                        "content": "<p>Use API Gateway User Pools</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 415,
            "attributes": {
                "createdAt": "2023-09-07T08:39:37.302Z",
                "updatedAt": "2023-09-07T08:39:37.302Z",
                "content": "<p>Two policies are attached to an IAM user. The first policy states that the user has explicitly been denied all access to EC2 instances. The second policy states that the user has been allowed permission for EC2:Describe action.</p>\n\n<p>When the user tries to use 'Describe' action on an EC2 instance using the CLI, what will be the output?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The user will be denied access because the policy has an explicit deny on it</strong> - User will be denied access because any explicit deny overrides the allow.</p>\n\n<p>Policy Evaluation explained:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q43-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The IAM user stands in an invalid state, because of conflicting policies</strong> - This is an incorrect statement. Access policies can have allow and deny permissions on them and based on policy rules they are evaluated. A user account does not get invalid because of policies.</p>\n\n<p><strong>The user will get access because it has an explicit allow</strong> - As discussed above, explicit deny overrides all other permissions and hence the user will be denied access.</p>\n\n<p><strong>The order of the policy matters. If policy 1 is before 2, then the user is denied access. If policy 2 is before 1, then the user is allowed access</strong> - If policies that apply to a request include an Allow statement and a Deny statement, the Deny statement trumps the Allow statement. The request is explicitly denied.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n",
                "options": [
                    {
                        "id": 1694,
                        "content": "<p>The IAM user stands in an invalid state, because of conflicting policies</p>",
                        "isValid": false
                    },
                    {
                        "id": 1695,
                        "content": "<p>The user will be denied access because one of the policies has an explicit deny on it</p>",
                        "isValid": true
                    },
                    {
                        "id": 1696,
                        "content": "<p>The user will get access because it has an explicit allow</p>",
                        "isValid": false
                    },
                    {
                        "id": 1697,
                        "content": "<p>The order of the policy matters. If policy 1 is before 2, then the user is denied access. If policy 2 is before 1, then the user is allowed access</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 416,
            "attributes": {
                "createdAt": "2023-09-07T08:39:37.382Z",
                "updatedAt": "2023-09-07T08:39:37.382Z",
                "content": "<p>A high-frequency stock trading firm is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon SQS. The development team at the company wants to minimize the costs of using SQS.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend to address the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use SQS long polling to retrieve messages from your Amazon SQS queues</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.</p>\n\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.</p>\n\n<p>Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the differences between Short Polling vs Long Polling:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q1-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use SQS short polling to retrieve messages from your Amazon SQS queues</strong> - With short polling, Amazon SQS sends the response right away, even if the query found no messages. You end up paying more because of the increased number of empty receives.</p>\n\n<p><strong>Use SQS visibility timeout to retrieve messages from your Amazon SQS queues</strong> - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.</p>\n\n<p><strong>Use SQS message timer to retrieve messages from your Amazon SQS queues</strong> - You can use message timers to set an initial invisibility period for a message added to a queue. So, if you send a message with a 60-second timer, the message isn't visible to consumers for its first 60 seconds in the queue. The default (minimum) delay for a message is 0 seconds. The maximum is 15 minutes. You cannot use message timer to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n",
                "options": [
                    {
                        "id": 1698,
                        "content": "<p>Use SQS long polling to retrieve messages from your Amazon SQS queues</p>",
                        "isValid": true
                    },
                    {
                        "id": 1699,
                        "content": "<p>Use SQS visibility timeout to retrieve messages from your Amazon SQS queues</p>",
                        "isValid": false
                    },
                    {
                        "id": 1700,
                        "content": "<p>Use SQS message timer to retrieve messages from your Amazon SQS queues</p>",
                        "isValid": false
                    },
                    {
                        "id": 1701,
                        "content": "<p>Use SQS short polling to retrieve messages from your Amazon SQS queues</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 417,
            "attributes": {
                "createdAt": "2023-09-07T08:39:37.457Z",
                "updatedAt": "2023-09-07T08:39:37.457Z",
                "content": "<p>Your company has a three-year contract with a healthcare provider. The contract states that monthly database backups must be retained for the duration of the contract for compliance purposes. Currently, the limit on backup retention for automated backups, on Amazon Relational Database Service (RDS), does not meet your requirements.</p>\n\n<p>Which of the following solutions can help you meet your requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a cron event in CloudWatch, which triggers an AWS Lambda function that triggers the database snapshot</strong> - There are multiple ways to run periodic jobs in AWS. CloudWatch Events with Lambda is the simplest of all solutions. To do this, create a CloudWatch Rule and select â€œScheduleâ€ as the Event Source. You can either use a cron expression or provide a fixed rate (such as every 5 minutes). Next, select â€œLambda Functionâ€ as the Target. Your Lambda will have the necessary code for snapshot functionality.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable RDS automatic backups</strong> - You can enable automatic backups but as of 2020, the retention period is 0 to 35 days.</p>\n\n<p><strong>Enable RDS Read replicas</strong> - Amazon RDS server's built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the read replica. Read replicas are useful for heavy read-only data workloads. These are not suitable for the given use-case.</p>\n\n<p><strong>Enable RDS Multi-AZ</strong> - Multi-AZ allows you to create a highly available application with RDS. It does not directly help in database backups or retention periods.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html</a></p>\n",
                "options": [
                    {
                        "id": 1702,
                        "content": "<p>Create a cron event in CloudWatch, which triggers an AWS Lambda function that triggers the database snapshot</p>",
                        "isValid": true
                    },
                    {
                        "id": 1703,
                        "content": "<p>Enable RDS Multi-AZ</p>",
                        "isValid": false
                    },
                    {
                        "id": 1704,
                        "content": "<p>Enable RDS Read replicas</p>",
                        "isValid": false
                    },
                    {
                        "id": 1705,
                        "content": "<p>Enable RDS automatic backups</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 418,
            "attributes": {
                "createdAt": "2023-09-07T08:39:37.541Z",
                "updatedAt": "2023-09-07T08:39:37.541Z",
                "content": "<p>A team is checking the viability of using AWS Step Functions for creating a banking workflow for loan approvals. The web application will also have human approval as one of the steps in the workflow.</p>\n\n<p>As a developer associate, which of the following would you identify as the key characteristics for AWS Step Function? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that can also support any human approval steps</strong> - Standard Workflows on AWS Step Functions are more suitable for long-running, durable, and auditable workflows where repeating workflow steps is expensive (e.g., restarting a long-running media transcode) or harmful (e.g., charging a credit card twice). Example workloads include training and deploying machine learning models, report generation, billing, credit card processing, and ordering and fulfillment processes. Step functions also support any human approval steps.</p>\n\n<p><em>You should use Express Workflows for workloads with high event rates and short duration</em>* - You should use Express Workflows for workloads with high event rates and short durations. Express Workflows support event rates of more than 100,000 per second.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that do not support any human approval steps</strong> - As Step functions support any human approval steps, so this option is incorrect.</p>\n\n<p><strong>Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of 180 days or 6 months</strong> - Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of one year.</p>\n\n<p><strong>Both Standard and Express Workflows support all service integrations, activities, and design patterns</strong> - Standard Workflows support all service integrations, activities, and design patterns. Express Workflows do not support activities, job-run (.sync), and Callback patterns.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/features/\">https://aws.amazon.com/step-functions/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/implementing-serverless-manual-approval-steps-in-aws-step-functions-and-amazon-api-gateway/\">https://aws.amazon.com/blogs/compute/implementing-serverless-manual-approval-steps-in-aws-step-functions-and-amazon-api-gateway/</a></p>\n",
                "options": [
                    {
                        "id": 1706,
                        "content": "<p>Both Standard and Express Workflows support all service integrations, activities, and design patterns</p>",
                        "isValid": false
                    },
                    {
                        "id": 1707,
                        "content": "<p>You should use Express Workflows for workloads with high event rates and short duration</p>",
                        "isValid": true
                    },
                    {
                        "id": 1708,
                        "content": "<p>Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of 180 days or 6 months</p>",
                        "isValid": false
                    },
                    {
                        "id": 1709,
                        "content": "<p>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that can also support any human approval steps</p>",
                        "isValid": true
                    },
                    {
                        "id": 1710,
                        "content": "<p>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that do not support any human approval steps</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 419,
            "attributes": {
                "createdAt": "2023-09-07T08:39:37.610Z",
                "updatedAt": "2023-09-07T08:39:37.610Z",
                "content": "<p>A digital marketing company has its website hosted on an Amazon S3 bucket A. The development team notices that the web fonts that are hosted on another S3 bucket B are not loading on the website.</p>\n\n<p>Which of the following solutions can be used to address this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure CORS on the bucket B that is hosting the web fonts to allow Bucket A origin to make the requests</strong></p>\n\n<p>Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.</p>\n\n<p>To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operation-specific information.</p>\n\n<p>For the given use-case, you would create a <code>&lt;CORSRule&gt;</code> in <code>&lt;CORSConfiguration&gt;</code> for bucket B to allow access from the S3 website origin hosted on bucket A.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable versioning on both the buckets to facilitate the correct functioning of the website</strong> - This option is a distractor and versioning will not help to address the web fonts loading issue on the website.</p>\n\n<p><strong>Update bucket policies on both bucket A and bucket B to allow successful loading of the web fonts on the website</strong> - It's not the bucket policies but the CORS configuration on bucket B that needs to be updated to allow web fonts to be loaded on the website. Updating bucket policies will not help to address the web fonts loading issue on the website.</p>\n\n<p><strong>Configure CORS on the bucket A that is hosting the website to allow any origin to respond to requests</strong> - The CORS configuration needs to be updated on bucket B to allow web fonts to be loaded on the website hosted on bucket A. So this option in incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>\n",
                "options": [
                    {
                        "id": 1711,
                        "content": "<p>Configure CORS on the bucket B that is hosting the web fonts to allow Bucket A origin to make the requests</p>",
                        "isValid": true
                    },
                    {
                        "id": 1712,
                        "content": "<p>Configure CORS on the bucket A that is hosting the website to allow any origin to respond to requests</p>",
                        "isValid": false
                    },
                    {
                        "id": 1713,
                        "content": "<p>Enable versioning on both the buckets to facilitate correct functioning of the website</p>",
                        "isValid": false
                    },
                    {
                        "id": 1714,
                        "content": "<p>Update bucket policies on both bucket A and bucket B to allow successful loading of the web fonts on the website</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 420,
            "attributes": {
                "createdAt": "2023-09-07T08:39:37.684Z",
                "updatedAt": "2023-09-07T08:39:37.684Z",
                "content": "<p>A developer wants to integrate user-specific file upload and download features in an application that uses both Amazon Cognito user pools and Cognito identity pools for secure access with Amazon S3. The developer also wants to ensure that only authorized users can access their own files and that the files are securely saved and retrieved. The files are 5 KB to 500 MB in size.</p>\n\n<p>What do you recommend as the most efficient solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Leverage an IAM policy with the Amazon Cognito identity prefix to restrict users to use their own folders in Amazon S3</strong></p>\n\n<p>Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. Amazon Cognito identity pools support the following identity providers:</p>\n\n<p>Public providers: Login with Amazon (identity pools), Facebook (identity pools), Google (identity pools), Sign in with Apple (identity pools).</p>\n\n<p>Amazon Cognito user pools</p>\n\n<p>OpenID Connect providers (identity pools)</p>\n\n<p>SAML identity providers (identity pools)</p>\n\n<p>Developer authenticated identities (identity pools)</p>\n\n<p>You can create an identity-based policy that allows Amazon Cognito users to access objects in a specific S3 bucket. This policy allows access only to objects with a name that includes Cognito, the name of the application, and the federated user's ID, represented by the ${cognito-identity.amazonaws.com:sub} variable.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q26-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use S3 Event Notifications to trigger a Lambda function that validates that the given file is uploaded and downloaded only by the authorized user</strong> - While it is certainly possible to build this solution, however, it is not the most optimal solution as it does not prevent an invalid upload of a file into another user's designated folder. So this option is incorrect.</p>\n\n<p><strong>Integrate Amazon API Gateway with a Lambda function that validates that the given file is uploaded to S3 and downloaded from S3 only by the authorized user</strong> - Again, it is certainly possible to build this solution, however, it is not the most optimal solution as it does not prevent an invalid upload of a file into another user's designated folder. So this option is incorrect.</p>\n\n<p><strong>Use CloudFront Lambda@Edge to validate that the given file is uploaded to S3 and downloaded from S3 only by the authorized user</strong> - This option assumes that the solution comprises a CloudFront distribution. This introduces inefficiency in the solution, as one needs to pay for CloudFront/Lambda@Edge and adds unnecessary hops in the data flow for both uploads and downloads.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-integrating-user-pools-with-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-integrating-user-pools-with-identity-pools.html</a></p>\n",
                "options": [
                    {
                        "id": 1715,
                        "content": "<p>Integrate Amazon API Gateway with a Lambda function that validates that the given file is uploaded to S3 and downloaded from S3 only by the authorized user</p>",
                        "isValid": false
                    },
                    {
                        "id": 1716,
                        "content": "<p>Leverage an IAM policy with the Amazon Cognito identity prefix to restrict users to use their own folders in Amazon S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 1717,
                        "content": "<p>Use CloudFront Lambda@Edge to validate that the given file is uploaded to S3 and downloaded from S3 only by the authorized user</p>",
                        "isValid": false
                    },
                    {
                        "id": 1718,
                        "content": "<p>Use S3 Event Notifications to trigger a Lambda function that validates that the given file is uploaded and downloaded only by the authorized user</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 421,
            "attributes": {
                "createdAt": "2023-09-07T08:39:37.764Z",
                "updatedAt": "2023-09-07T08:39:37.764Z",
                "content": "<p>You are a developer working with the AWS CLI to create Lambda functions that contain environment variables. Your functions will require over 50 environment variables consisting of sensitive information of database table names.</p>\n\n<p>What is the total set size/number of environment variables you can create for AWS Lambda?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The total size of all environment variables shouldn't exceed 4 KB. There is no limit on the number of variables</strong></p>\n\n<p>An environment variable is a pair of strings that are stored in a function's version-specific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. The total size of all environment variables doesn't exceed 4 KB. There is no limit defined on the number of variables that can be used.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The total size of all environment variables shouldn't exceed 8 KB. The maximum number of variables that can be created is 50</strong> - Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.</p>\n\n<p><strong>The total size of all environment variables shouldn't exceed 8 KB. There is no limit on the number of variables</strong> - Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.</p>\n\n<p><strong>The total size of all environment variables shouldn't exceed 4 KB. The maximum number of variables that can be created is 35</strong> - Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html</a></p>\n",
                "options": [
                    {
                        "id": 1719,
                        "content": "<p>The total size of all environment variables shouldn't exceed 8 KB. There is no limit on the number of variables</p>",
                        "isValid": false
                    },
                    {
                        "id": 1720,
                        "content": "<p>The total size of all environment variables shouldn't exceed 4 KB. The maximum number of variables that can be created is 35</p>",
                        "isValid": false
                    },
                    {
                        "id": 1721,
                        "content": "<p>The total size of all environment variables shouldn't exceed 4 KB. There is no limit on the number of variables</p>",
                        "isValid": true
                    },
                    {
                        "id": 1722,
                        "content": "<p>The total size of all environment variables shouldn't exceed 8 KB. The maximum number of variables that can be created is 50</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 422,
            "attributes": {
                "createdAt": "2023-09-07T08:39:37.841Z",
                "updatedAt": "2023-09-07T08:39:37.841Z",
                "content": "<p>An intern at an IT company is getting started with AWS Cloud and wants to understand the following Amazon S3 bucket access policy:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListAllS3Buckets\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:ListAllMyBuckets\"],\n            \"Resource\": \"arn:aws:s3:::*\"\n        },\n        {\n            \"Sid\": \"AllowBucketLevelActions\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::*\"\n        },\n        {\n            \"Sid\": \"AllowBucketObjectActions\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:PutObjectAcl\",\n                \"s3:GetObject\",\n                \"s3:GetObjectAcl\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": \"arn:aws:s3:::*/*\"\n        },\n        {\n            \"Sid\": \"RequireMFAForProductionBucket\",\n            \"Effect\": \"Deny\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::Production/*\",\n                \"arn:aws:s3:::Production\"\n            ],\n            \"Condition\": {\n                \"NumericGreaterThanIfExists\": {\"aws:MultiFactorAuthAge\": \"1800\"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>As a Developer Associate, can you help him identify what the policy is for?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Allows full S3 access, but explicitly denies access to the Production bucket if the user has not signed in using MFA within the last thirty minutes</strong> - This example shows how you might create a policy that allows an Amazon S3 user to access any bucket, including updating, adding, and deleting objects. However, it explicitly denies access to the Production bucket if the user has not signed in using multi-factor authentication (MFA) within the last thirty minutes. This policy grants the permissions necessary to perform this action in the console or programmatically using the AWS CLI or AWS API.</p>\n\n<p>This policy never allows programmatic access to the Production bucket using long-term user access keys. This is accomplished using the aws:MultiFactorAuthAge condition key with the NumericGreaterThanIfExists condition operator. This policy condition returns true if MFA is not present or if the age of the MFA is greater than 30 minutes. In those situations, access is denied. To access the Production bucket programmatically, the S3 user must use temporary credentials that were generated in the last 30 minutes using the GetSessionToken API operation.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Allows a user to manage a single Amazon S3 bucket and denies every other AWS action and resource if the user is not signed in using MFA within last thirty minutes</strong></p>\n\n<p><strong>Allows full S3 access to an Amazon Cognito user, but explicitly denies access to the Production bucket if the Cognito user is not authenticated</strong></p>\n\n<p><strong>Allows IAM users to access their own home directory in Amazon S3, programmatically and in the console</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_full-access-except-production.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_full-access-except-production.html</a></p>\n",
                "options": [
                    {
                        "id": 1723,
                        "content": "<p>Allows full S3 access to an Amazon Cognito user, but explicitly denies access to the Production bucket if the Cognito user is not authenticated</p>",
                        "isValid": false
                    },
                    {
                        "id": 1724,
                        "content": "<p>Allows a user to manage a single Amazon S3 bucket and denies every other AWS action and resource if the user is not signed in using MFA within last thirty minutes</p>",
                        "isValid": false
                    },
                    {
                        "id": 1725,
                        "content": "<p>Allows full S3 access, but explicitly denies access to the Production bucket if the user has not signed in using MFA within the last thirty minutes</p>",
                        "isValid": true
                    },
                    {
                        "id": 1726,
                        "content": "<p>Allows IAM users to access their own home directory in Amazon S3, programmatically and in the console</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 423,
            "attributes": {
                "createdAt": "2023-09-07T08:39:37.927Z",
                "updatedAt": "2023-09-07T08:39:37.927Z",
                "content": "<p>A media company uses Amazon Simple Queue Service (SQS) queue to manage their transactions. With changing business needs, the payload size of the messages is increasing. The Team Lead of the project is worried about the 256 KB message size limit that SQS has.</p>\n\n<p>What can be done to make the queue accept messages of a larger size?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use the SQS Extended Client</strong> - To manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the Amazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. Unless your application requires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queues, consider using Amazon S3 for storing your data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the MultiPart API</strong> - This is an incorrect statement. There is no multi-part API for Amazon Simple Queue Service.</p>\n\n<p><strong>Get a service limit increase from AWS</strong> - While it is possible to get service limits extended for certain AWS services, AWS already offers Extended Client to deal with queues that have larger messages.</p>\n\n<p><strong>Use gzip compression</strong> - You can compress the messages before sending them to the queue. The messages also need to be encoded after this to cater to SQS message standards. This adds bulk to the messages and will not be an optimal solution for the current scenario.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html</a></p>\n",
                "options": [
                    {
                        "id": 1727,
                        "content": "<p>Use the MultiPart API</p>",
                        "isValid": false
                    },
                    {
                        "id": 1728,
                        "content": "<p>Get a service limit increase from AWS</p>",
                        "isValid": false
                    },
                    {
                        "id": 1729,
                        "content": "<p>Use the SQS Extended Client</p>",
                        "isValid": true
                    },
                    {
                        "id": 1730,
                        "content": "<p>Use gzip compression</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 424,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.001Z",
                "updatedAt": "2023-09-07T08:39:38.001Z",
                "content": "<p>You company runs business logic on smaller software components that perform various functions. Some functions process information in a few seconds while others seem to take a long time to complete. Your manager asked you to decouple components that take a long time to ensure software applications stay responsive under load. You decide to configure Amazon Simple Queue Service (SQS) to work with your Elastic Beanstalk configuration.</p>\n\n<p>Which of the following Elastic Beanstalk environment should you choose to meet this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>Elastic BeanStalk Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><strong>Dedicated worker environment</strong> - If your AWS Elastic Beanstalk application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load.</p>\n\n<p>A long-running task is anything that substantially increases the time it takes to complete a request, such as processing images or videos, sending emails, or generating a ZIP archive. These operations can take only a second or two to complete, but a delay of a few seconds is a lot for a web request that would otherwise complete in less than 500 ms.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Single Instance Worker node</strong> - Worker machines in Kubernetes are called nodes. Amazon EKS worker nodes are standard Amazon EC2 instances. EKS worker nodes are not to be confused with the Elastic Beanstalk worker environment. Since we are talking about the Elastic Beanstalk environment, this is not the correct choice.</p>\n\n<p><strong>Load-balancing, Autoscaling environment</strong> - A load-balancing and autoscaling environment uses the Elastic Load Balancing and Amazon EC2 Auto Scaling services to provision the Amazon EC2 instances that are required for your deployed application. Amazon EC2 Auto Scaling automatically starts additional instances to accommodate increasing load on your application. If your application requires scalability with the option of running in multiple Availability Zones, use a load-balancing, autoscaling environment. This is not the right environment for the given use-case since it will add costs to the overall solution.</p>\n\n<p><strong>Single Instance with Elastic IP</strong> - A single-instance environment contains one Amazon EC2 instance with an Elastic IP address. A single-instance environment doesn't have a load balancer, which can help you reduce costs compared to a load-balancing, autoscaling environment. This is not a highly available architecture, because if that one instance goes down then your application is down. This is not recommended for production environments.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html</a></p>\n",
                "options": [
                    {
                        "id": 1731,
                        "content": "<p>Single Instance Worker node</p>",
                        "isValid": false
                    },
                    {
                        "id": 1732,
                        "content": "<p>Load-balancing, Autoscaling environment</p>",
                        "isValid": false
                    },
                    {
                        "id": 1733,
                        "content": "<p>Single Instance with Elastic IP</p>",
                        "isValid": false
                    },
                    {
                        "id": 1734,
                        "content": "<p>Dedicated worker environment</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 425,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.074Z",
                "updatedAt": "2023-09-07T08:39:38.074Z",
                "content": "<p>Your company is planning to move away from reserving EC2 instances and would like to adopt a more agile form of serverless architecture.</p>\n\n<p>Which of the following is the simplest and the least effort way of deploying the Docker containers on this serverless architecture?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Elastic Container Service (Amazon ECS) on Fargate</strong> - Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type.</p>\n\n<p>AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.</p>\n\n<p>ECS Fargate Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q15-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Elastic Container Service (Amazon ECS) on EC2</strong> - Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. ECS uses EC2 instances and hence cannot be called a serverless solution.</p>\n\n<p><strong>Amazon Elastic Kubernetes Service (Amazon EKS) on Fargate</strong> - Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed Kubernetes service. You can choose to run your EKS clusters using AWS Fargate, which is a serverless compute for containers. Since the use-case talks about the simplest and the least effort way to deploy Docker containers, EKS is not the best fit as you can use ECS Fargate to build a much easier solution. EKS is better suited to run Kubernetes on AWS without needing to install and operate your own Kubernetes control plane or worker nodes.</p>\n\n<p><strong>AWS Elastic Beanstalk</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. Beanstalk uses EC2 instances for its deployment, hence cannot be called a serverless architecture.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/eks/\">https://aws.amazon.com/eks/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n",
                "options": [
                    {
                        "id": 1735,
                        "content": "<p>AWS Elastic Beanstalk</p>",
                        "isValid": false
                    },
                    {
                        "id": 1736,
                        "content": "<p>Amazon Elastic Container Service (Amazon ECS) on Fargate</p>",
                        "isValid": true
                    },
                    {
                        "id": 1737,
                        "content": "<p>Amazon Elastic Container Service (Amazon ECS) on EC2</p>",
                        "isValid": false
                    },
                    {
                        "id": 1738,
                        "content": "<p>Amazon Elastic Kubernetes Service (Amazon EKS) on Fargate</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 426,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.150Z",
                "updatedAt": "2023-09-07T08:39:38.150Z",
                "content": "<p>As part of employee skills upgrade, the developers of your team have been delegated few responsibilities of DevOps engineers. Developers now have full control over modeling the entire software delivery process, from coding to deployment. As the team lead, you are now responsible for any manual approvals needed in the process.</p>\n\n<p>Which of the following approaches supports the given workflow?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create one CodePipeline for your entire flow and add a manual approval step</strong> - You can add an approval action to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop so someone can manually approve or reject the action. Approval actions can't be added to Source stages. Source stages can contain only source actions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create multiple CodePipelines for each environment and link them using AWS Lambda</strong> - You can create Lambda functions and add them as actions in your pipelines but the approval step is confined to a workflow process and cannot be outsourced to any other AWS service.</p>\n\n<p><strong>Create deeply integrated AWS CodePipelines for each environment</strong> - You can use an AWS CloudFormation template in conjunction with AWS CodePipeline and AWS CodeCommit to create a test environment that deploys to your production environment when the changes to your application are approved, helping you automate a continuous delivery workflow. This is a possible answer but not an optimized way of achieving what the client needs.</p>\n\n<p><strong>Use CodePipeline with Amazon Virtual Private Cloud</strong> - AWS CodePipeline supports Amazon Virtual Private Cloud (Amazon VPC) endpoints powered by AWS PrivateLink. This means you can connect directly to CodePipeline through a private endpoint in your VPC, keeping all traffic inside your VPC and the AWS network. This is a robust security feature but is of no value for our current use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/vpc-support.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/vpc-support.html</a></p>\n",
                "options": [
                    {
                        "id": 1739,
                        "content": "<p>Create one CodePipeline for your entire flow and add a manual approval step</p>",
                        "isValid": true
                    },
                    {
                        "id": 1740,
                        "content": "<p>Create multiple CodePipelines for each environment and link them using AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 1741,
                        "content": "<p>Create deeply integrated AWS CodePipelines for each environment</p>",
                        "isValid": false
                    },
                    {
                        "id": 1742,
                        "content": "<p>Use CodePipeline with Amazon Virtual Private Cloud</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 427,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.232Z",
                "updatedAt": "2023-09-07T08:39:38.232Z",
                "content": "<p>A HealthCare mobile app uses proprietary Machine Learning algorithms to provide early diagnosis using patient health metrics. To protect this sensitive data, the development team wants to transition to a scalable user management system with log-in/sign-up functionality that also supports Multi-Factor Authentication (MFA)</p>\n\n<p>Which of the following options can be used to implement a solution with the LEAST amount of development effort? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Use Amazon Cognito for user-management and facilitating the log-in/sign-up process</strong></p>\n\n<p><strong>Use Amazon Cognito to enable Multi-Factor Authentication (MFA) when users log-in</strong></p>\n\n<p>Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0.</p>\n\n<p>A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.</p>\n\n<p>Cognito user pools provide support for sign-up and sign-in services as well as security features such as multi-factor authentication (MFA).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q10-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Lambda functions and DynamoDB to create a custom solution for user management</strong></p>\n\n<p><strong>Use Lambda functions and RDS to create a custom solution for user management</strong></p>\n\n<p>As the problem statement mentions that the solution should require the least amount of development effort, so you cannot use Lambda functions with DynamoDB or RDS to create a custom solution. So both these options are incorrect.</p>\n\n<p><strong>Use Amazon SNS to send Multi-Factor Authentication (MFA) code via SMS to mobile app users</strong> - Amazon SNS cannot be used to send MFA codes via SMS to the user's mobile devices as this functionality is only meant to be used for IAM users. An SMS (short message service) MFA device can be any mobile device with a phone number that can receive standard SMS text messages. AWS will soon end support for SMS multi-factor authentication (MFA).</p>\n\n<p>Please see this for more details:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_sms.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_sms.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n",
                "options": [
                    {
                        "id": 1743,
                        "content": "<p>Use Amazon SNS to send Multi-Factor Authentication (MFA) code via SMS to mobile app users</p>",
                        "isValid": false
                    },
                    {
                        "id": 1744,
                        "content": "<p>Use Lambda functions and DynamoDB to create a custom solution for user management</p>",
                        "isValid": false
                    },
                    {
                        "id": 1745,
                        "content": "<p>Use Amazon Cognito to enable Multi-Factor Authentication (MFA) when users log-in</p>",
                        "isValid": true
                    },
                    {
                        "id": 1746,
                        "content": "<p>Use Amazon Cognito for user-management and facilitating the log-in/sign-up process</p>",
                        "isValid": true
                    },
                    {
                        "id": 1747,
                        "content": "<p>Use Lambda functions and RDS to create a custom solution for user management</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 428,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.303Z",
                "updatedAt": "2023-09-07T08:39:38.303Z",
                "content": "<p>A development team has created a new IAM user that has <code>s3:putObject</code> permission to write to an S3 bucket. This S3 bucket uses server-side encryption with AWS KMS managed keys (SSE-KMS) as the default encryption. Using the access key ID and the secret access key of the IAM user, the application received an access denied error when calling the <code>PutObject</code> API.</p>\n\n<p>As a Developer Associate, how would you resolve this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Correct the policy of the IAM user to allow the <code>kms:GenerateDataKey</code> action</strong> - You can protect data at rest in Amazon S3 by using three different modes of server-side encryption: SSE-S3, SSE-C, or SSE-KMS. SSE-KMS requires that AWS manage the data key but you manage the customer master key (CMK) in AWS KMS. You can choose a customer managed CMK or the AWS managed CMK for Amazon S3 in your account. If you choose to encrypt your data using the standard features, AWS KMS and Amazon S3 perform the following actions:</p>\n\n<ol>\n<li><p>Amazon S3 requests a plaintext data key and a copy of the key encrypted under the specified CMK.</p></li>\n<li><p>AWS KMS generates a data key, encrypts it under the CMK, and sends both the plaintext data key and the encrypted data key to Amazon S3.</p></li>\n<li><p>Amazon S3 encrypts the data using the data key and removes the plaintext key from memory as soon as possible after use.</p></li>\n<li><p>Amazon S3 stores the encrypted data key as metadata with the encrypted data.</p></li>\n</ol>\n\n<p>The error message indicates that your IAM user or role needs permission for the <code>kms:GenerateDataKey</code> action. This permission is required for buckets that use default encryption with a custom AWS KMS key.</p>\n\n<p>In the JSON policy documents, look for policies related to AWS KMS access. Review statements with \"Effect\": \"Allow\" to check if the user or role has permissions for the kms:GenerateDataKey action on the bucket's AWS KMS key. If this permission is missing, then add the permission to the appropriate policy.</p>\n\n<p>In the JSON policy documents, look for statements with \"Effect\": \"Deny\". Then, confirm that those statements don't deny the s3:PutObject action on the bucket. The statements must also not deny the IAM user or role access to the kms:GenerateDataKey action on the key used to encrypt the bucket. Additionally, make sure the necessary KMS and S3 permissions are not restricted using a VPC endpoint policy, service control policy, permissions boundary, or session policy.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Correct the policy of the IAM user to allow the <code>s3:Encrypt</code> action</strong> - This is an invalid action given only as a distractor.</p>\n\n<p><strong>Correct the bucket policy of the S3 bucket to allow the IAM user to upload encrypted objects</strong> - The user already has access to the bucket. What the user lacks is access to generate a KMS key, which is mandatory when a bucket is enabled for default encryption.</p>\n\n<p><strong>Correct the ACL of the S3 bucket to allow the IAM user to upload encrypted objects</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. It defines which AWS accounts or groups are granted access and the type of access. ACL is another way of giving access to S3 bucket objects. Permissions to use KMS keys will still be needed.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-error-kms/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-error-kms/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html</a></p>\n",
                "options": [
                    {
                        "id": 1748,
                        "content": "<p>Correct the ACL of the S3 bucket to allow the IAM user to upload encrypted objects</p>",
                        "isValid": false
                    },
                    {
                        "id": 1749,
                        "content": "<p>Correct the policy of the IAM user to allow the <code>s3:Encrypt</code> action</p>",
                        "isValid": false
                    },
                    {
                        "id": 1750,
                        "content": "<p>Correct the bucket policy of the S3 bucket to allow the IAM user to upload encrypted objects</p>",
                        "isValid": false
                    },
                    {
                        "id": 1751,
                        "content": "<p>Correct the policy of the IAM user to allow the <code>kms:GenerateDataKey</code> action</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 429,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.375Z",
                "updatedAt": "2023-09-07T08:39:38.375Z",
                "content": "<p>The development team at a company wants to encrypt a 111 GB object using AWS KMS.</p>\n\n<p>Which of the following represents the best solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Make a <code>GenerateDataKey</code> API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data</strong> - <code>GenerateDataKey</code> API, generates a unique symmetric data key for client-side encryption. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.</p>\n\n<p><code>GenerateDataKey</code> returns a unique data key for each request. The bytes in the plaintext key are not related to the caller or the CMK.</p>\n\n<p>To encrypt data outside of AWS KMS:</p>\n\n<ol>\n<li><p>Use the <code>GenerateDataKey</code> operation to get a data key.</p></li>\n<li><p>Use the plaintext data key (in the Plaintext field of the response) to encrypt your data outside of AWS KMS. Then erase the plaintext data key from memory.</p></li>\n<li><p>Store the encrypted data key (in the CiphertextBlob field of the response) with the encrypted data.</p></li>\n</ol>\n\n<p>To decrypt data outside of AWS KMS:</p>\n\n<ol>\n<li><p>Use the Decrypt operation to decrypt the encrypted data key. The operation returns a plaintext copy of the data key.</p></li>\n<li><p>Use the plaintext data key to decrypt data outside of AWS KMS, then erase the plaintext data key from memory.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Make a <code>GenerateDataKeyWithPlaintext</code> API call that returns an encrypted copy of a data key. Use a plaintext key to encrypt the data</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p><strong>Make an <code>Encrypt</code> API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material</strong> - <code>Encrypt</code> API is used to encrypt plaintext into ciphertext by using a customer master key (CMK). The Encrypt operation has two primary use cases:</p>\n\n<ol>\n<li><p>To encrypt small amounts of arbitrary data, such as a personal identifier or database password, or other sensitive information.</p></li>\n<li><p>To move encrypted data from one AWS Region to another.</p></li>\n</ol>\n\n<p>Neither of the two is useful for the given scenario.</p>\n\n<p><strong>Make a <code>GenerateDataKeyWithoutPlaintext</code> API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data</strong> - <code>GenerateDataKeyWithoutPlaintext</code> API, generates a unique symmetric data key. This operation returns a data key that is encrypted under a customer master key (CMK) that you specify.</p>\n\n<p><code>GenerateDataKeyWithoutPlaintext</code> is identical to the <code>GenerateDataKey</code> operation except that returns only the encrypted copy of the data key. This operation is useful for systems that need to encrypt data at some point, but not immediately. When you need to encrypt the data, you call the Decrypt operation on the encrypted copy of the key.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_Encrypt.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_Encrypt.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html</a></p>\n",
                "options": [
                    {
                        "id": 1752,
                        "content": "<p>Make a <code>GenerateDataKey</code> API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data</p>",
                        "isValid": true
                    },
                    {
                        "id": 1753,
                        "content": "<p>Make a <code>GenerateDataKeyWithPlaintext</code> API call that returns an encrypted copy of a data key. Use a plaintext key to encrypt the data</p>",
                        "isValid": false
                    },
                    {
                        "id": 1754,
                        "content": "<p>Make a <code>GenerateDataKeyWithoutPlaintext</code> API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data</p>",
                        "isValid": false
                    },
                    {
                        "id": 1755,
                        "content": "<p>Make an <code>Encrypt</code> API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 430,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.452Z",
                "updatedAt": "2023-09-07T08:39:38.452Z",
                "content": "<p>A developer wants to securely store an access token that allows a transaction-processing application running on Amazon EC2 instances to authenticate and send a chat message (via the chat API) to the company's support team when an invalid transaction is detected. While minimizing management overhead, the chat API access token must be encrypted both at rest and in transit, and also be accessible from other AWS accounts.</p>\n\n<p>What is the most efficient solution to address this scenario?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Leverage AWS Secrets Manager with an AWS KMS customer-managed key to store the access token as a secret and configure a resource-based policy for the secret to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Secrets Manager. Fetch the token from Secrets Manager and then use the decrypted access token to send the message to the chat</strong></p>\n\n<p>AWS Secrets Manager is an AWS service that encrypts and stores your secrets, and transparently decrypts and returns them to you in plaintext. It's designed especially to store application secrets, such as login credentials, that change periodically and should not be hard-coded or stored in plaintext in the application. In place of hard-coded credentials or table lookups, your application calls Secrets Manager.</p>\n\n<p>Secrets Manager also supports features that periodically rotate the secrets associated with commonly used databases. It always encrypts newly rotated secrets before they are stored.</p>\n\n<p>Secrets Manager integrates with AWS Key Management Service (AWS KMS) to encrypt every version of every secret value with a unique data key that is protected by an AWS KMS key. This integration protects your secrets under encryption keys that never leave AWS KMS unencrypted. It also enables you to set custom permissions on the KMS key and audit the operations that generate, encrypt, and decrypt the data keys that protect your secrets.</p>\n\n<p>To grant permission to retrieve secret values, you can attach policies to secrets or identities.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q46-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_examples.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_examples.html</a></p>\n\n<p>For the given use case, you can use the resource-based policy to the secret to allow access from other accounts. Then you need to update the IAM role of the EC2 instances with permissions to access Secrets Manager which will retrieve the token from Secrets Manager and use the decrypted access token to send the message to the support team via the chat API.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage AWS Systems Manager Parameter Store with an AWS KMS customer-managed key to store the access token as a SecureString parameter and configure a resource-based policy for the parameter to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Parameter Store. Fetch the token from Parameter Store using the <code>with decryption</code> flag and then use the decrypted access token to send the message to the chat</strong> - You cannot use a resource-based policy with a parameter in the Parameter Store. Parameter Store supports parameter policies that are available for parameters that use the advanced parameters tier. Parameter policies help you manage a growing set of parameters by allowing you to assign specific criteria to a parameter such as an expiration date or time to live. Parameter policies are especially helpful in forcing you to update or delete passwords and configuration data stored in Parameter Store, a capability of AWS Systems Manager. So this option is incorrect.</p>\n\n<p><strong>Store AWS KMS encrypted access token in a DynamoDB table and configure a resource-based policy for the DynamoDB table to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the DynamoDB table. Fetch the token from the Dynamodb table and then use the decrypted access token to send the message to the chat</strong> - You should note that DynamoDB does not support resource-based policies. Moreover, it's a security bad practice to keep sensitive access credentials in code, database or a flat file on a file system or object storage. Therefore, this option is incorrect.</p>\n\n<p><strong>Leverage SSE-KMS to store the access token as an encrypted object on S3 and configure a resource-based policy for the S3 bucket to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the S3 object. Fetch the token from S3 and then use the decrypted access token to send the message to the chat</strong> - It is considered a security bad practice to keep sensitive access credentials in code, database, or a flat file on a file system or object storage. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_examples.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_examples.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store-policies.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store-policies.html</a></p>\n",
                "options": [
                    {
                        "id": 1756,
                        "content": "<p>Leverage AWS Systems Manager Parameter Store with an AWS KMS customer-managed key to store the access token as a SecureString parameter and configure a resource-based policy for the parameter to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Parameter Store. Fetch the token from Parameter Store using the <code>with decryption</code> flag and then use the decrypted access token to send the message to the chat</p>",
                        "isValid": false
                    },
                    {
                        "id": 1757,
                        "content": "<p>Leverage AWS Secrets Manager with an AWS KMS customer-managed key to store the access token as a secret and configure a resource-based policy for the secret to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Secrets Manager. Fetch the token from Secrets Manager and then use the decrypted access token to send the message to the chat</p>",
                        "isValid": true
                    },
                    {
                        "id": 1758,
                        "content": "<p>Store AWS KMS encrypted access token in a DynamoDB table and configure a resource-based policy for the DynamoDB table to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the DynamoDB table. Fetch the token from the Dynamodb table and then use the decrypted access token to send the message to the chat</p>",
                        "isValid": false
                    },
                    {
                        "id": 1759,
                        "content": "<p>Leverage SSE-KMS to store the access token as an encrypted object on S3 and configure a resource-based policy for the S3 bucket to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the S3 object. Fetch the token from S3 and then use the decrypted access token to send the message to the chat</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 431,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.531Z",
                "updatedAt": "2023-09-07T08:39:38.531Z",
                "content": "<p>Your team lead has requested code review of your code for Lambda functions. Your code is written in Python and makes use of the Amazon Simple Storage Service (S3) to upload logs to an S3 bucket. After the review, your team lead has recommended reuse of execution context to improve the Lambda performance.</p>\n\n<p>Which of the following actions will help you implement the recommendation?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Move the Amazon S3 client initialization, out of your function handler</strong> - AWS best practices for Lambda suggest taking advantage of execution context reuse to improve the performance of your functions. Initialize SDK clients and database connections outside of the function handler, and cache static assets locally in the /tmp directory. Subsequent invocations processed by the same instance of your function can reuse these resources. This saves execution time and cost. To avoid potential data leaks across invocations, donâ€™t use the execution context to store user data, events, or other information with security implications.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use environment variables to pass operational parameters</strong> - This is one of the suggested best practices for Lambda. By using environment variables to pass operational parameters you can avoid hard-coding useful information. But, this is not the right answer for the current use-case, since it talks about reusing context.</p>\n\n<p><strong>Assign more RAM to the function</strong> - Increasing RAM will help speed up the process. But, in the current question, the reviewer has specifically mentioned about reusing context. Hence, this is not the right answer.</p>\n\n<p><strong>Enable X-Ray integration</strong> - You can use AWS X-Ray to visualize the components of your application, identify performance bottlenecks, and troubleshoot requests that resulted in an error. Your Lambda functions send trace data to X-Ray, and X-Ray processes the data to generate a service map and searchable trace summaries. This is a useful tool for troubleshooting. But, for the current use-case, we already know the bottleneck that needs to be fixed and that is the context reuse.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\">https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html</a></p>\n",
                "options": [
                    {
                        "id": 1760,
                        "content": "<p>Assign more RAM to the function</p>",
                        "isValid": false
                    },
                    {
                        "id": 1761,
                        "content": "<p>Move the Amazon S3 client initialization, out of your function handler</p>",
                        "isValid": true
                    },
                    {
                        "id": 1762,
                        "content": "<p>Enable X-Ray integration</p>",
                        "isValid": false
                    },
                    {
                        "id": 1763,
                        "content": "<p>Use environment variables to pass operational parameters</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 432,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.611Z",
                "updatedAt": "2023-09-07T08:39:38.611Z",
                "content": "<p>To meet compliance guidelines, a company needs to ensure replication of any data stored in its S3 buckets.</p>\n\n<p>Which of the following characteristics are correct while configuring an S3 bucket for replication? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Same-Region Replication (SRR) and Cross-Region Replication (CRR) can be configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags</strong> - Amazon S3 Replication (CRR and SRR) is configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags. You add a replication configuration on your source bucket by specifying a destination bucket in the same or different AWS region for replication.</p>\n\n<p><strong>S3 lifecycle actions are not replicated with S3 replication</strong> - With S3 Replication (CRR and SRR), you can establish replication rules to make copies of your objects into another storage class, in the same or a different region. Lifecycle actions are not replicated, and if you want the same lifecycle configuration applied to both source and destination buckets, enable the same lifecycle configuration on both.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Object tags cannot be replicated across AWS Regions using Cross-Region Replication</strong> - Object tags can be replicated across AWS Regions using Cross-Region Replication. For customers with Cross-Region Replication already enabled, new permissions are required for tags to replicate.</p>\n\n<p><strong>Once replication is enabled on a bucket, all old and new objects will be replicated</strong> - Replication only replicates the objects added to the bucket after replication is enabled on the bucket. Any objects present in the bucket before enabling replication are not replicated.</p>\n\n<p><strong>Replicated objects do not retain metadata</strong> - You can use replication to make copies of your objects that retain all metadata, such as the original object creation time and version IDs. This capability is important if you need to ensure that your replica is identical to the source object.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/replication.html\">https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/replication.html</a></p>\n",
                "options": [
                    {
                        "id": 1764,
                        "content": "<p>Replicated objects do not retain metadata</p>",
                        "isValid": false
                    },
                    {
                        "id": 1765,
                        "content": "<p>Object tags cannot be replicated across AWS Regions using Cross-Region Replication</p>",
                        "isValid": false
                    },
                    {
                        "id": 1766,
                        "content": "<p>Once replication is enabled on a bucket, all old and new objects will be replicated</p>",
                        "isValid": false
                    },
                    {
                        "id": 1767,
                        "content": "<p>Same-Region Replication (SRR) and Cross-Region Replication (CRR) can be configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags</p>",
                        "isValid": true
                    },
                    {
                        "id": 1768,
                        "content": "<p>S3 lifecycle actions are not replicated with S3 replication</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 433,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.686Z",
                "updatedAt": "2023-09-07T08:39:38.686Z",
                "content": "<p>A company has more than 100 million members worldwide enjoying 125 million hours of TV shows and movies each day. The company uses AWS for nearly all its computing and storage needs, which use more than 10,000 server instances on AWS. This results in an extremely complex and dynamic networking environment where applications are constantly communicating inside AWS and across the Internet. Monitoring and optimizing its network is critical for the company.</p>\n\n<p>The company needs a solution for ingesting and analyzing the multiple terabytes of real-time data its network generates daily in the form of flow logs. Which technology/service should the company use to ingest this data economically and has the flexibility to direct this data to other downstream systems?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Queue Service (SQS)</strong> - Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. AWS recommends using Amazon SQS for cases where individual message fail/success are important, message delays are needed and there is only one consumer for the messages received (if more than one consumers need to consume the message, then AWS suggests configuring more queues).</p>\n\n<p><strong>Amazon Kinesis Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards youâ€™re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration.</p>\n\n<p>Kinesis data streams is highly customizable and best suited for developers building custom applications or streaming data for specialized needs. Data Streams also provide greater flexibility in integrating downstream applications than Firehose. Data Streams is also a cost-effective option compared to Firehose. Therefore, KDS is the right solution.</p>\n\n<p><strong>AWS Glue</strong> - AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue provides all of the capabilities needed for data integration so that you can start analyzing your data and putting it to use in minutes instead of months. Glue is not best suited to handle real-time data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n",
                "options": [
                    {
                        "id": 1769,
                        "content": "<p>Amazon Kinesis Firehose</p>",
                        "isValid": false
                    },
                    {
                        "id": 1770,
                        "content": "<p>Amazon Simple Queue Service (SQS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 1771,
                        "content": "<p>Amazon Kinesis Data Streams</p>",
                        "isValid": true
                    },
                    {
                        "id": 1772,
                        "content": "<p>AWS Glue</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 434,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.765Z",
                "updatedAt": "2023-09-07T08:39:38.765Z",
                "content": "<p>You are working for a shipping company that is automating the creation of ECS clusters with an Auto Scaling Group using an AWS CloudFormation template that accepts cluster name as its parameters.  Initially, you launch the template with input value 'MainCluster', which deployed five instances across two availability zones. The second time, you launch the template with an input value 'SecondCluster'. However, the instances created in the second run were also launched in 'MainCluster' even after specifying a different cluster name.</p>\n\n<p>What is the root cause of this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The cluster name Parameter has not been updated in the file /etc/ecs/ecs.config during bootstrap</strong> - In the ecs.config file you have to configure the parameter ECS_CLUSTER='your_cluster_name' to register the container instance with a cluster named 'your_cluster_name'.</p>\n\n<p>Sample config for ECS Container Agent:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q35-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EC2 instance is missing IAM permissions to join the other clusters</strong> - EC2 instances are getting registered to the first cluster, so permissions are not an issue here and hence this statement is an incorrect choice for the current use case.</p>\n\n<p><strong>The ECS agent Docker image must be re-built to connect to the other clusters</strong> -  Since the first set of instances got created from the template without any issues, there is no issue with the ECS agent here.</p>\n\n<p><strong>The security groups on the EC2 instance are pointing to the wrong ECS cluster</strong> - Security groups govern the rules about the incoming network traffic to your ECS containers. The issue here is not about user access and hence is a wrong choice for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html</a></p>\n",
                "options": [
                    {
                        "id": 1773,
                        "content": "<p>The cluster name Parameter has not been updated in the file /etc/ecs/ecs.config during bootstrap</p>",
                        "isValid": true
                    },
                    {
                        "id": 1774,
                        "content": "<p>The security groups on the EC2 instance are pointing to the wrong ECS cluster</p>",
                        "isValid": false
                    },
                    {
                        "id": 1775,
                        "content": "<p>The ECS agent Docker image must be re-built to connect to the other clusters</p>",
                        "isValid": false
                    },
                    {
                        "id": 1776,
                        "content": "<p>The EC2 instance is missing IAM permissions to join the other clusters</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 435,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.836Z",
                "updatedAt": "2023-09-07T08:39:38.836Z",
                "content": "<p>A companyâ€™s e-commerce website is expecting hundreds of thousands of visitors on Black Friday. The marketing department is concerned that high volumes of orders might stress SQS leading to message failures. The company has approached you for the steps to be taken as a precautionary measure against the high volumes.</p>\n\n<p>What step will you suggest as a Developer Associate?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon SQS is highly scalable and does not need any intervention to handle the expected high volumes</strong></p>\n\n<p>Amazon SQS leverages the AWS cloud to dynamically scale, based on demand. SQS scales elastically with your application so you don't have to worry about capacity planning and pre-provisioning. For most standard queues (depending on queue traffic and message backlog), there can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue).</p>\n\n<p>Info on Queue Quotas:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q53-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Pre-configure the SQS queue to increase the capacity when messages hit a certain threshold</strong> - This is an incorrect statement. Amazon SQS scales dynamically, automatically provisioning the needed capacity.</p>\n\n<p><strong>Enable auto-scaling in the SQS queue</strong> - SQS queues are, by definition, auto-scalable and do not need any configuration changes for auto-scaling.</p>\n\n<p><strong>Convert the queue into FIFO ordered queue, since messages to the down system will be processed faster once they are ordered</strong> - This is a wrong statement. You cannot convert an existing standard queue to FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.</p>\n\n<p>Standard to FIFO queue conversion:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q53-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n",
                "options": [
                    {
                        "id": 1777,
                        "content": "<p>Convert the queue into FIFO ordered queue, since messages to the down system will be processed faster once they are ordered</p>",
                        "isValid": false
                    },
                    {
                        "id": 1778,
                        "content": "<p>Amazon SQS is highly scalable and does not need any intervention to handle the expected high volumes</p>",
                        "isValid": true
                    },
                    {
                        "id": 1779,
                        "content": "<p>Pre-configure the SQS queue to increase the capacity when messages hit a certain threshold</p>",
                        "isValid": false
                    },
                    {
                        "id": 1780,
                        "content": "<p>Enable auto-scaling in the SQS queue</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 436,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.909Z",
                "updatedAt": "2023-09-07T08:39:38.909Z",
                "content": "<p>An IT company has its serverless stack integrated with AWS X-Ray. The developer at the company has noticed a high volume of data going into X-Ray and the AWS monthly usage charges have skyrocketed as a result. The developer has requested changes to mitigate the issue.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to obtain tracing trends while reducing costs with minimal disruption?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your applicationâ€™s underlying components.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p><strong>Enable X-Ray sampling</strong></p>\n\n<p>To ensure efficient tracing and provide a representative sample of the requests that your application serves, the X-Ray SDK applies a sampling algorithm to determine which requests get traced. By default, the X-Ray SDK records the first request each second, and five percent of any additional requests. X-Ray sampling is enabled directly from the AWS console, hence your application code does not need to change.</p>\n\n<p>You can also customize the X-Ray sampling rules:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Filter Expressions in the X-Ray console</strong> - When you choose a time period of traces to view in the X-Ray console, you might get more results than the console can display. You can narrow the results to just the traces that you want to find by using a filter expression. This option is not correct because it does not reduce the volume of data sent into the X-Ray console.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q13-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html</a></p>\n\n<p><strong>Custom configuration for the X-Ray agents</strong> - You cannot do a custom configuration, instead you can do custom sampling rules. So this option is incorrect.</p>\n\n<p><strong>Implement a network sampling rule</strong> - This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html</a></p>\n",
                "options": [
                    {
                        "id": 1781,
                        "content": "<p>Enable X-Ray sampling</p>",
                        "isValid": true
                    },
                    {
                        "id": 1782,
                        "content": "<p>Custom configuration for the X-Ray agents</p>",
                        "isValid": false
                    },
                    {
                        "id": 1783,
                        "content": "<p>Implement a network sampling rule</p>",
                        "isValid": false
                    },
                    {
                        "id": 1784,
                        "content": "<p>Use Filter Expressions in the X-Ray console</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 437,
            "attributes": {
                "createdAt": "2023-09-07T08:39:38.986Z",
                "updatedAt": "2023-09-07T08:39:38.986Z",
                "content": "<p>A cybersecurity company is running a serverless backend with several compute-heavy workflows running on Lambda functions. The development team has noticed a performance lag after analyzing the performance metrics for the Lambda functions.</p>\n\n<p>As a Developer Associate, which of the following options would you suggest as the BEST solution to address the compute-heavy workloads?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Increase the amount of memory available to the Lambda functions</strong></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>In the AWS Lambda resource model, you choose the amount of memory you want for your function which allocates proportional CPU power and other resources. This means you will have access to more compute power when you choose one of the new larger settings. You can set your memory in 64MB increments from 128MB to 3008MB. You access these settings when you create a function or update its configuration. The settings are available using the AWS Management Console, AWS CLI, or SDKs.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a></p>\n\n<p>Therefore, by increasing the amount of memory available to the Lambda functions, you can run the compute-heavy workflows.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Invoke the Lambda functions asynchronously to process the compute-heavy workflows</strong> - When you invoke a function asynchronously, you don't wait for a response from the function code. You hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors and can send invocation records to a downstream resource to chain together components of your application. The method of invocation has no bearing on the Lambda function's ability to process the compute-heavy workflows.</p>\n\n<p><strong>Use reserved concurrency to account for the compute-heavy workflows</strong></p>\n\n<p><strong>Use provisioned concurrency to account for the compute-heavy workflows</strong></p>\n\n<p>Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency. The type of concurrency has no bearing on the Lambda function's ability to process the compute-heavy workflows. So both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a></p>\n",
                "options": [
                    {
                        "id": 1785,
                        "content": "<p>Invoke the Lambda functions asynchronously to process the compute-heavy workflows</p>",
                        "isValid": false
                    },
                    {
                        "id": 1786,
                        "content": "<p>Use provisioned concurrency to account for the compute-heavy workflows</p>",
                        "isValid": false
                    },
                    {
                        "id": 1787,
                        "content": "<p>Use reserved concurrency to account for the compute-heavy workflows</p>",
                        "isValid": false
                    },
                    {
                        "id": 1788,
                        "content": "<p>Increase the amount of memory available to the Lambda functions</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 438,
            "attributes": {
                "createdAt": "2023-09-07T08:39:39.061Z",
                "updatedAt": "2023-09-07T08:39:39.061Z",
                "content": "<p>Your company hosts a static website on Amazon Simple Storage Service (S3) written in HTML5. The website targets aviation enthusiasts and it has grown a worldwide audience with hundreds of thousands of visitors accessing the website now on a monthly basis. While users in the United States have a great user experience, users from other parts of the world are experiencing slow responses and lag.</p>\n\n<p>Which service can mitigate this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon CloudFront</strong></p>\n\n<p>Storing your static content with S3 provides a lot of advantages. But to help optimize your applicationâ€™s performance and security while effectively managing cost, AWS recommends that you also set up Amazon CloudFront to work with your S3 bucket to serve and protect the content. CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>By caching your content in Edge Locations, CloudFront reduces the load on your S3 bucket and helps ensure a faster response for your users when they request content. In addition, data transfer out for content by using CloudFront is often more cost-effective than serving files directly from S3, and there is no data transfer fee from S3 to CloudFront.</p>\n\n<p>A security feature of CloudFront is Origin Access Identity (OAI), which restricts access to an S3 bucket and its content to only CloudFront and operations it performs.</p>\n\n<p>CloudFront Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q16-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon ElastiCache for Redis</strong> - Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, and Q&amp;A portals). ElastiCache is often used with databases that need millisecond latency. For the current scenario, we do not need a caching layer since the data load is not that heavy.</p>\n\n<p><strong>Use Amazon S3 Caching</strong> - This is a made-up option, given as a distractor.</p>\n\n<p><strong>Use Amazon S3 Transfer Acceleration</strong> - Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. However, S3 Transfer Acceleration leverages Amazon CloudFrontâ€™s globally distributed AWS Edge Locations. Each time S3 Transfer Acceleration is used to upload an object, AWS checks whether S3 Transfer Acceleration is likely to be faster than a regular Amazon S3 transfer. If it finds that S3 Transfer Acceleration might not be significantly faster, AWS shifts back to normal Amazon S3 transfer mode. So, this is not the right option for our use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n",
                "options": [
                    {
                        "id": 1789,
                        "content": "<p>Use Amazon S3 Transfer Acceleration</p>",
                        "isValid": false
                    },
                    {
                        "id": 1790,
                        "content": "<p>Use Amazon CloudFront</p>",
                        "isValid": true
                    },
                    {
                        "id": 1791,
                        "content": "<p>Use Amazon ElastiCache for Redis</p>",
                        "isValid": false
                    },
                    {
                        "id": 1792,
                        "content": "<p>Use Amazon S3 Caching</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 439,
            "attributes": {
                "createdAt": "2023-09-07T08:39:39.140Z",
                "updatedAt": "2023-09-07T08:39:39.140Z",
                "content": "<p>A telecom service provider stores its critical customer data on Amazon Simple Storage Service (Amazon S3).</p>\n\n<p>Which of the following options can be used to control access to data stored on Amazon S3? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Bucket policies, Identity and Access Management (IAM) policies</strong></p>\n\n<p><strong>Query String Authentication, Access Control Lists (ACLs)</strong></p>\n\n<p>Customers may use four mechanisms for controlling access to Amazon S3 resources: Identity and Access Management (IAM) policies, bucket policies, Access Control Lists (ACLs), and Query String Authentication.</p>\n\n<p>IAM enables organizations with multiple employees to create and manage multiple users under a single AWS account. With IAM policies, customers can grant IAM users fine-grained control to their Amazon S3 bucket or objects while also retaining full control over everything the users do.</p>\n\n<p>With bucket policies, customers can define rules which apply broadly across all requests to their Amazon S3 resources, such as granting write privileges to a subset of Amazon S3 resources. Customers can also restrict access based on an aspect of the request, such as HTTP referrer and IP address.</p>\n\n<p>With ACLs, customers can grant specific permissions (i.e. READ, WRITE, FULL_CONTROL) to specific users for an individual bucket or object.</p>\n\n<p>With Query String Authentication, customers can create a URL to an Amazon S3 object which is only valid for a limited time. Using query parameters to authenticate requests is useful when you want to express a request entirely in a URL. This method is also referred as presigning a URL.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Permissions boundaries, Identity and Access Management (IAM) policies</strong></p>\n\n<p><strong>Query String Authentication, Permissions boundaries</strong></p>\n\n<p><strong>IAM database authentication, Bucket policies</strong></p>\n\n<p>Permissions boundary - A Permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. When you use a policy to set the permissions boundary for a user, it limits the user's permissions but does not provide permissions on its own.</p>\n\n<p>IAM database authentication - IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. It is a database authentication technique and cannot be used to authenticate for S3.</p>\n\n<p>Therefore, all three options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-overview.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-overview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p>\n",
                "options": [
                    {
                        "id": 1793,
                        "content": "<p>Query String Authentication, Permissions boundaries</p>",
                        "isValid": false
                    },
                    {
                        "id": 1794,
                        "content": "<p>IAM database authentication, Bucket policies</p>",
                        "isValid": false
                    },
                    {
                        "id": 1795,
                        "content": "<p>Permissions boundaries, Identity and Access Management (IAM) policies</p>",
                        "isValid": false
                    },
                    {
                        "id": 1796,
                        "content": "<p>Bucket policies, Identity and Access Management (IAM) policies</p>",
                        "isValid": true
                    },
                    {
                        "id": 1797,
                        "content": "<p>Query String Authentication, Access Control Lists (ACLs)</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 440,
            "attributes": {
                "createdAt": "2023-09-07T08:39:39.214Z",
                "updatedAt": "2023-09-07T08:39:39.214Z",
                "content": "<p>Your company uses an Application Load Balancer to route incoming end-user traffic to applications hosted on Amazon EC2 instances. The applications capture incoming request information and store it in the Amazon Relational Database Service (RDS) running on Microsoft SQL Server DB engines.</p>\n\n<p>As part of new compliance rules, you need to capture the client's IP address. How will you achieve this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use the header X-Forwarded-For</strong> - The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can get the Client IP addresses from server access logs</strong> - As discussed above, Load Balancers intercept traffic between clients and servers, so server access logs will contain only the IP address of the load balancer.</p>\n\n<p><strong>Use the header X-Forwarded-From</strong> - This is a made-up option and given as a distractor.</p>\n\n<p><strong>You can get the Client IP addresses from Elastic Load Balancing logs</strong> - Elastic Load Balancing logs requests sent to the load balancer, including requests that never made it to the targets. For example, if a client sends a malformed request, or there are no healthy targets to respond to the request, the request is still logged. So, this is not the right option if we wish to collect the IP addresses of the clients that have access to the instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n",
                "options": [
                    {
                        "id": 1798,
                        "content": "<p>You can get the Client IP addresses from server access logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1799,
                        "content": "<p>Use the header X-Forwarded-For</p>",
                        "isValid": true
                    },
                    {
                        "id": 1800,
                        "content": "<p>You can get the Client IP addresses from Elastic Load Balancing logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1801,
                        "content": "<p>Use the header X-Forwarded-From</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 441,
            "attributes": {
                "createdAt": "2023-09-07T08:39:39.292Z",
                "updatedAt": "2023-09-07T08:39:39.292Z",
                "content": "<p>The Development team at a media company is working on securing their databases.</p>\n\n<p>Which of the following AWS database engines can be configured with IAM Database Authentication? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p>You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication.  With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM.</p>\n\n<p><strong>RDS MySQL</strong> - IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS.</p>\n\n<p><strong>RDS PostGreSQL</strong> - IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>RDS Oracle</strong></p>\n\n<p><strong>RDS SQL Server</strong></p>\n\n<p>These two options contradict the details in the explanation above, so these are incorrect.</p>\n\n<p><strong>RDS Db2</strong> - This option has been added as a distractor. Db2 is a family of data management products, including database servers, developed by IBM. RDS does not support Db2 database engine.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p>\n",
                "options": [
                    {
                        "id": 1802,
                        "content": "<p>RDS MySQL</p>",
                        "isValid": true
                    },
                    {
                        "id": 1803,
                        "content": "<p>RDS Db2</p>",
                        "isValid": false
                    },
                    {
                        "id": 1804,
                        "content": "<p>RDS Oracle</p>",
                        "isValid": false
                    },
                    {
                        "id": 1805,
                        "content": "<p>RDS PostGreSQL</p>",
                        "isValid": true
                    },
                    {
                        "id": 1806,
                        "content": "<p>RDS SQL Server</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 442,
            "attributes": {
                "createdAt": "2023-09-07T08:39:39.368Z",
                "updatedAt": "2023-09-07T08:39:39.368Z",
                "content": "<p>The development team at a social media company is considering using Amazon ElastiCache to boost the performance of their existing databases.</p>\n\n<p>As a Developer Associate, which of the following use-cases would you recommend as the BEST fit for ElastiCache? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use ElastiCache to improve latency and throughput for read-heavy application workloads</strong></p>\n\n<p><strong>Use ElastiCache to improve performance of compute-intensive workloads</strong></p>\n\n<p>Amazon ElastiCache allows you to run in-memory data stores in the AWS cloud. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/images/ElastiCache-Caching.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a></p>\n\n<p>Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, and Q&amp;A portals) or compute-intensive workloads (such as a recommendation engine) by allowing you to store the objects that are often read in the cache.</p>\n\n<p>Overview of Amazon ElastiCache features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q3-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/features/\">https://aws.amazon.com/elasticache/features/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use ElastiCache to improve latency and throughput for write-heavy application workloads</strong> - As mentioned earlier in the explanation, Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads. Caching is not a good fit for write-heavy applications as the cache goes stale at a very fast rate.</p>\n\n<p><strong>Use ElastiCache to improve performance of Extract-Transform-Load (ETL) workloads</strong> - ETL workloads involve reading and transforming high volume data which is not a good fit for caching. You should use AWS Glue or Amazon EMR to facilitate ETL workloads.</p>\n\n<p><strong>Use ElastiCache to run highly complex JOIN queries</strong> - Complex JSON queries can be run on relational databases such as RDS or Aurora. ElastiCache is not a good fit for this use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/features/\">https://aws.amazon.com/elasticache/features/</a></p>\n",
                "options": [
                    {
                        "id": 1807,
                        "content": "<p>Use ElastiCache to improve performance of Extract-Transform-Load (ETL) workloads</p>",
                        "isValid": false
                    },
                    {
                        "id": 1808,
                        "content": "<p>Use ElastiCache to improve performance of compute-intensive workloads</p>",
                        "isValid": true
                    },
                    {
                        "id": 1809,
                        "content": "<p>Use ElastiCache to improve latency and throughput for read-heavy application workloads</p>",
                        "isValid": true
                    },
                    {
                        "id": 1810,
                        "content": "<p>Use ElastiCache to run highly complex JOIN queries</p>",
                        "isValid": false
                    },
                    {
                        "id": 1811,
                        "content": "<p>Use ElastiCache to improve latency and throughput for write-heavy application workloads</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 443,
            "attributes": {
                "createdAt": "2023-09-07T08:39:39.442Z",
                "updatedAt": "2023-09-07T08:39:39.442Z",
                "content": "<p>An organization is moving its on-premises resources to the cloud. Source code will be moved to AWS CodeCommit and AWS CodeBuild will be used for compiling the source code using Apache Maven as a build tool. The organization wants the build environment should allow for scaling and running builds in parallel.</p>\n\n<p>Which of the following options should the organization choose for their requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>CodeBuild scales automatically, the organization does not have to do anything for scaling or for parallel builds</strong> - AWS CodeBuild is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more. You can also customize build environments in CodeBuild to use your own build tools. CodeBuild scales automatically to meet peak build requests.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Choose a high-performance instance type for your CodeBuild instances</strong> - For the current requirement, this is will not make any difference.</p>\n\n<p><strong>Run CodeBuild in an Auto Scaling Group</strong> - AWS CodeBuild is a managed service and scales automatically, does not need Auto Scaling Group to scale it up.</p>\n\n<p><strong>Enable CodeBuild Auto Scaling</strong> - This has been added as a distractor. CodeBuild scales automatically to meet peak build requests.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-compute-types.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-compute-types.html</a></p>\n",
                "options": [
                    {
                        "id": 1812,
                        "content": "<p>CodeBuild scales automatically, the organization does not have to do anything for scaling or for parallel builds</p>",
                        "isValid": true
                    },
                    {
                        "id": 1813,
                        "content": "<p>Choose a high-performance instance type for your CodeBuild instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 1814,
                        "content": "<p>Run CodeBuild in an Auto Scaling group</p>",
                        "isValid": false
                    },
                    {
                        "id": 1815,
                        "content": "<p>Enable CodeBuild Auto Scaling</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 444,
            "attributes": {
                "createdAt": "2023-09-07T08:39:39.509Z",
                "updatedAt": "2023-09-07T08:39:39.509Z",
                "content": "<p>Recently, you started an online learning platform using AWS Lambda and AWS Gateway API. Your first version was successful, and you began developing new features for the second version. You would like to gradually introduce the second version by routing only 10% of the incoming traffic to the new Lambda version.</p>\n\n<p>Which solution should you opt for?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Lambda aliases</strong> - A Lambda alias is like a pointer to a specific Lambda function version. You can create one or more aliases for your AWS Lambda function. Users can access the function version using the alias ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function.\nEvent sources such as Amazon S3 invoke your Lambda function. These event sources maintain a mapping that identifies the function to invoke when events occur. If you specify a Lambda function alias in the mapping configuration, you don't need to update the mapping when the function version changes. This is the right choice for the current requirement.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Tags to distinguish the different versions</strong> - You can tag Lambda functions to organize them by owner, project or department. Tags are freeform key-value pairs that are supported across AWS services for use in filtering resources and adding detail to billing reports. This does not address the given use-case.</p>\n\n<p><strong>Use environment variables</strong> - You can use environment variables to store secrets securely and adjust your function's behavior without updating code. An environment variable is a pair of strings that are stored in a function's version-specific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. For example, you can use environment variables to point to test, development or production databases by passing it as an environment variable during runtime. This option does not address the given use-case.</p>\n\n<p><strong>Deploy your Lambda in a VPC</strong> - Amazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you've defined. This adds another layer of security for your entire architecture. Not the right choice for the given scenario.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-tags.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-tags.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html</a></p>\n",
                "options": [
                    {
                        "id": 1816,
                        "content": "<p>Deploy your Lambda in a VPC</p>",
                        "isValid": false
                    },
                    {
                        "id": 1817,
                        "content": "<p>Use Tags to distinguish the different versions</p>",
                        "isValid": false
                    },
                    {
                        "id": 1818,
                        "content": "<p>Use environment variables</p>",
                        "isValid": false
                    },
                    {
                        "id": 1819,
                        "content": "<p>Use AWS Lambda aliases</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 445,
            "attributes": {
                "createdAt": "2023-09-07T08:39:39.580Z",
                "updatedAt": "2023-09-07T08:39:39.580Z",
                "content": "<p>A company wants to automate and orchestrate a multi-source high-volume flow of data in a scalable data management solution built using AWS services. The solution must ensure that the business rules and transformations run in sequence, handle reprocessing of data in case of errors, and require minimal maintenance.</p>\n\n<p>Which AWS service should the company use to manage and automate the orchestration of the data flows?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS Step Functions</strong></p>\n\n<p>AWS Step Functions is a visual workflow service that helps developers use AWS services to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines.</p>\n\n<p><img src=\"https://d1.awsstatic.com/video-thumbs/Step-Functions/AWS_Step_Functions_HIW.bc3d2930f00dd0401269367b8e8617a7dba5915c.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong> - Amazon Kinesis Data Streams is a serverless streaming data service that makes it easy to capture, process, and store data streams at any scale.</p>\n\n<p><strong>AWS Glue</strong> - AWS Glue is a serverless data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources for analytics, machine learning (ML), and application development.</p>\n\n<p><strong>AWS Batch</strong> - AWS Batch is a set of batch management capabilities that enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements of the batch jobs submitted.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p><a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p>\n\n<p><a href=\"https://aws.amazon.com/batch/faqs/\">https://aws.amazon.com/batch/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 1820,
                        "content": "<p>AWS Batch</p>",
                        "isValid": false
                    },
                    {
                        "id": 1821,
                        "content": "<p>AWS Step Functions</p>",
                        "isValid": true
                    },
                    {
                        "id": 1822,
                        "content": "<p>AWS Glue</p>",
                        "isValid": false
                    },
                    {
                        "id": 1823,
                        "content": "<p>Amazon Kinesis Data Streams</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 446,
            "attributes": {
                "createdAt": "2023-09-07T08:39:39.654Z",
                "updatedAt": "2023-09-07T08:39:39.654Z",
                "content": "<p>An IT company has migrated to a serverless application stack on the AWS Cloud with the compute layer being implemented via Lambda functions. The engineering managers would like to actively troubleshoot any failures in the Lambda functions.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>\"The developers should insert logging statements in the Lambda function code which are then available via CloudWatch logs\"</p>\n\n<p>When you invoke a Lambda function, two types of error can occur. Invocation errors occur when the invocation request is rejected before your function receives it. Function errors occur when your function's code or runtime returns an error. Depending on the type of error, the type of invocation, and the client or service that invokes the function, the retry behavior, and the strategy for managing errors varies.</p>\n\n<p>Lambda function failures are commonly caused by:</p>\n\n<p>Permissions issues\nCode issues\nNetwork issues\nThrottling\nInvoke API 500 and 502 errors</p>\n\n<p>You can insert logging statements into your code to help you validate that your code is working as expected. Lambda automatically integrates with CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function, which is named <code>/aws/lambda/&lt;function name&gt;</code>.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q9-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html\">https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Use CloudWatch Events to identify and notify any failures in the Lambda code\" - Typically Lambda functions are triggered as a response to a CloudWatch Event. CloudWatch Events cannot identify and notify failures in the Lambda code.</p>\n\n<p>\"Use CodeCommit to identify and notify any failures in the Lambda code\"</p>\n\n<p>\"Use CodeDeploy to identify and notify any failures in the Lambda code\"</p>\n\n<p>AWS CodeCommit is a fully-managed source control service that hosts secure Git-based repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem.</p>\n\n<p>AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.</p>\n\n<p>Neither CodeCommit nor CodeDeploy can identify and notify failures in the Lambda code.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html\">https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html</a></p>\n",
                "options": [
                    {
                        "id": 1824,
                        "content": "<p>Use CloudWatch Events to identify and notify any failures in the Lambda code</p>",
                        "isValid": false
                    },
                    {
                        "id": 1825,
                        "content": "<p>Use CodeDeploy to identify and notify any failures in the Lambda code</p>",
                        "isValid": false
                    },
                    {
                        "id": 1826,
                        "content": "<p>The developers should insert logging statements in the Lambda function code which are then available via CloudWatch logs</p>",
                        "isValid": true
                    },
                    {
                        "id": 1827,
                        "content": "<p>Use CodeCommit to identify and notify any failures in the Lambda code</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 447,
            "attributes": {
                "createdAt": "2023-09-07T08:39:39.730Z",
                "updatedAt": "2023-09-07T08:39:39.730Z",
                "content": "<p>A developer is designing an AWS CloudFormation template for deploying Amazon EC2 instances in numerous AWS accounts. The developer needs to select EC2 instances from a list of pre-approved instance types.</p>\n\n<p>What measures could the developer take to integrate the list of authorized instance types into the CloudFormation template?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure a parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template</strong></p>\n\n<p>You can use the Parameters section to customize your templates. Parameters enable you to input custom values to your template each time you create or update a stack.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q57-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a></p>\n\n<p>AllowedValues refers to an array containing the list of values allowed for the parameter. When applied to a parameter of type String, the parameter value must be one of the allowed values. When applied to a parameter of type CommaDelimitedList, each value in the list must be one of the specified allowed values.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure separate parameters for each EC2 instance type in the CloudFormation template</strong> - Creating separate parameters for each instance type is semantically incorrect as the underlying value will point to the same resource but have multiple inputs.</p>\n\n<p><strong>Configure a mapping having a list of EC2 instance types as parameters in the CloudFormation template</strong> - The Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function to retrieve values in a map. A mapping is not a list, rather, it consists of key value pairs. You can't include parameters, pseudo parameters, or intrinsic functions in the Mappings section. So, this option is incorrect.</p>\n\n<p><strong>Configure a pseudo parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template</strong> - Pseudo parameters are parameters that are predefined by AWS CloudFormation. You don't declare them in your template.  So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html</a></p>\n",
                "options": [
                    {
                        "id": 1828,
                        "content": "<p>Configure separate parameters for each EC2 instance type in the CloudFormation template</p>",
                        "isValid": false
                    },
                    {
                        "id": 1829,
                        "content": "<p>Configure a mapping having a list of EC2 instance types as parameters in the CloudFormation template</p>",
                        "isValid": false
                    },
                    {
                        "id": 1830,
                        "content": "<p>Configure a parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template</p>",
                        "isValid": true
                    },
                    {
                        "id": 1831,
                        "content": "<p>Configure a pseudo parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 448,
            "attributes": {
                "createdAt": "2023-09-07T08:39:39.806Z",
                "updatedAt": "2023-09-07T08:39:39.806Z",
                "content": "<p>Your web application architecture consists of multiple Amazon EC2 instances running behind an Elastic Load Balancer with an Auto Scaling group having the desired capacity of 5 EC2 instances. You would like to integrate AWS CodeDeploy for automating application deployment. The deployment should re-route traffic from your application's original environment to the new environment.</p>\n\n<p>Which of the following options will meet your deployment criteria?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Opt for Blue/Green deployment</strong> - A Blue/Green deployment is used to update your applications while minimizing interruptions caused by the changes of a new application version. CodeDeploy provisions your new application version alongside the old version before rerouting your production traffic. The behavior of your deployment depends on which compute platform you use:</p>\n\n<ol>\n<li>AWS Lambda: Traffic is shifted from one version of a Lambda function to a new version of the same Lambda function.</li>\n<li>Amazon ECS: Traffic is shifted from a task set in your Amazon ECS service to an updated, replacement task set in the same Amazon ECS service.</li>\n<li>EC2/On-Premises: Traffic is shifted from one set of instances in the original environment to a replacement set of instances.</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Opt for Rolling deployment</strong> - This deployment type is present for AWS Elastic Beanstalk and not for EC2 instances directly.</p>\n\n<p><strong>Opt for Immutable deployment</strong> - This deployment type is present for AWS Elastic Beanstalk and not for EC2 instances directly.</p>\n\n<p><strong>Opt for In-place deployment</strong> - Under this deployment type, the application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview-blue-green\">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview-blue-green</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n",
                "options": [
                    {
                        "id": 1832,
                        "content": "<p>Opt for Immutable deployment</p>",
                        "isValid": false
                    },
                    {
                        "id": 1833,
                        "content": "<p>Opt for Rolling deployment</p>",
                        "isValid": false
                    },
                    {
                        "id": 1834,
                        "content": "<p>Opt for In-place deployment</p>",
                        "isValid": false
                    },
                    {
                        "id": 1835,
                        "content": "<p>Opt for Blue/Green deployment</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 449,
            "attributes": {
                "createdAt": "2023-09-07T08:39:39.876Z",
                "updatedAt": "2023-09-07T08:39:39.876Z",
                "content": "<p>You have been asked by your Team Lead to enable detailed monitoring of the Amazon EC2 instances your team uses. As a Developer working on AWS CLI, which of the below command will you run?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong><code>aws ec2 monitor-instances --instance-ids i-1234567890abcdef0</code></strong> - This enables detailed monitoring for a running instance.</p>\n\n<p>EC2 detailed monitoring:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q19-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cli/latest/reference/ec2/monitor-instances.html\">https://docs.aws.amazon.com/cli/latest/reference/ec2/monitor-instances.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>aws ec2 run-instances --image-id ami-09092360 --monitoring Enabled=true</code></strong> - This syntax is used to enable detailed monitoring when launching an instance from AWS CLI.</p>\n\n<p><strong><code>aws ec2 run-instances --image-id ami-09092360 --monitoring State=enabled</code></strong> - This is an invalid syntax</p>\n\n<p><strong><code>aws ec2 monitor-instances --instance-id i-1234567890abcdef0</code></strong> - This is an invalid syntax</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/ec2/run-instances.html\">https://docs.aws.amazon.com/cli/latest/reference/ec2/run-instances.html</a></p>\n",
                "options": [
                    {
                        "id": 1836,
                        "content": "<p>aws ec2 monitor-instances --instance-ids i-1234567890abcdef0</p>",
                        "isValid": true
                    },
                    {
                        "id": 1837,
                        "content": "<p>aws ec2 run-instances --image-id ami-09092360 --monitoring State=enabled</p>",
                        "isValid": false
                    },
                    {
                        "id": 1838,
                        "content": "<p>aws ec2 monitor-instances --instance-id i-1234567890abcdef0</p>",
                        "isValid": false
                    },
                    {
                        "id": 1839,
                        "content": "<p>aws ec2 run-instances --image-id ami-09092360 --monitoring Enabled=true</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 450,
            "attributes": {
                "createdAt": "2023-09-07T08:39:39.951Z",
                "updatedAt": "2023-09-07T08:39:39.951Z",
                "content": "<p>A company has a workload that requires 14,000 consistent IOPS for data that must be durable and secure. The compliance standards of the company state that the data should be secure at every stage of its lifecycle on all of the EBS volumes they use.</p>\n\n<p>Which of the following statements are true regarding data security on EBS?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Amazon EBS works with AWS KMS to encrypt and decrypt your EBS volume. You can encrypt both the boot and data volumes of an EC2 instance. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:</p>\n\n<ol>\n<li><p>Data at rest inside the volume</p></li>\n<li><p>All data moving between the volume and the instance</p></li>\n<li><p>All snapshots created from the volume</p></li>\n<li><p>All volumes created from those snapshots</p></li>\n</ol>\n\n<p><strong>EBS volumes support both in-flight encryption and encryption at rest using KMS</strong> - This is a correct statement. Encryption operations occur on the servers that host EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached EBS storage.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EBS volumes support in-flight encryption but do not support encryption at rest</strong> - This is an incorrect statement. As discussed above, all data moving between the volume and the instance is encrypted.</p>\n\n<p><strong>EBS volumes do not support in-flight encryption but do support encryption at rest using KMS</strong> - This is an incorrect statement. As discussed above, data at rest is also encrypted.</p>\n\n<p><strong>EBS volumes don't support any encryption</strong> - This is an incorrect statement. Amazon EBS encryption offers a straight-forward encryption solution for your EBS resources associated with your EC2 instances. With Amazon EBS encryption, you aren't required to build, maintain, and secure your own key management infrastructure.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n",
                "options": [
                    {
                        "id": 1840,
                        "content": "<p>EBS volumes support both in-flight encryption and encryption at rest using KMS</p>",
                        "isValid": true
                    },
                    {
                        "id": 1841,
                        "content": "<p>EBS volumes do not support in-flight encryption but do support encryption at rest using KMS</p>",
                        "isValid": false
                    },
                    {
                        "id": 1842,
                        "content": "<p>EBS volumes don't support any encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 1843,
                        "content": "<p>EBS volumes support in-flight encryption but does not support encryption at rest</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 451,
            "attributes": {
                "createdAt": "2023-09-07T08:39:40.019Z",
                "updatedAt": "2023-09-07T08:39:40.019Z",
                "content": "<p>You have a workflow process that pulls code from AWS CodeCommit and deploys to EC2 instances associated with tag group ProdBuilders. You would like to configure the instances to archive no more than two application revisions to conserve disk space.</p>\n\n<p>Which of the following will allow you to implement this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>\"CodeDeploy Agent\"</p>\n\n<p>The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent archives revisions and log files on instances. The CodeDeploy agent cleans up these artifacts to conserve disk space. You can use the :max_revisions: option in the agent configuration file to specify the number of application revisions to the archive by entering any positive integer. CodeDeploy also archives the log files for those revisions. All others are deleted, except for the log file of the last successful deployment.</p>\n\n<p>More info here:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q48-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS CloudWatch Log Agent</strong> - The CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances. This is an incorrect choice for the current use case.</p>\n\n<p><strong>Integrate with AWS CodePipeline</strong> - AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodeCommit and CodePipeline are already integrated services. CodePipeline cannot help in version control and management of archives on an EC2 instance.</p>\n\n<p><strong>Have a load balancer in front of your instances</strong> - Load Balancer helps balance incoming traffic across different EC2 instances. It is an incorrect choice for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html</a></p>\n",
                "options": [
                    {
                        "id": 1844,
                        "content": "<p>Have a load balancer in front of your instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 1845,
                        "content": "<p>AWS CloudWatch Log Agent</p>",
                        "isValid": false
                    },
                    {
                        "id": 1846,
                        "content": "<p>Integrate with AWS CodePipeline</p>",
                        "isValid": false
                    },
                    {
                        "id": 1847,
                        "content": "<p>CodeDeploy Agent</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 452,
            "attributes": {
                "createdAt": "2023-09-07T08:39:40.095Z",
                "updatedAt": "2023-09-07T08:39:40.095Z",
                "content": "<p>As a developer, you are looking at creating a custom configuration for Amazon EC2 instances running in an Auto Scaling group. The solution should allow the group to auto-scale based on the metric of 'average RAM usage' for your Amazon EC2 instances.</p>\n\n<p>Which option provides the best solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a custom metric in CloudWatch and make your instances send data to it using PutMetricData. Then, create an alarm based on this metric</strong> - You can create a custom CloudWatch metric for your EC2 Linux instance statistics by creating a script through the AWS Command Line Interface (AWS CLI). Then, you can monitor that metric by pushing it to CloudWatch.</p>\n\n<p>You can publish your own metrics to CloudWatch using the AWS CLI or an API. Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p>\n\n<p>High-resolution metrics can give you more immediate insight into your application's sub-minute activity. But, every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a high-resolution metric can lead to higher charges.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a custom alarm for your ASG and make your instances trigger the alarm using PutAlarmData API</strong> - This solution will not work, your instances must be aware of each other's RAM utilization status, to know when the average RAM would be too high to trigger the alarm.</p>\n\n<p><strong>Enable detailed monitoring for EC2 and ASG to get the RAM usage data and create a CloudWatch Alarm on top of it</strong> - By enabling detailed monitoring you define the frequency at which the metric data has to be sent to CloudWatch, from 5 minutes to 1-minute frequency window. But, you still need to create and collect the custom metric you wish to track.</p>\n\n<p><strong>Migrate your application to AWS Lambda</strong> - This option has been added as a distractor. You cannot use Lambda for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html#CloudWatchAlarm\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html#CloudWatchAlarm</a></p>\n",
                "options": [
                    {
                        "id": 1848,
                        "content": "<p>Migrate your application to AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 1849,
                        "content": "<p>Create a custom alarm for your ASG and make your instances trigger the alarm using PutAlarmData API</p>",
                        "isValid": false
                    },
                    {
                        "id": 1850,
                        "content": "<p>Create a custom metric in CloudWatch and make your instances send data to it using PutMetricData. Then, create an alarm based on this metric</p>",
                        "isValid": true
                    },
                    {
                        "id": 1851,
                        "content": "<p>Enable detailed monitoring for EC2 and ASG to get the RAM usage data and create a CloudWatch Alarm on top of it</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 453,
            "attributes": {
                "createdAt": "2023-09-07T08:39:40.175Z",
                "updatedAt": "2023-09-07T08:39:40.175Z",
                "content": "<p>As a Senior Developer, you manage 10 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for PostgreSQL. You need to make this architecture resilient for disaster recovery.</p>\n\n<p>Which of the following features will help you prepare for database disaster recovery? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use cross-Region Read Replicas</strong></p>\n\n<p>In addition to using Read Replicas to reduce the load on your source DB instance, you can also use Read Replicas to implement a DR solution for your production DB environment. If the source DB instance fails, you can promote your Read Replica to a standalone source server. Read Replicas can also be created in a different Region than the source database. Using a cross-Region Read Replica can help ensure that you get back up and running if you experience a regional availability issue.</p>\n\n<p><strong>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region</strong></p>\n\n<p>Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology.</p>\n\n<p>The automated backup feature of Amazon RDS enables point-in-time recovery for your database instance. Amazon RDS will backup your database and transaction logs and store both for a user-specified retention period. If itâ€™s a Multi-AZ configuration, backups occur on the standby to reduce I/O impact on the primary. Automated backups are limited to a single AWS Region while manual snapshots and Read Replicas are supported across multiple Regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable the automated backup feature of Amazon RDS  in a multi-AZ deployment that creates backups across multiple Regions</strong> - This is an incorrect statement. Automated backups are limited to a single AWS Region while manual snapshots and Read Replicas are supported across multiple Regions.</p>\n\n<p><strong>Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</strong> - Amazon RDS Provisioned IOPS Storage is an SSD-backed storage option designed to deliver fast, predictable, and consistent I/O performance. This storage type enhances the performance of the RDS database, but this isn't a disaster recovery option.</p>\n\n<p><strong>Use database cloning feature of the RDS DB cluster</strong> - This option has been added as a distractor. Database cloning is only available for Aurora and not for RDS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/\">https://aws.amazon.com/rds/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p>\n",
                "options": [
                    {
                        "id": 1852,
                        "content": "<p>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region</p>",
                        "isValid": true
                    },
                    {
                        "id": 1853,
                        "content": "<p>Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 1854,
                        "content": "<p>Enable the automated backup feature of Amazon RDS  in a multi-AZ deployment that creates backups across multiple Regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 1855,
                        "content": "<p>Use cross-Region Read Replicas</p>",
                        "isValid": true
                    },
                    {
                        "id": 1856,
                        "content": "<p>Use database cloning feature of the RDS DB cluster</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 454,
            "attributes": {
                "createdAt": "2023-09-07T08:39:40.245Z",
                "updatedAt": "2023-09-07T08:39:40.245Z",
                "content": "<p>Your web application reads and writes data to your DynamoDB table. The table is provisioned with 400 Write Capacity Units (WCUâ€™s) shared across 4 partitions. One of the partitions receives 250 WCU/second while others receive much less. You receive the error 'ProvisionedThroughputExceededException'.</p>\n\n<p>What is the likely cause of this error?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>You have a hot partition</strong></p>\n\n<p>It's not always possible to distribute read and write activity evenly. When data access is imbalanced, a \"hot\" partition can receive a higher volume of read and write traffic compared to other partitions.\nTo better accommodate uneven access patterns, DynamoDB adaptive capacity enables your application to continue reading and writing to hot partitions without being throttled, provided that traffic does not exceed your tableâ€™s total provisioned capacity or the partition maximum capacity.</p>\n\n<p>ProvisionedThroughputExceededException explained:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p>Hot partition explained:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q41-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudWatch monitoring is lagging</strong> - The error is specific to DynamoDB itself and not to any connected service. CloudWatch is a fully managed service from AWS and does not result in throttling.</p>\n\n<p><strong>Configured IAM policy is wrong</strong> - The error is not associated with authorization but to exceeding something pre-configured value. So, it's clearly not a permissions issue.</p>\n\n<p><strong>Write-capacity units (WCUâ€™s) are applied across to all your DynamoDB tables and this needs reconfiguration</strong> - This statement is incorrect. Read Capacity Units and Write Capacity Units are specific to one table.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n",
                "options": [
                    {
                        "id": 1857,
                        "content": "<p>CloudWatch monitoring is lagging</p>",
                        "isValid": false
                    },
                    {
                        "id": 1858,
                        "content": "<p>Configured IAM policy is wrong</p>",
                        "isValid": false
                    },
                    {
                        "id": 1859,
                        "content": "<p>Write Capacity Units (WCUâ€™s) are applied across to all your DynamoDB tables and this needs reconfiguration</p>",
                        "isValid": false
                    },
                    {
                        "id": 1860,
                        "content": "<p>You have a hot partition</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 455,
            "attributes": {
                "createdAt": "2023-09-07T08:39:40.317Z",
                "updatedAt": "2023-09-07T08:39:40.317Z",
                "content": "<p>A developer wants to enable X-Ray tracing on an on-premises Linux server running a custom application that is accessed through Amazon API Gateway.</p>\n\n<p>What is the most efficient solution that requires minimal configuration?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Install and run the X-Ray daemon on the on-premises servers to capture and relay the data to the X-Ray service</strong></p>\n\n<p>The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service.</p>\n\n<p>To run the X-Ray daemon locally, on-premises, or on other AWS services, download it, run it, and then give it permission to upload segment documents to X-Ray.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Install and run the X-Ray SDK on the on-premises servers to capture and relay the data to the X-Ray service</strong> - As mentioned above, you need to run the X-Ray daemon on the on-premises servers and give it the required permission to upload X-Ray data to the X-Ray service. So this option is incorrect.</p>\n\n<p><strong>Install and run the CloudWatch Unified Agent on the on-premises servers to capture and relay the X-Ray data to the X-Ray service using the PutTraceSegments API call</strong> - This option has been added as a distractor. CloudWatch Agent cannot relay X-Ray data to the X-Ray service using the PutTraceSegments API call.</p>\n\n<p><strong>Configure a Lambda function to analyze the incoming traffic data on the on-premises servers and then relay the X-Ray data to the X-Ray service using the PutTelemetryRecords API call</strong> - This option is incorrect as the Lambda function cannot process the X-Ray data for an on-premises instance and then relay it to the X-Ray service.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html</a></p>\n",
                "options": [
                    {
                        "id": 1861,
                        "content": "<p>Configure a Lambda function to analyze the incoming traffic data on the on-premises servers and then relay the X-Ray data to the X-Ray service using the PutTelemetryRecords API call</p>",
                        "isValid": false
                    },
                    {
                        "id": 1862,
                        "content": "<p>Install and run the CloudWatch Unified Agent on the on-premises servers to capture and relay the X-Ray data to the X-Ray service using the PutTraceSegments API call</p>",
                        "isValid": false
                    },
                    {
                        "id": 1863,
                        "content": "<p>Install and run the X-Ray daemon on the on-premises servers to capture and relay the data to the X-Ray service</p>",
                        "isValid": true
                    },
                    {
                        "id": 1864,
                        "content": "<p>Install and run the X-Ray SDK on the on-premises servers to capture and relay the data to the X-Ray service</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 456,
            "attributes": {
                "createdAt": "2023-09-07T08:39:40.399Z",
                "updatedAt": "2023-09-07T08:39:40.399Z",
                "content": "<p>A developer from your team has configured the load balancer to route traffic equally between instances or across Availability Zones. However, Elastic Load Balancing (ELB) routes more traffic to one instance or Availability Zone than the others.</p>\n\n<p>Why is this happening and how can it be fixed? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Sticky sessions are enabled for the load balancer</strong> - This can be the reason for potential unequal traffic routing by the load balancer. Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies.</p>\n\n<p>When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, the load balancer detects the cookie and routes the request to the same target.</p>\n\n<p>If you use duration-based session stickiness, configure an appropriate cookie expiration time for your specific use case. If you set session stickiness from individual applications, use session cookies instead of persistent cookies where possible.</p>\n\n<p><strong>Instances of a specific capacity type arenâ€™t equally distributed across Availability Zones</strong> - A Classic Load Balancer with HTTP or HTTPS listeners might route more traffic to higher-capacity instance types. This distribution aims to prevent lower-capacity instance types from having too many outstanding requests. Itâ€™s a best practice to use similar instance types and configurations to reduce the likelihood of capacity gaps and traffic imbalances.</p>\n\n<p>A traffic imbalance might also occur if you have instances of similar capacities running on different Amazon Machine Images (AMIs). In this scenario, the imbalance of the traffic in favor of higher-capacity instance types is desirable.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>There could be short-lived TCP connections between clients and instances</strong> - This is an incorrect statement. Long-lived TCP connections between clients and instances can potentially lead to unequal distribution of traffic by the load balancer. Long-lived TCP connections between clients and instances cause uneven traffic load distribution by design. As a result, new instances take longer to reach connection equilibrium. Be sure to check your metrics for long-lived TCP connections that might be causing routing issues in the load balancer.</p>\n\n<p><strong>For Application Load Balancers, cross-zone load balancing is disabled by default</strong> - This is an incorrect statement. With Application Load Balancers, cross-zone load balancing is always enabled.</p>\n\n<p><strong>After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer, thereby receiving random bursts of traffic</strong> - This is an incorrect statement. After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer. However, even though they remain registered, the load balancer does not route traffic to them.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elb-fix-unequal-traffic-routing/\">https://aws.amazon.com/premiumsupport/knowledge-center/elb-fix-unequal-traffic-routing/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#availability-zones\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#availability-zones</a></p>\n",
                "options": [
                    {
                        "id": 1865,
                        "content": "<p>Instances of a specific capacity type arenâ€™t equally distributed across Availability Zones</p>",
                        "isValid": true
                    },
                    {
                        "id": 1866,
                        "content": "<p>For Application Load Balancers, cross-zone load balancing is disabled by default</p>",
                        "isValid": false
                    },
                    {
                        "id": 1867,
                        "content": "<p>There could be short-lived TCP connections between clients and instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 1868,
                        "content": "<p>Sticky sessions are enabled for the load balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 1869,
                        "content": "<p>After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer, thereby receiving random bursts of traffic</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 457,
            "attributes": {
                "createdAt": "2023-09-07T08:39:40.474Z",
                "updatedAt": "2023-09-07T08:39:40.474Z",
                "content": "<p>A junior developer working on ECS instances terminated a container instance in Amazon Elastic Container Service (Amazon ECS) as per instructions from the team lead. But the container instance continues to appear as a resource in the ECS cluster.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to fix this behavior?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>You terminated the container instance while it was in STOPPED state, that lead to this synchronization issues</strong> - If you terminate a container instance while it is in the STOPPED state, that container instance isn't automatically removed from the cluster. You will need to deregister your container instance in the STOPPED state by using the Amazon ECS console or AWS Command Line Interface. Once deregistered, the container instance will no longer appear as a resource in your Amazon ECS cluster.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You terminated the container instance while it was in RUNNING state, that lead to this synchronization issues</strong> -  This is an incorrect statement. If you terminate a container instance in the RUNNING state, that container instance is automatically removed, or deregistered, from the cluster.</p>\n\n<p><strong>The container instance has been terminated with AWS CLI, whereas, for ECS instances, Amazon ECS CLI should be used to avoid any synchronization issues</strong> - This is incorrect and has been added as a distractor.</p>\n\n<p><strong>A custom software on the container instance could have failed and resulted in the container hanging in an unhealthy state till restarted again</strong> - This is an incorrect statement. It is already mentioned in the question that the developer has terminated the instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/deregister-ecs-instance/\">https://aws.amazon.com/premiumsupport/knowledge-center/deregister-ecs-instance/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html</a></p>\n",
                "options": [
                    {
                        "id": 1870,
                        "content": "<p>You terminated the container instance while it was in STOPPED state, that lead to this synchronization issues</p>",
                        "isValid": true
                    },
                    {
                        "id": 1871,
                        "content": "<p>A custom software on the container instance could have failed and resulted in the container hanging in an unhealthy state till restarted again</p>",
                        "isValid": false
                    },
                    {
                        "id": 1872,
                        "content": "<p>The container instance has been terminated with AWS CLI, whereas, for ECS instances, Amazon ECS CLI should be used to avoid any synchronization issues</p>",
                        "isValid": false
                    },
                    {
                        "id": 1873,
                        "content": "<p>You terminated the container instance while it was in RUNNING state, that lead to this synchronization issues</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 458,
            "attributes": {
                "createdAt": "2023-09-07T08:39:40.566Z",
                "updatedAt": "2023-09-07T08:39:40.566Z",
                "content": "<p>A company has several Linux-based EC2 instances that generate various log files which need to be analyzed for security and compliance purposes. The company wants to use Kinesis Data Streams (KDS) to analyze this log data.</p>\n\n<p>Which of the following is the most optimal way of sending log data from the EC2 instances to KDS?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Install and configure Kinesis Agent on each of the instances</strong></p>\n\n<p>Kinesis Agent is a stand-alone Java software application that offers an easy way to collect and send data to Kinesis Data Streams. The agent continuously monitors a set of files and sends new data to your stream. The agent handles file rotation, checkpointing, and retry upon failures. It delivers all of your data in a reliable, timely, and simple manner. It also emits Amazon CloudWatch metrics to help you better monitor and troubleshoot the streaming process.</p>\n\n<p>You can install the agent on Linux-based server environments such as web servers, log servers, and database servers. After installing the agent, configure it by specifying the files to monitor and the stream for the data. After the agent is configured, it durably collects data from the files and reliably sends it to the stream.</p>\n\n<p>The agent can also pre-process the records parsed from monitored files before sending them to your stream. You can enable this feature by adding the dataProcessingOptions configuration setting to your file flow. One or more processing options can be added and they will be performed in the specified order.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Run cron job on each of the instances to collect log data and send it to Kinesis Data Streams</strong> - This solution is possible, though not an optimal one. This solution requires writing custom code and tracking file/log changes, retry failures and so on. Kinesis Agent is built to handle all these requirements and integrates with Data Streams.</p>\n\n<p><strong>Install AWS SDK on each of the instances and configure it to send the necessary files to Kinesis Data Streams</strong> - Kinesis Data Streams APIs that are available in the AWS SDKs, helps you manage many aspects of Kinesis Data Streams, including creating streams, resharding, and putting and getting records. You will need to write custom code to handle new data in the log files and send it over to your stream. Kinesis Agent does it easily, as it is designed to continuously monitor a set of files and send new data to your stream.</p>\n\n<p><strong>Use Kinesis Producer Library (KPL) to collect and ingest data from each EC2 instance</strong> - The KPL is an easy-to-use, highly configurable library that helps you write to a Kinesis data stream. It acts as an intermediary between your producer application code and the Kinesis Data Streams API actions. This is not optimal compared to Kinesis Agent which is designed to continuously monitor a set of files and send new data to your stream.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html\">https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html</a></p>\n",
                "options": [
                    {
                        "id": 1874,
                        "content": "<p>Install AWS SDK on each of the instances and configure it to send the necessary files to Kinesis Data Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 1875,
                        "content": "<p>Run cron job on each of the instances to collect log data and send it to Kinesis Data Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 1876,
                        "content": "<p>Install and configure Kinesis Agent on each of the instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 1877,
                        "content": "<p>Use Kinesis Producer Library (KPL) to collect and ingest data from each EC2 instance</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 459,
            "attributes": {
                "createdAt": "2023-09-07T08:39:40.635Z",
                "updatedAt": "2023-09-07T08:39:40.635Z",
                "content": "<p>You are getting ready for an event to show off your Alexa skill written in JavaScript. As you are testing your voice activation commands you find that some intents are not invoking as they should and you are struggling to figure out what is happening. You included the following code <code>console.log(JSON.stringify(this.event))</code> in hopes of getting more details about the request to your Alexa skill.</p>\n\n<p>You would like the logs stored in an Amazon Simple Storage Service (S3) bucket named <code>MyAlexaLog</code>. How do you achieve this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use CloudWatch integration feature with S3</strong></p>\n\n<p>You can export log data from your CloudWatch log groups to an Amazon S3 bucket and use this data in custom processing and analysis, or to load onto other systems.</p>\n\n<p>Exporting CloudWatch Log Data to Amazon S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudWatch integration feature with Kinesis</strong> - You can use both to do custom processing or analysis but with S3 you don't have to process anything. Instead, you configure the CloudWatch settings to send logs to S3.</p>\n\n<p><strong>Use CloudWatch integration feature with Lambda</strong> - You can use both to do custom processing or analysis but with S3 you don't have to process anything. Instead, you configure the CloudWatch settings to send logs to S3.</p>\n\n<p><strong>Use CloudWatch integration feature with Glue</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. Glue is not the right fit for the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html</a></p>\n",
                "options": [
                    {
                        "id": 1878,
                        "content": "<p>Use CloudWatch integration feature with Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 1879,
                        "content": "<p>Use CloudWatch integration feature with Kinesis</p>",
                        "isValid": false
                    },
                    {
                        "id": 1880,
                        "content": "<p>Use CloudWatch integration feature with Glue</p>",
                        "isValid": false
                    },
                    {
                        "id": 1881,
                        "content": "<p>Use CloudWatch integration feature with S3</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 460,
            "attributes": {
                "createdAt": "2023-09-07T08:39:40.710Z",
                "updatedAt": "2023-09-07T08:39:40.710Z",
                "content": "<p>An e-commerce company has implemented AWS CodeDeploy as part of its AWS cloud CI/CD strategy. The company has configured automatic rollbacks while deploying a new version of its flagship application to Amazon EC2.</p>\n\n<p>What occurs if the deployment of the new version fails?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>A new deployment of the last known working version of the application is deployed with a new deployment ID</strong></p>\n\n<p>AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.</p>\n\n<p>CodeDeploy rolls back deployments by redeploying a previously deployed revision of an application as a new deployment. These rolled-back deployments are technically new deployments, with new deployment IDs, rather than restored versions of a previous deployment.</p>\n\n<p>To roll back an application to a previous revision, you just need to deploy that revision. AWS CodeDeploy keeps track of the files that were copied for the current revision and removes them before starting a new deployment, so there is no difference between redeploy and rollback. However, you need to make sure that the previous revisions are available for rollback.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The last known working deployment is automatically restored using the snapshot stored in Amazon S3</strong> - CodeDeploy deployment does not have a snapshot stored on S3, so this option is incorrect.</p>\n\n<p><strong>AWS CodePipeline promotes the most recent working deployment with a SUCCEEDED status to production</strong> - The use-case does not talk about using CodePipeline, so this option just acts as a distractor.</p>\n\n<p><strong>CodeDeploy switches the Route 53 alias records back to the known good green deployment and terminates the failed blue deployment</strong> - The use-case does not talk about the blue/green deployment, so this option has just been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html</a></p>\n",
                "options": [
                    {
                        "id": 1882,
                        "content": "<p>AWS CodePipeline promotes the most recent working deployment with a SUCCEEDED status to production</p>",
                        "isValid": false
                    },
                    {
                        "id": 1883,
                        "content": "<p>A new deployment of the last known working version of the application is deployed with a new deployment ID</p>",
                        "isValid": true
                    },
                    {
                        "id": 1884,
                        "content": "<p>CodeDeploy switches the Route 53 alias records back to the known good green deployment and terminates the failed blue deployment</p>",
                        "isValid": false
                    },
                    {
                        "id": 1885,
                        "content": "<p>The last known working deployment is automatically restored using the snapshot stored in Amazon S3</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 461,
            "attributes": {
                "createdAt": "2023-09-07T08:39:40.780Z",
                "updatedAt": "2023-09-07T08:39:40.780Z",
                "content": "<p>Your company leverages Amazon CloudFront to provide content via the internet to customers with low latency. Aside from latency, security is another concern and you are looking for help in enforcing end-to-end connections using HTTPS so that content is protected.</p>\n\n<p>Which of the following options is available for HTTPS in AWS CloudFront?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Between clients and CloudFront as well as between CloudFront and backend</strong></p>\n\n<p>For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so connections are encrypted when CloudFront communicates with viewers.</p>\n\n<p>Requiring HTTPS for Communication Between Viewers and CloudFront:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q51-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p>\n\n<p>You also can configure CloudFront to use HTTPS to get objects from your origin, so connections are encrypted when CloudFront communicates with your origin.</p>\n\n<p>Requiring HTTPS for Communication Between CloudFront and Your Custom Origin:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q51-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Between clients and CloudFront only</strong> - This is incorrect as you can choose to require HTTPS between CloudFront and your origin.</p>\n\n<p><strong>Between CloudFront and backend only</strong> - This is incorrect as you can choose to require HTTPS between viewers and CloudFront.</p>\n\n<p><strong>Neither between clients and CloudFront nor between CloudFront and backend</strong> - This is incorrect as you can choose HTTPS settings both for communication between viewers and CloudFront as well as between CloudFront and your origin.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/secure-connections-supported-viewer-protocols-ciphers.html#secure-connections-supported-ciphers-cloudfront-to-origin\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/secure-connections-supported-viewer-protocols-ciphers.html#secure-connections-supported-ciphers-cloudfront-to-origin</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html</a></p>\n",
                "options": [
                    {
                        "id": 1886,
                        "content": "<p>Neither between clients and CloudFront nor between CloudFront and backend</p>",
                        "isValid": false
                    },
                    {
                        "id": 1887,
                        "content": "<p>Between clients and CloudFront only</p>",
                        "isValid": false
                    },
                    {
                        "id": 1888,
                        "content": "<p>Between clients and CloudFront as well as between CloudFront and backend</p>",
                        "isValid": true
                    },
                    {
                        "id": 1889,
                        "content": "<p>Between CloudFront and backend only</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 462,
            "attributes": {
                "createdAt": "2023-09-07T08:39:40.856Z",
                "updatedAt": "2023-09-07T08:39:40.856Z",
                "content": "<p>You work as a developer doing contract work for the government on AWS gov cloud. Your applications use Amazon Simple Queue Service (SQS) for its message queue service. Due to recent hacking attempts, security measures have become stricter and require you to store data in encrypted queues.</p>\n\n<p>Which of the following steps can you take to meet your requirements without making changes to the existing code?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable SQS KMS encryption</strong></p>\n\n<p>Server-side encryption (SSE) lets you transmit sensitive data in encrypted queues. SSE protects the contents of messages in queues using keys managed in AWS Key Management Service (AWS KMS).</p>\n\n<p>AWS KMS combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use Amazon SQS with AWS KMS, the data keys that encrypt your message data are also encrypted and stored with the data they protect.</p>\n\n<p>You can choose to have SQS encrypt messages stored in both Standard and FIFO queues using an encryption key provided by AWS Key Management Service (KMS).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the SSL endpoint</strong> - The given use-case needs encryption at rest. When using SSL, the data is encrypted during transit, but the data needs to be encrypted at rest as well, so this option is incorrect.</p>\n\n<p><strong>Use Client-side encryption</strong> - For additional security, you can build your application to encrypt messages before they are placed in a message queue but will require a code change, so this option is incorrect.</p>\n\n<p><em>*Use Secrets Manager *</em> - AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB. Secrets Manager cannot be used for encrypting data at rest.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-server-side-encryption.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-server-side-encryption.html</a></p>\n",
                "options": [
                    {
                        "id": 1890,
                        "content": "<p>Use Client side encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 1891,
                        "content": "<p>Use Secrets Manager</p>",
                        "isValid": false
                    },
                    {
                        "id": 1892,
                        "content": "<p>Enable SQS KMS encryption</p>",
                        "isValid": true
                    },
                    {
                        "id": 1893,
                        "content": "<p>Use the SSL endpoint</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 463,
            "attributes": {
                "createdAt": "2023-09-07T08:39:40.933Z",
                "updatedAt": "2023-09-07T08:39:40.933Z",
                "content": "<p>A firm uses AWS DynamoDB to store information about peopleâ€™s favorite sports teams and allow the information to be searchable from their home page. There is a daily requirement that all 10 million records in the table should be deleted then re-loaded at 2:00 AM each night.</p>\n\n<p>Which option is an efficient way to delete with minimal costs?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Delete then re-create the table</strong></p>\n\n<p>The DeleteTable operation deletes a table and all of its items. After a <code>DeleteTable</code> request, the specified table is in the <code>DELETING</code> state until DynamoDB completes the deletion.</p>\n\n<p><strong>Scan and call DeleteItem</strong> - Scan is a very slow operation for 10 million items and this is not the best-fit option for the given use-case.</p>\n\n<p><strong>Scan and call BatchDeleteItem</strong> - Scan is a very slow operation for 10 million items and this is not the best-fit option for the given use-case.</p>\n\n<p><strong>Call PurgeTable</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/dynamodb/delete-table.html\">https://docs.aws.amazon.com/cli/latest/reference/dynamodb/delete-table.html</a></p>\n",
                "options": [
                    {
                        "id": 1894,
                        "content": "<p>Delete then re-create the table</p>",
                        "isValid": true
                    },
                    {
                        "id": 1895,
                        "content": "<p>Call PurgeTable</p>",
                        "isValid": false
                    },
                    {
                        "id": 1896,
                        "content": "<p>Scan and call DeleteItem</p>",
                        "isValid": false
                    },
                    {
                        "id": 1897,
                        "content": "<p>Scan and call BatchDeleteItem</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 464,
            "attributes": {
                "createdAt": "2023-09-07T08:39:41.008Z",
                "updatedAt": "2023-09-07T08:39:41.008Z",
                "content": "<p>You have a Java-based application running on EC2 instances loaded with AWS CodeDeploy agents. You are considering different options for deployment, one is the flexibility that allows for incremental deployment of your new application versions and replaces existing versions in the EC2 instances. The other option is a strategy in which an Auto Scaling group is used to perform a deployment.</p>\n\n<p>Which of the following options will allow you to deploy in this manner? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>In-place Deployment</strong></p>\n\n<p>The application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete.</p>\n\n<p><strong>Blue/green Deployment</strong></p>\n\n<p>With a blue/green deployment, you provision a new set of instances on which CodeDeploy installs the latest version of your application. CodeDeploy then re-routes load balancer traffic from an existing set of instances running the previous version of your application to the new set of instances running the latest version. After traffic is re-routed to the new instances, the existing instances can be terminated.</p>\n\n<p>CodeDeploy Deployment Types:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q45-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Cattle Deployment</strong> - This is a good option if you have cattle in a farm</p>\n\n<p><strong>Warm Standby Deployment</strong> - This is not a valid CodeDeploy deployment option. The term \"Warm Standby\" is used to describe a Disaster Recovery scenario in which a scaled-down version of a fully functional environment is always running in the cloud.</p>\n\n<p><strong>Pilot Light Deployment</strong> - This is not a valid CodeDeploy deployment option. \"Pilot Light\" is a Disaster Recovery approach where you simply replicate part of your IT structure for a limited set of core services so that the AWS cloud environment seamlessly takes over in the event of a disaster.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/\">https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/</a></p>\n",
                "options": [
                    {
                        "id": 1898,
                        "content": "<p>In-place Deployment</p>",
                        "isValid": true
                    },
                    {
                        "id": 1899,
                        "content": "<p>Pilot Light Deployment</p>",
                        "isValid": false
                    },
                    {
                        "id": 1900,
                        "content": "<p>Warm Standby Deployment</p>",
                        "isValid": false
                    },
                    {
                        "id": 1901,
                        "content": "<p>Cattle Deployment</p>",
                        "isValid": false
                    },
                    {
                        "id": 1902,
                        "content": "<p>Blue/green Deployment</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 465,
            "attributes": {
                "createdAt": "2023-09-07T08:39:41.079Z",
                "updatedAt": "2023-09-07T08:39:41.079Z",
                "content": "<p>Your company manages hundreds of EC2 instances running on Linux OS. The instances are configured in several Availability Zones in the eu-west-3 region. Your manager has requested to collect system memory metrics on all EC2 instances using a script.</p>\n\n<p>Which of the following solutions will help you collect this data?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>\"Use a cron job on the instances that pushes the EC2 RAM statistics as a Custom metric into CloudWatch\"</p>\n\n<p>The Amazon CloudWatch Monitoring Scripts for Amazon Elastic Compute Cloud (Amazon EC2) Linux-based instances demonstrate how to produce and consume Amazon CloudWatch custom metrics. These Perl scripts comprise a fully functional example that reports memory, swap, and disk space utilization metrics for a Linux instance. You can set a cron schedule for metrics reported to CloudWatch and report memory utilization to CloudWatch every x minutes.</p>\n\n<p>Incorrect options:</p>\n\n<p>\"Extract RAM statistics using the instance metadata\" - Instance metadata is data about your instance that you can use to configure or manage the running instance. Instance metadata is divided into categories, for example, hostname, events, and security groups. The instance metadata can only provide the ID of the RAM disk specified at launch time. So this option is incorrect.</p>\n\n<p>\"Extract RAM statistics from the standard CloudWatch metrics for EC2 instances\" - Amazon EC2 sends metrics to Amazon CloudWatch. By default, each data point covers the 5 minutes that follow the start time of activity for the instance. If you've enabled detailed monitoring, each data point covers the next minute of activity from the start time. The standard CloudWatch metrics don't have any metrics for memory utilization details.</p>\n\n<p>\"Extract RAM statistics using X-Ray\" - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your applicationâ€™s underlying components.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p>X-Ray cannot be used to extract RAM statistics for EC2 instances.</p>\n\n<p>For more information visit https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html</p>\n",
                "options": [
                    {
                        "id": 1903,
                        "content": "<p>Extract RAM statistics using X-Ray</p>",
                        "isValid": false
                    },
                    {
                        "id": 1904,
                        "content": "<p>Use a cron job on the instances that pushes the EC2 RAM statistics as a Custom metric into CloudWatch</p>",
                        "isValid": true
                    },
                    {
                        "id": 1905,
                        "content": "<p>Extract RAM statistics from the standard CloudWatch metrics for EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 1906,
                        "content": "<p>Extract RAM statistics using the instance metadata</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 466,
            "attributes": {
                "createdAt": "2023-09-07T08:39:41.148Z",
                "updatedAt": "2023-09-07T08:39:41.148Z",
                "content": "<p>A development team uses the AWS SDK for Java to maintain an application that stores data in AWS DynamoDB. The application makes use of <code>Scan</code> operations to return several items from a 25 GB table. There is no possibility of creating indexes to retrieve these items predictably. Developers are trying to get these specific rows from DynamoDB as fast as possible.</p>\n\n<p>Which of the following options can be used to improve the performance of the Scan operation?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use parallel scans</strong></p>\n\n<p>By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. The larger the table or index being scanned, the more time the Scan takes to complete. To address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel.</p>\n\n<p>To make use of a parallel Scan feature, you will need to run multiple worker threads or processes in parallel. Each worker will be able to scan a separate partition of a table concurrently with the other workers.</p>\n\n<p>How DynamoDB parallel Scan works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q11-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a ProjectionExpression</strong> - A projection expression is a string that identifies the attributes you want. To retrieve a single attribute, specify its name. For multiple attributes, the names must be comma-separated</p>\n\n<p><strong>Use a FilterExpression</strong> - If you need to further refine the Scan results, you can optionally provide a filter expression. A filter expression determines which items within the Scan results should be returned to you. All of the other results are discarded.</p>\n\n<p>A filter expression is applied after a Scan finishes, but before the results are returned. Therefore, a Scan consumes the same amount of read capacity, regardless of whether a filter expression is present.</p>\n\n<p><strong>Use a Query</strong> - This could work if we were able to create an index, but the question says: \"There is no possibility of creating indexes to retrieve these items predictably\". As such, we cannot use a Query.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan</a></p>\n",
                "options": [
                    {
                        "id": 1907,
                        "content": "<p>Use a Query</p>",
                        "isValid": false
                    },
                    {
                        "id": 1908,
                        "content": "<p>Use parallel scans</p>",
                        "isValid": true
                    },
                    {
                        "id": 1909,
                        "content": "<p>Use a ProjectionExpression</p>",
                        "isValid": false
                    },
                    {
                        "id": 1910,
                        "content": "<p>Use a FilterExpression</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 467,
            "attributes": {
                "createdAt": "2023-09-07T08:39:41.236Z",
                "updatedAt": "2023-09-07T08:39:41.236Z",
                "content": "<p>A data analytics company with its IT infrastructure on the AWS Cloud wants to build and deploy its flagship application as soon as there are any changes to the source code.</p>\n\n<p>As a Developer Associate, which of the following options would you suggest to trigger the deployment? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Keep the source code in an AWS CodeCommit repository and start AWS CodePipeline whenever a change is pushed to the CodeCommit repository</strong></p>\n\n<p><strong>Keep the source code in an Amazon S3 bucket and start AWS CodePipeline whenever a file in the S3 bucket is updated</strong></p>\n\n<p>AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define.</p>\n\n<p>How CodePipeline Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CodePipeLine.7b8dd19eb6478b7f6f747d936c2f0b0b66757bbf.png\">\nvia - <a href=\"https://aws.amazon.com/codepipeline/\">https://aws.amazon.com/codepipeline/</a></p>\n\n<p>Using change detection methods that you specify, you can make your pipeline start when a change is made to a repository. You can also make your pipeline start on a schedule.</p>\n\n<p>When you use the console to create a pipeline that has a CodeCommit source repository or S3 source bucket, CodePipeline creates an Amazon CloudWatch Events rule that starts your pipeline when the source changes. This is the recommended change detection method.</p>\n\n<p>If you use the AWS CLI to create the pipeline, the change detection method defaults to starting the pipeline by periodically checking the source (CodeCommit, Amazon S3, and GitHub source providers only). AWS recommends that you disable periodic checks and create the rule manually.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Keep the source code in Amazon EFS and start AWS CodePipeline whenever a file is updated</strong></p>\n\n<p><strong>Keep the source code in an Amazon EBS volume and start AWS CodePipeline whenever there are updates to the source code</strong></p>\n\n<p>Both EFS and EBS are not supported as valid source providers for CodePipeline to check for any changes to the source code, hence these two options are incorrect.</p>\n\n<p><strong>Keep the source code in an Amazon S3 bucket and set up AWS CodePipeline to recur at an interval of every 15 minutes</strong> - As mentioned in the explanation above, although you could have the change detection method start the pipeline by periodically checking the S3 bucket, but this method is inefficient.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html</a></p>\n",
                "options": [
                    {
                        "id": 1911,
                        "content": "<p>Keep the source code in an Amazon S3 bucket and set up AWS CodePipeline to recur at an interval of every 15 minutes</p>",
                        "isValid": false
                    },
                    {
                        "id": 1912,
                        "content": "<p>Keep the source code in an Amazon S3 bucket and start AWS CodePipeline whenever a file in the S3 bucket is updated</p>",
                        "isValid": true
                    },
                    {
                        "id": 1913,
                        "content": "<p>Keep the source code in Amazon EFS and start AWS CodePipeline whenever a file is updated</p>",
                        "isValid": false
                    },
                    {
                        "id": 1914,
                        "content": "<p>Keep the source code in an AWS CodeCommit repository and start AWS CodePipeline whenever a change is pushed to the CodeCommit repository</p>",
                        "isValid": true
                    },
                    {
                        "id": 1915,
                        "content": "<p>Keep the source code in an Amazon EBS volume and start AWS CodePipeline whenever there are updates to the source code</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 468,
            "attributes": {
                "createdAt": "2023-09-07T08:39:41.321Z",
                "updatedAt": "2023-09-07T08:39:41.321Z",
                "content": "<p>A firm maintains a highly available application that receives HTTPS traffic from mobile devices and web browsers. The main Developer would like to set up the Load Balancer routing to route traffic from web servers to smart.com/api and from mobile devices to smart.com/mobile. A developer advises that the previous recommendation is not needed and that requests should be sent to api.smart.com and mobile.smart.com instead.</p>\n\n<p>Which of the following routing options were discussed in the given use-case? (select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Path based</strong></p>\n\n<p>You can create a listener with rules to forward requests based on the URL path. This is known as path-based routing. If you are running microservices, you can route traffic to multiple back-end services using path-based routing. For example, you can route general requests to one target group and request to render images to another target group.</p>\n\n<p>This path-based routing allows you to route requests to, for example, /api to one set of servers (also known as target groups) and /mobile to another set. Segmenting your traffic in this way gives you the ability to control the processing environment for each category of requests. Perhaps /api requests are best processed on Compute Optimized instances, while /mobile requests are best handled by Memory Optimized instances.</p>\n\n<p><strong>Host based</strong></p>\n\n<p>You can create Application Load Balancer rules that route incoming traffic based on the domain name specified in the Host header. Requests to api.example.com can be sent to one target group, requests to mobile.example.com to another, and all others (by way of a default rule) can be sent to a third. You can also create rules that combine host-based routing and path-based routing. This would allow you to route requests to api.example.com/production and api.example.com/sandbox to distinct target groups.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q54-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#rule-condition-types\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#rule-condition-types</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Client IP</strong> - This option has been added as a distractor. Routing is not based on the client's IP address.</p>\n\n<p><strong>Web browser version</strong> - Routing has nothing to do with the client's web browser, if it was then there is something sneaky going on.</p>\n\n<p><strong>Cookie value</strong> - Application Load Balancers support load balancer-generated cookies only and you cannot modify them. When routing sticky sessions to route requests to the same target then cookies are needed to be supported by the client's browser.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/\">https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/</a></p>\n",
                "options": [
                    {
                        "id": 1916,
                        "content": "<p>Client IP</p>",
                        "isValid": false
                    },
                    {
                        "id": 1917,
                        "content": "<p>Host based</p>",
                        "isValid": true
                    },
                    {
                        "id": 1918,
                        "content": "<p>Cookie value</p>",
                        "isValid": false
                    },
                    {
                        "id": 1919,
                        "content": "<p>Web browser version</p>",
                        "isValid": false
                    },
                    {
                        "id": 1920,
                        "content": "<p>Path based</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 469,
            "attributes": {
                "createdAt": "2023-09-07T08:39:41.393Z",
                "updatedAt": "2023-09-07T08:39:41.393Z",
                "content": "<p>As a Full-stack Web Developer, you are involved with every aspect of a companyâ€™s platform from development with PHP and JavaScript to the configuration of NoSQL databases with Amazon DynamoDB. You are not concerned about your response receiving stale data from your database and need to perform 16 eventually consistent reads per second of 12 KB in size each.</p>\n\n<p>How many read capacity units (RCUs) do you need?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Before proceeding with the calculations, please review the following:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q64-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q64-i2.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p><strong>24</strong></p>\n\n<p>One read capacity unit represents <strong>two</strong> eventually consistent reads per second, for an item up to <strong>4 KB</strong> in size.\nSo that means that for an item of 12KB in size, we need 3 RCU (12 KB / 4 KB) for <strong>two</strong> eventually consistent reads per second. As we need 16 eventually consistent reads per second, we need 3 * (16 / 2) = 24 RCU.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>12</strong></p>\n\n<p><strong>192</strong></p>\n\n<p><strong>48</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html</a></p>\n",
                "options": [
                    {
                        "id": 1921,
                        "content": "<p>48</p>",
                        "isValid": false
                    },
                    {
                        "id": 1922,
                        "content": "<p>192</p>",
                        "isValid": false
                    },
                    {
                        "id": 1923,
                        "content": "<p>24</p>",
                        "isValid": true
                    },
                    {
                        "id": 1924,
                        "content": "<p>12</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 470,
            "attributes": {
                "createdAt": "2023-09-07T08:39:41.467Z",
                "updatedAt": "2023-09-07T08:39:41.467Z",
                "content": "<p>An organization recently began using AWS CodeCommit for its source control service. A compliance security team visiting the organization was auditing the software development process and noticed developers making many git push commands within their development machines. The compliance team requires that encryption be used for this activity.</p>\n\n<p>How can the organization ensure source code is encrypted in transit and at rest?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Repositories are automatically encrypted at rest</strong></p>\n\n<p>Data in AWS CodeCommit repositories is encrypted in transit and at rest. When data is pushed into an AWS CodeCommit repository (for example, by calling git push), AWS CodeCommit encrypts the received data as it is stored in the repository.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q28-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/encryption.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable KMS encryption</strong> - You don't have to. The first time you create an AWS CodeCommit repository in a new region in your AWS account, CodeCommit creates an AWS-managed key in that same region in AWS Key Management Service (AWS KMS) that is used only by CodeCommit.</p>\n\n<p><strong>Use AWS Lambda as a hook to encrypt the pushed code</strong> - This is not needed as CodeCommit handles it for you.</p>\n\n<p><strong>Use a git command line hook to encrypt the code client-side</strong> - This is not needed as CodeCommit handles it for you.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/encryption.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/encryption.html</a></p>\n\n<p>For more information visit https://docs.aws.amazon.com/codecommit/latest/userguide/encryption.html</p>\n",
                "options": [
                    {
                        "id": 1925,
                        "content": "<p>Use a git command line hook to encrypt the code client side</p>",
                        "isValid": false
                    },
                    {
                        "id": 1926,
                        "content": "<p>Use AWS Lambda as a hook to encrypt the pushed code</p>",
                        "isValid": false
                    },
                    {
                        "id": 1927,
                        "content": "<p>Enable KMS encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 1928,
                        "content": "<p>Repositories are automatically encrypted at rest</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 471,
            "attributes": {
                "createdAt": "2023-09-07T08:39:41.535Z",
                "updatedAt": "2023-09-07T08:39:41.535Z",
                "content": "<p>A developer has pushed a Lambda function that pushes data into an RDS MySQL database with the following Python code:</p>\n\n<pre><code>def handler(event, context):\n    mysql = mysqlclient.connect()\n    data = event['data']\n    mysql.execute(f\"INSERT INTO foo (bar) VALUES (${data});\")\n    mysql.close()\n    return\n</code></pre>\n\n<p>On the first execution, the Lambda function takes 2 seconds to execute. On the second execution and all the subsequent ones, the Lambda function takes 1.9 seconds to execute.</p>\n\n<p>What can be done to improve the execution time of the Lambda function?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Move the database connection out of the handler</strong></p>\n\n<p>Here at every Lambda function execution, the database connection handler will be created, and then closed. These connections steps are expensive in terms of time, and thus should be moved out of the <code>handler</code> function so that they are kept in the function execution context, and re-used across function calls. This is what the function should look like in the end:</p>\n\n<pre><code>mysql = mysqlclient.connect()\n\ndef handler(event, context):\n    data = event['data']\n    mysql.execute(f\"INSERT INTO foo (bar) VALUES (${data});\")\n    return\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upgrade the MySQL instance type</strong> - The bottleneck here is the MySQL connection object, not the MySQL instance itself.</p>\n\n<p><strong>Change the runtime to Node.js</strong> - Re-writing the function in another runtime won't improve the performance.</p>\n\n<p><strong>Increase the Lambda function RAM</strong> - While this may help speed-up the Lambda function, as increasing the RAM also increases the CPU allocated to your function, it only makes sense if RAM or CPU is a critical factor in the Lambda function performance. Here, the connection object is at fault.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html\">https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html</a></p>\n",
                "options": [
                    {
                        "id": 1929,
                        "content": "<p>Increase the Lambda function RAM</p>",
                        "isValid": false
                    },
                    {
                        "id": 1930,
                        "content": "<p>Change the runtime to Node.js</p>",
                        "isValid": false
                    },
                    {
                        "id": 1931,
                        "content": "<p>Upgrade the MySQL instance type</p>",
                        "isValid": false
                    },
                    {
                        "id": 1932,
                        "content": "<p>Move the database connection out of the handler</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 472,
            "attributes": {
                "createdAt": "2023-09-07T08:39:41.605Z",
                "updatedAt": "2023-09-07T08:39:41.605Z",
                "content": "<p>You are storing your video files in a separate S3 bucket than your main static website in an S3 bucket. When accessing the video URLs directly the users can view the videos on the browser, but they can't play the videos while visiting the main website.</p>\n\n<p>What is the root cause of this problem?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable CORS</strong></p>\n\n<p>Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.</p>\n\n<p>To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operation-specific information.</p>\n\n<p>For the given use-case, you would create a <code>&lt;CORSRule&gt;</code> in <code>&lt;CORSConfiguration&gt;</code> for bucket B to allow access from the S3 website origin hosted on bucket A.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the bucket policy</strong> - A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy that grants permissions. With this policy, you can do things such as allow one IP address to access the video file in the S3 bucket. In this scenario, we know that's not the case because it works using the direct URL but it doesn't work when you click on a link to access the video.</p>\n\n<p><strong>Amend the IAM policy</strong> - You attach IAM policies to IAM users, groups, or roles, which are then subject to the permissions you've defined. This scenario refers to public users of a website and they need not have an IAM user account.</p>\n\n<p><strong>Disable Server-Side Encryption</strong> - Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it, if the video file is encrypted at rest then there is nothing you need to do because AWS handles encrypt and decrypt. Disabling encryption is not an issue because you can access the video directly using an URL but not from the main website.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>\n",
                "options": [
                    {
                        "id": 1933,
                        "content": "<p>Change the bucket policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 1934,
                        "content": "<p>Enable CORS</p>",
                        "isValid": true
                    },
                    {
                        "id": 1935,
                        "content": "<p>Disable Server-Side Encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 1936,
                        "content": "<p>Amend the IAM policy</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 473,
            "attributes": {
                "createdAt": "2023-09-07T08:39:41.677Z",
                "updatedAt": "2023-09-07T08:39:41.677Z",
                "content": "<p>You are assigned as the new project lead for a web application that processes orders for customers. You want to integrate event-driven processing anytime data is modified or deleted and use a serverless approach using AWS Lambda for processing stream events.</p>\n\n<p>Which of the following databases should you choose from?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>DynamoDB</strong></p>\n\n<p>A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table, and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real-time.</p>\n\n<p>Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified.</p>\n\n<p>DynamoDB Streams Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>RDS</strong> - By itself, RDS cannot be used to stream events like DynamoDB, so this option is ruled out. However, you can use Amazon Kinesis for streaming data from RDS.</p>\n\n<p>Please refer to this excellent blog for more details on using Kinesis for streaming data from RDS:\n<a href=\"https://aws.amazon.com/blogs/database/streaming-changes-in-a-database-with-amazon-kinesis/\">https://aws.amazon.com/blogs/database/streaming-changes-in-a-database-with-amazon-kinesis/</a></p>\n\n<p><strong>ElastiCache</strong> - ElastiCache works as an in-memory data store and cache, it cannot be used to stream data like DynamoDB.</p>\n\n<p><strong>Kinesis</strong> - Kinesis is not a database, so this option is ruled out.</p>\n\n<p>Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources.</p>\n\n<p>How Kinesis Data Streams Work\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n",
                "options": [
                    {
                        "id": 1937,
                        "content": "<p>RDS</p>",
                        "isValid": false
                    },
                    {
                        "id": 1938,
                        "content": "<p>ElastiCache</p>",
                        "isValid": false
                    },
                    {
                        "id": 1939,
                        "content": "<p>Kinesis</p>",
                        "isValid": false
                    },
                    {
                        "id": 1940,
                        "content": "<p>DynamoDB</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 474,
            "attributes": {
                "createdAt": "2023-09-07T08:39:41.748Z",
                "updatedAt": "2023-09-07T08:39:41.748Z",
                "content": "<p>A company developed an app-based service for citizens to book transportation rides in the local community. The platform is running on AWS EC2 instances and uses Amazon Relational Database Service (RDS) for storing transportation data. A new feature has been requested where receipts would be emailed to customers with PDF attachments retrieved from Amazon Simple Storage Service (S3).</p>\n\n<p>Which of the following options will provide EC2 instances with the right permissions to upload files to Amazon S3 and generate S3 Signed URL?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM Role for EC2</strong></p>\n\n<p>IAM roles have been incorporated so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.</p>\n\n<p>Amazon EC2 uses an instance profile as a container for an IAM role. When you create an IAM role using the IAM console, the console creates an instance profile automatically and gives it the same name as the role to which it corresponds.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EC2 User Data</strong> - You can specify user data when you launch an instance and you would not want to hard code the AWS credentials in the user data.</p>\n\n<p><strong>Run <code>aws configure</code> on the EC2 instance</strong> - When you first configure the CLI you have to run this command, afterward you should not need to if you want to obtain credentials to authenticate to other AWS services. An IAM role will receive temporary credentials for you so you can focus on using the CLI to get access to other AWS services if you have the permissions.</p>\n\n<p><strong>CloudFormation</strong> - AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p>\n",
                "options": [
                    {
                        "id": 1941,
                        "content": "<p>EC2 User Data</p>",
                        "isValid": false
                    },
                    {
                        "id": 1942,
                        "content": "<p>CloudFormation</p>",
                        "isValid": false
                    },
                    {
                        "id": 1943,
                        "content": "<p>Create an IAM Role for EC2</p>",
                        "isValid": true
                    },
                    {
                        "id": 1944,
                        "content": "<p>Run <code>aws configure</code> on the EC2 instance</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 475,
            "attributes": {
                "createdAt": "2023-09-07T08:39:41.825Z",
                "updatedAt": "2023-09-07T08:39:41.825Z",
                "content": "<p>A .NET developer team works with many ASP.NET web applications that use EC2 instances to host them on IIS. The deployment process needs to be configured so that multiple versions of the application can run in AWS Elastic Beanstalk. One version would be used for development, testing, and another version for load testing.</p>\n\n<p>Which of the following methods do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Define a dev environment with a single instance and a 'load test' environment that has settings close to production environment</strong></p>\n\n<p>AWS Elastic Beanstalk makes it easy to create new environments for your application. You can create and manage separate environments for development, testing, and production use, and you can deploy any version of your application to any environment. Environments can be long-running or temporary. When you terminate an environment, you can save its configuration to recreate it later.</p>\n\n<p>It is common practice to have many environments for the same application. You can deploy multiple environments when you need to run multiple versions of an application. So for the given use-case, you can set up 'dev' and 'load test' environment.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q48-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.html</a></p>\n\n<p><strong>You cannot have multiple development environments in Elastic Beanstalk, just one development, and one production environment</strong> - Incorrect, use the Create New Environment wizard in the AWS Management Console for BeanStalk to guide you on this.</p>\n\n<p><strong>Use only one Beanstalk environment and perform configuration changes using an Ansible script</strong> - Ansible is an open-source deployment tool that integrates with AWS. It allows us to deploy the infrastructure. Elastic Beanstalk provisions the servers that you need for hosting the application and it also handles multiple environments, so Beanstalk is a better option.</p>\n\n<p><strong>Create an Application Load Balancer to route based on hostname so you can pass on parameters to the development Elastic Beanstalk environment. Create a file in .ebextensions/ to know how to handle the traffic coming from the ALB</strong> - This is not a good design if you need to load test because you will have two versions on the same instances and may not be able to access resources in the system due to the load testing.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.environments.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.environments.html</a></p>\n",
                "options": [
                    {
                        "id": 1945,
                        "content": "<p>Create an Application Load Balancer to route based on hostname so you can pass on parameters to the development Elastic Beanstalk environment. Create a file in .ebextensions/ to know how to handle the traffic coming from the ALB</p>",
                        "isValid": false
                    },
                    {
                        "id": 1946,
                        "content": "<p>You cannot have multiple development environments in Elastic Beanstalk, just one development and one production environment</p>",
                        "isValid": false
                    },
                    {
                        "id": 1947,
                        "content": "<p>Use only one Beanstalk environment and perform configuration changes using an Ansible script</p>",
                        "isValid": false
                    },
                    {
                        "id": 1948,
                        "content": "<p>Define a dev environment with a single instance and a 'load test' environment that has settings close to production environment</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 476,
            "attributes": {
                "createdAt": "2023-09-07T08:39:41.895Z",
                "updatedAt": "2023-09-07T08:39:41.895Z",
                "content": "<p>You have a web application hosted on EC2 that makes GET and PUT requests for objects stored in Amazon Simple Storage Service (S3) using the SDK for PHP. As the security team completed the final review of your application for vulnerabilities, they noticed that your application uses hardcoded IAM access key and secret access key to gain access to AWS services. They recommend you leverage a more secure setup, which should use temporary credentials if possible.</p>\n\n<p>Which of the following options can be used to address the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use an IAM Instance Role</strong></p>\n\n<p>An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. The AWS SDK will use the EC2 metadata service to obtain temporary credentials thanks to the IAM instance role. This is the most secure and common setup when deploying any kind of applications onto an EC2 instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use environment variables</strong> - This is another option if you configure AWS CLI on the EC2 instance. When configuring the AWS CLI you will set the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables. This practice may not be bad for one instance but once you start running more EC2 instances this is not a good practice because you may have to change credentials on each instance whereas an IAM Role gets temporary permissions.</p>\n\n<p><strong>Hardcode the credentials in the application code</strong> - It will work for sure, but it's not a good practice from a security point of view.</p>\n\n<p><strong>Use the SSM parameter store</strong> - With parameter store you can store data such as passwords. The problem is that you need the SDK to access parameter store and without credentials, you cannot use the SDK. Use parameter store for other uses such as database connection strings or other secret codes when you have already authenticated to AWS.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html</a></p>\n",
                "options": [
                    {
                        "id": 1949,
                        "content": "<p>Use the SSM parameter store</p>",
                        "isValid": false
                    },
                    {
                        "id": 1950,
                        "content": "<p>Hardcode the credentials in the application code</p>",
                        "isValid": false
                    },
                    {
                        "id": 1951,
                        "content": "<p>Use an IAM Instance Role</p>",
                        "isValid": true
                    },
                    {
                        "id": 1952,
                        "content": "<p>Use environment variables</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 477,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.002Z",
                "updatedAt": "2023-09-07T08:39:42.002Z",
                "content": "<p>An IT company is using AWS CloudFormation to manage its IT infrastructure. It has created a template to provision a stack with a VPC and a subnet. The output value of this subnet has to be used in another stack.</p>\n\n<p>As a Developer Associate, which of the following options would you suggest to provide this information to another stack?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use 'Export' field in the Output section of the stack's template</strong></p>\n\n<p>To share information between stacks, export a stack's output values. Other stacks that are in the same AWS account and region can import the exported values.</p>\n\n<p>To export a stack's output value, use the Export field in the Output section of the stack's template. To import those values, use the Fn::ImportValue function in the template for the other stacks.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use 'Expose' field in the Output section of the stack's template</strong> - 'Expose' is a made-up option, and only given as a distractor.</p>\n\n<p><strong>Use Fn::ImportValue</strong> - To import the values exported by another stack, we use the Fn::ImportValue function in the template for the other stacks. This function is not useful for the current scenario.</p>\n\n<p><strong>Use Fn::Transform</strong> - The intrinsic function Fn::Transform specifies a macro to perform custom processing on part of a stack template. Macros enable you to perform custom processing on templates, from simple actions like find-and-replace operations to extensive transformations of entire templates. This function is not useful for the current scenario.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html</a></p>\n",
                "options": [
                    {
                        "id": 1953,
                        "content": "<p>Use 'Expose' field in the Output section of the stack's template</p>",
                        "isValid": false
                    },
                    {
                        "id": 1954,
                        "content": "<p>Use Fn::ImportValue</p>",
                        "isValid": false
                    },
                    {
                        "id": 1955,
                        "content": "<p>Use 'Export' field in the Output section of the stack's template</p>",
                        "isValid": true
                    },
                    {
                        "id": 1956,
                        "content": "<p>Use Fn::Transform</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 478,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.083Z",
                "updatedAt": "2023-09-07T08:39:42.083Z",
                "content": "<p>A developer wants a seamless ability to return to older versions of a Lambda function that is being deployed.</p>\n\n<p>Which of the following solutions offers the LEAST operational overhead?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use a Lambda function alias that can point to the different versions</strong></p>\n\n<p>You can use versions to manage the deployment of your functions. For example, you can publish a new version of a function for beta testing without affecting users of the stable production version. Lambda creates a new version of your function each time that you publish the function. The new version is a copy of the unpublished version of the function.</p>\n\n<p>By publishing a version of your function, you can store your code and configuration as a separate resource that cannot be changed.</p>\n\n<p>A Lambda alias is like a pointer to a specific function version. Users can access the function version using the alias Amazon Resource Name (ARN). Each alias has a unique ARN. An alias can point only to a function version, not to another alias. You can update an alias to point to the different versions of the Lambda function.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a Route 53 weighted policy that can point to the different Lambda function versions</strong> - This option is a distractor, as Route 53 cannot be used for the given use case. Route 53 weighted policy lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software.</p>\n\n<p><strong>Use CodeDeploy to configure blue/green deployments for the different Lambda function versions</strong> - A deployment to the AWS Lambda compute platform is always a blue/green deployment. You do not specify a deployment type option. When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application. You can shift traffic using a canary, linear, or all-at-once deployment configuration. Once deployed, you cannot go back to the previous versions of your Lambda function. So this option is incorrect.</p>\n\n<p><strong>Use Lambda function layers that can point to the different versions</strong> - Lambda layers provide a convenient way to package libraries and other dependencies that you can use with your Lambda functions. Using layers reduces the size of uploaded deployment archives and makes it faster to deploy your code. You cannot use the Lambda function layers to point to the different versions of the Lambda function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html#deployment-configuration-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html#deployment-configuration-lambda</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html</a></p>\n",
                "options": [
                    {
                        "id": 1957,
                        "content": "<p>Use Lambda function layers that can point to the different versions</p>",
                        "isValid": false
                    },
                    {
                        "id": 1958,
                        "content": "<p>Use a Lambda function alias that can point to the different versions</p>",
                        "isValid": true
                    },
                    {
                        "id": 1959,
                        "content": "<p>Use a Route 53 weighted policy that can point to the different Lambda function versions</p>",
                        "isValid": false
                    },
                    {
                        "id": 1960,
                        "content": "<p>Use CodeDeploy to configure blue/green deployments for the different Lambda function versions</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 479,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.157Z",
                "updatedAt": "2023-09-07T08:39:42.157Z",
                "content": "<p>A developer is migrating an on-premises application to AWS Cloud. The application currently processes user uploads and uploads them to a local directory on the server. All such file uploads must be saved and then made available to all instances in an Auto Scaling group.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 and make code changes in the application so all uploads are put on S3</strong></p>\n\n<p>Amazon S3 is an object storage built to store and retrieve any amount of data from anywhere on the Internet. Itâ€™s a simple storage service that offers an extremely durable, highly available, and infinitely scalable data storage infrastructure at very low costs.</p>\n\n<p>Amazon S3 provides a simple web service interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. Using this web service, you can easily build applications that make use of Internet storage.</p>\n\n<p>You can use S3 PutObject API from the application to upload the objects in a single bucket, which is then accessible from all instances.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon EBS and configure the application AMI to use a snapshot of the same EBS instance while launching new instances</strong> - Using EBS to share data between instances is not possible because EBS volume is tied to an instance by definition. Creating a snapshot would only manage to move the stale data into the new instances.</p>\n\n<p><strong>Use Instance Store type of EC2 instances and share the files via file synchronization software</strong></p>\n\n<p><strong>Use Amazon EBS as the storage volume and share the files via file synchronization software</strong></p>\n\n<p>Technically you could use file synchronization software on EC2 instances with EBS or Instance Store type, but that involves a lot of development effort and still would not be as production-ready as just using S3. So both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 1961,
                        "content": "<p>Use Amazon EBS as the storage volume and share the files via file synchronization software</p>",
                        "isValid": false
                    },
                    {
                        "id": 1962,
                        "content": "<p>Use Instance Store type of EC2 instances and share the files via file synchronization software</p>",
                        "isValid": false
                    },
                    {
                        "id": 1963,
                        "content": "<p>Use Amazon S3 and make code changes in the application so all uploads are put on S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 1964,
                        "content": "<p>Use Amazon EBS and configure the application AMI to use a snapshot of the same EBS instance while launching new instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 480,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.237Z",
                "updatedAt": "2023-09-07T08:39:42.237Z",
                "content": "<p>A cybersecurity company is publishing critical log data to a log group in Amazon CloudWatch Logs, which was created 3 months ago. The company must encrypt the log data using an AWS KMS customer master key (CMK), so any future data can be encrypted to meet the companyâ€™s security guidelines.</p>\n\n<p>How can the company address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use the AWS CLI <code>associate-kms-key</code> command and specify the KMS key ARN</strong></p>\n\n<p>Log group data is always encrypted in CloudWatch Logs. You can optionally use AWS AWS Key Management Service for this encryption. If you do, the encryption is done using an AWS KMS (AWS KMS) customer master key (CMK). Encryption using AWS KMS is enabled at the log group level, by associating a CMK with a log group, either when you create the log group or after it exists.</p>\n\n<p>After you associate a CMK with a log group, all newly ingested data for the log group is encrypted using the CMK. This data is stored in an encrypted format throughout its retention period. CloudWatch Logs decrypts this data whenever it is requested. CloudWatch Logs must have permissions for the CMK whenever encrypted data is requested.</p>\n\n<p>To associate the CMK with an existing log group, you can use the <code>associate-kms-key</code> command.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable the encrypt feature on the log group via the CloudWatch Logs console</strong> - CloudWatch Logs console does not have an option to enable encryption for a log group.</p>\n\n<p><strong>Use the AWS CLI <code>describe-log-groups</code> command and specify the KMS key ARN</strong> - You can use the <code>describe-log-groups</code> command to find whether a log group already has a CMK associated with it.</p>\n\n<p><strong>Use the AWS CLI <code>create-log-group</code> command and specify the KMS key ARN</strong> - You can use the <code>create-log-group</code> command to associate the CMK with a log group when you create it.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html</a></p>\n",
                "options": [
                    {
                        "id": 1965,
                        "content": "<p>Enable the encrypt feature on the log group via the CloudWatch Logs console</p>",
                        "isValid": false
                    },
                    {
                        "id": 1966,
                        "content": "<p>Use the AWS CLI <code>associate-kms-key</code> command and specify the KMS key ARN</p>",
                        "isValid": true
                    },
                    {
                        "id": 1967,
                        "content": "<p>Use the AWS CLI <code>create-log-group</code> command and specify the KMS key ARN</p>",
                        "isValid": false
                    },
                    {
                        "id": 1968,
                        "content": "<p>Use the AWS CLI <code>describe-log-groups</code> command and specify the KMS key ARN</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 481,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.313Z",
                "updatedAt": "2023-09-07T08:39:42.313Z",
                "content": "<p>You are a system administrator whose company recently moved its production application to AWS and migrated data from MySQL to AWS DynamoDB. You are adding new tables to AWS DynamoDB and need to allow your application to query your data by the primary key and an alternate key. This option must be added when first creating tables otherwise changes cannot be made afterward.</p>\n\n<p>Which of the following actions should you take?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an LSI</strong></p>\n\n<p>LSI stands for Local Secondary Index. Some applications only need to query data using the base table's primary key; however, there may be situations where an alternate sort key would be helpful. To give your application a choice of sort keys, you can create one or more local secondary indexes on a table and issue Query or Scan requests against these indexes.</p>\n\n<p>Differences between GSI and LSI:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Call Scan</strong> - Scan is an operation on the data. Once you create your local secondary indexes on a table you can then issue Scan requests again.</p>\n\n<p><strong>Create a GSI</strong> - GSI (Global Secondary Index) is an index with a partition key and a sort key that can be different from those on the base table.</p>\n\n<p><strong>Migrate away from DynamoDB</strong> - Migrating to another database that is not NoSQL may cause you to make changes that require substantial code changes.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n",
                "options": [
                    {
                        "id": 1969,
                        "content": "<p>Create a GSI</p>",
                        "isValid": false
                    },
                    {
                        "id": 1970,
                        "content": "<p>Call Scan</p>",
                        "isValid": false
                    },
                    {
                        "id": 1971,
                        "content": "<p>Migrate away from DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 1972,
                        "content": "<p>Create a LSI</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 482,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.381Z",
                "updatedAt": "2023-09-07T08:39:42.381Z",
                "content": "<p>An order management system uses a cron job to poll for any new orders. Every time a new order is created, the cron job sends this order data as a message to the message queues to facilitate downstream order processing in a reliable way. To reduce costs and improve performance, the company wants to move this functionality to AWS cloud.</p>\n\n<p>Which of the following is the most optimal solution to meet this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Simple Notification Service (SNS) to push notifications when an order is created. Configure different Amazon Simple Queue Service (SQS) queues to receive these messages for downstream processing</strong></p>\n\n<p>Amazon SNS works closely with Amazon Simple Queue Service (Amazon SQS). These services provide different benefits for developers. Amazon SNS allows applications to send time-critical messages to multiple subscribers through a â€œpushâ€ mechanism, eliminating the need to periodically check or â€œpollâ€ for updates. Amazon SQS is a message queue service used by distributed applications to exchange messages through a polling model, and can be used to decouple sending and receiving componentsâ€”without requiring each component to be concurrently available.</p>\n\n<p>Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also stored in an Amazon SQS queue for other applications to process at a later time.</p>\n\n<p>When you subscribe an Amazon SQS queue to an Amazon SNS topic, you can publish a message to the topic and Amazon SNS sends an Amazon SQS message to the subscribed queue. The Amazon SQS message contains the subject and message that were published to the topic along with metadata about the message in a JSON document.</p>\n\n<p>SNS-SQS fanout is the right solution for this use case.</p>\n\n<p>Sample SNS-SQS Fanout message:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q35-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure different Amazon Simple Queue Service (SQS) queues to poll for new orders</strong> - Amazon SQS cannot be used as a polling service, as messages need to be pushed to the queue, which are then handled by the queue consumers.</p>\n\n<p><strong>Use Amazon Simple Notification Service (SNS) to push notifications and use AWS Lambda functions to process the information received from SNS</strong> - Amazon SNS and AWS Lambda are integrated so you can invoke Lambda functions with Amazon SNS notifications. When a message is published to an SNS topic that has a Lambda function subscribed to it, the Lambda function is invoked with the payload of the published message. For the given scenario, we need a service that can store the message data pushed by SNS, for further processing. AWS Lambda does not have capacity to store the message data. In case a Lambda function is unable to process a specific message, it will be left unprocessed. Hence this option is not correct.</p>\n\n<p><strong>Use Amazon Simple Notification Service (SNS) to push notifications to Kinesis Data Firehose delivery streams for processing the data for downstream applications</strong> - You can subscribe Amazon Kinesis Data Firehose delivery streams to SNS topics, which allows you to send notifications to additional storage and analytics endpoints. However, Kinesis is built for real-time processing of big data. Whereas, SQS is meant for decoupling dependent systems with easy methods to transmit data/messages. SQS also is a cheaper option when compared to Firehose. Therefore this option is not the right fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-lambda-as-subscriber.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-lambda-as-subscriber.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-firehose-as-subscriber.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-firehose-as-subscriber.html</a></p>\n",
                "options": [
                    {
                        "id": 1973,
                        "content": "<p>Configure different Amazon Simple Queue Service (SQS) queues to poll for new orders</p>",
                        "isValid": false
                    },
                    {
                        "id": 1974,
                        "content": "<p>Use Amazon Simple Notification Service (SNS) to push notifications to Kinesis Data Firehose delivery streams for processing the data for downstream applications</p>",
                        "isValid": false
                    },
                    {
                        "id": 1975,
                        "content": "<p>Use Amazon Simple Notification Service (SNS) to push notifications and use AWS Lambda functions to process the information received from SNS</p>",
                        "isValid": false
                    },
                    {
                        "id": 1976,
                        "content": "<p>Use Amazon Simple Notification Service (SNS) to push notifications when an order is created. Configure different Amazon Simple Queue Service (SQS) queues to receive these messages for downstream processing</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 483,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.454Z",
                "updatedAt": "2023-09-07T08:39:42.454Z",
                "content": "<p>A website serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from an application load balancer. The user base is spread across the world and latency should be minimized for a better user experience.</p>\n\n<p>Which technology/service can help access the static and dynamic content while keeping the data latency low?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure CloudFront with multiple origins to serve both static and dynamic content at low latency to global users</strong></p>\n\n<p>Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.</p>\n\n<p>You can configure a single CloudFront web distribution to serve different types of requests from multiple origins.</p>\n\n<p>Steps to configure CLoudFront for multiple origins:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q40-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-distribution-serve-content/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-distribution-serve-content/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudFront's Lambda@Edge feature to server data from S3 buckets and load balancer programmatically on-the-fly</strong> - AWS Lambda@Edge is a general-purpose serverless compute feature that supports a wide range of computing needs and customizations. Lambda@Edge is best suited for computationally intensive operations. This is not relevant for the given use case.</p>\n\n<p><strong>Use Global Accelerator to transparently switch between S3 bucket and load balancer for different data needs</strong> - AWS Global Accelerator is a networking service that improves the performance of your usersâ€™ traffic by up to 60% using Amazon Web Servicesâ€™ global network infrastructure.</p>\n\n<p>With Global Accelerator, you are provided two global static public IPs that act as a fixed entry point to your application, improving availability. On the back end, add or remove your AWS application endpoints, such as Application Load Balancers, Network Load Balancers, EC2 Instances, and Elastic IPs without making user-facing changes. Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint to mitigate endpoint failure.</p>\n\n<p>CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover.</p>\n\n<p>Global Accelerator is not relevant for the given use-case.</p>\n\n<p><strong>Use CloudFront's Origin Groups to group both static and dynamic requests into one request for further processing</strong> - You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an Origin Group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin. Origin Groups are for origin failure scenarios and not for request routing.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-overview.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-overview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n",
                "options": [
                    {
                        "id": 1977,
                        "content": "<p>Use CloudFront's Lambda@Edge feature to server data from S3 buckets and load balancer programmatically on-the-fly</p>",
                        "isValid": false
                    },
                    {
                        "id": 1978,
                        "content": "<p>Configure CloudFront with multiple origins to serve both static and dynamic content at low latency to global users</p>",
                        "isValid": true
                    },
                    {
                        "id": 1979,
                        "content": "<p>Use Global Accelerator to transparently switch between S3 bucket and load balancer for different data needs</p>",
                        "isValid": false
                    },
                    {
                        "id": 1980,
                        "content": "<p>Use CloudFront's Origin Groups to group both static and dynamic requests into one request for further processing</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 484,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.526Z",
                "updatedAt": "2023-09-07T08:39:42.526Z",
                "content": "<p>An IT company has a web application running on Amazon EC2 instances that needs read-only access to an Amazon DynamoDB table.</p>\n\n<p>As a Developer Associate, what is the best-practice solution you would recommend to accomplish this task?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM role with an AmazonDynamoDBReadOnlyAccess policy and apply it to the EC2 instance profile</strong></p>\n\n<p>As an AWS security best practice, you should not create an IAM user and pass the user's credentials to the application or embed the credentials in the application. Instead, create an IAM role that you attach to the EC2 instance to give temporary security credentials to applications running on the instance. When an application uses these credentials in AWS, it can perform all of the operations that are allowed by the policies attached to the role.</p>\n\n<p>So for the given use-case, you should create an IAM role with an AmazonDynamoDBReadOnlyAccess policy and apply it to the EC2 instance profile.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q9-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new IAM user with access keys. Attach an inline policy to the IAM user with read-only access to DynamoDB. Place the keys in the code. For security, redeploy the code whenever the keys rotate</strong></p>\n\n<p><strong>Create an IAM user with Administrator access and configure AWS credentials for this user on the given EC2 instance</strong></p>\n\n<p><strong>Run application code with AWS account root user credentials to ensure full access to all AWS services</strong></p>\n\n<p>As mentioned in the explanation above, it is dangerous to pass an IAM user's credentials to the application or embed the credentials in the application. The security implications are even higher when you use an IAM user with admin privileges or use the AWS account root user. So all three options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html</a></p>\n",
                "options": [
                    {
                        "id": 1981,
                        "content": "<p>Run application code with AWS account root user credentials to ensure full access to all AWS services</p>",
                        "isValid": false
                    },
                    {
                        "id": 1982,
                        "content": "<p>Create an IAM role with an AmazonDynamoDBReadOnlyAccess policy and apply it to the EC2 instance profile</p>",
                        "isValid": true
                    },
                    {
                        "id": 1983,
                        "content": "<p>Create an IAM user with Administrator access and configure AWS credentials for this user on the given EC2 instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 1984,
                        "content": "<p>Create a new IAM user with access keys. Attach an inline policy to the IAM user with read-only access to DynamoDB. Place the keys in the code. For security, redeploy the code whenever the keys rotate</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 485,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.593Z",
                "updatedAt": "2023-09-07T08:39:42.593Z",
                "content": "<p>What is the run order of the hooks for in-place deployments using CodeDeploy?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Application Stop -&gt; Before Install -&gt; Application Start -&gt; ValidateService</strong></p>\n\n<p>In CodeDeploy, a deployment is a process of installing content on one or more instances. This content can consist of code, web and configuration files, executables, packages, scripts, and so on. CodeDeploy deploys content that is stored in a source repository, according to the configuration rules you specify.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q12-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n\n<p>The content in the 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment. The 'hooks' section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q12-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Before Install -&gt; Application Stop -&gt; ValidateService -&gt; Application Start</strong></p>\n\n<p><strong>Application Stop -&gt; Before Install -&gt; ValidateService -&gt; Application Start</strong></p>\n\n<p><strong>Before Install -&gt; Application Stop -&gt; Application Start -&gt; ValidateService</strong></p>\n\n<p>As explained above, these three options contradict the correct order of hooks, so these are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n",
                "options": [
                    {
                        "id": 1985,
                        "content": "<p>Before Install -&gt; Application Stop -&gt; ValidateService -&gt; Application Start</p>",
                        "isValid": false
                    },
                    {
                        "id": 1986,
                        "content": "<p>Application Stop -&gt; Before Install -&gt; Application Start -&gt; ValidateService</p>",
                        "isValid": true
                    },
                    {
                        "id": 1987,
                        "content": "<p>Before Install -&gt; Application Stop -&gt; Application Start -&gt; ValidateService</p>",
                        "isValid": false
                    },
                    {
                        "id": 1988,
                        "content": "<p>Application Stop -&gt; Before Install -&gt; ValidateService -&gt; Application Start</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 486,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.667Z",
                "updatedAt": "2023-09-07T08:39:42.667Z",
                "content": "<p>The development team at a company is looking at building an AWS CloudFormation template that self-populates the AWS Region variable while deploying the CloudFormation template.</p>\n\n<p>What is the MOST operationally efficient way to determine the Region in which the template is being deployed?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use the AWS::Region pseudo parameter</strong></p>\n\n<p>Pseudo parameters are parameters that are predefined by AWS CloudFormation. You don't declare them in your template. Use them the same way as you would a parameter, as the argument for the Ref function.</p>\n\n<p>You can access pseudo parameters in a CloudFormation template like so:</p>\n\n<pre><code>Outputs:\n  MyStacksRegion:\n    Value: !Ref \"AWS::Region\"\n</code></pre>\n\n<p>The <code>AWS::Region</code> pseudo parameter returns a string representing the Region in which the encompassing resource is being created, such as us-west-2.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudFormation parameter for Region and let the desired value be populated at the time of deployment</strong> - Although it is certainly possible to use a CloudFormation parameter to populate the desired value of the Region at the time of deployment, however, this is not operationally efficient, as you can directly use the AWS::Region pseudo parameter for this.</p>\n\n<p><strong>Set up a mapping containing the key and the named values for all AWS Regions and then have the CloudFormation template auto-select the desired value</strong> - The Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a Region, you can create a mapping that uses the Region name as a key and contains the values you want to specify for each specific Region. You use the Fn::FindInMap intrinsic function to retrieve values in a map. This option is incorrect as the CloudFormation template cannot auto-select the desired value of the Region from a mapping.</p>\n\n<p><strong>Create an AWS Lambda-backed custom resource for Region and let the desired value be populated at the time of deployment by the Lambda</strong> - Custom resources enable you to write custom provisioning logic in templates that AWS CloudFormation runs anytime you create, update (if you changed the custom resource), or delete stacks. When you associate a Lambda function with a custom resource, the function is invoked whenever the custom resource is created, updated, or deleted. This option is a distractor, as Region is not a custom resource that needs to be provisioned.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html</a></p>\n",
                "options": [
                    {
                        "id": 1989,
                        "content": "<p>Create an AWS Lambda-backed custom resource for Region and let the desired value be populated at the time of deployment by the Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 1990,
                        "content": "<p>Set up a mapping containing the key and the named values for all AWS Regions and then have the CloudFormation template auto-select the desired value</p>",
                        "isValid": false
                    },
                    {
                        "id": 1991,
                        "content": "<p>Use the AWS::Region pseudo parameter</p>",
                        "isValid": true
                    },
                    {
                        "id": 1992,
                        "content": "<p>Create a CloudFormation parameter for Region and let the desired value be populated at the time of deployment</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 487,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.742Z",
                "updatedAt": "2023-09-07T08:39:42.742Z",
                "content": "<p>Consider the following IAM policy:</p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Deny\",\n      \"Action\": \"s3:*\",\n      \"Resource\": \"arn:aws:s3:::EXAMPLE-BUCKET/private*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:PutObject\", \"s3:GetObject\"]\n      \"Resource\": \"arn:aws:s3:::EXAMPLE-BUCKET/*\",\n    }\n  ]\n}\n</code></pre>\n\n<p>Which of the following statements is correct per the given policy?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The policy provides PutObject and GetObject access to all objects in the <code>EXAMPLE-BUCKET</code> bucket except the objects that start with <code>private</code></strong></p>\n\n<p>The first statement denies access to any objects that start with <code>private</code> in the <code>EXAMPLE-BUCKET</code> bucket. The second statement allows PutObject and GetObject access to all objects in the <code>EXAMPLE-BUCKET</code> bucket. So the net effect is to allow PutObject and GetObject access to all objects in the <code>EXAMPLE-BUCKET</code> bucket except the objects that start with <code>private</code>.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The policy provides PutObject and GetObject access to all buckets except the <code>EXAMPLE-BUCKET/private</code> bucket</strong></p>\n\n<p><strong>The policy provides PutObject and GetObject access to all objects in the <code>EXAMPLE-BUCKET</code> bucket as well as provides access to all s3 actions on objects starting with <code>private</code> in the <code>EXAMPLE-BUCKET</code> bucket</strong></p>\n\n<p><strong>The policy denies PutObject and GetObject access to all buckets except the <code>EXAMPLE-BUCKET/private</code> bucket</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a></p>\n",
                "options": [
                    {
                        "id": 1993,
                        "content": "<p>The policy provides PutObject and GetObject access to all objects in the <code>EXAMPLE-BUCKET</code> bucket except the objects that start with <code>private</code></p>",
                        "isValid": true
                    },
                    {
                        "id": 1994,
                        "content": "<p>The policy provides PutObject and GetObject access to all objects in the <code>EXAMPLE-BUCKET</code> bucket as well as provides access to all s3 actions on objects starting with <code>private</code> in the <code>EXAMPLE-BUCKET</code> bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 1995,
                        "content": "<p>The policy denies PutObject and GetObject access to all buckets except the <code>EXAMPLE-BUCKET/private</code> bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 1996,
                        "content": "<p>The policy provides PutObject and GetObject access to all buckets except the <code>EXAMPLE-BUCKET/private</code> bucket</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 488,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.813Z",
                "updatedAt": "2023-09-07T08:39:42.813Z",
                "content": "<p>A financial services company with over 10,000 employees has hired you as the new Senior Developer. Initially caching was enabled to reduce the number of calls made to all API endpoints and improve the latency of requests to the companyâ€™s API Gateway.</p>\n\n<p>For testing purposes, you would like to invalidate caching for the API clients to get the most recent responses. Which of the following should you do?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Using the Header Cache-Control: max-age=0</strong></p>\n\n<p>A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the Cache-Control: max-age=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q63-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the Request parameter: ?bypass_cache=1</strong> - Method parameters take query string but this is not one of them.</p>\n\n<p><strong>Using the Header Bypass-Cache=1</strong> - This is a made-up option.</p>\n\n<p><strong>Using the request parameter ?cache-control-max-age=0</strong> - To invalidate cache it requires a header and not a request parameter.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching</a></p>\n",
                "options": [
                    {
                        "id": 1997,
                        "content": "<p>Using the Header Bypass-Cache=1</p>",
                        "isValid": false
                    },
                    {
                        "id": 1998,
                        "content": "<p>Use the Request parameter: ?bypass_cache=1</p>",
                        "isValid": false
                    },
                    {
                        "id": 1999,
                        "content": "<p>Using the request parameter ?cache-control-max-age=0</p>",
                        "isValid": false
                    },
                    {
                        "id": 2000,
                        "content": "<p>Using the Header Cache-Control: max-age=0</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 489,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.886Z",
                "updatedAt": "2023-09-07T08:39:42.886Z",
                "content": "<p>A communication platform serves millions of customers and deploys features in a production environment on AWS via CodeDeploy. You are reviewing scripts for the deployment process located in the AppSpec file.</p>\n\n<p>Which of the following options lists the correct order of lifecycle events?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>DownloadBundle =&gt; BeforeInstall =&gt; ApplicationStart =&gt; ValidateService</strong></p>\n\n<p>AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.</p>\n\n<p>You can specify one or more scripts to run in a hook. Each hook for a lifecycle event is specified with a string on a separate line.</p>\n\n<p>Please review the correct order of lifecycle events:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>BeforeInstall =&gt; ApplicationStart =&gt; DownloadBundle =&gt; ValidateService</strong></p>\n\n<p><strong>ValidateService =&gt; BeforeInstall =&gt;DownloadBundle =&gt; ApplicationStart</strong></p>\n\n<p><strong>BeforeInstall =&gt; ValidateService =&gt;DownloadBundle =&gt; ApplicationStart</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these options are not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order</a></p>\n",
                "options": [
                    {
                        "id": 2001,
                        "content": "<p>BeforeInstall =&gt; ValidateService =&gt;DownloadBundle =&gt; ApplicationStart</p>",
                        "isValid": false
                    },
                    {
                        "id": 2002,
                        "content": "<p>BeforeInstall =&gt; ApplicationStart =&gt; DownloadBundle =&gt; ValidateService</p>",
                        "isValid": false
                    },
                    {
                        "id": 2003,
                        "content": "<p>DownloadBundle =&gt; BeforeInstall =&gt; ApplicationStart =&gt; ValidateService</p>",
                        "isValid": true
                    },
                    {
                        "id": 2004,
                        "content": "<p>ValidateService =&gt; BeforeInstall =&gt;DownloadBundle =&gt; ApplicationStart</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 490,
            "attributes": {
                "createdAt": "2023-09-07T08:39:42.958Z",
                "updatedAt": "2023-09-07T08:39:42.958Z",
                "content": "<p>Your AWS CodeDeploy deployment to T2 instances succeed. The new application revision makes API calls to Amazon S3 however the application is not working as expected due to authorization exceptions and you were assigned to troubleshoot the issue.</p>\n\n<p>Which of the following should you do?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Fix the IAM permissions for the EC2 instance role</strong></p>\n\n<p>You should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute long-term credentials (such as a user name and password or access keys) to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. In this case, make sure your role has access to the S3 bucket.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Fix the IAM permissions for the CodeDeploy service role</strong> - The fact that CodeDeploy deployed the application to EC2 instances tells us that there was no issue between those two. The actual issue is between the EC2 instances and S3.</p>\n\n<p><strong>Make the S3 bucket public</strong> - This is not a good practice, you should strive to provide least privilege access. You may have files in here that should not be allowed public access and you are opening the door to security breaches.</p>\n\n<p><strong>Enable CodeDeploy Proxy</strong> - This is not correct as we don't need to look into CodeDeploy settings but rather between EC2 and S3 permissions.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html</a></p>\n",
                "options": [
                    {
                        "id": 2005,
                        "content": "<p>Enable CodeDeploy Proxy</p>",
                        "isValid": false
                    },
                    {
                        "id": 2006,
                        "content": "<p>Fix the IAM permissions for the EC2 instance role</p>",
                        "isValid": true
                    },
                    {
                        "id": 2007,
                        "content": "<p>Fix the IAM permissions for the CodeDeploy service role</p>",
                        "isValid": false
                    },
                    {
                        "id": 2008,
                        "content": "<p>Make the S3 bucket public</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 491,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.036Z",
                "updatedAt": "2023-09-07T08:39:43.036Z",
                "content": "<p>As part of internal regulations, you must ensure that all communications to Amazon S3 are encrypted.</p>\n\n<p>For which of the following encryption mechanisms will a request get rejected if the connection is not using HTTPS?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>SSE-C</strong></p>\n\n<p>Server-side encryption is about protecting data at rest. Server-side encryption encrypts only the object data, not object metadata. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your encryption keys.</p>\n\n<p>When you upload an object, Amazon S3 uses the encryption key you provide to apply AES-256 encryption to your data and removes the encryption key from memory. When you retrieve an object, you must provide the same encryption key as part of your request. Amazon S3 first verifies that the encryption key you provided matches and then decrypts the object before returning the object data to you.</p>\n\n<p>Amazon S3 will reject any requests made over HTTP when using SSE-C. For security considerations, AWS recommends that you consider any key you send erroneously using HTTP to be compromised.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q25-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-KMS</strong> - It is not mandatory to use HTTPS.</p>\n\n<p><strong>Client-Side Encryption</strong> - Client-side encryption is the act of encrypting data before sending it to Amazon S3. It is not mandatory to use HTTPS for this.</p>\n\n<p><strong>SSE-S3</strong> - It is not mandatory to use HTTPS. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html</a></p>\n",
                "options": [
                    {
                        "id": 2009,
                        "content": "<p>SSE-KMS</p>",
                        "isValid": false
                    },
                    {
                        "id": 2010,
                        "content": "<p>SSE-S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 2011,
                        "content": "<p>Client Side Encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 2012,
                        "content": "<p>SSE-C</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 492,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.106Z",
                "updatedAt": "2023-09-07T08:39:43.106Z",
                "content": "<p>Your company manages MySQL databases on EC2 instances to have full control. Applications on other EC2 instances managed by an ASG make requests to these databases to get information that displays data on dashboards viewed on mobile phones, tablets, and web browsers.</p>\n\n<p>Your manager would like to scale your Auto Scaling group based on the number of requests per minute. How can you achieve this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>You create a CloudWatch custom metric and build an alarm to scale your ASG</strong></p>\n\n<p>Here we need to scale on the metric \"number of requests per minute\", which is a custom metric we need to create, as it's not readily available in CloudWatch.</p>\n\n<p>Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Attach an Elastic Load Balancer</strong> - This is not what you need for auto-scaling. An Elastic Load Balancer distributes workloads across multiple compute resources and checks your instances' health status to name a few, but it does not automatically increase and decrease the number of instances based on the application requirement.</p>\n\n<p><strong>Attach additional Elastic File Storage</strong> - This is a file storage service designed for performance. Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth. This cannot be used to facilitate auto-scaling.</p>\n\n<p>How EFS Works:\n<img src=\"https://d1.awsstatic.com/r2018/b/EFS/product-page-diagram-Amazon-EFS-Launch_How-It-Works.cf947858f0ef3557b9fc14077bdf3f65b3f9ff43.png\">\nvia - <a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a></p>\n\n<p><strong>You enable detailed monitoring and use that to scale your ASG</strong> - The detailed monitoring metrics won't provide information about database /application-level requests per minute, so this option is not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p>\n",
                "options": [
                    {
                        "id": 2013,
                        "content": "<p>You enable detailed monitoring and use that to scale your ASG</p>",
                        "isValid": false
                    },
                    {
                        "id": 2014,
                        "content": "<p>Attach additional Elastic File Storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 2015,
                        "content": "<p>Attach an Elastic Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 2016,
                        "content": "<p>You create a CloudWatch custom metric and build an alarm to scale your ASG</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 493,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.180Z",
                "updatedAt": "2023-09-07T08:39:43.180Z",
                "content": "<p>You have a popular web application that accesses data stored in an Amazon Simple Storage Service (S3) bucket. Developers use the SDK to maintain the application and add new features. Security compliance requests that all new objects uploaded to S3 be encrypted using SSE-S3 at the time of upload. Which of the following headers must the developers add to their request?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>'x-amz-server-side-encryption': 'AES256'</strong></p>\n\n<p>Server-side encryption protects data at rest. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available to encrypt your data, 256-bit Advanced Encryption Standard (AES-256).</p>\n\n<p>SSE-S3 Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html</a></p>\n\n<p><strong>'x-amz-server-side-encryption': 'SSE-S3'</strong> - SSE-S3 (Amazon S3-Managed Keys) is an option available but it's not a valid header value.</p>\n\n<p><strong>'x-amz-server-side-encryption': 'SSE-KMS'</strong> - SSE-KMS (AWS KMS-Managed Keys) is an option available but it's not a valid header value. A valid value would be 'aws:kms'</p>\n\n<p><strong>'x-amz-server-side-encryption': 'aws:kms'</strong> - Server-side encryption is the encryption of data at its destination by the application or service that receives it. AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. Amazon S3 uses AWS KMS customer master keys (CMKs) to encrypt your Amazon S3 objects. AWS KMS encrypts only the object data. Any object metadata is not encrypted.</p>\n\n<p>This is a valid header value and you can use if you need more control over your keys like create, rotating, disabling them using AWS KMS. Otherwise, if you wish to let AWS S3 manage your keys just stick with SSE-S3.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p>\n",
                "options": [
                    {
                        "id": 2017,
                        "content": "<p>'x-amz-server-side-encryption': 'SSE-KMS'</p>",
                        "isValid": false
                    },
                    {
                        "id": 2018,
                        "content": "<p>'x-amz-server-side-encryption': 'AES256'</p>",
                        "isValid": true
                    },
                    {
                        "id": 2019,
                        "content": "<p>'x-amz-server-side-encryption': 'aws:kms'</p>",
                        "isValid": false
                    },
                    {
                        "id": 2020,
                        "content": "<p>'x-amz-server-side-encryption': 'SSE-S3'</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 494,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.263Z",
                "updatedAt": "2023-09-07T08:39:43.263Z",
                "content": "<p>You are working on a project that has over 100 dependencies. Every time your AWS CodeBuild runs a build step it has to resolve Java dependencies from external Ivy repositories which take a long time. Your manager wants to speed this process up in AWS CodeBuild.</p>\n\n<p>Which of the following will help you do this with minimal effort?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Cache dependencies on S3</strong></p>\n\n<p>AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you donâ€™t need to provision, manage, and scale your build servers.</p>\n\n<p>Downloading dependencies is a critical phase in the build process. These dependent files can range in size from a few KBs to multiple MBs. Because most of the dependent files do not change frequently between builds, you can noticeably reduce your build time by caching dependencies in S3.</p>\n\n<p>Best Practices for Caching Dependencies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q7-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/\">https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Reduce the number of dependencies</strong> - This is ideal but sometimes you may not have control over this as your application needs those dependencies, so this option is ruled out.</p>\n\n<p><strong>Ship all the dependencies as part of the source code</strong> - This is not a good practice as doing this will increase your build time. If your dependencies are not changing then its best to cache them.</p>\n\n<p><strong>Use Instance Store type of EC2 instances to facilitate internal dependency cache</strong> - An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.</p>\n\n<p>Instance Store cannot be used to facilitate the internal dependency cache for the code build process.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/\">https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/</a></p>\n",
                "options": [
                    {
                        "id": 2021,
                        "content": "<p>Reduce the number of dependencies</p>",
                        "isValid": false
                    },
                    {
                        "id": 2022,
                        "content": "<p>Ship all the dependencies as part of the source code</p>",
                        "isValid": false
                    },
                    {
                        "id": 2023,
                        "content": "<p>Cache dependencies on S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 2024,
                        "content": "<p>Use Instance Store type of EC2 instances to facilitate internal dependency cache</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 495,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.344Z",
                "updatedAt": "2023-09-07T08:39:43.344Z",
                "content": "<p>For an application that stores personal health information (PHI) in an encrypted Amazon RDS for MySQL DB instance, a developer wants to improve its performance by caching frequently accessed data and adding the ability to sort or rank the cached datasets.</p>\n\n<p>What is the best approach to meet these requirements subject to the constraint that the PHI stays encrypted at all times?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Store the frequently accessed data in an Amazon ElastiCache for Redis instance with encryption enabled for data in transit and at rest</strong></p>\n\n<p>Amazon ElastiCache for Redis is a Redis-compatible in-memory data structure service that can be used as a data store or cache. It delivers the ease of use and power of Redis along with the availability, reliability, scalability, security, and performance suitable for the most demanding applications.</p>\n\n<p>In addition to strings, Redis supports lists, sets, sorted sets, hashes, bit arrays, and hyperlog logs. Applications can use these more advanced data structures to support a variety of use cases. For example, you can use Redis Sorted Sets to easily implement a game leaderboard that keeps a list of players sorted by their rank.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the frequently accessed data in an Amazon ElastiCache for Memcached instance with encryption enabled for data in transit and at rest</strong> - Memcached is designed for simplicity and it does not offer support for advanced data structures and operations such as sort or rank.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q60-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p><strong>Migrate the frequently accessed data to DynamoDB Accelerator (DAX) that has encryption enabled for data in transit and at rest</strong> - DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. DAX cannot be used with RDS MySQL as a caching service, so this option is incorrect.</p>\n\n<p><strong>Migrate the frequently accessed data to an EC2 Instance Store that has encryption enabled for data in transit and at rest</strong> - This option is incorrect. EC2 instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content. It can also be used to store temporary data that you replicate across a fleet of instances, such as a load-balanced pool of web servers.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p>\n",
                "options": [
                    {
                        "id": 2025,
                        "content": "<p>Store the frequently accessed data in an Amazon ElastiCache for Redis instance with encryption enabled for data in transit and at rest</p>",
                        "isValid": true
                    },
                    {
                        "id": 2026,
                        "content": "<p>Migrate the frequently accessed data to an EC2 Instance Store that has encryption enabled for data in transit and at rest</p>",
                        "isValid": false
                    },
                    {
                        "id": 2027,
                        "content": "<p>Store the frequently accessed data in an Amazon ElastiCache for Memcached instance with encryption enabled for data in transit and at rest</p>",
                        "isValid": false
                    },
                    {
                        "id": 2028,
                        "content": "<p>Migrate the frequently accessed data to DynamoDB Accelerator (DAX) that has encryption enabled for data in transit and at rest</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 496,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.423Z",
                "updatedAt": "2023-09-07T08:39:43.423Z",
                "content": "<p>A development team is storing sensitive customer data in S3 that will require encryption at rest. The encryption keys must be rotated at least annually.</p>\n\n<p>What is the easiest way to implement a solution for this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS KMS with automatic key rotation</strong> - Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. You have three mutually exclusive options, depending on how you choose to manage the encryption keys: Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS), Server-Side Encryption with Customer-Provided Keys (SSE-C).</p>\n\n<p>When you use server-side encryption with AWS KMS (SSE-KMS), you can use the default AWS managed CMK, or you can specify a customer managed CMK that you have already created. If you don't specify a customer managed CMK, Amazon S3 automatically creates an AWS managed CMK in your AWS account the first time that you add an object encrypted with SSE-KMS to a bucket. By default, Amazon S3 uses this CMK for SSE-KMS.</p>\n\n<p>You can choose to have AWS KMS automatically rotate CMKs every year, provided that those keys were generated within AWS KMS HSMs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Encrypt the data before sending it to Amazon S3</strong> - The act of encrypting data before sending it to Amazon S3 is called Client-Side encryption. You will have to handle the key generation, maintenance and rotation process. Hence, this is not the right choice here.</p>\n\n<p><strong>Import a custom key into AWS KMS and automate the key rotation on an annual basis by using a Lambda function</strong> - When you import a custom key, you are responsible for maintaining a copy of your imported keys in your key management infrastructure so that you can re-import them at any time. Also, automatic key rotation is not supported for imported keys. Using Lambda functions to rotate keys is a possible solution, but not an optimal one for the current use case.</p>\n\n<p><strong>Use SSE-C with automatic key rotation on an annual basis</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. The keys are not stored anywhere in Amazon S3. There is no automatic key rotation facility for this option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p>\n",
                "options": [
                    {
                        "id": 2029,
                        "content": "<p>Use SSE-C with automatic key rotation on an annual basis</p>",
                        "isValid": false
                    },
                    {
                        "id": 2030,
                        "content": "<p>Use AWS KMS with automatic key rotation</p>",
                        "isValid": true
                    },
                    {
                        "id": 2031,
                        "content": "<p>Encrypt the data before sending it to Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 2032,
                        "content": "<p>Import a custom key into AWS KMS and automate the key rotation on an annual basis by using a Lambda function</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 497,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.492Z",
                "updatedAt": "2023-09-07T08:39:43.492Z",
                "content": "<p>You have an Amazon Kinesis Data Stream with 10 shards, and from the metrics, you are well below the throughput utilization of 10 MB per second to send data. You send 3 MB per second of data and yet you are receiving ProvisionedThroughputExceededException errors frequently.</p>\n\n<p>What is the likely cause of this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The partition key that you have selected isn't distributed enough</strong></p>\n\n<p>Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs.</p>\n\n<p>A Kinesis data stream is a set of shards. A shard is a uniquely identified sequence of data records in a stream. A stream is composed of one or more shards, each of which provides a fixed unit of capacity.</p>\n\n<p>The partition key is used by Kinesis Data Streams to distribute data across shards. Kinesis Data Streams segregates the data records that belong to a stream into multiple shards, using the partition key associated with each data record to determine the shard to which a given data record belongs.</p>\n\n<p>Kinesis Data Streams Overview:\n<img src=\"https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>For the given use-case, as the partition key is not distributed enough, all the data is getting skewed at a few specific shards and not leveraging the entire cluster of shards.</p>\n\n<p>You can also use metrics to determine which are your \"hot\" or \"cold\" shards, that is, shards that are receiving much more data, or much less data, than expected. You could then selectively split the hot shards to increase capacity for the hash keys that target those shards. Similarly, you could merge cold shards to make better use of their unused capacity.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Metrics are slow to update</strong> - Metrics are a CloudWatch concept. This option has been added as a distractor.</p>\n\n<p><strong>You have too many shards</strong> - Too many shards is not the issue as you would see a LimitExceededException in that case.</p>\n\n<p><strong>The data retention period is too long</strong> - Your streaming data is retained for up to 365 days. The data retention period is not an issue causing this error.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html</a></p>\n",
                "options": [
                    {
                        "id": 2033,
                        "content": "<p>Metrics are slow to update</p>",
                        "isValid": false
                    },
                    {
                        "id": 2034,
                        "content": "<p>The data retention period is too long</p>",
                        "isValid": false
                    },
                    {
                        "id": 2035,
                        "content": "<p>You have too many shards</p>",
                        "isValid": false
                    },
                    {
                        "id": 2036,
                        "content": "<p>The partition key that you have selected isn't distributed enough</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 498,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.560Z",
                "updatedAt": "2023-09-07T08:39:43.560Z",
                "content": "<p>Your company has been hired to build a resilient mobile voting app for an upcoming music award show that expects to have 5 to 20 million viewers. The mobile voting app will be marketed heavily months in advance so you are expected to handle millions of messages in the system. You are configuring Amazon Simple Queue Service (SQS) queues for your architecture that should receive messages from 20 KB to 200 KB.</p>\n\n<p>Is it possible to send these messages to SQS?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Yes, the max message size is 256KB</strong></p>\n\n<p>The minimum message size is 1 byte (1 character). The maximum is 262,144 bytes (256 KB).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q43-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Yes, the max message size is 512KB</strong> - The max size is 256KB</p>\n\n<p><strong>No, the max message size is 128KB</strong> - The max size is 256KB</p>\n\n<p><strong>No, the max message size is 64KB</strong> - The max size is 256KB</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 2037,
                        "content": "<p>Yes, the max message size is 512KB</p>",
                        "isValid": false
                    },
                    {
                        "id": 2038,
                        "content": "<p>Yes, the max message size is 256KB</p>",
                        "isValid": true
                    },
                    {
                        "id": 2039,
                        "content": "<p>No, the max message size is 128KB</p>",
                        "isValid": false
                    },
                    {
                        "id": 2040,
                        "content": "<p>No, the max message size is 64KB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 499,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.628Z",
                "updatedAt": "2023-09-07T08:39:43.628Z",
                "content": "<p>You have configured a Network ACL and a Security Group for the load balancer and Amazon EC2 instances to allow inbound traffic on port 80. However, users are still unable to connect to your website after launch.</p>\n\n<p>Which additional configuration is required to make the website accessible to all users over the internet?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Add a rule to the Network ACLs to allow outbound traffic on ports 1024 - 65535</strong></p>\n\n<p>A Network Access Control List (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.</p>\n\n<p>When you create a custom Network ACL and associate it with a subnet, by default, this custom Network ACL denies all inbound and outbound traffic until you add rules. A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. Network ACLs are stateless, which means that responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa).</p>\n\n<p>The client that initiates the request chooses the ephemeral port range. The range varies depending on the client's operating system. Requests originating from Elastic Load Balancing use ports 1024-65535. List of ephemeral port ranges:</p>\n\n<ol>\n<li><p>Many Linux kernels (including the Amazon Linux kernel) use ports 32768-61000.</p></li>\n<li><p>Requests originating from Elastic Load Balancing use ports 1024-65535.</p></li>\n<li><p>Windows operating systems through Windows Server 2003 use ports 1025-5000.</p></li>\n<li><p>Windows Server 2008 and later versions use ports 49152-65535.</p></li>\n<li><p>A NAT gateway uses ports 1024-65535.</p></li>\n</ol>\n\n<p>AWS Lambda functions use ports 1024-65535.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add a rule to the Network ACLs to allow outbound traffic on ports 1025 - 5000</strong> - As discussed above, Windows operating systems through Windows Server 2003 use ports 1025-5000. ELB uses the port range 1024-65535.</p>\n\n<p><strong>Add a rule to the Network ACLs to allow outbound traffic on ports 32768 - 61000</strong> - As discussed above, Linux kernels (including the Amazon Linux kernel) use ports 1025-5000. ELB uses the port range 1024-65535.</p>\n\n<p><strong>Add a rule to the Security Group allowing outbound traffic on port 80</strong> - A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. Security groups act at the instance level, not the subnet level. Security groups are stateful â€” if you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules. Responses to allowed inbound traffic are allowed to flow out, regardless of outbound rules.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p>\n",
                "options": [
                    {
                        "id": 2041,
                        "content": "<p>Add a rule to the Network ACLs to allow outbound traffic on ports 32768 - 61000</p>",
                        "isValid": false
                    },
                    {
                        "id": 2042,
                        "content": "<p>Add a rule to the Network ACLs to allow outbound traffic on ports 1024 - 65535</p>",
                        "isValid": true
                    },
                    {
                        "id": 2043,
                        "content": "<p>Add a rule to the Network ACLs to allow outbound traffic on ports 1025 - 5000</p>",
                        "isValid": false
                    },
                    {
                        "id": 2044,
                        "content": "<p>Add a rule to the Security Group allowing outbound traffic on port 80</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 500,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.698Z",
                "updatedAt": "2023-09-07T08:39:43.698Z",
                "content": "<p>DevOps engineers are developing an order processing system where notifications are sent to a department whenever an order is placed for a product. The system also pushes identical notifications of the new order to a processing module that would allow EC2 instances to handle the fulfillment of the order. In the case of processing errors, the messages should be allowed to be re-processed at a later stage. The order processing system should be able to scale transparently without the need for any manual or programmatic provisioning of resources.</p>\n\n<p>Which of the following solutions can be used to address this use-case in the most cost-efficient way?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>SNS + SQS</strong></p>\n\n<p>Amazon SNS enables message filtering and fanout to a large number of subscribers, including serverless functions, queues, and distributed systems. Additionally, Amazon SNS fans out notifications to end users via mobile push messages, SMS, and email.</p>\n\n<p>How SNS Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/SNS/product-page-diagram_SNS_how-it-works_1.53a464980bf0d5a868b141e9a8b2acf12abc503f.png\">\nvia - <a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Because each buffered request can be processed independently, Amazon SQS can scale transparently to handle the load without any provisioning instructions from you.</p>\n\n<p>SNS and SQS can be used to create a fanout messaging scenario in which messages are \"pushed\" to multiple subscribers, which eliminates the need to periodically check or poll for updates and enables parallel asynchronous processing of the message by the subscribers. SQS can allow for later re-processing and dead letter queues. This is called the fan-out pattern.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SNS + Kinesis</strong> - You can use Amazon Kinesis Data Streams to collect and process large streams of data records in real-time. Kinesis Data Streams stores records from 24 hours (by default) to 8760 hours (365 days). However, you need to manually provision shards in case the load increases or you need to use CloudWatch alarms to set up auto scaling for the shards. Since Kinesis only supports transparent scaling in the on-demand mode, however, it is not cost efficient for the given use case, so this option is not the right fit for the given use case.</p>\n\n<p><strong>SNS + Lambda</strong> - Amazon SNS and AWS Lambda are integrated so you can invoke Lambda functions with Amazon SNS notifications. The Lambda function receives the message payload as an input parameter and can manipulate the information in the message, publish the message to other SNS topics, or send the message to other AWS services. However, your EC2 instances cannot \"poll\" from Lambda functions and as such, this would not work.</p>\n\n<p><strong>SQS + SES</strong> - This will not work as the messages need to be processed twice (once for sending the notification and later for order fulfillment) and SQS only allows for one consuming application.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/tutorials/send-fanout-event-notifications/\">https://aws.amazon.com/getting-started/tutorials/send-fanout-event-notifications/</a></p>\n",
                "options": [
                    {
                        "id": 2045,
                        "content": "<p>SNS + Kinesis</p>",
                        "isValid": false
                    },
                    {
                        "id": 2046,
                        "content": "<p>SQS + SES</p>",
                        "isValid": false
                    },
                    {
                        "id": 2047,
                        "content": "<p>SNS + SQS</p>",
                        "isValid": true
                    },
                    {
                        "id": 2048,
                        "content": "<p>SNS + Lambda</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 501,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.771Z",
                "updatedAt": "2023-09-07T08:39:43.771Z",
                "content": "<p>A development team had enabled and configured CloudTrail for all the Amazon S3 buckets used in a project. The project manager owns all the S3 buckets used in the project. However, the manager noticed that he did not receive any object-level API access logs when the data was read by another AWS account.</p>\n\n<p>What could be the reason for this behavior/error?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The bucket owner also needs to be object owner to get the object access logs</strong></p>\n\n<p>If the bucket owner is also the object owner, the bucket owner gets the object access logs. Otherwise, the bucket owner must get permissions, through the object ACL, for the same object API to get the same object-access API logs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudTrail always delivers object-level API access logs to the requester and not to object owner</strong> - CloudTrail always delivers object-level API access logs to the requester. In addition, CloudTrail also delivers the same logs to the bucket owner only if the bucket owner has permissions for the same API actions on that object.</p>\n\n<p><strong>CloudTrail needs to be configured on both the AWS accounts for receiving the access logs in cross-account access</strong></p>\n\n<p><strong>The meta-data of the bucket is in an invalid state and needs to be corrected by the bucket owner from AWS console to fix the issue</strong></p>\n\n<p>These two options are incorrect and are given only as distractors.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging-s3-info.html#cloudtrail-object-level-crossaccount\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging-s3-info.html#cloudtrail-object-level-crossaccount</a></p>\n",
                "options": [
                    {
                        "id": 2049,
                        "content": "<p>CloudTrail always delivers object-level API access logs to the requester and not to object owner</p>",
                        "isValid": false
                    },
                    {
                        "id": 2050,
                        "content": "<p>CloudTrail needs to be configured on both the AWS accounts for receiving the access logs in cross-account access</p>",
                        "isValid": false
                    },
                    {
                        "id": 2051,
                        "content": "<p>The meta-data of the bucket is in an invalid state and needs to be corrected by the bucket owner from AWS console to fix the issue</p>",
                        "isValid": false
                    },
                    {
                        "id": 2052,
                        "content": "<p>The bucket owner also needs to be object owner to get the object access logs</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 502,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.841Z",
                "updatedAt": "2023-09-07T08:39:43.841Z",
                "content": "<p>An IT company uses a blue/green deployment policy to provision new Amazon EC2 instances in an Auto Scaling group behind a new Application Load Balancer for each new application version. The current set up requires the users to log in after every new deployment.</p>\n\n<p>As a Developer Associate, what advice would you give to the company for resolving this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use ElastiCache to maintain user sessions</strong></p>\n\n<p>Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.</p>\n\n<p>To address scalability and to provide a shared data storage for sessions that can be accessed from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution to for this is to leverage an In-Memory Key/Value store such as Redis and Memcached via ElastiCache.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/caching-session-management-diagram-v2.c6856e6de83c4222dbc4853d9ff873f5542a86d8.PNG\">\nvia - <a href=\"https://aws.amazon.com/caching/session-management/\">https://aws.amazon.com/caching/session-management/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use rolling updates instead of a blue/green deployment</strong> - With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. It leaves the rest of the instances in the environment running the old version of the application. When processing a batch, Elastic Beanstalk detaches all instances in the batch from the load balancer, deploys the new application version, and then reattaches the instances.</p>\n\n<p>This means that some of the users can experience session disruptions when the instances maintaining the sessions were detached as part of the given batch. So this option is incorrect.</p>\n\n<p><strong>Enable sticky sessions in the Application Load Balancer</strong> - As the Application Load Balancer itself is replaced on each new deployment, so maintaining sticky sessions via the Application Load Balancer will not work.</p>\n\n<p><strong>Use multicast to replicate session information</strong> - This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/caching/session-management/\">https://aws.amazon.com/caching/session-management/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method</a></p>\n",
                "options": [
                    {
                        "id": 2053,
                        "content": "<p>Use multicast to replicate session information</p>",
                        "isValid": false
                    },
                    {
                        "id": 2054,
                        "content": "<p>Use ElastiCache to maintain user sessions</p>",
                        "isValid": true
                    },
                    {
                        "id": 2055,
                        "content": "<p>Use rolling updates instead of a blue/green deployment</p>",
                        "isValid": false
                    },
                    {
                        "id": 2056,
                        "content": "<p>Enable sticky sessions in the Application Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 503,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.918Z",
                "updatedAt": "2023-09-07T08:39:43.918Z",
                "content": "<p>An organization uses Alexa as its intelligent assistant to improve productivity throughout the organization. A group of developers manages custom Alexa Skills written in Node.Js to control conference-room equipment settings and start meetings using voice activation. The manager has requested developers that all functions code should be monitored for error rates with the possibility of creating alarms on top of them.</p>\n\n<p>Which of the following options should be chosen? (select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p>CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, and visualizes it using automated dashboards so you can get a unified view of your AWS resources, applications, and services that run in AWS and on-premises. You can correlate your metrics and logs to better understand the health and performance of your resources. You can also create alarms based on metric value thresholds you specify, or that can watch for anomalous metric behavior based on machine learning algorithms.</p>\n\n<p>How CloudWatch works:\n<img src=\"https://d1.awsstatic.com/product-marketing/cloudwatch/product-page-diagram_Cloudwatch_v4.55c15d1cc086395cbd5ad279a2f1fc37e8452e77.png\">\nvia - <a href=\"https://aws.amazon.com/cloudwatch/\">https://aws.amazon.com/cloudwatch/</a></p>\n\n<p><strong>CloudWatch Metrics</strong></p>\n\n<p>Amazon CloudWatch monitors your Amazon Web Services (AWS) resources and the applications you run on AWS in real-time. You can use CloudWatch to collect and track metrics, which are variables you can measure for your resources and applications. Metric data is kept for 15 months, enabling you to view both up-to-the-minute data and historical data.</p>\n\n<p>CloudWatch retains metric data as follows:</p>\n\n<p>Data points with a period of less than 60 seconds are available for 3 hours. These data points are high-resolution custom metrics.\nData points with a period of 60 seconds (1 minute) are available for 15 days\nData points with a period of 300 seconds (5 minute) are available for 63 days\nData points with a period of 3600 seconds (1 hour) are available for 455 days (15 months)</p>\n\n<p><strong>CloudWatch Alarms</strong></p>\n\n<p>You can use an alarm to automatically initiate actions on your behalf. An alarm watches a single metric over a specified time, and performs one or more specified actions, based on the value of the metric relative to a threshold over time. The action is a notification sent to an Amazon SNS topic or an Auto Scaling policy. You can also add alarms to dashboards.</p>\n\n<p>CloudWatch alarms send notifications or automatically make changes to the resources you are monitoring based on rules that you define. Alarms work together with CloudWatch Metrics.</p>\n\n<p>A metric alarm has the following possible states:</p>\n\n<p>OK â€“ The metric or expression is within the defined threshold.</p>\n\n<p>ALARM â€“ The metric or expression is outside of the defined threshold.</p>\n\n<p>INSUFFICIENT_DATA â€“ The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>X-Ray</strong> - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your applicationâ€™s underlying components.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p>X-Ray cannot be used to capture metrics and set up alarms as per the given use-case, so this option is incorrect.</p>\n\n<p><strong>CloudTrail</strong> - CloudWatch is a monitoring service whereas CloudTrail is more of an audit service where you can find API calls made on services and by whom.</p>\n\n<p>How CloudTrail Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><strong>Systems Manager</strong> - Using AWS Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. Systems Manager cannot be used to capture metrics and set up alarms as per the given use-case, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/\">https://aws.amazon.com/cloudwatch/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html</a></p>\n",
                "options": [
                    {
                        "id": 2057,
                        "content": "<p>SSM</p>",
                        "isValid": false
                    },
                    {
                        "id": 2058,
                        "content": "<p>X-Ray</p>",
                        "isValid": false
                    },
                    {
                        "id": 2059,
                        "content": "<p>CloudWatch Alarms</p>",
                        "isValid": true
                    },
                    {
                        "id": 2060,
                        "content": "<p>CloudTrail</p>",
                        "isValid": false
                    },
                    {
                        "id": 2061,
                        "content": "<p>CloudWatch Metrics</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 504,
            "attributes": {
                "createdAt": "2023-09-07T08:39:43.996Z",
                "updatedAt": "2023-09-07T08:39:43.996Z",
                "content": "<p>A company wants to add geospatial capabilities to the cache layer, along with query capabilities and an ability to horizontally scale. The company uses Amazon RDS as the database tier.</p>\n\n<p>Which solution is optimal for this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Leverage the capabilities offered by ElastiCache for Redis with cluster mode enabled</strong></p>\n\n<p>You can use Amazon ElastiCache to accelerate your high volume application workloads by caching your data in-memory providing sub-millisecond data retrieval performance. When used in conjunction with any database including Amazon RDS or Amazon DynamoDB, ElastiCache can alleviate the pressure associated with heavy request loads, increase overall application performance and reduce costs associated with scaling for throughput on other databases.</p>\n\n<p>Amazon ElastiCache makes it easy to deploy and manage a highly available and scalable in-memory data store in the cloud. Among the open source in-memory engines available for use with ElastiCache is Redis, which added powerful geospatial capabilities in its newer versions.</p>\n\n<p>You can leverage ElastiCache for Redis with cluster mode enabled to enhance reliability and availability with little change to your existing workload. Cluster Mode comes with the primary benefit of horizontal scaling up and down of your Redis cluster, with almost zero impact on the performance of the cluster.</p>\n\n<p>Enabling Cluster Mode provides a number of additional benefits in scaling your cluster. In short, it allows you to scale in or out the number of shards (horizontal scaling) versus scaling up or down the node type (vertical scaling). This means that Cluster Mode can scale to very large amounts of storage (potentially 100s of terabytes) across up to 90 shards, whereas a single node can only store as much data in memory as the instance type has capacity for.</p>\n\n<p>Cluster Mode also allows for more flexibility when designing new workloads with unknown storage requirements or heavy write activity. In a read-heavy workload, one can scale a single shard by adding read replicas, up to five, but a write-heavy workload can benefit from additional write endpoints when cluster mode is enabled.</p>\n\n<p>Geospatial on Amazon ElastiCache for Redis:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q38-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/\">https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage the capabilities offered by ElastiCache for Redis with cluster mode disabled</strong> - For a production workload, you should consider using a configuration that includes replication to enhance the protection of your data. Also, only vertical scaling is possible when cluster mode is disabled. The use case mentions horizontal scaling as a requirement, hence disabling cluster mode is not an option.</p>\n\n<p><strong>Use CloudFront caching to cater to demands of increasing workloads</strong> - One of the purposes of using CloudFront is to reduce the number of requests that your origin server must respond to directly. With CloudFront caching, more objects are served from CloudFront edge locations, which are closer to your users. This reduces the load on your origin server and reduces latency. However, the use case mentions that in-memory caching is needed for enhancing the performance of the application. So, this option is incorrect.</p>\n\n<p><strong>Migrate to Amazon DynamoDB to utilize the automatically integrated DynamoDB Accelerator (DAX) along with query capability features</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement â€“ from milliseconds to microseconds â€“ even at millions of requests per second. Database migration is a more elaborate effort compared to implementing and optimizing the caching layer.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/\">https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/amazon-elasticache-utilizing-redis-geospatial-capabilities/\">https://aws.amazon.com/blogs/database/amazon-elasticache-utilizing-redis-geospatial-capabilities/</a></p>\n",
                "options": [
                    {
                        "id": 2062,
                        "content": "<p>Leverage the capabilities offered by ElastiCache for Redis with cluster mode enabled</p>",
                        "isValid": true
                    },
                    {
                        "id": 2063,
                        "content": "<p>Use CloudFront caching to cater to demands of increasing workloads</p>",
                        "isValid": false
                    },
                    {
                        "id": 2064,
                        "content": "<p>Migrate to Amazon DynamoDB to utilize the automatically integrated DynamoDB Accelerator (DAX) along with query capability features</p>",
                        "isValid": false
                    },
                    {
                        "id": 2065,
                        "content": "<p>Leverage the capabilities offered by ElastiCache for Redis with cluster mode disabled</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 505,
            "attributes": {
                "createdAt": "2023-09-07T08:39:44.068Z",
                "updatedAt": "2023-09-07T08:39:44.068Z",
                "content": "<p>After reviewing your monthly AWS bill you notice that the cost of using Amazon SQS has gone up substantially after creating new queues; however, you know that your queue clients do not have a lot of traffic and are receiving empty responses.</p>\n\n<p>Which of the following actions should you take?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use LongPolling</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.</p>\n\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.</p>\n\n<p>Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the differences between Short Polling vs Long Polling:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q14-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q14-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the VisibilityTimeout</strong> - Because there is no guarantee that a consumer received a message, the consumer must delete it. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout. Visibility timeout will not help with cost reduction.</p>\n\n<p><strong>Use a FIFO queue</strong> - FIFO queues are designed to enhance messaging between applications when the order of operations and events has to be enforced. FIFO queues will not help with cost reduction. In fact, they are costlier than standard queues.</p>\n\n<p><strong>Decrease DelaySeconds</strong> - This is similar to VisibilityTimeout. The difference is that a message is hidden when it is first added to a queue for DelaySeconds, whereas for visibility timeouts a message is hidden only after it is consumed from the queue. DelaySeconds will not help with cost reduction.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n",
                "options": [
                    {
                        "id": 2066,
                        "content": "<p>Use a FIFO queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 2067,
                        "content": "<p>Increase the VisibilityTimeout</p>",
                        "isValid": false
                    },
                    {
                        "id": 2068,
                        "content": "<p>Use LongPolling</p>",
                        "isValid": true
                    },
                    {
                        "id": 2069,
                        "content": "<p>Decrease DelaySeconds</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 506,
            "attributes": {
                "createdAt": "2023-09-07T08:39:44.142Z",
                "updatedAt": "2023-09-07T08:39:44.142Z",
                "content": "<p>Your company is in the process of building a DevOps culture and is moving all of its on-premise resources to the cloud using serverless architectures and automated deployments. You have created a CloudFormation template in YAML that uses an AWS Lambda function to pull HTML files from GitHub and place them into an Amazon Simple Storage Service (S3) bucket that you specify.</p>\n\n<p>Which of the following AWS CLI commands can you use to upload AWS Lambda functions and AWS CloudFormation templates to AWS?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong><code>cloudformation package</code> and <code>cloudformation deploy</code></strong></p>\n\n<p>AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p>The <code>cloudformation package</code> command packages the local artifacts (local paths) that your AWS CloudFormation template references. The command will upload local artifacts, such as your source code for your AWS Lambda function.</p>\n\n<p>The <code>cloudformation deploy</code> command deploys the specified AWS CloudFormation template by creating and then executing a changeset.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>cloudformation package</code> and <code>cloudformation upload</code></strong> - The <code>cloudformation upload</code> command does not exist.</p>\n\n<p><strong><code>cloudformation zip</code> and <code>cloudformation upload</code></strong> - Both commands do not exist, this is a made-up option.</p>\n\n<p><strong><code>cloudformation zip</code> and <code>cloudformation deploy</code></strong> - The <code>cloudformation zip</code> command does not exist, this is a made-up option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html\">https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html</a></p>\n",
                "options": [
                    {
                        "id": 2070,
                        "content": "<p><code>cloudformation package</code> and <code>cloudformation deploy</code></p>",
                        "isValid": true
                    },
                    {
                        "id": 2071,
                        "content": "<p><code>cloudformation zip</code> and <code>cloudformation deploy</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2072,
                        "content": "<p><code>cloudformation package</code> and <code>cloudformation upload</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2073,
                        "content": "<p><code>cloudformation zip</code> and <code>cloudformation upload</code></p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 507,
            "attributes": {
                "createdAt": "2023-09-07T08:39:44.210Z",
                "updatedAt": "2023-09-07T08:39:44.210Z",
                "content": "<p>You are designing a high-performance application that requires millions of connections. You have several EC2 instances running Apache2 web servers and the application will require capturing the userâ€™s source IP address and source port without the use of X-Forwarded-For.</p>\n\n<p>Which of the following options will meet your needs?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Network Load Balancer</strong></p>\n\n<p>A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration. Incoming connections remain unmodified, so application software need not support X-Forwarded-For.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q50-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Application Load Balancer</strong> - An Application Load Balancer functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI) model. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply and then selects a target from the target group for the rule action.</p>\n\n<p>One of many benefits of the Application Load Balancer is its support for path-based routing. You can configure rules for your listener that forward requests based on the URL in the request. This enables you to structure your application as smaller services, and route requests to the correct service based on the content of the URL. For needs relating to network traffic go with Network Load Balancer.</p>\n\n<p><strong>Elastic Load Balancer</strong> - Elastic Load Balancing is the service itself that offers different types of load balancers.</p>\n\n<p><strong>Classic Load Balancer</strong> - It is a basic load balancer that distributes traffic. If your account was created before 2013-12-04, your account supports EC2-Classic instances and you will benefit in using this type of load balancer. The classic load balancer can be used regardless of when your account was created and whether you use EC2-Classic or whether your instances are in a VPC but just remember its the basic load balancer AWS offers and not advanced as the others.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p>\n",
                "options": [
                    {
                        "id": 2074,
                        "content": "<p>Classic Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 2075,
                        "content": "<p>Application Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 2076,
                        "content": "<p>Network Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 2077,
                        "content": "<p>Elastic Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 508,
            "attributes": {
                "createdAt": "2023-09-07T08:39:44.280Z",
                "updatedAt": "2023-09-07T08:39:44.280Z",
                "content": "<p>Your mobile application needs to perform API calls to DynamoDB. You do not want to store AWS secret and access keys onto the mobile devices and need all the calls to DynamoDB made with a different identity per mobile device.</p>\n\n<p>Which of the following services allows you to achieve this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>\"Cognito Identity Pools\"</p>\n\n<p>Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token. Identity pools provide AWS credentials to grant your users access to other AWS services.</p>\n\n<p>Cognito Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q58-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Cognito User Pools\" - AWS Cognito User Pools is there to authenticate users for your applications which looks similar to Cognito Identity Pools. The difference is that Identity Pools allows a way to authorize your users to use the various AWS services and User Pools is not about authorizing to AWS services but to provide add sign-up and sign-in functionality to web and mobile applications.</p>\n\n<p>\"Cognito Sync\" - You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status.</p>\n\n<p>\"IAM\" - This is not a good solution because it would require you to have an IAM user for each mobile device which is not a good practice or manageable way of handling deployment.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q58-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n",
                "options": [
                    {
                        "id": 2078,
                        "content": "<p>Cognito Sync</p>",
                        "isValid": false
                    },
                    {
                        "id": 2079,
                        "content": "<p>Cognito User Pools</p>",
                        "isValid": false
                    },
                    {
                        "id": 2080,
                        "content": "<p>IAM</p>",
                        "isValid": false
                    },
                    {
                        "id": 2081,
                        "content": "<p>Cognito Identity Pools</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 509,
            "attributes": {
                "createdAt": "2023-09-07T08:39:44.365Z",
                "updatedAt": "2023-09-07T08:39:44.365Z",
                "content": "<p>A developer is configuring an Application Load Balancer (ALB) to direct traffic to the application's EC2 instances and Lambda functions.</p>\n\n<p>Which of the following characteristics of the ALB can be identified as correct? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>An ALB has three possible target types: Instance, IP and Lambda</strong></p>\n\n<p>When you create a target group, you specify its target type, which determines the type of target you specify when registering targets with this target group. After you create a target group, you cannot change its target type. The following are the possible target types:</p>\n\n<ol>\n<li><code>Instance</code> - The targets are specified by instance ID</li>\n<li><code>IP</code> - The targets are IP addresses</li>\n<li><code>Lambda</code> - The target is a Lambda function</li>\n</ol>\n\n<p><strong>You can not specify publicly routable IP addresses to an ALB</strong></p>\n\n<p>When the target type is IP, you can specify IP addresses from specific CIDR blocks only. You can't specify publicly routable IP addresses.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If you specify targets using an instance ID, traffic is routed to instances using any private IP address from one or more network interfaces</strong> - If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance.</p>\n\n<p><strong>If you specify targets using IP addresses, traffic is routed to instances using the primary private IP address</strong> - If you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port.</p>\n\n<p><strong>An ALB has three possible target types: Hostname, IP and Lambda</strong> - This is incorrect, as described in the correct explanation above.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 2082,
                        "content": "<p>An ALB has three possible target types: Instance, IP and Lambda</p>",
                        "isValid": true
                    },
                    {
                        "id": 2083,
                        "content": "<p>If you specify targets using IP addresses, traffic is routed to instances using the primary private IP address</p>",
                        "isValid": false
                    },
                    {
                        "id": 2084,
                        "content": "<p>If you specify targets using an instance ID, traffic is routed to instances using any private IP address from one or more network interfaces</p>",
                        "isValid": false
                    },
                    {
                        "id": 2085,
                        "content": "<p>An ALB has three possible target types: Hostname, IP and Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 2086,
                        "content": "<p>You can not specify publicly routable IP addresses to an ALB</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 510,
            "attributes": {
                "createdAt": "2023-09-07T08:39:44.436Z",
                "updatedAt": "2023-09-07T08:39:44.436Z",
                "content": "<p>Your development team uses the AWS SDK for Java on a web application that uploads files to several Amazon Simple Storage Service (S3) buckets using the SSE-KMS encryption mechanism. Developers are reporting that they are receiving permission errors when trying to push their objects over HTTP. Which of the following headers should they include in their request?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>'x-amz-server-side-encryption': 'aws:kms'</strong></p>\n\n<p>Server-side encryption is the encryption of data at its destination by the application or service that receives it. AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. Amazon S3 uses AWS KMS customer master keys (CMKs) to encrypt your Amazon S3 objects. AWS KMS encrypts only the object data. Any object metadata is not encrypted.</p>\n\n<p>If the request does not include the x-amz-server-side-encryption header, then the request is denied.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q22-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>'x-amz-server-side-encryption': 'SSE-S3'</strong> - This is an invalid header value. The correct value is 'x-amz-server-side-encryption': 'AES256'. This refers to Server-Side Encryption with Amazon S3-Managed Encryption Keys (SSE-S3).</p>\n\n<p><strong>'x-amz-server-side-encryption': 'SSE-KMS'</strong> - Invalid header value. SSE-KMS is an encryption option.</p>\n\n<p><strong>'x-amz-server-side-encryption': 'AES256'</strong> - This is the correct header value if you are using SSE-S3 server-side encryption.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n",
                "options": [
                    {
                        "id": 2087,
                        "content": "<p>'x-amz-server-side-encryption': 'SSE-S3'</p>",
                        "isValid": false
                    },
                    {
                        "id": 2088,
                        "content": "<p>'x-amz-server-side-encryption': 'AES256'</p>",
                        "isValid": false
                    },
                    {
                        "id": 2089,
                        "content": "<p>'x-amz-server-side-encryption': 'SSE-KMS'</p>",
                        "isValid": false
                    },
                    {
                        "id": 2090,
                        "content": "<p>'x-amz-server-side-encryption': 'aws:kms'</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 511,
            "attributes": {
                "createdAt": "2023-09-07T08:39:44.510Z",
                "updatedAt": "2023-09-07T08:39:44.510Z",
                "content": "<p>A development team is considering Amazon ElastiCache for Redis as its in-memory caching solution for its relational database.</p>\n\n<p>Which of the following options are correct while configuring ElastiCache? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>All the nodes in a Redis cluster must reside in the same region</strong></p>\n\n<p>All the nodes in a Redis cluster (cluster mode enabled or cluster mode disabled) must reside in the same region.</p>\n\n<p><strong>While using Redis with cluster mode enabled, you cannot manually promote any of the replica nodes to primary</strong></p>\n\n<p>While using Redis with cluster mode enabled, there are some limitations:</p>\n\n<ol>\n<li><p>You cannot manually promote any of the replica nodes to primary.</p></li>\n<li><p>Multi-AZ is required.</p></li>\n<li><p>You can only change the structure of a cluster, the node type, and the number of nodes by restoring from a backup.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>While using Redis with cluster mode enabled, asynchronous replication mechanisms are used to keep the read replicas synchronized with the primary. If cluster mode is disabled, the replication mechanism is done synchronously</strong> - When you add a read replica to a cluster, all of the data from the primary is copied to the new node. From that point on, whenever data is written to the primary, the changes are asynchronously propagated to all the read replicas, for both the Redis offerings (cluster mode enabled or cluster mode disabled).</p>\n\n<p><strong>If you have no replicas and a node fails, you experience no loss of data when using Redis with cluster mode enabled</strong> - If you have no replicas and a node fails, you experience loss of all data in that node's shard, when using Redis with cluster mode enabled. If you have no replicas and the node fails, you experience total data loss in Redis with cluster mode disabled.</p>\n\n<p><strong>You can scale write capacity for Redis by adding replica nodes</strong> - This increases only the read capacity of the Redis cluster, write capacity is not enhanced by read replicas.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis.Groups.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis.Groups.html</a></p>\n",
                "options": [
                    {
                        "id": 2091,
                        "content": "<p>If you have no replicas and a node fails, you experience no loss of data when using Redis with cluster mode enabled</p>",
                        "isValid": false
                    },
                    {
                        "id": 2092,
                        "content": "<p>While using Redis with cluster mode enabled, asynchronous replication mechanisms are used to keep the read replicas synchronized with the primary. If cluster mode is disabled, the replication mechanism is done synchronously</p>",
                        "isValid": false
                    },
                    {
                        "id": 2093,
                        "content": "<p>You can scale write capacity for Redis by adding replica nodes</p>",
                        "isValid": false
                    },
                    {
                        "id": 2094,
                        "content": "<p>While using Redis with cluster mode enabled, you cannot manually promote any of the replica nodes to primary</p>",
                        "isValid": true
                    },
                    {
                        "id": 2095,
                        "content": "<p>All the nodes in a Redis cluster must reside in the same region</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 512,
            "attributes": {
                "createdAt": "2023-09-07T08:39:44.594Z",
                "updatedAt": "2023-09-07T08:39:44.594Z",
                "content": "<p>An Amazon Simple Queue Service (SQS) has to be configured between two AWS accounts for shared access to the queue. AWS account A has the SQS queue in its account and AWS account B has to be given access to this queue.</p>\n\n<p>Which of the following options need to be combined to allow this cross-account access? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>The account A administrator creates an IAM role and attaches a permissions policy</strong></p>\n\n<p><strong>The account A administrator attaches a trust policy to the role that identifies account B as the principal who can assume the role</strong></p>\n\n<p><strong>The account B administrator delegates the permission to assume the role to any users in account B</strong></p>\n\n<p>To grant cross-account permissions, you need to attach an identity-based permissions policy to an IAM role. For example, the AWS account A administrator can create a role to grant cross-account permissions to AWS account B as follows:</p>\n\n<ol>\n<li><p>The account A administrator creates an IAM role and attaches a permissions policyâ€”that grants permissions on resources in account Aâ€”to the role.</p></li>\n<li><p>The account A administrator attaches a trust policy to the role that identifies account B as the principal who can assume the role.</p></li>\n<li><p>The account B administrator delegates the permission to assume the role to any users in account B. This allows users in account B to create or access queues in account A.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>The account B administrator creates an IAM role and attaches a trust policy to the role with account B as the principal</strong> - As mentioned above, the account A administrator needs to create an IAM role and then attach a permissions policy. So, this option is incorrect.</p>\n\n<p><strong>The account A administrator delegates the permission to assume the role to any users in account A</strong> - This is irrelevant, as users in account B need to be given access.</p>\n\n<p><strong>The account A administrator attaches a trust policy to the role that identifies account B as the AWS service principal who can assume the role</strong> - AWS service principal is given as principal in the trust policy when you need to grant the permission to assume the role to an AWS service. The given use case talks about giving permission to another account. So, service principal is not an option here.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-overview-of-managing-access.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-overview-of-managing-access.html</a></p>\n",
                "options": [
                    {
                        "id": 2096,
                        "content": "<p>The account A administrator delegates the permission to assume the role to any users in account A</p>",
                        "isValid": false
                    },
                    {
                        "id": 2097,
                        "content": "<p>The account B administrator creates an IAM role and attaches a trust policy to the role with account B as the principal</p>",
                        "isValid": false
                    },
                    {
                        "id": 2098,
                        "content": "<p>The account A administrator attaches a trust policy to the role that identifies account B as the AWS service principal who can assume the role</p>",
                        "isValid": false
                    },
                    {
                        "id": 2099,
                        "content": "<p>The account A administrator attaches a trust policy to the role that identifies account B as the principal who can assume the role</p>",
                        "isValid": true
                    },
                    {
                        "id": 2100,
                        "content": "<p>The account B administrator delegates the permission to assume the role to any users in account B</p>",
                        "isValid": true
                    },
                    {
                        "id": 2101,
                        "content": "<p>The account A administrator creates an IAM role and attaches a permissions policy</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 513,
            "attributes": {
                "createdAt": "2023-09-07T08:39:44.666Z",
                "updatedAt": "2023-09-07T08:39:44.666Z",
                "content": "<p>You are a manager for a tech company that has just hired a team of developers to work on the company's AWS infrastructure. All the developers are reporting to you that when using the AWS CLI to execute commands it fails with the following exception: You are not authorized to perform this operation. Encoded authorization failure message: 6h34GtpmGjJJUm946eDVBfzWQJk6z5GePbbGDs9Z2T8xZj9EZtEduSnTbmrR7pMqpJrVYJCew2m8YBZQf4HRWEtrpncANrZMsnzk.</p>\n\n<p>Which of the following actions will help developers decode the message?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS STS decode-authorization-message</strong></p>\n\n<p>Use decode-authorization-message to decode additional information about the authorization status of a request from an encoded message returned in response to an AWS request. If a user is not authorized to perform an action that was requested, the request returns a Client.UnauthorizedOperation response (an HTTP 403 response). The message is encoded because the details of the authorization status can constitute privileged information that the user who requested the operation should not see. To decode an authorization status message, a user must be granted permissions via an IAM policy to request the DecodeAuthorizationMessage (sts:DecodeAuthorizationMessage) action.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS IAM decode-authorization-message</strong> - The IAM service does not have this command, as it's a made-up option.</p>\n\n<p><strong>Use KMS decode-authorization-message</strong> - The KMS service does not have this command, as it's a made-up option.</p>\n\n<p><strong>AWS Cognito Decoder</strong> - The Cognito service does not have this command, as it's a made-up option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html\">https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html</a></p>\n",
                "options": [
                    {
                        "id": 2102,
                        "content": "<p>AWS Cognito Decoder</p>",
                        "isValid": false
                    },
                    {
                        "id": 2103,
                        "content": "<p>AWS STS decode-authorization-message</p>",
                        "isValid": true
                    },
                    {
                        "id": 2104,
                        "content": "<p>Use KMS decode-authorization-message</p>",
                        "isValid": false
                    },
                    {
                        "id": 2105,
                        "content": "<p>AWS IAM decode-authorization-message</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 514,
            "attributes": {
                "createdAt": "2023-09-07T08:39:44.732Z",
                "updatedAt": "2023-09-07T08:39:44.732Z",
                "content": "<p>The development team at a company wants to insert vendor records into an Amazon DynamoDB table as soon as the vendor uploads a new file into an Amazon S3 bucket.</p>\n\n<p>As a Developer Associate, which set of steps would you recommend to achieve this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an S3 event to invoke a Lambda function that inserts records into DynamoDB</strong></p>\n\n<p>The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the notification subresource that is associated with a bucket.</p>\n\n<p>Amazon S3 APIs such as PUT, POST, and COPY can create an object. Using these event types, you can enable notification when an object is created using a specific API, or you can use the s3:ObjectCreated:* event type to request notification regardless of the API that was used to create an object.</p>\n\n<p>For the given use-case, you would create an S3 event notification that triggers a Lambda function whenever we have a PUT object operation in the S3 bucket. The Lambda function in turn would execute custom code to inserts records into DynamoDB.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q15-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Write a cron job that will execute a Lambda function at a scheduled time and insert the records into DynamoDB</strong> - This is not efficient because there may not be any unprocessed file in the S3 bucket when the cron triggers the Lambda on schedule. So this is not the correct option.</p>\n\n<p><strong>Set up an event with Amazon CloudWatch Events that will monitor the S3 bucket and then insert the records into DynamoDB</strong> - The CloudWatch event cannot directly insert records into DynamoDB as it's not a supported target type. The CloudWatch event needs to use something like a Lambda function to insert the records into DynamoDB.</p>\n\n<p><strong>Develop a Lambda function that will poll the S3 bucket and then insert the records into DynamoDB</strong> - This is not efficient because there may not be any unprocessed file in the S3 bucket when the Lambda function polls the S3 bucket at a given time interval. So this is not the correct option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n",
                "options": [
                    {
                        "id": 2106,
                        "content": "<p>Set up an event with Amazon CloudWatch Events that will monitor the S3 bucket and then insert the records into DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 2107,
                        "content": "<p>Develop a Lambda function that will poll the S3 bucket and then insert the records into DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 2108,
                        "content": "<p>Write a cron job that will execute a Lambda function at a scheduled time and insert the records into DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 2109,
                        "content": "<p>Create an S3 event to invoke a Lambda function that inserts records into DynamoDB</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 515,
            "attributes": {
                "createdAt": "2023-09-07T08:39:44.807Z",
                "updatedAt": "2023-09-07T08:39:44.807Z",
                "content": "<p>You are planning to build a fleet of EBS-optimized EC2 instances to handle the load of your new application. Due to security compliance, your organization wants any secret strings used in the application to be encrypted to prevent exposing values as clear text.</p>\n\n<p>The solution requires that decryption events be audited and API calls to be simple. How can this be achieved? (select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Store the secret as SecureString in SSM Parameter Store</strong></p>\n\n<p>With AWS Systems Manager Parameter Store, you can create SecureString parameters, which are parameters that have a plaintext parameter name and an encrypted parameter value. Parameter Store uses AWS KMS to encrypt and decrypt the parameter values of Secure String parameters. Also, if you are using customer-managed CMKs, you can use IAM policies and key policies to manage to encrypt and decrypt permissions. To retrieve the decrypted value you only need to do one API call.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q47-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html</a></p>\n\n<p><strong>Audit using CloudTrail</strong></p>\n\n<p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides an event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services.</p>\n\n<p>CloudTrail will allow you to see all API calls made to SSM and KMS.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Encrypt first with KMS then store in SSM Parameter store</strong> - This could work but will require two API calls to get the decrypted value instead of one. So this is not the right option.</p>\n\n<p><strong>Store the secret as PlainText in SSM Parameter Store</strong> - Plaintext parameters are not secure and shouldn't be used to store secrets.</p>\n\n<p><strong>Audit using SSM Audit Trail</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html</a></p>\n",
                "options": [
                    {
                        "id": 2110,
                        "content": "<p>Store the secret as SecureString in SSM Parameter Store</p>",
                        "isValid": true
                    },
                    {
                        "id": 2111,
                        "content": "<p>Audit using CloudTrail</p>",
                        "isValid": true
                    },
                    {
                        "id": 2112,
                        "content": "<p>Encrypt first with KMS then store in SSM Parameter store</p>",
                        "isValid": false
                    },
                    {
                        "id": 2113,
                        "content": "<p>Store the secret as PlainText in SSM Parameter Store</p>",
                        "isValid": false
                    },
                    {
                        "id": 2114,
                        "content": "<p>Audit using SSM Audit Trail</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 516,
            "attributes": {
                "createdAt": "2023-09-07T08:39:44.875Z",
                "updatedAt": "2023-09-07T08:39:44.875Z",
                "content": "<p>You have uploaded a zip file to AWS Lambda that contains code files written in Node.Js. When your function is executed you receive the following output, 'Error: Memory Size: 10,240 MB Max Memory Used'.</p>\n\n<p>Which of the following explains the problem?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Your Lambda function ran out of RAM</strong></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\">\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p>The maximum amount of memory available to the Lambda function at runtime is 10,240 MB. Your Lambda function was deployed with 10,240 MB of RAM, but it seems your code requested or used more than that, so the Lambda function failed.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q27-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Your zip file is corrupt</strong> - A memory size error states that Lambda was able to extract so the file is not corrupt</p>\n\n<p><strong>The uncompressed zip file exceeds AWS Lambda limits</strong> - This is not correct as your function was able to execute.</p>\n\n<p><strong>You have uploaded a zip file larger than 50 MB to AWS Lambda</strong> -  This is not correct as your lambda function was able to execute</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a></p>\n",
                "options": [
                    {
                        "id": 2115,
                        "content": "<p>Your Lambda function ran out of RAM</p>",
                        "isValid": true
                    },
                    {
                        "id": 2116,
                        "content": "<p>The uncompressed zip file exceeds AWS Lambda limits</p>",
                        "isValid": false
                    },
                    {
                        "id": 2117,
                        "content": "<p>You have uploaded a zip file larger than 50 MB to AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 2118,
                        "content": "<p>Your zip file is corrupt</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 517,
            "attributes": {
                "createdAt": "2023-09-07T08:39:44.958Z",
                "updatedAt": "2023-09-07T08:39:44.958Z",
                "content": "<p>A user has an IAM policy as well as an Amazon SQS policy that apply to his account. The IAM policy grants his account permission for the <code>ReceiveMessage</code> action on <code>example_queue</code>, whereas the Amazon SQS policy gives his account permission for the <code>SendMessage</code> action on the same queue.</p>\n\n<p>Considering the permissions above, which of the following options are correct? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>The user can send a <code>ReceiveMessage</code> request to <code>example_queue</code>, the IAM policy allows this action</strong></p>\n\n<p>The user has both an IAM policy and an Amazon SQS policy that apply to his account. The IAM policy grants his account permission for the <code>ReceiveMessage</code> action on <code>example_queue</code>, whereas the Amazon SQS policy gives his account permission for the <code>SendMessage</code> action on the same queue.</p>\n\n<p>How IAM policy and SQS policy work in tandem:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html</a></p>\n\n<p><strong>If you add a policy that denies the user access to all actions for the queue, the policy will override the other two policies and the user will not have access to <code>example_queue</code></strong></p>\n\n<p>To remove the user's full access to the queue, the easiest thing to do is to add a policy that denies him access to all actions for the queue. This policy overrides the other two because an explicit deny always overrides an allow.</p>\n\n<p>You can also add an additional statement to the Amazon SQS policy that denies the user any type of access to the queue. It has the same effect as adding an IAM policy that denies the user access to the queue.</p>\n\n<p>How IAM policy and SQS policy work in tandem:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q34-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If the user sends a <code>SendMessage</code> request to <code>example_queue</code>, the IAM policy will deny this action</strong> - If the user sends a <code>SendMessage</code> request to <code>example_queue</code>, the Amazon SQS policy allows the action. The IAM policy has no explicit deny on this action, so it plays no part.</p>\n\n<p><strong>Either of IAM policies or Amazon SQS policies should be used to grant permissions. Both cannot be used together</strong> - There are two ways to give your users permissions to your Amazon SQS resources: using the Amazon SQS policy system and using the IAM policy system. You can use one or the other, or both. For the most part, you can achieve the same result with either one.</p>\n\n<p><strong>Adding only an IAM policy to deny the user of all actions on the queue is not enough. The SQS policy should also explicitly deny all action</strong> - The user can be denied access using any one of the policies. Explicit deny in any policy will override all other allow actions defined using either of the policies.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html</a></p>\n",
                "options": [
                    {
                        "id": 2119,
                        "content": "<p>If you add a policy that denies the user access to all actions for the queue, the policy will override the other two policies and the user will not have access to <code>example_queue</code></p>",
                        "isValid": true
                    },
                    {
                        "id": 2120,
                        "content": "<p>The user can send a <code>ReceiveMessage</code> request to <code>example_queue</code>, the IAM policy allows this action</p>",
                        "isValid": true
                    },
                    {
                        "id": 2121,
                        "content": "<p>If the user sends a <code>SendMessage</code> request to <code>example_queue</code>, the IAM policy will deny this action</p>",
                        "isValid": false
                    },
                    {
                        "id": 2122,
                        "content": "<p>Adding only an IAM policy to deny the user of all actions on the queue is not enough. The SQS policy should also explicitly deny all action</p>",
                        "isValid": false
                    },
                    {
                        "id": 2123,
                        "content": "<p>Either of IAM policies or Amazon SQS policies should be used to grant permissions. Both cannot be used together</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 518,
            "attributes": {
                "createdAt": "2023-09-07T08:39:45.029Z",
                "updatedAt": "2023-09-07T08:39:45.029Z",
                "content": "<p>A senior cloud engineer designs and deploys online fraud detection solutions for credit card companies processing millions of transactions daily. The Elastic Beanstalk application sends files to Amazon S3 and then sends a message to an Amazon SQS queue containing the path of the uploaded file in S3. The engineer wants to postpone the delivery of any new messages to the queue for at least 10 seconds.</p>\n\n<p>Which SQS feature should the engineer leverage?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use DelaySeconds parameter</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Delay queues let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Implement application-side delay</strong> - You can customize your application to delay sending messages but it is not a robust solution. You can run into a scenario where your application crashes before sending a message, then that message would be lost.</p>\n\n<p><strong>Use visibility timeout parameter</strong> - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p><strong>Enable LongPolling</strong> - Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. You cannot use LongPolling to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a></p>\n",
                "options": [
                    {
                        "id": 2124,
                        "content": "<p>Use DelaySeconds parameter</p>",
                        "isValid": true
                    },
                    {
                        "id": 2125,
                        "content": "<p>Use visibility timeout parameter</p>",
                        "isValid": false
                    },
                    {
                        "id": 2126,
                        "content": "<p>Enable LongPolling</p>",
                        "isValid": false
                    },
                    {
                        "id": 2127,
                        "content": "<p>Implement application-side delay</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 519,
            "attributes": {
                "createdAt": "2023-09-07T08:39:45.111Z",
                "updatedAt": "2023-09-07T08:39:45.111Z",
                "content": "<p>You have moved your on-premise infrastructure to AWS and are in the process of configuring an AWS Elastic Beanstalk deployment environment for production, development, and testing. You have configured your production environment to use a rolling deployment to prevent your application from becoming unavailable to users. For the development and testing environment, you would like to deploy quickly and are not concerned about downtime.</p>\n\n<p>Which of the following deployment policies meet your needs?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>All at once</strong></p>\n\n<p>This is the quickest deployment method. Suitable if you can accept a short loss of service, and if quick deployments are important to you. With this method, Elastic Beanstalk deploys the new application version to each instance. Then, the web proxy or application server might need to restart. As a result, your application might be unavailable to users (or have low availability) for a short time.</p>\n\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q26-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Rolling</strong> - With this method, your application is deployed to your environment one batch of instances at a time. Most bandwidth is retained throughout the deployment. Avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. Suitable if you can't accept any period of completely lost service.</p>\n\n<p><strong>Rolling with additional batches</strong> - With this method, Elastic Beanstalk launches an extra batch of instances, then performs a rolling deployment. Launching the extra batch takes time, and ensures that the same bandwidth is retained throughout the deployment. This policy also avoids any reduced availability, although at a cost of an even longer deployment time compared to the Rolling method. Finally, this option is suitable if you must maintain the same bandwidth throughout the deployment.</p>\n\n<p><strong>Immutable</strong> - A slower deployment method, that ensures your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. With this method, Elastic Beanstalk performs an immutable update to deploy your application. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n",
                "options": [
                    {
                        "id": 2128,
                        "content": "<p>Rolling</p>",
                        "isValid": false
                    },
                    {
                        "id": 2129,
                        "content": "<p>All at once</p>",
                        "isValid": true
                    },
                    {
                        "id": 2130,
                        "content": "<p>Rolling with additional batches</p>",
                        "isValid": false
                    },
                    {
                        "id": 2131,
                        "content": "<p>Immutable</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 520,
            "attributes": {
                "createdAt": "2023-09-07T08:39:45.213Z",
                "updatedAt": "2023-09-07T08:39:45.213Z",
                "content": "<p>A company would like to migrate the existing application code from a GitHub repository to AWS CodeCommit.</p>\n\n<p>As an AWS Certified Developer Associate, which of the following would you recommend for migrating the cloned repository to CodeCommit over HTTPS?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Git credentials generated from IAM</strong> - CodeCommit repositories are Git-based and support the basic functionalities of Git such as Git credentials. AWS recommends that you use an IAM user when working with CodeCommit. You can access CodeCommit with other identity types, but the other identity types are subject to limitations.</p>\n\n<p>The simplest way to set up connections to AWS CodeCommit repositories is to configure Git credentials for CodeCommit in the IAM console, and then use those credentials for HTTPS connections. You can also use these same credentials with any third-party tool or individual development environment (IDE) that supports HTTPS authentication using a static user name and password.</p>\n\n<p>An IAM user is an identity within your Amazon Web Services account that has specific custom permissions. For example, an IAM user can have permissions to create and manage Git credentials for accessing CodeCommit repositories. This is the recommended user type for working with CodeCommit. You can use an IAM user name and password to sign in to secure AWS webpages like the AWS Management Console, AWS Discussion Forums, or the AWS Support Center.</p>\n\n<p>Authentication and access control for AWS CodeCommit:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q52-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use IAM Multi-Factor authentication</strong> - AWS Multi-Factor Authentication (MFA) is a simple best practice that adds an extra layer of protection on top of your user name and password. With MFA enabled, when a user signs in to an AWS Management Console, they will be prompted for their user name and password (the first factorâ€”what they know), as well as for an authentication code from their AWS MFA device (the second factorâ€”what they have). Taken together, these multiple factors provide increased security for your AWS account settings and resources.</p>\n\n<p><strong>Use IAM user secret access key and access key ID</strong> - Access keys are long-term credentials for an IAM user or the AWS account root user. You can use access keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK). As a best practice, AWS suggests using temporary security credentials (IAM roles) instead of access keys.</p>\n\n<p><strong>Use authentication offered by GitHub secure tokens</strong> - Personal access tokens (PATs) are an alternative to using passwords for authentication to GitHub when using the GitHub API or the command line. This option is specific to GitHub only and hence not useful for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html</a></p>\n",
                "options": [
                    {
                        "id": 2132,
                        "content": "<p>Use Git credentials generated from IAM</p>",
                        "isValid": true
                    },
                    {
                        "id": 2133,
                        "content": "<p>Use authentication offered by GitHub secure tokens</p>",
                        "isValid": false
                    },
                    {
                        "id": 2134,
                        "content": "<p>Use IAM user secret access key and access key ID</p>",
                        "isValid": false
                    },
                    {
                        "id": 2135,
                        "content": "<p>Use IAM Multi-Factor authentication</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 521,
            "attributes": {
                "createdAt": "2023-09-07T08:39:45.314Z",
                "updatedAt": "2023-09-07T08:39:45.314Z",
                "content": "<p>You have been hired at a company that needs an experienced developer to help with a continuous integration/continuous delivery (CI/CD) workflow on AWS. You configure the companyâ€™s workflow to run an AWS CodePipeline pipeline whenever the applicationâ€™s source code changes in a repository hosted in AWS Code Commit and compiles source code with AWS Code Build. You are configuring ProjectArtifacts in your build stage.</p>\n\n<p>Which of the following should you do?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Give AWS CodeBuild permissions to upload the build output to your Amazon S3 bucket</strong></p>\n\n<p>If you choose ProjectArtifacts and your value type is S3 then the build project stores build output in Amazon Simple Storage Service (Amazon S3). For that, you will need to give AWS CodeBuild permissions to upload.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS CodeBuild to store output artifacts on EC2 servers</strong> - EC2 servers are not a valid output location, so this option is ruled out.</p>\n\n<p><strong>Give AWS CodeCommit permissions to upload the build output to your Amazon S3 bucket</strong> - AWS CodeCommit is the repository that holds source code and has no control over compiling the source code, so this option is incorrect.</p>\n\n<p><strong>Contact AWS Support to allow AWS CodePipeline to manage build outputs</strong> - You can set AWS CodePipeline to manage its build output locations instead of AWS CodeBuild. There is no need to contact AWS Support.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/create-project.html#create-project-cli\">https://docs.aws.amazon.com/codebuild/latest/userguide/create-project.html#create-project-cli</a></p>\n",
                "options": [
                    {
                        "id": 2136,
                        "content": "<p>Give AWS CodeBuild permissions to upload the build output to your Amazon S3 bucket</p>",
                        "isValid": true
                    },
                    {
                        "id": 2137,
                        "content": "<p>Contact AWS Support to allow AWS CodePipeline to manage build outputs</p>",
                        "isValid": false
                    },
                    {
                        "id": 2138,
                        "content": "<p>Give AWS CodeCommit permissions to upload the build output to your Amazon S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 2139,
                        "content": "<p>Configure AWS CodeBuild to store output artifacts on EC2 servers</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 522,
            "attributes": {
                "createdAt": "2023-09-07T08:39:45.403Z",
                "updatedAt": "2023-09-07T08:39:45.403Z",
                "content": "<p>A voting system hosted on-premise was recently migrated to AWS to lower cost, gain scalability, and to better serve thousands of concurrent users. When one of the AWS resource state changes, it generates an event and will need to trigger AWS Lambda. The AWS resource whose state changes and AWS Lambda does not have direct integration.</p>\n\n<p>Which of the following methods can be used to trigger AWS Lambda?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>CloudWatch Events Rules with AWS Lambda</strong></p>\n\n<p>You can create a Lambda function and direct CloudWatch Events to execute it on a regular schedule. You can specify a fixed rate (for example, execute a Lambda function every hour or 15 minutes), or you can specify a Cron expression.</p>\n\n<p>CloudWatch Events Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q29-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p>Schedule Expressions for CloudWatch Events Rules:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q29-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Lambda Custom Sources</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p><strong>Open a support ticket with AWS</strong> - You can, although the AWS support team will not add a custom configuration for you, they will step you through creating event rule with Lambda.</p>\n\n<p><strong>Cron jobs to trigger AWS Lambda to check the state of your service</strong> - You would need an additional server for your cron job instead you should consider using a cron expression with CloudWatch.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html</a></p>\n",
                "options": [
                    {
                        "id": 2140,
                        "content": "<p>CloudWatch Events Rules with AWS Lambda</p>",
                        "isValid": true
                    },
                    {
                        "id": 2141,
                        "content": "<p>Open a support ticket with AWS</p>",
                        "isValid": false
                    },
                    {
                        "id": 2142,
                        "content": "<p>AWS Lambda Custom Sources</p>",
                        "isValid": false
                    },
                    {
                        "id": 2143,
                        "content": "<p>Cron jobs to trigger AWS Lambda to check the state of your service</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 523,
            "attributes": {
                "createdAt": "2023-09-07T08:39:45.507Z",
                "updatedAt": "2023-09-07T08:39:45.507Z",
                "content": "<p>You would like to paginate the results of an S3 List to show 100 results per page to your users and minimize the number of API calls that you will use.</p>\n\n<p>Which CLI options should you use? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>--max-items</strong></p>\n\n<p><strong>--starting-token</strong></p>\n\n<p>For commands that can return a large list of items, the AWS Command Line Interface (AWS CLI) has three options to control the number of items included in the output when the AWS CLI calls a service's API to populate the list.</p>\n\n<p><code>--page-size</code></p>\n\n<p><code>--max-items</code></p>\n\n<p><code>--starting-token</code></p>\n\n<p>By default, the AWS CLI uses a page size of 1000 and retrieves all available items. For example, if you run <code>aws s3api list-objects</code> on an Amazon S3 bucket that contains 3,500 objects, the AWS CLI makes four calls to Amazon S3, handling the service-specific pagination logic for you in the background and returning all 3,500 objects in the final output.</p>\n\n<p>Here's an example: <code>aws s3api list-objects --bucket my-bucket --max-items 100 --starting-token eyJNYXJrZXIiOiBudWxsLCAiYm90b190cnVuY2F0ZV9hbW91bnQiOiAxfQ==</code></p>\n\n<p>Incorrect options:</p>\n\n<p>\"--page-size\" - You can use the <code>--page-size</code> option to specify that the AWS CLI requests a smaller number of items from each call to the AWS service. The CLI still retrieves the full list but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call.</p>\n\n<p>\"--next-token\" - This is a made-up option and has been added as a distractor.</p>\n\n<p>\"--limit\" - This is a made-up option and has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html\">https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html</a></p>\n",
                "options": [
                    {
                        "id": 2144,
                        "content": "<p>--max-items</p>",
                        "isValid": true
                    },
                    {
                        "id": 2145,
                        "content": "<p>--next-token</p>",
                        "isValid": false
                    },
                    {
                        "id": 2146,
                        "content": "<p>--page-size</p>",
                        "isValid": false
                    },
                    {
                        "id": 2147,
                        "content": "<p>--starting-token</p>",
                        "isValid": true
                    },
                    {
                        "id": 2148,
                        "content": "<p>--limit</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 524,
            "attributes": {
                "createdAt": "2023-09-07T08:39:45.583Z",
                "updatedAt": "2023-09-07T08:39:45.583Z",
                "content": "<p>You are creating a web application in which users can follow each other. Some users will be more popular than others and thus their data will be requested very often. Currently, the user data sits in RDS and it has been recommended by your Developer to use ElastiCache as a caching layer to improve the read performance. The whole dataset of users cannot sit in ElastiCache without incurring tremendous costs and therefore you would like to cache only the most often requested users profiles there. As your website is high traffic, it is accepted to have stale data for users for a while, as long as the stale data is less than a minute old.</p>\n\n<p>What caching strategy do you recommend implementing?</p>",
                "answerExplanation": "<p>Correct option</p>\n\n<p><strong>Use a Lazy Loading strategy with TTL</strong></p>\n\n<p>Lazy loading is a caching strategy that loads data into the cache only when necessary. Whenever your application requests data, it first requests the ElastiCache cache. If the data exists in the cache and is current, ElastiCache returns the data to your application. If the data doesn't exist in the cache or has expired, your application requests the data from your data store. Your datastore then returns the data to your application.</p>\n\n<p>In this case, data that is actively requested by users will be cached in ElastiCache, and thanks to the TTL, we can expire that data after a minute to limit the data staleness.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q42-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading</a></p>\n\n<p>Incorrect option:</p>\n\n<p><strong>Use a Lazy Loading strategy without TTL</strong> - This fits the read requirements, but won't help expiring stale data, so we need TTL.</p>\n\n<p><strong>Use a Write Through strategy with TTL</strong></p>\n\n<p><strong>Use a Write Through strategy without TTL</strong></p>\n\n<p>The problem with these two options for the write-through strategy is that we would fill up the cache with unnecessary data and as mentioned in the question we don't have enough space in the cache to fit all the dataset. Therefore we can't use a write-through strategy.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading</a></p>\n",
                "options": [
                    {
                        "id": 2149,
                        "content": "<p>Use a Lazy Loading strategy without TTL</p>",
                        "isValid": false
                    },
                    {
                        "id": 2150,
                        "content": "<p>Use a Write Through strategy with TTL</p>",
                        "isValid": false
                    },
                    {
                        "id": 2151,
                        "content": "<p>Use a Write Through strategy without TTL</p>",
                        "isValid": false
                    },
                    {
                        "id": 2152,
                        "content": "<p>Use a Lazy Loading strategy with TTL</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 525,
            "attributes": {
                "createdAt": "2023-09-07T08:39:45.658Z",
                "updatedAt": "2023-09-07T08:39:45.658Z",
                "content": "<p>You have created a DynamoDB table to support your application and provisioned RCU and WCU to it so that your application has been running for over a year now without any throttling issues. Your application now requires a second type of query over your table and as such, you have decided to create an LSI and a GSI on a new table to support that use case. One month after having implemented such indexes, it seems your table is experiencing throttling.</p>\n\n<p>Upon looking at the table's metrics, it seems the RCU and WCU provisioned are still sufficient. What's happening?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The GSI is throttling so you need to provision more RCU and WCU to the GSI</strong></p>\n\n<p>DynamoDB supports two types of secondary indexes:</p>\n\n<p>Global secondary index â€” An index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions. A global secondary index is stored in its own partition space away from the base table and scales separately from the base table.</p>\n\n<p>Local secondary index â€” An index that has the same partition key as the base table, but a different sort key. A local secondary index is \"local\" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value.</p>\n\n<p>Differences between GSI and LSI:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q30-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n\n<p>If you perform heavy write activity on the table, but a global secondary index on that table has insufficient write capacity, then the write activity on the table will be throttled. To avoid potential throttling, the provisioned write capacity for a global secondary index should be equal or greater than the write capacity of the base table since new updates will write to both the base table and global secondary index.</p>\n\n<p>Incorrect options</p>\n\n<p><strong>The LSI is throttling so you need to provision more RCU and WCU to the LSI</strong> - LSI use the RCU and WCU of the main table, so you can't provision more RCU and WCU to the LSI.</p>\n\n<p><strong>Adding both an LSI and a GSI to a table is not recommended by AWS best practices as this is a known cause for creating throttles</strong> - This option has been added as a distractor. It is fine to have LSI and GSI together.</p>\n\n<p><strong>Metrics are lagging in your CloudWatch dashboard and you should see the RCU and WCU peaking for the main table in a few minutes</strong> - This could be a reason, but in this case, the GSI is at fault as the application has been running fine for months.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.ThroughputConsiderations\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.ThroughputConsiderations</a></p>\n",
                "options": [
                    {
                        "id": 2153,
                        "content": "<p>Adding both an LSI and a GSI to a table is not recommended by AWS best practices as this is a known cause for creating throttles</p>",
                        "isValid": false
                    },
                    {
                        "id": 2154,
                        "content": "<p>The LSI is throttling so you need to provision more RCU and WCU to the LSI</p>",
                        "isValid": false
                    },
                    {
                        "id": 2155,
                        "content": "<p>The GSI is throttling so you need to provision more RCU and WCU to the GSI</p>",
                        "isValid": true
                    },
                    {
                        "id": 2156,
                        "content": "<p>Metrics are lagging in your CloudWatch dashboard and you should see the RCU and WCU peaking for the main table in a few minutes</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 526,
            "attributes": {
                "createdAt": "2023-09-07T08:39:45.730Z",
                "updatedAt": "2023-09-07T08:39:45.730Z",
                "content": "<p>A development team has a mix of applications hosted on-premises as well as on EC2 instances. The on-premises application controls all applications deployed on the EC2 instances. In case of any errors, the team wants to leverage Amazon CloudWatch to monitor and troubleshoot the on-premises application.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure the CloudWatch agent on the on-premises server by using IAM user credentials with permissions for CloudWatch</strong></p>\n\n<p>The CloudWatch agent enables you to do the following:</p>\n\n<p>Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.</p>\n\n<p>Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p>\n\n<p>To enable the CloudWatch agent to send data from an on-premises server, you must specify the access key and secret key of the IAM user that you created earlier.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure CloudWatch Logs to directly read the logs from the on-premises server</strong> - This is a made-up option as you cannot have CloudWatch Logs directly communicate with the on-premises server. You have to go via the CloudWatch Agent.</p>\n\n<p><strong>Upload log files from the on-premises server to an EC2 instance which further forwards the logs to CloudWatch</strong></p>\n\n<p><strong>Upload log files from the on-premises server to S3 and let CloudWatch process the files from S3</strong></p>\n\n<p>Both these options require significant customizations and still will not be as neatly integrated with CloudWatch as compared to just using the CloudWatch Agent which is available off-the-shelf.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p>\n",
                "options": [
                    {
                        "id": 2157,
                        "content": "<p>Configure the CloudWatch agent on the on-premises server by using IAM user credentials with permissions for CloudWatch</p>",
                        "isValid": true
                    },
                    {
                        "id": 2158,
                        "content": "<p>Upload log files from the on-premises server to an EC2 instance which further forwards the logs to CloudWatch</p>",
                        "isValid": false
                    },
                    {
                        "id": 2159,
                        "content": "<p>Upload log files from the on-premises server to S3 and let CloudWatch process the files from S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 2160,
                        "content": "<p>Configure CloudWatch Logs to directly read the logs from the on-premises server</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 527,
            "attributes": {
                "createdAt": "2023-09-07T08:39:45.798Z",
                "updatedAt": "2023-09-07T08:39:45.798Z",
                "content": "<p>Your Lambda function must use the Node.js drivers to connect to your RDS PostgreSQL database in your VPC.</p>\n\n<p>How do you bundle your Lambda function to add the dependencies?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\">\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p><strong>Put the function and the dependencies in one folder and zip them together</strong></p>\n\n<p>A deployment package is a ZIP archive that contains your function code and dependencies. You need to create a deployment package if you use the Lambda API to manage functions, or if you need to include libraries and dependencies other than the AWS SDK. You can upload the package directly to Lambda, or you can use an Amazon S3 bucket, and then upload it to Lambda. If the deployment package is larger than 50 MB, you must use Amazon S3. This is the standard way of packaging Lambda functions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Zip the function as-is with a package.json file so that AWS Lambda can resolve the dependencies for you</strong></p>\n\n<p><strong>Upload the code through the AWS console and upload the dependencies as a zip</strong></p>\n\n<p><strong>Zip the function and the dependencies separately and upload them in AWS Lambda as two parts</strong></p>\n\n<p>These three options are incorrect as there's only one way of deploying a Lambda function, which is to provide the zip file with all dependencies that it'll need.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html\">https://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html</a></p>\n",
                "options": [
                    {
                        "id": 2161,
                        "content": "<p>Zip the function and the dependencies separately and upload them in AWS Lambda as two parts</p>",
                        "isValid": false
                    },
                    {
                        "id": 2162,
                        "content": "<p>Put the function and the dependencies in one folder and zip them together</p>",
                        "isValid": true
                    },
                    {
                        "id": 2163,
                        "content": "<p>Zip the function as-is with a package.json file so that AWS Lambda can resolve the dependencies for you</p>",
                        "isValid": false
                    },
                    {
                        "id": 2164,
                        "content": "<p>Upload the code through the AWS console and upload the dependencies as a zip</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 528,
            "attributes": {
                "createdAt": "2023-09-07T08:39:45.871Z",
                "updatedAt": "2023-09-07T08:39:45.871Z",
                "content": "<p>You would like your Elastic Beanstalk environment to expose an HTTPS endpoint instead of an HTTP endpoint to get in-flight encryption between your clients and your web servers.</p>\n\n<p>What must be done to set up HTTPS on Beanstalk?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>The simplest way to use HTTPS with an Elastic Beanstalk environment is to assign a server certificate to your environment's load balancer. When you configure your load balancer to terminate HTTPS, the connection between the client and the load balancer is secure. Backend connections between the load balancer and EC2 instances use HTTP, so no additional configuration of the instances is required.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q12-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html</a></p>\n\n<p><strong>Create a config file in the .ebextensions folder to configure the Load Balancer</strong></p>\n\n<p>To update your AWS Elastic Beanstalk environment to use HTTPS, you need to configure an HTTPS listener for the load balancer in your environment. Two types of load balancers support an HTTPS listener: Classic Load Balancer and Application Load Balancer.</p>\n\n<p>Example <code>.ebextensions/securelistener-alb.config</code></p>\n\n<p>Use this example when your environment has an Application Load Balancer. The example uses options in the aws:elbv2:listener namespace to configure an HTTPS listener on port 443 with the specified certificate. The listener routes traffic to the default process.</p>\n\n<pre><code>option_settings:\n  aws:elbv2:listener:443:\n    ListenerEnabled: 'true'\n    Protocol: HTTPS\n    SSLCertificateArns: arn:aws:acm:us-east-2:1234567890123:certificate/####################################\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a separate CloudFormation template to load the SSL certificate onto the Load Balancer</strong> - A separate CloudFormation template won't be able to mutate the state of a Load Balancer managed by Elastic Beanstalk, so this option is incorrect.</p>\n\n<p><strong>Open up the port 80 for the security group</strong> - Port 80 is for HTTP traffic, so this option is incorrect.</p>\n\n<p><strong>Configure Health Checks</strong> - Health Checks are not related to SSL certificates, so this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html</a></p>\n",
                "options": [
                    {
                        "id": 2165,
                        "content": "<p>Create a config file in the .ebextensions folder to configure the Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 2166,
                        "content": "<p>Open up the port 80 for the security group</p>",
                        "isValid": false
                    },
                    {
                        "id": 2167,
                        "content": "<p>Use a separate CloudFormation template to load the SSL certificate onto the Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 2168,
                        "content": "<p>Configure Health Checks</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 529,
            "attributes": {
                "createdAt": "2023-09-07T08:39:45.957Z",
                "updatedAt": "2023-09-07T08:39:45.957Z",
                "content": "<p>A data analytics company ingests a large number of messages and stores them in an RDS database using Lambda. Because of the increased payload size, it is taking more than 15 minutes to process each message.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend to process each message in the MOST scalable way?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Provision EC2 instances in an Auto Scaling group to poll the messages from an SQS queue</strong></p>\n\n<p>As each message takes more than 15 minutes to process, therefore the development team cannot use Lambda for message processing. To build a scalable solution, the EC2 instances must be provisioning via an Auto Scaling group to handle variations in the message processing workload.</p>\n\n<p>Amazon EC2 Auto Scaling Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provision an EC2 instance to poll the messages from an SQS queue</strong> - Just using a single EC2 instance may not be sufficient to handle a sudden spike in the number of incoming messages.</p>\n\n<p><strong>Contact AWS Support to increase the Lambda timeout to 60 minutes</strong> - AWS Support cannot increase the Lambda timeout upper limit.</p>\n\n<p><strong>Use DynamoDB instead of RDS as database</strong> - This option has been added as a distractor, as changing the database would have no impact on the Lambda timeout while processing the message.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n",
                "options": [
                    {
                        "id": 2169,
                        "content": "<p>Use DynamoDB instead of RDS as database</p>",
                        "isValid": false
                    },
                    {
                        "id": 2170,
                        "content": "<p>Provision an EC2 instance to poll the messages from an SQS queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 2171,
                        "content": "<p>Provision EC2 instances in an Auto Scaling group to poll the messages from an SQS queue</p>",
                        "isValid": true
                    },
                    {
                        "id": 2172,
                        "content": "<p>Contact AWS Support to increase the Lambda timeout to 60 minutes</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 530,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.037Z",
                "updatedAt": "2023-09-07T08:39:46.037Z",
                "content": "<p>An e-commerce company has deployed its application on AWS Elastic Beanstalk. The Auto Scaling group associated with the Beanstalk environment has three Amazon EC2 instances. When the number of instances falls below two, it severely impacts the performance of the web application. The company currently uses the default all-at-once deployment policy and is looking for an effective strategy for future deployments.</p>\n\n<p>Which of the following represents the most cost-effective deployment strategy for the company?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Opt for rolling with additional batch deployment strategy. Set the batch size parameter to 1</strong></p>\n\n<p>With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. It leaves the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version.</p>\n\n<p>To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.</p>\n\n<p>For the rolling and rolling with additional batch deployment policies, you can configure Batch size â€“ The size of the set of instances to deploy in each batch. Choose Percentage to configure a percentage of the total number of EC2 instances in the Auto Scaling group (up to 100 percent), or choose Fixed to configure a fixed number of instances (up to the maximum instance count in your environment's Auto Scaling configuration).</p>\n\n<p>Example configuration for rolling With additional batch:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Opt for a traffic-splitting deployment strategy with a traffic split parameter set to 50% of the total traffic</strong> - Traffic-splitting deployments let you perform canary testing as part of your application deployment. In a traffic-splitting deployment, Elastic Beanstalk launches a full set of new instances just like during an immutable deployment. It then forwards a specified percentage of incoming client traffic to the new application version for a specified evaluation period. This turns out to be a costly solution for the given use case.</p>\n\n<p><strong>Configure an Elastic Load Balancer to front the Auto Scaling Group and choose two different Availability Zones (AZs) for deployment</strong> - Adding AZs makes the entire configuration resilient for failures post-deployment. However, it does not represent a valid deployment strategy for the given use case. This option has been added as a distractor.</p>\n\n<p><strong>Opt for a rolling deployment strategy. Set the batch size to 2</strong> - As already discussed, with rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. With a batch size set to 2, only 1 instance will be left for traffic during deployment, impacting the performance of the e-commerce application. Hence, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n",
                "options": [
                    {
                        "id": 2173,
                        "content": "<p>Opt for rolling with additional batch deployment strategy. Set the batch size parameter to 1</p>",
                        "isValid": true
                    },
                    {
                        "id": 2174,
                        "content": "<p>Opt for traffic-splitting deployment strategy with traffic split parameter set to 50% of the total traffic</p>",
                        "isValid": false
                    },
                    {
                        "id": 2175,
                        "content": "<p>Opt for rolling deployment strategy. Set the batch size to 2</p>",
                        "isValid": false
                    },
                    {
                        "id": 2176,
                        "content": "<p>Configure an Elastic Load Balancer to front the Auto Scaling Group and choose two different Availability Zones (AZs) for deployment</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 531,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.109Z",
                "updatedAt": "2023-09-07T08:39:46.109Z",
                "content": "<p>A company's e-commerce application becomes slow when traffic spikes. The application has a three-tier architecture (web, application and database tier) that uses synchronous transactions. The development team at the company has identified certain bottlenecks in the application tier and it is looking for a long term solution to improve the application's performance.</p>\n\n<p>As a developer associate, which of the following solutions would you suggest to meet the required application response times while accounting for any traffic spikes?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer</strong> - A horizontally scalable system is one that can increase capacity by adding more computers to the system. This is in contrast to a vertically scalable system, which is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory or storage.</p>\n\n<p>Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different computers.</p>\n\n<p>Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed.</p>\n\n<p>To use Elastic Load Balancing with your Auto Scaling group, you attach the load balancer to your Auto Scaling group to register the group with the load balancer. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group.</p>\n\n<p>When you use Elastic Load Balancing with your Auto Scaling group, it's not necessary to register individual EC2 instances with the load balancer. Instances that are launched by your Auto Scaling group are automatically registered with the load balancer. Likewise, instances that are terminated by your Auto Scaling group are automatically deregistered from the load balancer.</p>\n\n<p>This option will require fewer design changes, it's mostly configuration changes and the ability for the web/application tier to be able to communicate across instances. Hence, this is the right solution for the current use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage SQS with asynchronous AWS Lambda calls to decouple the application and data tiers</strong> - This is incorrect as it uses asynchronous AWS Lambda calls and the application uses synchronous transactions. The question says there should be no change in the application architecture.</p>\n\n<p><strong>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS</strong> - The issue is not with the persistence layer at all. This option has only been used as a distractor.</p>\n\n<p>You can deploy scalable Oracle Real Application Clusters (RAC) on Amazon EC2 using Amazon Machine Images (AMI) on AWS Marketplace. Oracle RAC is a shared-everything database cluster technology from Oracle that allows a single database (a set of data files) to be concurrently accessed and served by one or many database server instances.</p>\n\n<p><strong>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size</strong> - Vertical scaling is just a band-aid solution and will not work long term.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/\">https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/</a></p>\n",
                "options": [
                    {
                        "id": 2177,
                        "content": "<p>Leverage SQS with asynchronous AWS Lambda calls to decouple the application and data tiers</p>",
                        "isValid": false
                    },
                    {
                        "id": 2178,
                        "content": "<p>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS</p>",
                        "isValid": false
                    },
                    {
                        "id": 2179,
                        "content": "<p>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size</p>",
                        "isValid": false
                    },
                    {
                        "id": 2180,
                        "content": "<p>Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 532,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.183Z",
                "updatedAt": "2023-09-07T08:39:46.183Z",
                "content": "<p>A developer has defined a Lambda integration in Amazon API Gateway using a stage variable. However, when the developer invokes the API method, it consistently returns an \"Internal server error\" and a 500 status code.</p>\n\n<p>What steps should the developer take to fix the issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>If you create a stage variable to call a Lambda function through your API, you must add the required permissions. Update your Lambda function's resource-based AWS Identity and Access Management (IAM) policy so that it grants invoke permission to the API Gateway</strong></p>\n\n<p>If your Lambda function's resource-based policy doesn't include permissions for your API to invoke the function, API Gateway returns an Internal server error message.</p>\n\n<p>If you create a stage variable to call a function through your API, you must add the required permissions by doing one of the following:</p>\n\n<p>Update your Lambda function's resource-based AWS Identity and Access Management (IAM) policy so that it grants invoke permission to API Gateway.</p>\n\n<p>OR</p>\n\n<p>Create an IAM role that API Gateway can assume to invoke your Lambda function.</p>\n\n<p>If you build an API Gateway API with standard Lambda integration using the API Gateway console, the console adds the required permissions automatically.</p>\n\n<p>Example resource-based policy that grants invoke permission to API Gateway:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q48-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-lambda-stage-variable-500/\">https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-lambda-stage-variable-500/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>API calls can't exceed the maximum allowed API request rate per account and per Region. Implement error retries and exponential backoffs to fix the error</strong> - When API calls exceed the maximum allowed API request rate per account and per Region, the error received is a \"Rate Exceeded\" error and not an \"Internal server error\". Hence, this option is irrelevant to the given use case.</p>\n\n<p><strong>If you create a stage variable to call a function through your API, you must add the required permissions. Create an IAM role that your Lambda function can assume when invoking the respective AWS resources</strong> - This option is incorrect. However, creating an IAM role that API Gateway can assume to invoke the Lambda function is another solution to fix the given problem.</p>\n\n<p><strong>When setting the Lambda function as the value of a stage variable, use the function's ARN and not the function alias for setting up the value</strong> - This statement is incorrect. When setting a Lambda function as the value of a stage variable, use the function's local name, possibly including its alias or version specification. Do not use the function's ARN. The API Gateway console assumes the stage variable value for a Lambda function as the unqualified function name and expands the given stage variable into an ARN.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-set-stage-variables-aws-console.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-set-stage-variables-aws-console.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-lambda-stage-variable-500/\">https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-lambda-stage-variable-500/</a></p>\n",
                "options": [
                    {
                        "id": 2181,
                        "content": "<p>If you create a stage variable to call a Lambda function through your API, you must add the required permissions. Update your Lambda function's resource-based AWS Identity and Access Management (IAM) policy so that it grants invoke permission to the API Gateway</p>",
                        "isValid": true
                    },
                    {
                        "id": 2182,
                        "content": "<p>If you create a stage variable to call a function through your API, you must add the required permissions. Create an IAM role that your Lambda function can assume when invoking the respective AWS resources</p>",
                        "isValid": false
                    },
                    {
                        "id": 2183,
                        "content": "<p>When setting the Lambda function as the value of a stage variable, use the function's ARN and not the function alias for setting up the value</p>",
                        "isValid": false
                    },
                    {
                        "id": 2184,
                        "content": "<p>API calls can't exceed the maximum allowed API request rate per account and per Region. Implement error retries and exponential backoffs to fix the error</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 533,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.251Z",
                "updatedAt": "2023-09-07T08:39:46.251Z",
                "content": "<p>You are responsible for an application that runs on multiple Amazon EC2 instances. In front of the instances is an Internet-facing load balancer that takes requests from clients over the internet and distributes them to the EC2 instances. A health check is configured to ping the index.html page found in the root directory for the health status. When accessing the website via the internet visitors of the website receive timeout errors.</p>\n\n<p>What should be checked first to resolve the issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Security Groups</strong></p>\n\n<p>A security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance.</p>\n\n<p>Check the security group rules of your EC2 instance. You need a security group rule that allows inbound traffic from your public IPv4 address on the proper port.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>IAM Roles</strong> - Usually you run into issues with authorization of APIs with roles but not for timeout, so this option does not fit the given use-case.</p>\n\n<p><strong>The application is down</strong> - Although you can set a health check for application ping or HTTP, timeouts are usually caused by blocked firewall access.</p>\n\n<p><strong>The ALB is warming up</strong> - ALB has a slow start mode which allows a warm-up period before being able to respond to requests with optimal performance. So this is not the issue.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html#TroubleshootingInstancesConnectionTimeout\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html#TroubleshootingInstancesConnectionTimeout</a></p>\n",
                "options": [
                    {
                        "id": 2185,
                        "content": "<p>The ALB is warming up</p>",
                        "isValid": false
                    },
                    {
                        "id": 2186,
                        "content": "<p>Security Groups</p>",
                        "isValid": true
                    },
                    {
                        "id": 2187,
                        "content": "<p>IAM Roles</p>",
                        "isValid": false
                    },
                    {
                        "id": 2188,
                        "content": "<p>The application is down</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 534,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.330Z",
                "updatedAt": "2023-09-07T08:39:46.330Z",
                "content": "<p>You would like to retrieve a subset of your dataset stored in S3 with the CSV format. You would like to retrieve a month of data and only 3 columns out of the 10.</p>\n\n<p>You need to minimize compute and network costs for this, what should you use?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>S3 Select</strong></p>\n\n<p>S3 Select enables applications to retrieve only a subset of data from an object by using simple SQL expressions. By using S3 Select to retrieve only the data needed by your application, you can achieve drastic performance increases in many cases you can get as much as a 400% improvement.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q19-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/s3-glacier-select/\">https://aws.amazon.com/blogs/aws/s3-glacier-select/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>S3 Inventory</strong> - Amazon S3 inventory is one of the tools Amazon S3 provides to help manage your storage. You can use it to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs.</p>\n\n<p><strong>S3 Analytics</strong> - By using Amazon S3 analytics storage class analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class.</p>\n\n<p><strong>S3 Access Logs</strong> - Server access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/s3-glacier-select/\">https://aws.amazon.com/blogs/aws/s3-glacier-select/</a></p>\n",
                "options": [
                    {
                        "id": 2189,
                        "content": "<p>S3 Access Logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 2190,
                        "content": "<p>S3 Analytics</p>",
                        "isValid": false
                    },
                    {
                        "id": 2191,
                        "content": "<p>S3 Select</p>",
                        "isValid": true
                    },
                    {
                        "id": 2192,
                        "content": "<p>S3 Inventory</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 535,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.417Z",
                "updatedAt": "2023-09-07T08:39:46.417Z",
                "content": "<p>A developer has created a new Application Load Balancer but has not registered any targets with the target groups.</p>\n\n<p>Which of the following errors would be generated by the Load Balancer?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>HTTP 503: Service unavailable</strong></p>\n\n<p>The Load Balancer generates the <code>HTTP 503: Service unavailable</code> error when the target groups for the load balancer have no registered targets.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>HTTP 500: Internal server error</strong></p>\n\n<p><strong>HTTP 502: Bad gateway</strong></p>\n\n<p><strong>HTTP 504: Gateway timeout</strong></p>\n\n<p>Here is a summary of the possible causes for these error types:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html</a></p>\n",
                "options": [
                    {
                        "id": 2193,
                        "content": "<p>HTTP 504: Gateway timeout</p>",
                        "isValid": false
                    },
                    {
                        "id": 2194,
                        "content": "<p>HTTP 503: Service unavailable</p>",
                        "isValid": true
                    },
                    {
                        "id": 2195,
                        "content": "<p>HTTP 500: Internal server error</p>",
                        "isValid": false
                    },
                    {
                        "id": 2196,
                        "content": "<p>HTTP 502: Bad gateway</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 536,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.486Z",
                "updatedAt": "2023-09-07T08:39:46.486Z",
                "content": "<p>You are running a public DNS service on an EC2 instance where the DNS name is pointing to the IP address of the instance. You wish to upgrade your DNS service but would like to do it without any downtime.</p>\n\n<p>Which of the following options will help you accomplish this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Route 53 is a DNS managed by AWS, but nothing prevents you from running your own DNS (it's just a software) on an EC2 instance. The trick of this question is that it's about EC2, running some software that needs a fixed IP, and not about Route 53 at all.</p>\n\n<p><strong>Elastic IP</strong></p>\n\n<p>DNS services are identified by a public IP, so you need to use Elastic IP.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Load Balancer and an auto-scaling group</strong> - Load balancers do not provide an IP, instead they provide a DNS name, so this option is ruled out.</p>\n\n<p><strong>Provide a static private IP</strong> - If you provide a private IP it will not be accessible from the internet, so this option is incorrect.</p>\n\n<p><strong>Use Route 53</strong> - Route 53 is a DNS service from AWS but the use-case talks about offering a DNS service using an EC2 instance, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html#using-instance-addressing-eips-associating-different\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html#using-instance-addressing-eips-associating-different</a></p>\n",
                "options": [
                    {
                        "id": 2197,
                        "content": "<p>Use Route 53</p>",
                        "isValid": false
                    },
                    {
                        "id": 2198,
                        "content": "<p>Provide a static private IP</p>",
                        "isValid": false
                    },
                    {
                        "id": 2199,
                        "content": "<p>Create a Load Balancer and an auto scaling group</p>",
                        "isValid": false
                    },
                    {
                        "id": 2200,
                        "content": "<p>Elastic IP</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 537,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.553Z",
                "updatedAt": "2023-09-07T08:39:46.553Z",
                "content": "<p>You have been collecting AWS X-Ray traces across multiple applications and you would now like to index your XRay traces to search and filter through them efficiently.</p>\n\n<p>What should you use in your instrumentation?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Annotations</strong></p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your applicationâ€™s underlying components.</p>\n\n<p>You can use X-Ray to collect data across AWS Accounts. The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p>Annotations are simple key-value pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API.</p>\n\n<p>X-Ray indexes up to 50 annotations per trace.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Metadata</strong> - Metadata are key-value pairs with values of any type, including objects and lists, but that is not indexed. Use metadata to record data you want to store in the trace but don't need to use for searching traces.</p>\n\n<p><strong>Segments</strong> - The computing resources running your application logic send data about their work as segments. A segment provides the resource's name, details about the request, and details about the work done.</p>\n\n<p><strong>Sampling</strong> - To ensure efficient tracing and provide a representative sample of the requests that your application serves, the X-Ray SDK applies a sampling algorithm to determine which requests get traced. By default, the X-Ray SDK records the first request each second, and five percent of any additional requests.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html</a></p>\n",
                "options": [
                    {
                        "id": 2201,
                        "content": "<p>Sampling</p>",
                        "isValid": false
                    },
                    {
                        "id": 2202,
                        "content": "<p>Annotations</p>",
                        "isValid": true
                    },
                    {
                        "id": 2203,
                        "content": "<p>Metadata</p>",
                        "isValid": false
                    },
                    {
                        "id": 2204,
                        "content": "<p>Segments</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 538,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.623Z",
                "updatedAt": "2023-09-07T08:39:46.623Z",
                "content": "<p>A developer created an online shopping application that runs on EC2 instances behind load balancers. The same web application version is hosted on several EC2 instances and the instances run in an Auto Scaling group. The application uses STS to request credentials but after an hour your application stops working.</p>\n\n<p>What is the most likely cause of this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Your application needs to renew the credentials after 1 hour when they expire</strong></p>\n\n<p>AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). By default, AWS Security Token Service (STS) is available as a global service, and all AWS STS requests go to a single endpoint at https://sts.amazonaws.com.</p>\n\n<p>Credentials that are created by using account credentials can range from 900 seconds (15 minutes) up to a maximum of 3,600 seconds (1 hour), with a default of 1 hour. Hence you need to renew the credentials post expiry.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q45-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Your IAM policy is wrong</strong> - If your policy was wrong, a reboot would not solve the issue.</p>\n\n<p><strong>A lambda function revokes your access every hour</strong> - Revoking can be done by an IAM policy. Lambda function cannot revoke access.</p>\n\n<p><strong>The IAM service is experiencing downtime once an hour</strong> - The IAM service is reliable as it's managed by AWS.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html</a></p>\n",
                "options": [
                    {
                        "id": 2205,
                        "content": "<p>The IAM service is experiencing downtime once an hour</p>",
                        "isValid": false
                    },
                    {
                        "id": 2206,
                        "content": "<p>Your application needs to renew the credentials after 1 hour when they expire</p>",
                        "isValid": true
                    },
                    {
                        "id": 2207,
                        "content": "<p>Your IAM policy is wrong</p>",
                        "isValid": false
                    },
                    {
                        "id": 2208,
                        "content": "<p>A lambda function revokes your access every hour</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 539,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.693Z",
                "updatedAt": "2023-09-07T08:39:46.693Z",
                "content": "<p>A company ingests real-time data into its on-premises data center and subsequently a daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 GB.</p>\n\n<p>Which of the following is the fastest way to upload the daily compressed file into S3?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Upload the compressed file using multipart upload with S3 transfer acceleration</strong></p>\n\n<p>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFrontâ€™s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</p>\n\n<p>Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance. If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upload the compressed file in a single operation</strong> - In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput - you can upload parts in parallel to improve throughput. Therefore, this option is not correct.</p>\n\n<p><strong>Upload the compressed file using multipart upload</strong> - Although using multipart upload would certainly speed up the process, combining with S3 transfer acceleration would further improve the transfer speed. Therefore just using multipart upload is not the correct option.</p>\n\n<p><strong>FTP the compressed file into an EC2 instance that runs in the same region as the S3 bucket. Then transfer the file from the EC2 instance into the S3 bucket</strong> -  This is a roundabout process of getting the file into S3 and added as a distractor. Although it is technically feasible to follow this process, it would involve a lot of scripting and certainly would not be the fastest way to get the file into S3.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p>\n",
                "options": [
                    {
                        "id": 2209,
                        "content": "<p>Upload the compressed file in a single operation</p>",
                        "isValid": false
                    },
                    {
                        "id": 2210,
                        "content": "<p>FTP the compressed file into an EC2 instance that runs in the same region as the S3 bucket. Then transfer the file from the EC2 instance into the S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 2211,
                        "content": "<p>Upload the compressed file using multipart upload</p>",
                        "isValid": false
                    },
                    {
                        "id": 2212,
                        "content": "<p>Upload the compressed file using multipart upload with S3 transfer acceleration</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 540,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.764Z",
                "updatedAt": "2023-09-07T08:39:46.764Z",
                "content": "<p>Your company is new to cloud computing and would like to host a static HTML5 website on the cloud and be able to access it via domain www.mycompany.com. You have created a bucket in Amazon Simple Storage Service (S3), enabled website hosting, and set the index.html as the default page. Finally, you create an Alias record in Amazon Route 53 that points to the S3 website endpoint of your S3 bucket.</p>\n\n<p>When you test the domain www.mycompany.com you get the following error: 'HTTP response code 403 (Access Denied)'. What can you do to resolve this error?</p>",
                "answerExplanation": "<p>Correct answer</p>\n\n<p><strong>Create a bucket policy</strong></p>\n\n<p>Bucket policy is an access policy option available for you to grant permission to your Amazon S3 resources. It uses JSON-based access policy language.</p>\n\n<p>If you want to configure an existing bucket as a static website that has public access, you must edit block public access settings for that bucket. You may also have to edit your account-level block public access settings.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q52-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p>\n\n<p>Incorrect:</p>\n\n<p><strong>Create an IAM role</strong> - This will not help because IAM roles are attached to services and in this case, we have public users.</p>\n\n<p><strong>Enable CORS</strong> - CORS defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. Here we are not dealing with cross domains.</p>\n\n<p><strong>Enable Encryption</strong> - For the most part, encryption does not have an effect on access denied/forbidden errors. On the website endpoint, if a user requests an object that doesn't exist, Amazon S3 returns HTTP response code 404 (Not Found). If the object exists but you haven't granted read permission on it, the website endpoint returns HTTP response code 403 (Access Denied).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteAccessPermissionsReqd.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteAccessPermissionsReqd.html</a></p>\n",
                "options": [
                    {
                        "id": 2213,
                        "content": "<p>Enable CORS</p>",
                        "isValid": false
                    },
                    {
                        "id": 2214,
                        "content": "<p>Create an IAM role</p>",
                        "isValid": false
                    },
                    {
                        "id": 2215,
                        "content": "<p>Create a bucket policy</p>",
                        "isValid": true
                    },
                    {
                        "id": 2216,
                        "content": "<p>Enable Encryption</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 541,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.836Z",
                "updatedAt": "2023-09-07T08:39:46.836Z",
                "content": "<p>You would like to run the X-Ray daemon for your Docker containers deployed using AWS Fargate.</p>\n\n<p>What do you need to do to ensure the setup will work? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Deploy the X-Ray daemon agent as a sidecar container</strong></p>\n\n<p>In Amazon ECS, create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file to allow your application to communicate with the daemon container.</p>\n\n<p>As we are using AWS Fargate, we do not have control over the underlying EC2 instance and thus we can't deploy the agent on the EC2 instance or run an X-Ray agent container as a daemon set (only available for ECS classic).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q17-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html</a></p>\n\n<p><strong>Provide the correct IAM task role to the X-Ray container</strong></p>\n\n<p>For Fargate, we can only provide IAM roles to tasks, which is also the best security practice should we use EC2 instances.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the X-Ray daemon agent as a daemon set on ECS</strong> - As explained above, since we are using AWS Fargate, we do not have control over the underlying EC2 instance and thus we can't run an X-Ray agent container as a daemon set.</p>\n\n<p><strong>Deploy the X-Ray daemon agent as a process on your EC2 instance</strong></p>\n\n<p><strong>Provide the correct IAM instance role to the EC2 instance</strong></p>\n\n<p>As we are using AWS Fargate, we do not have control over the underlying EC2 instance, so both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html</a></p>\n",
                "options": [
                    {
                        "id": 2217,
                        "content": "<p>Deploy the X-Ray daemon agent as a process on your EC2 instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 2218,
                        "content": "<p>Provide the correct IAM instance role to the EC2 instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 2219,
                        "content": "<p>Provide the correct IAM task role to the X-Ray container</p>",
                        "isValid": true
                    },
                    {
                        "id": 2220,
                        "content": "<p>Deploy the X-Ray daemon agent as a daemon set on ECS</p>",
                        "isValid": false
                    },
                    {
                        "id": 2221,
                        "content": "<p>Deploy the X-Ray daemon agent as a sidecar container</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 542,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.907Z",
                "updatedAt": "2023-09-07T08:39:46.907Z",
                "content": "<p>You would like to have a one-stop dashboard for all the CI/CD needs of one of your projects. You don't need heavy control of the individual configuration of each component in your CI/CD, but need to be able to get a holistic view of your projects.</p>\n\n<p>Which service do you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>CodeStar</strong></p>\n\n<p>AWS CodeStar enables you to quickly develop, build, and deploy applications on AWS. AWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place. With AWS CodeStar, you can set up your entire continuous delivery toolchain in minutes, allowing you to start releasing code faster. AWS CodeStar makes it easy for your whole team to work together securely, allowing you to easily manage access and add owners, contributors, and viewers to your projects. Each AWS CodeStar project comes with a project management dashboard, including an integrated issue tracking capability powered by Atlassian JIRA Software. With the AWS CodeStar project dashboard, you can easily track progress across your entire software development process, from your backlog of work items to teamsâ€™ recent code deployments.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CodeBuild</strong></p>\n\n<p><strong>CodeDeploy</strong></p>\n\n<p><strong>CodePipeline</strong></p>\n\n<p>All these options are individual services encompassed by CodeStar when you deploy a project. They have to be used individually and don't provide a unified \"project\" view.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/codestar/\">https://aws.amazon.com/codestar/</a></p>\n",
                "options": [
                    {
                        "id": 2222,
                        "content": "<p>CodePipeline</p>",
                        "isValid": false
                    },
                    {
                        "id": 2223,
                        "content": "<p>CodeStar</p>",
                        "isValid": true
                    },
                    {
                        "id": 2224,
                        "content": "<p>CodeDeploy</p>",
                        "isValid": false
                    },
                    {
                        "id": 2225,
                        "content": "<p>CodeBuild</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 543,
            "attributes": {
                "createdAt": "2023-09-07T08:39:46.984Z",
                "updatedAt": "2023-09-07T08:39:46.984Z",
                "content": "<p>A business-critical application is hosted on an Amazon EC2 instance and the latest update is in the testing phase. The business has requested a solution to track the average response time of the application and send a notification to the manager if it exceeds a particular threshold. The update will eventually be implemented in the production environment where the solution will be deployed on multiple EC2 instances.</p>\n\n<p>Which of the following options would you combine to address the requirements of the business? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Configure the application to write the response time to a log file on the EC2 instance. Install and configure the Amazon CloudWatch agent on the EC2 instance to stream the application logs to CloudWatch Logs. Create a metric filter for the response time from the log file</strong></p>\n\n<p><strong>Create a CloudWatch alarm to send an Amazon Simple Notification Service (Amazon SNS) notification when the average of the response time metric exceeds the threshold</strong></p>\n\n<p>You can collect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch agent. The unified CloudWatch agent enables you to do the following:</p>\n\n<ol>\n<li><p>Collect internal system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances.</p></li>\n<li><p>Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.</p></li>\n<li><p>Retrieve custom metrics from your applications or services using the StatsD and collectd protocols. StatsD is supported on both Linux servers and servers running Windows Server. collectd is supported only on Linux servers.</p></li>\n<li><p>Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p></li>\n</ol>\n\n<p>You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is CWAgent, although you can specify a different namespace when you configure the agent.</p>\n\n<p>A metric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. The action can be sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Amazon EC2 Auto Scaling action, or creating an OpsItem or incident in Systems Manager. For this use case, we will configure the metric alarm action to send an SNS notification when the alarm is in an ALARM state.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the application to write the response time to a log file on the instance. Install and configure the Amazon Inspector agent on the EC2 instances to read the logs and send the response time to Amazon EventBridge</strong> - This statement is incorrect. Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container workloads for software vulnerabilities and unintended network exposure. Inspector is not a log-collecting agent.</p>\n\n<p><strong>Configure an EventBridge custom rule to send an Amazon Simple Notification Service (Amazon SNS) notification when the average of the response time metric exceeds the threshold</strong> - Amazon EventBridge integrates with Amazon CloudWatch so that when CloudWatch alarms are triggered, a matching EventBridge rule can execute targets. So we are using CloudWatch alarms in this scenario too and hence a straightforward way would be to use CloudWatch alarm.</p>\n\n<p><strong>Install and configure AWS Systems Manager Agent (SSM Agent) on the EC2 instances to monitor the response time and send the data to Amazon CloudWatch as a custom metric</strong> - This statement is incorrect. AWS Systems Manager Agent (SSM Agent) is Amazon software that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, on-premises servers, and virtual machines (VMs). SSM Agent makes it possible for the Systems Manager to update, manage, and configure these resources. SSM agent is required on each instance that needs to be managed from the Systems Manager service. SSM agents cannot collect log data and push it to CloudWatch logs. However, Systems Manager can be used to automate the task of CloudWatch agent installation on the instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html</a></p>\n",
                "options": [
                    {
                        "id": 2226,
                        "content": "<p>Configure the application to write the response time to a log file on the instance. Install and configure the Amazon Inspector agent on the EC2 instances to read the logs and send the response time to Amazon EventBridge</p>",
                        "isValid": false
                    },
                    {
                        "id": 2227,
                        "content": "<p>Configure an EventBridge custom rule to send an Amazon Simple Notification Service (Amazon SNS) notification when the average of the response time metric exceeds the threshold</p>",
                        "isValid": false
                    },
                    {
                        "id": 2228,
                        "content": "<p>Configure the application to write the response time to a log file on the EC2 instance. Install and configure the Amazon CloudWatch agent on the EC2 instance to stream the application logs to CloudWatch Logs. Create a metric filter for the response time from the log file</p>",
                        "isValid": true
                    },
                    {
                        "id": 2229,
                        "content": "<p>Install and configure AWS Systems Manager Agent (SSM Agent) on the EC2 instances to monitor the response time and send the data to Amazon CloudWatch as a custom metric</p>",
                        "isValid": false
                    },
                    {
                        "id": 2230,
                        "content": "<p>Create a CloudWatch alarm to send an Amazon Simple Notification Service (Amazon SNS) notification when the average of the response time metric exceeds the threshold</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 544,
            "attributes": {
                "createdAt": "2023-09-07T08:39:47.059Z",
                "updatedAt": "2023-09-07T08:39:47.059Z",
                "content": "<p>A company has a new media application that utilizes an Amazon CloudFront distribution that accesses the S3 bucket by using an origin access identity (OAI). The S3 bucket has an explicit access denial for all other users. A developer wants to allow access to the login page for unauthenticated users while ensuring the security of all private content that has restricted viewer access.</p>\n\n<p>Which of the following will you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure a second cache behavior to the distribution having the same origin as the default cache behavior and have the path pattern for the second cache behavior as the path of the login page with viewer access as unrestricted. Keep the default cache behaviorâ€™s settings unchanged</strong></p>\n\n<p>Cache behavior describes how CloudFront processes requests. You must create at least as many cache behaviors (including the default cache behavior) as you have origins if you want CloudFront to serve objects from all of the origins. Each cache behavior specifies the one origin from which you want CloudFront to get objects. If you have two origins and only the default cache behavior, the default cache behavior will cause CloudFront to get objects from one of the origins, but the other origin is never used.</p>\n\n<p>The pattern (for example, images/*.jpg) specifies which requests to apply the behavior to. When CloudFront receives a viewer request, the requested path is compared with path patterns in the order in which cache behaviors are listed in the distribution. The path pattern for the default cache behavior is * and cannot be changed. If the request for an object does not match the path pattern for any cache behaviors, CloudFront applies the behavior in the default cache behavior.</p>\n\n<p>For the given use case, you need to add a second cache behavior to the distribution having the same origin as the default cache behavior and list it above the default cache behavior in the distribution. The second cache behavior should have the path pattern as the path of the login page with viewer access set as unrestricted. This would allow access to the login page for unauthenticated users. Since the default cache behaviorâ€™s settings remain unchanged, it ensures the security of all private content that continues to have restricted viewer access.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a second cache behavior to the distribution having the same origin as the default cache behavior and have the path pattern for the second cache behavior as * with viewer access as restricted. Modify the default cache behaviorâ€™s path pattern to the path of the login page and have the viewer access as unrestricted</strong> - This option is incorrect since the path pattern for the default cache behavior is always * and cannot be changed.</p>\n\n<p><strong>Configure a new distribution having the same origin as the original distribution and set the path pattern for the default cache behavior of the new distribution as the path of the login page with viewer access as unrestricted. Keep the default cache behavior of the original distribution unchanged</strong> - If you have two origins and only the default cache behavior, the default cache behavior will cause CloudFront to get objects from one of the origins, but the other origin is never used. So this option is incorrect.</p>\n\n<p><strong>Configure a second origin as the failover origin for the default behavior of the original distribution and have the path pattern for the second origin as the path of the login page with viewer access as unrestricted. Keep the behavior for the primary origin unchanged</strong> - You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin. To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Finally, you create or update a cache behavior to use the origin group.</p>\n\n<p>This option is incorrect since the failover kicks in only when the primary is unavailable. Therefore, access to the login page and the rest of the content will never work together.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CacheBehavior.html\">https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CacheBehavior.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n",
                "options": [
                    {
                        "id": 2231,
                        "content": "<p>Configure a second cache behavior to the distribution having the same origin as the default cache behavior and have the path pattern for the second cache behavior as the path of the login page with viewer access as unrestricted. Keep the default cache behaviorâ€™s settings unchanged</p>",
                        "isValid": true
                    },
                    {
                        "id": 2232,
                        "content": "<p>Configure a second cache behavior to the distribution having the same origin as the default cache behavior and have the path pattern for the second cache behavior as * with viewer access as restricted. Modify the default cache behaviorâ€™s path pattern to the path of the login page and have the viewer access as unrestricted</p>",
                        "isValid": false
                    },
                    {
                        "id": 2233,
                        "content": "<p>Configure a new distribution having the same origin as the original distribution and set the path pattern for the default cache behavior of the new distribution as the path of the login page with viewer access as unrestricted. Keep the default cache behavior of the original distribution unchanged</p>",
                        "isValid": false
                    },
                    {
                        "id": 2234,
                        "content": "<p>Configure a second origin as the failover origin for the default behavior of the original distribution and have the path pattern for the second origin as the path of the login page with viewer access as unrestricted. Keep the behavior for the primary origin unchanged</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 545,
            "attributes": {
                "createdAt": "2023-09-07T08:39:47.131Z",
                "updatedAt": "2023-09-07T08:39:47.131Z",
                "content": "<p>A financial services company uses Amazon S3 to store transformed and anonymized customer data that is generated by a daily batch job. The development team has been tasked to build a solution that analyzes the output of the daily job for any sensitive financial information about the company's customers.</p>\n\n<p>As an AWS Certified Developer Associate, which of the following options would you recommend to address this use case MOST efficiently?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/Financial</strong></p>\n\n<p>Amazon Macie is a data security service that discovers sensitive data by using machine learning and pattern matching, provides visibility into data security risks, and enables automated protection against those risks. To help you manage the security posture of your organization's Amazon Simple Storage Service (Amazon S3) data estate, Macie provides you with an inventory of your S3 buckets, and automatically evaluates and monitors the buckets for security and access control. If Macie detects a potential issue with the security or privacy of your data, such as a bucket that becomes publicly accessible, Macie generates a finding for you to review and remediate as necessary.</p>\n\n<p>Macie also automates the discovery and reporting of sensitive data to provide you with a better understanding of the data that your organization stores in Amazon S3. To detect sensitive data, you can use built-in criteria and techniques that Macie provides, custom criteria that you define, or a combination of the two. If Macie detects sensitive data in an S3 object, Macie generates a finding to notify you of the sensitive data that Macie found.</p>\n\n<p>Macie generates a sensitive data finding when it detects sensitive data in an S3 object that it analyzes to discover sensitive data. This includes analysis that Macie performs when you run a sensitive data discovery job and when it performs automated sensitive data discovery.</p>\n\n<p>For the given use case, you can use Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/Financial which implies that the S3 object contains financial information, such as bank account numbers or credit card numbers.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/macie/latest/user/findings-types.html\">https://docs.aws.amazon.com/macie/latest/user/findings-types.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/Personal</strong> - The object contains personally identifiable information (such as mailing addresses or driver's license identification numbers), personal health information (such as health insurance or medical identification numbers), or a combination of the two.</p>\n\n<p><strong>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/CustomIdentifier</strong> - The object contains text that matches the detection criteria of one or more custom data identifiers. The object might contain more than one type of sensitive data.</p>\n\n<p>As explained above, for the given use case, you need to look for any sensitive data findings of type SensitiveData:S3Object/Financial. So both these options are incorrect.</p>\n\n<p><strong>Configure a S3 event notification for every object upload that triggers a Lambda function based custom application to detect sensitive customer information</strong> - Maintaining a custom application to detect sensitive information would be highly inefficient as you need to consistently upgrade the application to handle new formats and types of financial information. This is better handled by leveraging Macie's Findings.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/macie/latest/user/what-is-macie.html\">https://docs.aws.amazon.com/macie/latest/user/what-is-macie.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/macie/latest/user/findings-types.html\">https://docs.aws.amazon.com/macie/latest/user/findings-types.html</a></p>\n",
                "options": [
                    {
                        "id": 2235,
                        "content": "<p>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/Financial</p>",
                        "isValid": true
                    },
                    {
                        "id": 2236,
                        "content": "<p>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/Personal</p>",
                        "isValid": false
                    },
                    {
                        "id": 2237,
                        "content": "<p>Configure a S3 event notification for every object upload that triggers a Lambda function based Python script to detect sensitive customer information</p>",
                        "isValid": false
                    },
                    {
                        "id": 2238,
                        "content": "<p>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/CustomIdentifier</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 546,
            "attributes": {
                "createdAt": "2023-09-07T08:39:47.200Z",
                "updatedAt": "2023-09-07T08:39:47.200Z",
                "content": "<p>An e-commerce company has multiple EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a DynamoDB table.</p>\n\n<p>How would you go about providing private access to these AWS resources which are not part of this custom VPC?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</strong></p>\n\n<p>Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.</p>\n\n<p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p>\n\n<p>There are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.</p>\n\n<p>A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported:</p>\n\n<p>Amazon S3</p>\n\n<p>DynamoDB</p>\n\n<p>You should note that S3 now supports both gateway endpoints as well as the interface endpoints.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB service using the private IP address</strong></p>\n\n<p><strong>Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address</strong></p>\n\n<p>DynamoDB does not support interface endpoints, so these two options are incorrect.</p>\n\n<p><strong>Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an API endpoint for S3 and then connect to the S3 service using the private IP address</strong> - There is no such thing as an API endpoint for S3. API endpoints are used with AWS API Gateway. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html</a></p>\n",
                "options": [
                    {
                        "id": 2239,
                        "content": "<p>Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB service using the private IP address</p>",
                        "isValid": false
                    },
                    {
                        "id": 2240,
                        "content": "<p>Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address</p>",
                        "isValid": false
                    },
                    {
                        "id": 2241,
                        "content": "<p>Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</p>",
                        "isValid": true
                    },
                    {
                        "id": 2242,
                        "content": "<p>Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an API endpoint for S3 and then connect to the S3 service using the private IP address</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 547,
            "attributes": {
                "createdAt": "2023-09-07T08:39:47.271Z",
                "updatedAt": "2023-09-07T08:39:47.271Z",
                "content": "<p>You are deploying Lambda functions that operate on your S3 buckets to read files and extract key metadata. The Lambda functions are managed using SAM.</p>\n\n<p>Which Policy should you insert in your serverless model template to give buckets read access?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>S3ReadPolicy</strong></p>\n\n<p>The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications on AWS.</p>\n\n<p>A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda functionâ€”it can include additional resources such as APIs, databases, and event source mappings.</p>\n\n<p>AWS SAM allows you to choose from a list of policy templates to scope the permissions of your Lambda functions to the resources that are used by your application.</p>\n\n<p>AWS SAM applications in the AWS Serverless Application Repository that use policy templates don't require any special customer acknowledgments to deploy the application from the AWS Serverless Application Repository.</p>\n\n<p>S3ReadPolicy =&gt; Gives read-only permission to objects in an Amazon S3 bucket.</p>\n\n<p>S3CrudPolicy =&gt; Gives create, read, update, and delete permission to objects in an Amazon S3 bucket.</p>\n\n<p>SQSPollerPolicy =&gt; Permits to poll an Amazon SQS Queue.</p>\n\n<p>LambdaInvokePolicy =&gt; Permits to invoke a Lambda function, alias, or version.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SQSPollerPolicy</strong></p>\n\n<p><strong>S3CrudPolicy</strong></p>\n\n<p><strong>LambdaInvokePolicy</strong></p>\n\n<p>These three options contradict the explanation provided earlier. So these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-policy-templates.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-policy-templates.html</a></p>\n",
                "options": [
                    {
                        "id": 2243,
                        "content": "<p>S3ReadPolicy</p>",
                        "isValid": true
                    },
                    {
                        "id": 2244,
                        "content": "<p>SQSPollerPolicy</p>",
                        "isValid": false
                    },
                    {
                        "id": 2245,
                        "content": "<p>S3CrudPolicy</p>",
                        "isValid": false
                    },
                    {
                        "id": 2246,
                        "content": "<p>LambdaInvokePolicy</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 548,
            "attributes": {
                "createdAt": "2023-09-07T08:39:47.343Z",
                "updatedAt": "2023-09-07T08:39:47.343Z",
                "content": "<p>The development team at an e-commerce company is preparing for the upcoming Thanksgiving sale. The product manager wants the development team to implement appropriate caching strategy on Amazon ElastiCache to withstand traffic spikes on the website during the sale. A key requirement is to facilitate consistent updates to the product prices and product description, so that the cache never goes out of sync with the backend.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.</p>\n\n<p>Broadly, you can set up two types of caching strategies:</p>\n\n<ol>\n<li><p>Lazy Loading</p></li>\n<li><p>Write-Through</p></li>\n</ol>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q62-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q62-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p>\n\n<p><strong>Use a caching strategy to write to the backend first and then invalidate the cache</strong></p>\n\n<p>This option is similar to the write-through strategy wherein the application writes to the backend first and then invalidate the cache. As the cache gets invalidated, the caching engine would then fetch the latest value from the backend, thereby making sure that the product prices and product description stay consistent with the backend.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a caching strategy to update the cache and the backend at the same time</strong> - The cache and the backend cannot be updated at the same time via a single atomic operation as these are two separate systems. Therefore this option is incorrect.</p>\n\n<p><strong>Use a caching strategy to write to the backend first and wait for the cache to expire via TTL</strong> - This strategy could work if the TTL is really short. However, for the duration of this TTL, the cache would be out of sync with the backend, hence this option is not correct for the given use-case.</p>\n\n<p><strong>Use a caching strategy to write to the cache directly and sync the backend at a later time</strong> - This option is given as a distractor as this strategy is not viable to address the given use-case. The product prices and description on the cache must always stay consistent with the backend. You cannot sync the backend at a later time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p>\n",
                "options": [
                    {
                        "id": 2247,
                        "content": "<p>Use a caching strategy to write to the backend first and wait for the cache to expire via TTL</p>",
                        "isValid": false
                    },
                    {
                        "id": 2248,
                        "content": "<p>Use a caching strategy to update the cache and the backend at the same time</p>",
                        "isValid": false
                    },
                    {
                        "id": 2249,
                        "content": "<p>Use a caching strategy to write to the cache directly and sync the backend at a later time</p>",
                        "isValid": false
                    },
                    {
                        "id": 2250,
                        "content": "<p>Use a caching strategy to write to the backend first and then invalidate the cache</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 549,
            "attributes": {
                "createdAt": "2023-09-07T08:39:47.420Z",
                "updatedAt": "2023-09-07T08:39:47.420Z",
                "content": "<p>Your team lead has finished creating a CodeBuild project in the management console and a build spec has been defined for the project. After the build is run, CodeBuild fails to pull a Docker image into the build environment.</p>\n\n<p>What is the most likely cause?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Missing IAM permissions for the CodeBuild Service</strong></p>\n\n<p>By default, IAM users don't have permission to create or modify Amazon Elastic Container Registry (Amazon ECR) resources or perform tasks using the Amazon ECR API. A user who uses the AWS CodeBuild console must have a minimum set of permissions that allows the user to describe other AWS resources for the AWS account.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q46-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Docker image is missing some tags</strong> - Tags are optional for naming purposes</p>\n\n<p><strong>CodeBuild cannot work with custom Docker images</strong> - Custom docker images are supported, so this option is incorrect.</p>\n\n<p><strong>The Docker image is too big</strong> - It is good to properly design the image but in this case, it does not affect the CodeBuild. You can also look at multi-stage builds, which are a new feature requiring Docker 17.05 or higher on the daemon and client. Multistage builds are useful to anyone who has struggled to optimize Dockerfiles while keeping them easy to read and maintain.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html</a></p>\n",
                "options": [
                    {
                        "id": 2251,
                        "content": "<p>The Docker image is missing some tags</p>",
                        "isValid": false
                    },
                    {
                        "id": 2252,
                        "content": "<p>The Docker image is too big</p>",
                        "isValid": false
                    },
                    {
                        "id": 2253,
                        "content": "<p>CodeBuild cannot work with custom Docker images</p>",
                        "isValid": false
                    },
                    {
                        "id": 2254,
                        "content": "<p>Missing IAM permissions for the CodeBuild Service</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 550,
            "attributes": {
                "createdAt": "2023-09-07T08:39:47.505Z",
                "updatedAt": "2023-09-07T08:39:47.505Z",
                "content": "<p>You are a developer working on AWS Lambda functions that are triggered by Amazon API Gateway and would like to perform testing on a low volume of traffic for new API versions.</p>\n\n<p>Which of the following features will accomplish this task?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Canary Deployment</strong></p>\n\n<p>In a canary release deployment, total API traffic is separated at random into a production release and a canary release with a preconfigured ratio. Typically, the canary release receives a small percentage of API traffic and the production release takes up the rest. The updated API features are only visible to API traffic through the canary. You can adjust the canary traffic percentage to optimize test coverage or performance.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q50-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Stage Variables</strong> - They act like environment variables and can be used in your API setup.</p>\n\n<p><strong>Mapping Templates</strong> - Its a script to map the payload from a method request to the corresponding integration request and also maps the integration response to the corresponding method response.</p>\n\n<p><strong>Custom Authorizers</strong> - Used for authentication purposes and must return AWS Identity and Access Management (IAM) policies.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p>\n",
                "options": [
                    {
                        "id": 2255,
                        "content": "<p>Mapping Templates</p>",
                        "isValid": false
                    },
                    {
                        "id": 2256,
                        "content": "<p>Stage Variables</p>",
                        "isValid": false
                    },
                    {
                        "id": 2257,
                        "content": "<p>Canary Deployment</p>",
                        "isValid": true
                    },
                    {
                        "id": 2258,
                        "content": "<p>Custom Authorizers</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 551,
            "attributes": {
                "createdAt": "2023-09-07T08:39:47.578Z",
                "updatedAt": "2023-09-07T08:39:47.578Z",
                "content": "<p>Your organization has set up a full CI/CD pipeline leveraging CodePipeline and the deployment is done on Elastic Beanstalk. This pipeline has worked for over a year now but you are approaching the limits of Elastic Beanstalk in terms of how many versions can be stored in the service.</p>\n\n<p>How can you remove older versions that are not used by Elastic Beanstalk so that new versions can be created for your applications?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use a Lifecycle Policy</strong></p>\n\n<p>Each time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application version. If you don't delete versions that you no longer use, you will eventually reach the application version limit and be unable to create new versions of that application.</p>\n\n<p>You can avoid hitting the limit by applying an application version lifecycle policy to your applications. A lifecycle policy tells Elastic Beanstalk to delete old application versions or to delete application versions when the total number of versions for an application exceeds a specified number.</p>\n\n<p>Elastic Beanstalk applies an application's lifecycle policy each time you create a new application version and deletes up to 100 versions each time the lifecycle policy is applied. Elastic Beanstalk deletes old versions after creating the new version and does not count the new version towards the maximum number of versions defined in the policy.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q15-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html</a></p>\n\n<p>Incorrect options:\n<strong>Setup an <code>.ebextensions</code> files</strong> - You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains. This does not help with managing versions.</p>\n\n<p><strong>Define a Lambda function</strong> - This could work but would require a lot of manual scripting, to achieve the same desired effect as the Lifecycle Policy EB feature.</p>\n\n<p><strong>Use Worker Environments</strong> - This won't help. If your application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html</a></p>\n",
                "options": [
                    {
                        "id": 2259,
                        "content": "<p>Setup an <code>.ebextensions</code> file</p>",
                        "isValid": false
                    },
                    {
                        "id": 2260,
                        "content": "<p>Use Worker Environments</p>",
                        "isValid": false
                    },
                    {
                        "id": 2261,
                        "content": "<p>Define a Lambda function</p>",
                        "isValid": false
                    },
                    {
                        "id": 2262,
                        "content": "<p>Use a Lifecycle Policy</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 552,
            "attributes": {
                "createdAt": "2023-09-07T08:39:47.655Z",
                "updatedAt": "2023-09-07T08:39:47.655Z",
                "content": "<p>An e-commerce application posts its order transactions in bulk to an accounting application for further processing. Due to changes in the compliance rules, all the transactions are being encrypted with AWS Key Management Service (AWS KMS) key before posting to the accounting application. Post this change, the testers have raised tickets regarding the application receiving a ThrottlingException error.</p>\n\n<p>What measures should a developer take to fix this issue MOST optimally? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Reduce the rate of requests and consider using the backoff and retry logic</strong></p>\n\n<p>Each AWS SDK implements automatic retry logic. The AWS SDK for Java automatically retries requests, and you can configure the retry settings. In addition to simple retries, each AWS SDK implements an exponential backoff algorithm for better flow control. The idea behind exponential backoff is to use progressively longer waits between retries for consecutive error responses. You should implement a maximum delay interval, as well as a maximum number of retries. The maximum delay interval and the maximum number of retries are not necessarily fixed values and should be set based on the operation being performed, as well as other local factors, such as network latency.</p>\n\n<p><strong>Use the data key caching feature with the AWS Encryption SDK encryption library</strong></p>\n\n<p>Data key caching stores data keys and related cryptographic material in a cache. When you encrypt or decrypt data, the AWS Encryption SDK looks for a matching data key in the cache. If it finds a match, it uses the cached data key rather than generating a new one. Data key caching can improve performance, reduce cost, and help you stay within service limits as your application scales.</p>\n\n<p>Your application can benefit from data key caching if:\n1. It can reuse data keys.\n2. It generates numerous data keys.\n3. Your cryptographic operations are unacceptably slow, expensive, limited, or resource-intensive.</p>\n\n<p>Data key caching is an optional feature of the AWS Encryption SDK that you should use cautiously. By default, the AWS Encryption SDK generates a new data key for every encryption operation. This technique supports cryptographic best practices, which discourage excessive reuse of data keys. In general, use data key caching only when it is required to meet your performance goals. Then, use the data key caching security thresholds to ensure that you use the minimum amount of caching required to meet your cost and performance goals.</p>\n\n<p>Best practices to troubleshoot ThrottlingException errors:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q16-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/kms-throttlingexception-error/\">https://aws.amazon.com/premiumsupport/knowledge-center/kms-throttlingexception-error/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Send AWS CloudTrail events generated by AWS KMS to Amazon CloudWatch Logs</strong> - Deeper analysis of CloudTrail data sent to CloudWatch logs can help spot throttled API calls. While it is certainly possible to track API usage using CloudTrail data, it is not an optimal way to fix the ThrottlingException error.</p>\n\n<p><strong>Write queries in Amazon CloudWatch Logs Insights to track your API request usage and submit an AWS Support case to request a quota increase</strong> - Historically, to understand how close to a request rate quota you were, you had to perform three tasks: (i) send AWS CloudTrail events generated by AWS KMS to Amazon CloudWatch Logs; (ii) write queries in Amazon CloudWatch Logs Insights to track your API request usage; and (iii) submit an AWS Support case to request a quota increase. Now, you can view your AWS KMS API usage and request quota increases within the AWS Service Quotas console itself without doing any special configuration.</p>\n\n<p><strong>Use a bucket-level key for SSE-KMS which will decrease the requested traffic to AWS KMS thereby avoiding the ThrottlingException error</strong> - Amazon S3 bucket has not been mentioned in the given use case and hence this option is irrelevant.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/general/latest/gr/api-retries.html\">https://docs.aws.amazon.com/general/latest/gr/api-retries.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/data-key-caching.html\">https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/data-key-caching.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/manage-your-aws-kms-api-request-rates-using-service-quotas-and-amazon-cloudwatch/\">https://aws.amazon.com/blogs/security/manage-your-aws-kms-api-request-rates-using-service-quotas-and-amazon-cloudwatch/</a></p>\n",
                "options": [
                    {
                        "id": 2263,
                        "content": "<p>Send AWS CloudTrail events generated by AWS KMS to Amazon CloudWatch Logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 2264,
                        "content": "<p>Use the data key caching feature with the AWS Encryption SDK encryption library</p>",
                        "isValid": true
                    },
                    {
                        "id": 2265,
                        "content": "<p>Use a bucket-level key for SSE-KMS which will decrease the requested traffic to AWS KMS thereby avoiding the ThrottlingException error</p>",
                        "isValid": false
                    },
                    {
                        "id": 2266,
                        "content": "<p>Write queries in Amazon CloudWatch Logs Insights to track your API request usage and submit an AWS Support case to request a quota increase</p>",
                        "isValid": false
                    },
                    {
                        "id": 2267,
                        "content": "<p>Reduce the rate of requests and consider using the backoff and retry logic</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 553,
            "attributes": {
                "createdAt": "2023-09-07T08:39:47.727Z",
                "updatedAt": "2023-09-07T08:39:47.727Z",
                "content": "<p>A popular mobile app retrieves data from an AWS DynamoDB table that was provisioned with read-capacity units (RCUâ€™s) that are evenly shared across four partitions. One of those partitions is receiving more traffic than the other partitions, causing hot partition issues.</p>\n\n<p>What technology will allow you to reduce the read traffic on your AWS DynamoDB table with minimal effort?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>DynamoDB DAX</strong></p>\n\n<p>Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement: from milliseconds to microseconds: even at millions of requests per second.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DynamoDB Streams</strong> - A stream record contains information about a data modification to a single item in a DynamoDB table. This is not the correct option for the given use-case.</p>\n\n<p><strong>ElastiCache</strong> - ElastiCache can cache the results from anything but you will need to modify your code to check the cache before querying the main query store. As the given use-case mandates minimal effort, so this option is not correct.</p>\n\n<p><strong>More partitions</strong> - This option has been added as a distractor as DynamoDB handles that for you automatically.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n",
                "options": [
                    {
                        "id": 2268,
                        "content": "<p>ElastiCache</p>",
                        "isValid": false
                    },
                    {
                        "id": 2269,
                        "content": "<p>DynamoDB Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 2270,
                        "content": "<p>DynamoDB DAX</p>",
                        "isValid": true
                    },
                    {
                        "id": 2271,
                        "content": "<p>More partitions</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 554,
            "attributes": {
                "createdAt": "2023-09-07T08:39:47.800Z",
                "updatedAt": "2023-09-07T08:39:47.800Z",
                "content": "<p>Your organization has a single Amazon Simple Storage Service (S3) bucket that contains folders labeled with customer names. Several administrators have IAM access to the S3 bucket and versioning is enabled to easily recover from unintended user actions.</p>\n\n<p>Which of the following statements about versioning is <strong>NOT</strong> true based on this scenario?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Versioning can be enabled only for a specific folder</strong></p>\n\n<p>The versioning state applies to all (never some) of the objects in that bucket. The first time you enable a bucket for versioning, objects in it are thereafter always versioned and given a unique version ID.</p>\n\n<p>Versioning Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Overwriting a file increases its versions</strong> - If you overwrite an object (file), it results in a new object version in the bucket. You can always restore the previous version.</p>\n\n<p><strong>Deleting a file is a recoverable operation</strong> - Correct, when you delete an object (file), Amazon S3 inserts a delete marker, which becomes the current object version and you can restore the previous version.</p>\n\n<p><strong>Any file that was unversioned before enabling versioning will have the 'null' version</strong> - Objects stored in your bucket before you set the versioning state have a version ID of null. Those existing objects in your bucket do not change.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n",
                "options": [
                    {
                        "id": 2272,
                        "content": "<p>Overwriting a file increases its versions</p>",
                        "isValid": false
                    },
                    {
                        "id": 2273,
                        "content": "<p>Any file that was unversioned before enabling versioning will have the 'null' version</p>",
                        "isValid": false
                    },
                    {
                        "id": 2274,
                        "content": "<p>Versioning can be enabled only for a specific folder</p>",
                        "isValid": true
                    },
                    {
                        "id": 2275,
                        "content": "<p>Deleting a file is a recoverable operation</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 555,
            "attributes": {
                "createdAt": "2023-09-07T08:39:47.880Z",
                "updatedAt": "2023-09-07T08:39:47.880Z",
                "content": "<p>A media analytics company has built a streaming application on Lambda using Serverless Application Model (SAM).</p>\n\n<p>As a Developer Associate, which of the following would you identify as the correct order of execution to successfully deploy the application?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Develop the SAM template locally =&gt; upload the template to S3 =&gt; deploy your application to the cloud</strong></p>\n\n<p>The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML.</p>\n\n<p>You can develop and test your serverless application locally, and then you can deploy your application by using the sam deploy command. The sam deploy command zips your application artifacts, uploads them to Amazon Simple Storage Service (Amazon S3), and deploys your application to the AWS Cloud. AWS SAM uses AWS CloudFormation as the underlying deployment mechanism.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Develop the SAM template locally =&gt; upload the template to Lambda =&gt; deploy your application to the cloud</strong></p>\n\n<p><strong>Develop the SAM template locally =&gt; upload the template to CodeCommit =&gt; deploy your application to CodeDeploy</strong></p>\n\n<p><strong>Develop the SAM template locally =&gt; deploy the template to S3 =&gt; use your application in the cloud</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html</a></p>\n",
                "options": [
                    {
                        "id": 2276,
                        "content": "<p>Develop the SAM template locally =&gt; upload the template to CodeCommit =&gt; deploy your application to CodeDeploy</p>",
                        "isValid": false
                    },
                    {
                        "id": 2277,
                        "content": "<p>Develop the SAM template locally =&gt; upload the template to S3 =&gt; deploy your application to the cloud</p>",
                        "isValid": true
                    },
                    {
                        "id": 2278,
                        "content": "<p>Develop the SAM template locally =&gt; deploy the template to S3 =&gt; use your application in the cloud</p>",
                        "isValid": false
                    },
                    {
                        "id": 2279,
                        "content": "<p>Develop the SAM template locally =&gt; upload the template to Lambda =&gt; deploy your application to the cloud</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 556,
            "attributes": {
                "createdAt": "2023-09-07T08:39:47.958Z",
                "updatedAt": "2023-09-07T08:39:47.958Z",
                "content": "<p>Your AWS account is now growing to 200 users and you would like to provide each of these users a personal space in the S3 bucket 'my_company_space' with the prefix <code>/home/&lt;username&gt;</code>, where they have read/write access.</p>\n\n<p>How can you do this efficiently?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create one customer-managed policy with policy variables and attach it to a group of all users</strong></p>\n\n<p>You can assign access to \"dynamically calculated resources\" by using policy variables, a feature that lets you specify placeholders in a policy. When the policy is evaluated, the policy variables are replaced with values that come from the context of the request itself.</p>\n\n<p>This is ideal when you want want to generalize the policy so it works for many users without having to make a unique copy of the policy for each user. For example, consider writing a policy to allow each user to have access to his or her own objects in an Amazon S3 bucket, as in the previous example. But don't create a separate policy for each user that explicitly specifies the user's name as part of the resource. Instead, create a single group policy that works for any user in that group.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an S3 bucket policy and change it as users are added and removed</strong></p>\n\n<p>This doesn't scale and the S3 bucket policy size may be maxed out. The IAM policies bump up against a size limit (up to 2 kb for users, 5 kb for groups, and 10 kb for roles). S3 supports bucket policies of up 20 kb.</p>\n\n<p><strong>Create inline policies for each user as they are onboarded</strong>: This would work but doesn't scale and it's inefficient.</p>\n\n<p><strong>Create one customer-managed policy per user and attach them to the relevant users</strong>: This would work but doesn't scale and would be a nightmare to manage.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html</a></p>\n",
                "options": [
                    {
                        "id": 2280,
                        "content": "<p>Create one customer-managed policy with policy variables and attach it to a group of all users</p>",
                        "isValid": true
                    },
                    {
                        "id": 2281,
                        "content": "<p>Create inline policies for each user as they are onboarded</p>",
                        "isValid": false
                    },
                    {
                        "id": 2282,
                        "content": "<p>Create an S3 bucket policy and change it as users are added and removed</p>",
                        "isValid": false
                    },
                    {
                        "id": 2283,
                        "content": "<p>Create one customer-managed policy per user and attach them to the relevant users</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 557,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.032Z",
                "updatedAt": "2023-09-07T08:39:48.032Z",
                "content": "<p>You are implementing a banking application in which you need to update the Exchanges DynamoDB table and the AccountBalance DynamoDB table at the same time or not at all.</p>\n\n<p>Which DynamoDB feature should you use?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>DynamoDB Transactions</strong></p>\n\n<p>You can use DynamoDB transactions to make coordinated all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, helping you to maintain data correctness in your applications.</p>\n\n<p>DynamoDB Transactions Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DynamoDB TTL</strong> - DynamoDB TTL allows you to expire data based on a timestamp, so this option is not correct.</p>\n\n<p><strong>DynamoDB Streams</strong> - DynamoDB Streams gives a changelog of changes that happened to your tables and then may even relay these to a Lambda function for further processing.</p>\n\n<p><strong>DynamoDB Indexes</strong> - GSI and LSI are used to allow you to query your tables using different partition/sort keys.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a></p>\n",
                "options": [
                    {
                        "id": 2284,
                        "content": "<p>DynamoDB Indexes</p>",
                        "isValid": false
                    },
                    {
                        "id": 2285,
                        "content": "<p>DynamoDB Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 2286,
                        "content": "<p>DynamoDB Transactions</p>",
                        "isValid": true
                    },
                    {
                        "id": 2287,
                        "content": "<p>DynamoDB TTL</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 558,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.100Z",
                "updatedAt": "2023-09-07T08:39:48.100Z",
                "content": "<p>Which environment variable can be used by AWS X-Ray SDK to ensure that the daemon is correctly discovered on ECS?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS_XRAY_DAEMON_ADDRESS</strong></p>\n\n<p>Set the host and port of the X-Ray daemon listener. By default, the SDK uses 127.0.0.1:2000 for both trace data (UDP) and sampling (TCP). Use this variable if you have configured the daemon to listen on a different port or if it is running on a different host.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS_XRAY_TRACING_NAME</strong> - This sets a service name that the SDK uses for segments.</p>\n\n<p><strong>AWS_XRAY_CONTEXT_MISSING</strong> - This should be set to LOG_ERROR to avoid throwing exceptions when your instrumented code attempts to record data when no segment is open.</p>\n\n<p><strong>AWS_XRAY_DEBUG_MODE</strong> - This should be set to TRUE to configure the SDK to output logs to the console, instead of configuring a logger.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-configuration.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-configuration.html</a></p>\n",
                "options": [
                    {
                        "id": 2288,
                        "content": "<p>AWS_XRAY_DAEMON_ADDRESS</p>",
                        "isValid": true
                    },
                    {
                        "id": 2289,
                        "content": "<p>AWS_XRAY_DEBUG_MODE</p>",
                        "isValid": false
                    },
                    {
                        "id": 2290,
                        "content": "<p>AWS_XRAY_CONTEXT_MISSING</p>",
                        "isValid": false
                    },
                    {
                        "id": 2291,
                        "content": "<p>AWS_XRAY_TRACING_NAME</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 559,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.183Z",
                "updatedAt": "2023-09-07T08:39:48.183Z",
                "content": "<p>You would like your Elastic Beanstalk environment to expose an HTTPS endpoint and an HTTP endpoint. The HTTPS endpoint should be used to get in-flight encryption between your clients and your web servers, while the HTTP endpoint should only be used to redirect traffic to HTTPS and support URLs starting with http://.</p>\n\n<p>What must be done to configure this setup? (Select three)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Assign an SSL certificate to the Load Balancer</strong></p>\n\n<p>This ensures that the Load Balancer can expose an HTTPS endpoint.</p>\n\n<p><strong>Open up port 80 &amp; port 443</strong></p>\n\n<p>This ensures that the Load Balancer will allow both the HTTP (80) and HTTPS (443) protocol for incoming connections</p>\n\n<p><strong>Configure your EC2 instances to redirect HTTP traffic to HTTPS</strong></p>\n\n<p>This ensures traffic originating from HTTP onto the Load Balancer forces a redirect to HTTPS by the EC2 instances before being correctly served, thus ensuring the traffic served is fully encrypted.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Only open up port 80</strong> - This is not correct as it would not allow HTTPS traffic (port 443).</p>\n\n<p><strong>Only open up port 443</strong> - This is not correct as it would not allow HTTP traffic (port 80).</p>\n\n<p><strong>Configure your EC2 instances to redirect HTTPS traffic to HTTP</strong> - This is not correct as it would force HTTP traffic, instead of HTTPS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-httpredirect.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-httpredirect.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html</a></p>\n",
                "options": [
                    {
                        "id": 2292,
                        "content": "<p>Only open up port 443</p>",
                        "isValid": false
                    },
                    {
                        "id": 2293,
                        "content": "<p>Configure your EC2 instances to redirect HTTP traffic to HTTPS</p>",
                        "isValid": true
                    },
                    {
                        "id": 2294,
                        "content": "<p>Open up port 80 &amp; port 443</p>",
                        "isValid": true
                    },
                    {
                        "id": 2295,
                        "content": "<p>Configure your EC2 instances to redirect HTTPS traffic to HTTP</p>",
                        "isValid": false
                    },
                    {
                        "id": 2296,
                        "content": "<p>Only open up port 80</p>",
                        "isValid": false
                    },
                    {
                        "id": 2297,
                        "content": "<p>Assign an SSL certificate to the Load Balancer</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 560,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.248Z",
                "updatedAt": "2023-09-07T08:39:48.248Z",
                "content": "<p>An IT company uses AWS CloudFormation templates to provision their AWS infrastructure for Amazon EC2, Amazon VPC, and Amazon S3 resources. Using cross-stack referencing, a developer creates a stack called <code>NetworkStack</code> which will export the <code>subnetId</code> that can be used when creating EC2 instances in another stack.</p>\n\n<p>To use the exported value in another stack, which of the following functions must be used?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong><code>!ImportValue</code></strong></p>\n\n<p>The intrinsic function <code>Fn::ImportValue</code> returns the value of an output exported by another stack. You typically use this function to create cross-stack references.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>!Ref</code></strong> - Returns the value of the specified parameter or resource.</p>\n\n<p><strong><code>!GetAtt</code></strong> - Returns the value of an attribute from a resource in the template.</p>\n\n<p><strong><code>!Sub</code></strong> - Substitutes variables in an input string with values that you specify.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html</a></p>\n",
                "options": [
                    {
                        "id": 2298,
                        "content": "<p><code>!Sub</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2299,
                        "content": "<p><code>!ImportValue</code></p>",
                        "isValid": true
                    },
                    {
                        "id": 2300,
                        "content": "<p><code>!Ref</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2301,
                        "content": "<p><code>!GetAtt</code></p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 561,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.319Z",
                "updatedAt": "2023-09-07T08:39:48.319Z",
                "content": "<p>A media company wants to migrate a video editing service to Amazon EC2 while following security best practices. The videos are sourced and read from a non-public S3 bucket.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up an EC2 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</strong></p>\n\n<p>As an AWS security best practice, you should not create an IAM user and pass the user's credentials to the application or embed the credentials in the application. Instead, create an IAM role that you attach to the EC2 instance to give temporary security credentials to applications running on the instance. When an application uses these credentials in AWS, it can perform all of the operations that are allowed by the policies attached to the role.</p>\n\n<p>So for the given use-case, you should create an IAM role with read-only permissions for the S3 bucket and apply it to the EC2 instance profile.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an IAM user with read-only permissions for the S3 bucket. Configure AWS credentials for this user via AWS CLI on the EC2 instance</strong></p>\n\n<p><strong>Set up an IAM user with read-only permissions for the S3 bucket. Configure the IAM user credentials in the user data of the EC2 instance</strong></p>\n\n<p>As mentioned in the explanation above, it is dangerous to pass an IAM user's credentials to the application or embed the credentials in the application or even configure these credentials in the user data of the EC2 instance. So both these options are incorrect.</p>\n\n<p><strong>Set up an S3 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</strong> - As the application is running on EC2 instances, therefore you need to set up an EC2 service role, not an S3 service role.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html</a></p>\n",
                "options": [
                    {
                        "id": 2302,
                        "content": "<p>Set up an S3 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</p>",
                        "isValid": false
                    },
                    {
                        "id": 2303,
                        "content": "<p>Set up an IAM user with read-only permissions for the S3 bucket. Configure AWS credentials for this user via AWS CLI on the EC2 instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 2304,
                        "content": "<p>Set up an IAM user with read-only permissions for the S3 bucket. Configure the IAM user credentials in the user data of the EC2 instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 2305,
                        "content": "<p>Set up an EC2 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 562,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.386Z",
                "updatedAt": "2023-09-07T08:39:48.386Z",
                "content": "<p>A financial services company has developed a REST API which is deployed in an Auto Scaling Group behind an Application Load Balancer. The API stores the data payload in DynamoDB and the static content is served through S3. Upon analyzing the usage pattern, it's found that 80% of the read requests are shared across all users.</p>\n\n<p>As a Developer Associate, how can you improve the application performance while optimizing the cost with the least development effort?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</strong></p>\n\n<p>DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvementâ€”from milliseconds to microsecondsâ€”even at millions of requests per second.</p>\n\n<p>DAX is tightly integrated with DynamoDBâ€”you simply provision a DAX cluster, use the DAX client SDK to point your existing DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache DynamoDB reads.</p>\n\n<p>CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file theyâ€™ve requested isnâ€™t yet cached, CloudFront retrieves it from your origin â€“ for example, the S3 bucket where youâ€™ve stored your content.</p>\n\n<p>So, you can use CloudFront to improve application performance to serve static content from S3.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.</p>\n\n<p>ElastiCache for Redis Overview:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p>Although, you can integrate Redis with DynamoDB, it's much more involved from a development perspective. For the given use-case, you should use DAX which is a much better fit.</p>\n\n<p><strong>Enable DAX for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database.</p>\n\n<p>ElastiCache cannot be used as a cache to serve static content from S3, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n",
                "options": [
                    {
                        "id": 2306,
                        "content": "<p>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 2307,
                        "content": "<p>Enable DAX for DynamoDB and ElastiCache Memcached for S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 2308,
                        "content": "<p>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 2309,
                        "content": "<p>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 563,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.457Z",
                "updatedAt": "2023-09-07T08:39:48.457Z",
                "content": "<p>Your company is shifting towards Elastic Container Service (ECS) to deploy applications. The process should be automated using the AWS CLI to create a service where at least ten instances of a task definition are kept running under the default cluster.</p>\n\n<p>Which of the following commands should be executed?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong><code>aws ecs create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></strong></p>\n\n<p>To create a new service you would use this command which creates a service in your default region called ecs-simple-service. The service uses the ecs-demo task definition and it maintains 10 instantiations of that task.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>aws ecr create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></strong> - This command is referencing a different service called Amazon Elastic Container Registry (ECR) which's is a fully-managed Docker container registry</p>\n\n<p><strong><code>docker-compose create ecs-simple-service</code></strong> - This is a docker command to create containers for a service.</p>\n\n<p><strong><code>aws ecs run-task --cluster default --task-definition ecs-demo</code></strong> - This is a valid command but used for starting a new task using a specified task definition.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/ecs/create-service.html\">https://docs.aws.amazon.com/cli/latest/reference/ecs/create-service.html</a></p>\n",
                "options": [
                    {
                        "id": 2310,
                        "content": "<p><code>docker-compose create ecs-simple-service</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2311,
                        "content": "<p><code>aws ecs run-task --cluster default --task-definition ecs-demo</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2312,
                        "content": "<p><code>aws ecs create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></p>",
                        "isValid": true
                    },
                    {
                        "id": 2313,
                        "content": "<p><code>aws ecr create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 564,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.534Z",
                "updatedAt": "2023-09-07T08:39:48.534Z",
                "content": "<p>As part of your video processing application, you are looking to perform a set of repetitive and scheduled tasks asynchronously. Your application is deployed on Elastic Beanstalk.</p>\n\n<p>Which Elastic Beanstalk environment should you set up for performing the repetitive tasks?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>Elastic BeanStalk Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q25-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><strong>Setup a Worker environment and a <code>cron.yaml</code> file</strong></p>\n\n<p>An environment is a collection of AWS resources running an application version. An environment that pulls tasks from an Amazon Simple Queue Service (Amazon SQS) queue runs in a worker environment tier.</p>\n\n<p>If your AWS Elastic Beanstalk application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load.</p>\n\n<p>For a worker environment, you need a <code>cron.yaml</code> file to define the cron jobs and do repetitive tasks.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q25-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Setup a Web Server environment and a <code>cron.yaml</code> file</strong></p>\n\n<p><strong>Setup a Worker environment and a <code>.ebextensions</code> file</strong></p>\n\n<p><strong>Setup a Web Server environment and a <code>.ebextensions</code> file</strong></p>\n\n<p><code>.ebextensions/</code> won't work to define cron jobs, and Web Server environments cannot be set up to perform repetitive and scheduled tasks. So these three options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html</a></p>\n",
                "options": [
                    {
                        "id": 2314,
                        "content": "<p>Setup a Worker environment and a <code>.ebextensions</code> file</p>",
                        "isValid": false
                    },
                    {
                        "id": 2315,
                        "content": "<p>Setup a Worker environment and a <code>cron.yaml</code> file</p>",
                        "isValid": true
                    },
                    {
                        "id": 2316,
                        "content": "<p>Setup a Web Server environment and a <code>cron.yaml</code> file</p>",
                        "isValid": false
                    },
                    {
                        "id": 2317,
                        "content": "<p>Setup a Web Server environment and a <code>.ebextensions</code> file</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 565,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.600Z",
                "updatedAt": "2023-09-07T08:39:48.600Z",
                "content": "<p>When your company first created an AWS account, you began with a single sign-in principal called a root user account that had complete access to all AWS services and resources.</p>\n\n<p>What should you do to adhere to best practices for using the root user account?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>It should be accessible by one admin only after enabling Multi-factor authentication</strong></p>\n\n<p>AWS Root Account Security Best Practices:\n<img src=\"\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials</a></p>\n\n<p>If you continue to use the root user credentials, we recommend that you follow the security best practice to enable multi-factor authentication (MFA) for your account. Because your root user can perform sensitive operations in your account, adding a layer of authentication helps you to better secure your account. Multiple types of MFA are available.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>It should be accessible by 3 to 6 members of the IT team</strong> - Only the owner of the AWS account should have access to the root account credentials. You should create an IT group with admin permissions via IAM and then assign a few users to this group.</p>\n\n<p><strong>It should be accessible using the access key id and secret access key</strong> - AWS recommends that you should not use the access key id and secret access key for the AWS account root user.</p>\n\n<p><strong>It should be accessible by no one, throw away the passwords after creating the account</strong> - You will still need to store the password somewhere for your root account.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials</a></p>\n",
                "options": [
                    {
                        "id": 2318,
                        "content": "<p>It should be accessible by no one, throw away the passwords after creating the account</p>",
                        "isValid": false
                    },
                    {
                        "id": 2319,
                        "content": "<p>It should be accessible using the access key id and secret access key</p>",
                        "isValid": false
                    },
                    {
                        "id": 2320,
                        "content": "<p>It should be accessible by one admin only after enabling Multi-factor authentication</p>",
                        "isValid": true
                    },
                    {
                        "id": 2321,
                        "content": "<p>It should be accessible by 3 to 6 members of the IT team</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 566,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.672Z",
                "updatedAt": "2023-09-07T08:39:48.672Z",
                "content": "<p>An EC2 instance has an IAM instance role attached to it, providing it read and write access to the S3 bucket 'my_bucket'. You have tested the IAM instance role and both reads and writes are working. You then remove the IAM role from the EC2 instance and test both read and write again. Writes stopped working but reads are still working.</p>\n\n<p>What is the likely cause of this behavior?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The S3 bucket policy authorizes reads</strong></p>\n\n<p>When evaluating an IAM policy of an EC2 instance doing actions on S3, the least-privilege union of both the IAM policy of the EC2 instance and the bucket policy of the S3 bucket are taken into account.</p>\n\n<p>For the given use-case, as IAM role has been removed, therefore only the S3 bucket policy comes into effect which authorizes reads.</p>\n\n<p>Here is a great reference blog for understanding the various scenarios for using IAM policy vs S3 bucket policy -\n<a href=\"https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/\">https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EC2 instance is using cached temporary IAM credentials</strong> - As the IAM instance role has been removed that wouldn't be the case</p>\n\n<p><strong>Removing an instance role from an EC2 instance can take a few minutes before being active</strong> - It is immediately active and even if it wasn't, it wouldn't make sense as we can still do reads but not writes.</p>\n\n<p><strong>When a read is done on a bucket, there's a grace period of 5 minutes to do the same read again</strong> - This is not true. Every single request is evaluated against IAM in the AWS model.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/\">https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/</a></p>\n",
                "options": [
                    {
                        "id": 2322,
                        "content": "<p>Removing an instance role from an EC2 instance can take a few minutes before being active</p>",
                        "isValid": false
                    },
                    {
                        "id": 2323,
                        "content": "<p>When a read is done on a bucket, there's a grace period of 5 minutes to do the same read again</p>",
                        "isValid": false
                    },
                    {
                        "id": 2324,
                        "content": "<p>The S3 bucket policy authorizes reads</p>",
                        "isValid": true
                    },
                    {
                        "id": 2325,
                        "content": "<p>The EC2 instance is using cached temporary IAM credentials</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 567,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.744Z",
                "updatedAt": "2023-09-07T08:39:48.744Z",
                "content": "<p>A company has hosted its restaurant review website on an Amazon EC2 instance. The website supports multiple languages and the preferred language is added as a query string parameter to the request. The directory structure and file names for all versions of the website are identical. The website responds with the chosen language's webpage when accessed directly. However, when the same webpage is accessed through the configured CloudFront distribution, it defaults to a single language.</p>\n\n<p>How will you fix this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a new cache policy for the CloudFront distribution and set the cache behavior to Query string forwarding and caching. In the Query string whitelist field include the language string. Update the CloudFront distribution to use the new cache policy</strong></p>\n\n<p>CloudFront can cache different versions of your content based on the values of query string parameters. Forward all, cache based on whitelist option should be chosen if your origin server returns different versions of your objects based on one or more query string parameters. Then specify the parameters that you want CloudFront to use as a basis for caching in the Query string whitelist field.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new cache policy for the CloudFront distribution and set the cache behavior to None to improve caching performance. Update the CloudFront distribution to use the new cache policy</strong> - None improves Caching. Choose this option if your origin returns the same version of an object regardless of the values of query string parameters. This increases the likelihood that CloudFront can serve a request from the cache, which improves performance and reduces the load on your origin.</p>\n\n<p><strong>Create a new cache policy for the CloudFront distribution and set the cache behavior to <code>Cache based on selected request headers</code>. Use <code>Whitelist Headers</code> as the caching criteria</strong> - <code>Cache based on selected request headers</code> is not a valid option since the use case mentions using query string parameters.</p>\n\n<p><strong>Choose the <code>Customize</code> option for the <code>Object Caching</code> setting and reduce the <code>Default TTL</code> value so that CloudFront forwards requests to your origin more frequently</strong> - Default TTL specifies the default amount of time, in seconds, that you want objects to stay in CloudFront caches before CloudFront forwards another request to your origin to determine whether the object has been updated. This option is irrelevant for the current use case since the response returned defaulting to the same language is not a TTL issue.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesQueryString\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesQueryString</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesQueryStringWhiteList\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesQueryStringWhiteList</a></p>\n",
                "options": [
                    {
                        "id": 2326,
                        "content": "<p>Create a new cache policy for the CloudFront distribution and set the cache behavior to <code>None</code> to improve caching performance. Update the CloudFront distribution to use the new cache policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 2327,
                        "content": "<p>Choose the <code>Customize</code> option for the <code>Object Caching</code> setting and reduce the <code>Default TTL</code> value so that CloudFront forwards requests to your origin more frequently</p>",
                        "isValid": false
                    },
                    {
                        "id": 2328,
                        "content": "<p>Create a new cache policy for the CloudFront distribution and set the cache behavior to <code>Query string forwarding and caching</code>. In the Query string whitelist field include the language string. Update the CloudFront distribution to use the new cache policy</p>",
                        "isValid": true
                    },
                    {
                        "id": 2329,
                        "content": "<p>Create a new cache policy for the CloudFront distribution and set the cache behavior to <code>Cache based on selected request headers</code>. Use <code>Whitelist Headers</code> as the caching criteria</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 568,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.814Z",
                "updatedAt": "2023-09-07T08:39:48.814Z",
                "content": "<p>You have created a test environment in Elastic Beanstalk and as part of that environment, you have created an RDS database.</p>\n\n<p>How can you make sure the database can be explored after the environment is destroyed?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Make a snapshot of the database before it gets deleted</strong></p>\n\n<p>Use an Elastic Beanstalk blue (environment A)/green (environment B) deployment to decouple an RDS DB instance from environment.</p>\n\n<p>Create a new Elastic Beanstalk environment (environment B) with the necessary information to connect to the RDS DB instance.</p>\n\n<p>Note: An RDS DB instance attached to an Elastic Beanstalk environment is ideal for development and testing environments. However, it's not ideal for production environments because the lifecycle of the database instance is tied to the lifecycle of your application environment. If you terminate the environment, then you lose your data because the RDS DB instance is deleted by the environment. For more information, see Using Elastic Beanstalk with Amazon RDS.</p>\n\n<p>This is the only way to recover the database data before it gets deleted by Elastic Beanstalk.</p>\n\n<p>Please review this excellent document that addresses this use-case :</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/\">https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Make a selective delete in Elastic Beanstalk</strong> - This is not a feature in Elastic Beanstalk.</p>\n\n<p><strong>Change the Elastic Beanstalk environment variables</strong> - Environment variables won't help with the provisioned RDS database.</p>\n\n<p><strong>Convert the Elastic Beanstalk environment to a worker environment</strong> - You can't convert Elastic Beanstalk environments, you can only change their settings.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/\">https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/</a></p>\n",
                "options": [
                    {
                        "id": 2330,
                        "content": "<p>Convert the Elastic Beanstalk environment to a worker environment</p>",
                        "isValid": false
                    },
                    {
                        "id": 2331,
                        "content": "<p>Change the Elastic Beanstalk environment variables</p>",
                        "isValid": false
                    },
                    {
                        "id": 2332,
                        "content": "<p>Make a snapshot of the database before it gets deleted</p>",
                        "isValid": true
                    },
                    {
                        "id": 2333,
                        "content": "<p>Make a selective delete in Elastic Beanstalk</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 569,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.884Z",
                "updatedAt": "2023-09-07T08:39:48.884Z",
                "content": "<p>An IT company leverages CodePipeline to automate its release pipelines. The development team wants to write a Lambda function that will send notifications for state changes within the pipeline.</p>\n\n<p>As a Developer Associate, which steps would you suggest to associate the Lambda function with the event source?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams.</p>\n\n<p>CloudWatch Events Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p><strong>Set up an Amazon CloudWatch Events rule that uses CodePipeline as an event source with the target as the Lambda function</strong></p>\n\n<p>You can use Amazon CloudWatch Events to detect and react to changes in the state of a pipeline, stage, or action. Then, based on rules you create, CloudWatch Events invokes one or more target actions when a pipeline, stage, or action enters the state you specify in a rule. For the given use-case, you can set up a rule that detects pipeline changes and invokes an AWS Lambda function.</p>\n\n<p>Amazon CloudWatch Events With CodePipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q3-i2.jpg\">\n<a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that monitors status changes in Code Pipeline and triggers the Lambda function</strong> - As mentioned in the explanation above, you need to use a CloudWatch event and not CloudWatch alarm for this use-case.</p>\n\n<p><strong>Use the Lambda console to configure a trigger that invokes the Lambda function with CodePipeline as the event source</strong> - You cannot create a trigger with CodePipeline as the event source via the Lambda Console.</p>\n\n<p><strong>Use the CodePipeline console to set up a trigger for the Lambda function</strong> - CodePipeline console cannot be used to configure a trigger for a Lambda function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n",
                "options": [
                    {
                        "id": 2334,
                        "content": "<p>Set up an Amazon CloudWatch alarm that monitors status changes in Code Pipeline and triggers the Lambda function</p>",
                        "isValid": false
                    },
                    {
                        "id": 2335,
                        "content": "<p>Use the CodePipeline console to set up a trigger for the Lambda function</p>",
                        "isValid": false
                    },
                    {
                        "id": 2336,
                        "content": "<p>Set up an Amazon CloudWatch Events rule that uses CodePipeline as an event source with the target as the Lambda function</p>",
                        "isValid": true
                    },
                    {
                        "id": 2337,
                        "content": "<p>Use the Lambda console to configure a trigger that invokes the Lambda function with CodePipeline as the event source</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 570,
            "attributes": {
                "createdAt": "2023-09-07T08:39:48.963Z",
                "updatedAt": "2023-09-07T08:39:48.963Z",
                "content": "<p>You are using AWS SQS FIFO queues to get the ordering of messages on a per <code>user_id</code> basis. On top of this, you would like to make sure that duplicate messages should not be sent to SQS as this would cause application failure.</p>\n\n<p>As a developer, which message parameter should you set for deduplicating messages?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS FIFO queues are designed to enhance messaging between applications when the order of operations and events has to be enforced.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q35-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n\n<p><strong>MessageDeduplicationId</strong></p>\n\n<p>The message deduplication ID is the token used for the deduplication of sent messages. If a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>MessageGroupId</strong> - The message group ID is the tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are always processed one by one, in a strict order relative to the message group (however, messages that belong to different message groups might be processed out of order).</p>\n\n<p><strong>ReceiveRequestAttemptId</strong> - This parameter applies only to FIFO (first-in-first-out) queues. The token is used for deduplication of ReceiveMessage calls. If a networking issue occurs after a ReceiveMessage action, and instead of a response you receive a generic error, you can retry the same action with an identical ReceiveRequestAttemptId to retrieve the same set of messages, even if their visibility timeout has not yet expired.</p>\n\n<p><strong>ContentBasedDeduplication</strong> - This is not a message parameter, but a queue setting. Enable content-based deduplication to instruct Amazon SQS to use an SHA-256 hash to generate the message deduplication ID using the body of the message - but not the attributes of the message.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html</a></p>\n",
                "options": [
                    {
                        "id": 2338,
                        "content": "<p>MessageGroupId</p>",
                        "isValid": false
                    },
                    {
                        "id": 2339,
                        "content": "<p>ReceiveRequestAttemptId</p>",
                        "isValid": false
                    },
                    {
                        "id": 2340,
                        "content": "<p>MessageDeduplicationId</p>",
                        "isValid": true
                    },
                    {
                        "id": 2341,
                        "content": "<p>ContentBasedDeduplication</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 571,
            "attributes": {
                "createdAt": "2023-09-07T08:39:49.041Z",
                "updatedAt": "2023-09-07T08:39:49.041Z",
                "content": "<p>You are using AWS SQS FIFO queues to get the ordering of messages on a per <code>user_id</code> basis.</p>\n\n<p>As a developer, which message parameter should you set the value of <code>user_id</code> to guarantee the ordering?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS FIFO queues are designed to enhance messaging between applications when the order of operations and events has to be enforced.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q33-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n\n<p><strong>MessageGroupId</strong></p>\n\n<p>The message group ID is the tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are always processed one by one, in a strict order relative to the message group (however, messages that belong to different message groups might be processed out of order).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>MessageDeduplicationId</strong> - The message deduplication ID is the token used for the deduplication of sent messages. If a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval.</p>\n\n<p><strong>MessageOrderId</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p><strong>MessageHash</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagegroupid-property.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagegroupid-property.html</a></p>\n",
                "options": [
                    {
                        "id": 2342,
                        "content": "<p>MessageDeduplicationId</p>",
                        "isValid": false
                    },
                    {
                        "id": 2343,
                        "content": "<p>MessageGroupId</p>",
                        "isValid": true
                    },
                    {
                        "id": 2344,
                        "content": "<p>MessageHash</p>",
                        "isValid": false
                    },
                    {
                        "id": 2345,
                        "content": "<p>MessageOrderId</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 572,
            "attributes": {
                "createdAt": "2023-09-07T08:39:49.118Z",
                "updatedAt": "2023-09-07T08:39:49.118Z",
                "content": "<p>You were assigned to a project that requires the use of the AWS CLI to build a project with AWS CodeBuild. Your project's root directory includes the buildspec.yml file to run build commands and would like your build artifacts to be automatically encrypted at the end.</p>\n\n<p>How should you configure CodeBuild to accomplish this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Specify a KMS key to use</strong></p>\n\n<p>AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.</p>\n\n<p>For AWS CodeBuild to encrypt its build output artifacts, it needs access to an AWS KMS customer master key (CMK). By default, AWS CodeBuild uses the AWS-managed CMK for Amazon S3 in your AWS account. The following environment variable provides these details:</p>\n\n<p>CODEBUILD_KMS_KEY_ID: The identifier of the AWS KMS key that CodeBuild is using to encrypt the build output artifact (for example, arn:aws:kms:region-ID:account-ID:key/key-ID or alias/key-alias).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an AWS Lambda Hook</strong> - Code hook is used for integration with Lambda and is not relevant for the given use-case.</p>\n\n<p><strong>Use the AWS Encryption SDK</strong> - The SDK just makes it easier for you to implement encryption best practices in your application and is not relevant for the given use-case.</p>\n\n<p><strong>Use In-Flight encryption (SSL)</strong> - SSL is usually for internet traffic which in this case will be using internal traffic through AWS and is not relevant for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html</a></p>\n",
                "options": [
                    {
                        "id": 2346,
                        "content": "<p>Use the AWS Encryption SDK</p>",
                        "isValid": false
                    },
                    {
                        "id": 2347,
                        "content": "<p>Use In Flight encryption (SSL)</p>",
                        "isValid": false
                    },
                    {
                        "id": 2348,
                        "content": "<p>Specify a KMS key to use</p>",
                        "isValid": true
                    },
                    {
                        "id": 2349,
                        "content": "<p>Use an AWS Lambda Hook</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 573,
            "attributes": {
                "createdAt": "2023-09-07T08:39:49.206Z",
                "updatedAt": "2023-09-07T08:39:49.206Z",
                "content": "<p>A business-critical mobile application uses Amazon Cognito user pools with multi-factor authentication (MFA) enabled for all its users. The application manages confidential data about the company's sales forecasts and product launches. Considering the highly critical nature of the application, the company wants to track every user login activity via a notification sent as an email to the security team.</p>\n\n<p>Which of the following would you recommend as the MOST optimal way of implementing this requirement within a short period?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Lambda function that uses Amazon Simple Email Service to send an email notification to the concerned security team. Configure this function as Amazon Cognito post-authentication Lambda trigger</strong></p>\n\n<p>Amazon Cognito invokes Post authentication Lambda trigger after signing in a user, you can add custom logic after Amazon Cognito authenticates the user.</p>\n\n<p>Post-authentication Lambda flows:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q37-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Lambda function that uses Amazon Simple Email Service to send an email notification to the concerned security team. Configure this function as Amazon Cognito pre-authentication Lambda trigger</strong> - Pre-authentication Lambda trigger: Amazon Cognito invokes this trigger when a user attempts to sign in so that you can create custom validation that accepts or denies the authentication request. This is not useful for the current use case since we want to track user login activity which happens post-authentication.</p>\n\n<p><strong>Configure an AWS Lambda function as a trigger to Amazon Cognito identity pools authenticated API operations. Create the Lambda function to utilize the Amazon Simple Email Service to send an email notification to the concerned security team</strong> - This statement is incorrect. Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. Cognito identity pools are for authorization and not for authentication.</p>\n\n<p><strong>Configure Amazon Cognito user pools authenticated API operations and MFA API operations to send all login data to Amazon Kinesis Data Streams. Configure an AWS Lambda function to analyze these streams and trigger an SNS notification to the security team based on user access</strong> - This is a made-up option given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/user-pools-API-operations.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/user-pools-API-operations.html</a></p>\n",
                "options": [
                    {
                        "id": 2350,
                        "content": "<p>Configure Amazon Cognito user pools authenticated API operations and MFA API operations to send all login data to Amazon Kinesis Data Streams. Configure an AWS Lambda function to analyze these streams and trigger an SNS notification to the security team based on user access</p>",
                        "isValid": false
                    },
                    {
                        "id": 2351,
                        "content": "<p>Configure an AWS Lambda function as a trigger to Amazon Cognito identity pools authenticated API operations. Create the Lambda function to utilize the Amazon Simple Email Service to send an email notification to the concerned security team</p>",
                        "isValid": false
                    },
                    {
                        "id": 2352,
                        "content": "<p>Create an AWS Lambda function that uses Amazon Simple Email Service to send an email notification to the concerned security team. Configure this function as Amazon Cognito post-authentication Lambda trigger</p>",
                        "isValid": true
                    },
                    {
                        "id": 2353,
                        "content": "<p>Create an AWS Lambda function that uses Amazon Simple Email Service to send an email notification to the concerned security team. Configure this function as Amazon Cognito pre-authentication Lambda trigger</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 574,
            "attributes": {
                "createdAt": "2023-09-07T08:39:49.283Z",
                "updatedAt": "2023-09-07T08:39:49.283Z",
                "content": "<p>You are running a web application where users can author blogs and share them with their followers. Most of the workflow is read based, but when a blog is updated, you would like to ensure that the latest data is served to the users (no stale data). The Developer has already suggested using ElastiCache to cope with the read load but has asked you to implement a caching strategy that complies with the requirements of the site.</p>\n\n<p>Which strategy would you recommend?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use a Write Through strategy</strong></p>\n\n<p>The write-through strategy adds data or updates data in the cache whenever data is written to the database.</p>\n\n<p>In a Write Through strategy, any new blog or update to the blog will be written to both the database layer and the caching layer, thus ensuring that the latest data is always served from the cache.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q29-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a Lazy Loading strategy without TTL</strong></p>\n\n<p>Lazy Loading is a caching strategy that loads data into the cache only when necessary. Whenever your application requests data, it first requests the ElastiCache cache. If the data exists in the cache and is current, ElastiCache returns the data to your application. If the data doesn't exist in the cache or has expired, your application requests the data from your data store.</p>\n\n<p><strong>Use a Lazy Loading strategy with TTL</strong></p>\n\n<p>In the case of Lazy Loading, the data is loaded onto the cache whenever the data is missing from the cache. In case the blog gets updated, it won't be updated from the cache unless that cache expires (in case you used a TTL). Time to live (TTL) is an integer value that specifies the number of seconds until the key expires. When an application attempts to read an expired key, it is treated as though the key is not found. The database is queried for the key and the cache is updated. Therefore, for a while, old data will be served to users which is a problem from a requirements perspective as we don't want any stale data.</p>\n\n<p><strong>Use DAX</strong> - This is a cache for DynamoDB based implementations, but in this question, we are considering ElastiCache. Therefore this option is not relevant.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p>\n",
                "options": [
                    {
                        "id": 2354,
                        "content": "<p>Use a Lazy Loading strategy without TTL</p>",
                        "isValid": false
                    },
                    {
                        "id": 2355,
                        "content": "<p>Use a Lazy Loading strategy with TTL</p>",
                        "isValid": false
                    },
                    {
                        "id": 2356,
                        "content": "<p>Use a Write Through strategy</p>",
                        "isValid": true
                    },
                    {
                        "id": 2357,
                        "content": "<p>Use DAX</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 575,
            "attributes": {
                "createdAt": "2023-09-07T08:39:49.361Z",
                "updatedAt": "2023-09-07T08:39:49.361Z",
                "content": "<p>Which of the following CLI options will allow you to retrieve a subset of the attributes coming from a DynamoDB scan?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>--projection-expression</strong></p>\n\n<p>A projection expression is a string that identifies the attributes you want. To retrieve a single attribute, specify its name. For multiple attributes, the names must be comma-separated.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html</a></p>\n\n<p>To read data from a table, you use operations such as GetItem, Query, or Scan. DynamoDB returns all of the item attributes by default. To get just some, rather than all of the attributes, use a projection expression.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>--filter-expression</strong> - If you need to further refine the Query results, you can optionally provide a filter expression. A filter expression determines which items within the Query results should be returned to you. All of the other results are discarded. A filter expression is applied after Query finishes, but before the results are returned. Therefore, a Query will consume the same amount of read capacity, regardless of whether a filter expression is present.</p>\n\n<p><strong>--page-size</strong> - You can use the --page-size option to specify that the AWS CLI requests a smaller number of items from each call to the AWS service. The CLI still retrieves the full list but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call.</p>\n\n<p><strong>--max-items</strong> - To include fewer items at a time in the AWS CLI output, use the --max-items option. The AWS CLI still handles pagination with the service as described above, but prints out only the number of items at a time that you specify.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html</a></p>\n",
                "options": [
                    {
                        "id": 2358,
                        "content": "<p>--page-size</p>",
                        "isValid": false
                    },
                    {
                        "id": 2359,
                        "content": "<p>--filter-expression</p>",
                        "isValid": false
                    },
                    {
                        "id": 2360,
                        "content": "<p>--projection-expression</p>",
                        "isValid": true
                    },
                    {
                        "id": 2361,
                        "content": "<p>--max-items</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 576,
            "attributes": {
                "createdAt": "2023-09-07T08:39:49.441Z",
                "updatedAt": "2023-09-07T08:39:49.441Z",
                "content": "<p>Your client wants to deploy a service on EC2 instances, and as EC2 instances are added into an ASG, each EC2 instance should be running 3 different Docker Containers simultaneously.</p>\n\n<p>What Elastic Beanstalk platform should they choose?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Docker multi-container platform</strong></p>\n\n<p>Docker is a container platform that allows you to define your software stack and store it in an image that can be downloaded from a remote repository. Use the Multicontainer Docker platform if you need to run multiple containers on each instance. The Multicontainer Docker platform does not include a proxy server. Elastic Beanstalk uses Amazon Elastic Container Service (Amazon ECS) to coordinate container deployments to multi-container Docker environments.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Docker single-container platform</strong> - Docker is a container platform that allows you to define your software stack and store it in an image that can be downloaded from a remote repository. Use the Single Container Docker platform if you only need to run a single Docker container on each instance in your environment. The single container platform includes an Nginx proxy server.</p>\n\n<p><strong>Custom Platform</strong> - Elastic Beanstalk supports custom platforms. A custom platform provides more advanced customization than a custom image in several ways. A custom platform lets you develop an entirely new platform from scratch, customizing the operating system, additional software, and scripts that Elastic Beanstalk runs on platform instances. This flexibility enables you to build a platform for an application that uses a language or other infrastructure software, for which Elastic Beanstalk doesn't provide a managed platform. Compare that to custom images, where you modify an Amazon Machine Image (AMI) for use with an existing Elastic Beanstalk platform, and Elastic Beanstalk still provides the platform scripts and controls the platform's software stack. Besides, with custom platforms, you use an automated, scripted way to create and maintain your customization, whereas with custom images you make the changes manually over a running instance.</p>\n\n<p><strong>Third Party Platform</strong> - This is a made-up option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/platforms/platforms-supported.html#platforms-supported.mcdocker\">https://docs.aws.amazon.com/elasticbeanstalk/latest/platforms/platforms-supported.html#platforms-supported.mcdocker</a></p>\n",
                "options": [
                    {
                        "id": 2362,
                        "content": "<p>Docker multi-container platform</p>",
                        "isValid": true
                    },
                    {
                        "id": 2363,
                        "content": "<p>Docker single-container platform</p>",
                        "isValid": false
                    },
                    {
                        "id": 2364,
                        "content": "<p>Custom platform</p>",
                        "isValid": false
                    },
                    {
                        "id": 2365,
                        "content": "<p>Third-party platform</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 577,
            "attributes": {
                "createdAt": "2023-09-07T08:39:49.514Z",
                "updatedAt": "2023-09-07T08:39:49.514Z",
                "content": "<p>Your Lambda function processes files for your customers and as part of that process, it creates a lot of intermediary files it needs to store on its disk and then discard.</p>\n\n<p>What is the best way to store temporary files for your Lambda functions that will be discarded when the function stops running?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use the local directory /tmp</strong></p>\n\n<p>This is 512MB of temporary space you can use for your Lambda functions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a tmp/ directory in the source zip file and use it</strong> - This option has been added as a distractor, as you can't access a directory within a zip file.</p>\n\n<p><strong>Use the local directory /opt</strong> - This option has been added as a distractor. This path is not accessible.</p>\n\n<p><strong>Use an S3 bucket</strong> - This won't be temporary after the Lambda function is deleted, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/limits.html\">https://docs.aws.amazon.com/lambda/latest/dg/limits.html</a></p>\n",
                "options": [
                    {
                        "id": 2366,
                        "content": "<p>Use the local directory /opt</p>",
                        "isValid": false
                    },
                    {
                        "id": 2367,
                        "content": "<p>Use an S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 2368,
                        "content": "<p>Use the local directory /tmp</p>",
                        "isValid": true
                    },
                    {
                        "id": 2369,
                        "content": "<p>Create a tmp/ directory in the source zip file and use it</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 578,
            "attributes": {
                "createdAt": "2023-09-07T08:39:49.586Z",
                "updatedAt": "2023-09-07T08:39:49.586Z",
                "content": "<p>You are working with a t2.small instance bastion host that has the AWS CLI installed to help manage all the AWS services installed on it. You would like to know the security group and the instance id of the current instance.</p>\n\n<p>Which of the following will help you fetch the needed information?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Query the metadata at http://169.254.169.254/latest/meta-data</strong> - Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you're writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application. To view all categories of instance metadata from within a running instance, use the following URI - <code>http://169.254.169.254/latest/meta-data/</code>. The IP address 169.254.169.254 is a link-local address and is valid only from the instance. All instance metadata is returned as text (HTTP content type text/plain).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM role and attach it to your EC2 instance that helps you perform a 'describe' API call</strong> - The AWS CLI has a describe-instances API call needs instance ID as an input. So, this will not work for the current use case wherein we do not know the instance ID.</p>\n\n<p><strong>Query the user data at http://169.254.169.254/latest/user-data</strong> - This address retrieves the user data that you specified when launching your instance.</p>\n\n<p><strong>Query the user data at http://254.169.254.169/latest/meta-data</strong> - The IP address specified is wrong.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html</a></p>\n\n<p><a href=\"https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/describe-instances.html\">https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/describe-instances.html</a></p>\n",
                "options": [
                    {
                        "id": 2370,
                        "content": "<p>Query the metadata at http://169.254.169.254/latest/meta-data</p>",
                        "isValid": true
                    },
                    {
                        "id": 2371,
                        "content": "<p>Query the user data at http://254.169.254.169/latest/meta-data</p>",
                        "isValid": false
                    },
                    {
                        "id": 2372,
                        "content": "<p>Create an IAM role and attach it to your EC2 instance that helps you perform a 'describe' API call</p>",
                        "isValid": false
                    },
                    {
                        "id": 2373,
                        "content": "<p>Query the user data at http://169.254.169.254/latest/user-data</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 579,
            "attributes": {
                "createdAt": "2023-09-07T08:39:49.655Z",
                "updatedAt": "2023-09-07T08:39:49.655Z",
                "content": "<p>A company has recently launched a new gaming application that the users are adopting rapidly. The company uses RDS MySQL as the database. The development team wants an urgent solution to this issue where the rapidly increasing workload might exceed the available database storage.</p>\n\n<p>As a developer associate, which of the following solutions would you recommend so that it requires minimum development effort to address this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable storage auto-scaling for RDS MySQL</strong></p>\n\n<p>If your workload is unpredictable, you can enable storage autoscaling for an Amazon RDS DB instance. With storage autoscaling enabled, when Amazon RDS detects that you are running out of free database space it automatically scales up your storage. Amazon RDS starts a storage modification for an autoscaling-enabled DB instance when these factors apply:</p>\n\n<p>Free available space is less than 10 percent of the allocated storage.</p>\n\n<p>The low-storage condition lasts at least five minutes.</p>\n\n<p>At least six hours have passed since the last storage modification.</p>\n\n<p>The maximum storage threshold is the limit that you set for autoscaling the DB instance. You can't set the maximum storage threshold for autoscaling-enabled instances to a value greater than the maximum allocated storage.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate RDS MySQL to Aurora which offers storage auto-scaling</strong> - Although Aurora offers automatic storage scaling, this option is ruled out since it involves significant systems administration effort to migrate from RDS MySQL to Aurora. It is much easier to just enable storage auto-scaling for RDS MySQL.</p>\n\n<p><strong>Migrate RDS MySQL database to DynamoDB which automatically allocates storage space when required</strong> - This option is ruled out since DynamoDB is a NoSQL database which implies significant development effort to change the application logic to connect and query data from the underlying database. It is much easier to just enable storage auto-scaling for RDS MySQL.</p>\n\n<p><strong>Create read replica for RDS MySQL</strong> - Read replicas make it easy to take advantage of supported engines' built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create multiple read replicas for a given source DB Instance and distribute your applicationâ€™s read traffic amongst them. This option acts as a distractor as read replicas cannot help to automatically scale storage for the primary database.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html</a></p>\n",
                "options": [
                    {
                        "id": 2374,
                        "content": "<p>Enable storage auto-scaling for RDS MySQL</p>",
                        "isValid": true
                    },
                    {
                        "id": 2375,
                        "content": "<p>Migrate RDS MySQL database to Aurora which offers storage auto-scaling</p>",
                        "isValid": false
                    },
                    {
                        "id": 2376,
                        "content": "<p>Create read replica for RDS MySQL</p>",
                        "isValid": false
                    },
                    {
                        "id": 2377,
                        "content": "<p>Migrate RDS MySQL database to DynamoDB which automatically allocates storage space when required</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 580,
            "attributes": {
                "createdAt": "2023-09-07T08:39:49.728Z",
                "updatedAt": "2023-09-07T08:39:49.728Z",
                "content": "<p>One of your Kinesis Stream is experiencing increased traffic due to a sale day. Therefore your Kinesis Administrator has split shards and thus you went from having 6 shards to having 10 shards in your Kinesis Stream. Your consuming application is running a KCL-based application on EC2 instances.</p>\n\n<p>What is the maximum number of EC2 instances that can be deployed to process the shards?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>10</strong></p>\n\n<p>Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs.</p>\n\n<p>A Kinesis data stream is a set of shards. A shard is a uniquely identified sequence of data records in a stream. A stream is composed of one or more shards, each of which provides a fixed unit of capacity.</p>\n\n<p>Kinesis Data Streams Overview:\n<img src=\"https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>Each KCL consumer application instance uses \"workers\" to process data in Kinesis shards.  At any given time, each shard of data records is bound to a particular worker via a lease. For the given use-case, an EC2 instance acts as the worker for the KCL application.  You can have at most one EC2 instance per shard in Kinesis for the given application. As we have 10 shards, the max number of EC2 instances is 10.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html\">https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>1</strong></p>\n\n<p><strong>6</strong></p>\n\n<p><strong>20</strong></p>\n\n<p>These three options contradict the explanation provided earlier. So these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-kcl.html\">https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-kcl.html</a></p>\n",
                "options": [
                    {
                        "id": 2378,
                        "content": "<p>6</p>",
                        "isValid": false
                    },
                    {
                        "id": 2379,
                        "content": "<p>1</p>",
                        "isValid": false
                    },
                    {
                        "id": 2380,
                        "content": "<p>20</p>",
                        "isValid": false
                    },
                    {
                        "id": 2381,
                        "content": "<p>10</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 581,
            "attributes": {
                "createdAt": "2023-09-07T08:39:49.798Z",
                "updatedAt": "2023-09-07T08:39:49.798Z",
                "content": "<p>A security company is requiring all developers to perform server-side encryption with customer-provided encryption keys when performing operations in AWS S3. Developers should write software with C# using the AWS SDK and implement the requirement in the PUT, GET, Head, and Copy operations.</p>\n\n<p>Which of the following encryption methods meets this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>SSE-C</strong></p>\n\n<p>You have the following options for protecting data at rest in Amazon S3:</p>\n\n<p>Server-Side Encryption â€“ Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.</p>\n\n<p>Client-Side Encryption â€“ Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>For the given use-case, the company wants to manage the encryption keys via its custom application and let S3 manage the encryption, therefore you must use Server-Side Encryption with Customer-Provided Keys (SSE-C).</p>\n\n<p>Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it writes to disks, and decryption, when you access your objects.</p>\n\n<p>Please review these three options for Server Side Encryption on S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q55-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-KMS</strong> - Server-Side Encryption with Customer Master Keys (CMKs) stored in AWS Key Management Service (SSE-KMS) is similar to SSE-S3. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Additionally, you can create and manage customer-managed CMKs or use AWS managed CMKs that are unique to you, your service, and your Region.</p>\n\n<p><strong>Client-Side Encryption</strong> - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p><strong>SSE-S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n",
                "options": [
                    {
                        "id": 2382,
                        "content": "<p>Client-Side Encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 2383,
                        "content": "<p>SSE-C</p>",
                        "isValid": true
                    },
                    {
                        "id": 2384,
                        "content": "<p>SSE-S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 2385,
                        "content": "<p>SSE-KMS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 582,
            "attributes": {
                "createdAt": "2023-09-07T08:39:49.877Z",
                "updatedAt": "2023-09-07T08:39:49.877Z",
                "content": "<p>The customer feedback functionality for a company's flagship web application is handled via an Amazon API Gateway based REST API that invokes an AWS Lambda function for further processing. Although the performance of the function is satisfactory, the development team has been tasked to optimize the startup time of the Lambda function to further improve the customer experience.</p>\n\n<p>How will you optimize the Lambda function for faster initialization?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure provisioned concurrency for the Lambda function to respond immediately to the function's invocations</strong></p>\n\n<p>When Lambda allocates an instance of your function, the runtime loads your function's code and runs the initialization code that you define outside of the handler. If your code and dependencies are large, or you create SDK clients during initialization, this process can take some time. When your function has not been used for some time, needs to scale up, or when you update a function, Lambda creates new execution environments. This causes the portion of requests that are served by new instances to have higher latency than the rest, otherwise known as a cold start.</p>\n\n<p>By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with low latency. Lambda functions configured with provisioned concurrency run with consistent start-up latency, making them ideal for building interactive mobile or web backends, latency-sensitive microservices, and synchronously invoked APIs.</p>\n\n<p>Functions with Provisioned Concurrency differ from on-demand functions in some important ways:</p>\n\n<ol>\n<li><p>Initialization code does not need to be optimized. Since this happens long before the invocation, lengthy initialization does not impact the latency of invocations. If you are using runtimes that typically take longer to initialize, like Java, the performance of these can benefit from using Provisioned Concurrency.</p></li>\n<li><p>Initialization code is run more frequently than the total number of invocations. Since Lambda is highly available, for every one unit of Provisioned Concurrency, there are a minimum of two execution environments prepared in separate Availability Zones. This is to ensure that your code is available in the event of a service disruption. As environments are reaped and load balancing occurs, Lambda over-provisions environments to ensure availability. You are not charged for this activity. If your code initializer implements logging, you will see additional log files anytime that this code is run, even though the main handler is not invoked.</p></li>\n<li><p>Provisioned Concurrency cannot be used with the $LATEST version. This feature can only be used with published versions and aliases of a function. If you see cold starts for functions configured to use Provisioned Concurrency, you may be invoking the $LATEST version, instead of the version or alias with Provisioned Concurrency configured.</p></li>\n</ol>\n\n<p>Reducing cold starts with Provisioned Concurrency:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q14-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/\">https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure reserved concurrency to guarantee the maximum number of concurrent instances of the Lambda function</strong> - Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can use that concurrency. There is no charge for configuring reserved concurrency for a function. Whereas, provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations.</p>\n\n<p><strong>Configure an interface VPC endpoint powered by AWS PrivateLink to access the Amazon API Gateway REST API with milliseconds latency</strong> - An interface VPC endpoint can be used to connect your VPC resources to the AWS Lambda function without crossing the public internet. VPC endpoint is irrelevant to the current discussion.</p>\n\n<p><strong>Enable API caching in Amazon API Gateway to cache AWS Lambda function response</strong> - With caching, you can reduce the number of calls made to your AWS Lambda function and also improve the latency of requests to your API. Caching is best-effort and applications making frequent API calls to retrieve static data can benefit from a caching layer. Caching does not reduce the initialization time Lambda takes and hence is not an optimal solution for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/\">https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p>\n",
                "options": [
                    {
                        "id": 2386,
                        "content": "<p>Configure reserved concurrency to guarantee the maximum number of concurrent instances of the Lambda function</p>",
                        "isValid": false
                    },
                    {
                        "id": 2387,
                        "content": "<p>Enable API caching in Amazon API Gateway to cache AWS Lambda function response</p>",
                        "isValid": false
                    },
                    {
                        "id": 2388,
                        "content": "<p>Configure provisioned concurrency for the Lambda function to respond immediately to the function's invocations</p>",
                        "isValid": true
                    },
                    {
                        "id": 2389,
                        "content": "<p>Configure an interface VPC endpoint powered by AWS PrivateLink to access the Amazon API Gateway REST API with milliseconds latency</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 583,
            "attributes": {
                "createdAt": "2023-09-07T08:39:49.954Z",
                "updatedAt": "2023-09-07T08:39:49.954Z",
                "content": "<p>Applications running on EC2 instances process messages from an SQS queue but sometimes they experience errors due to messages not being processed.</p>\n\n<p>To isolate the messages, which option will help with further debugging?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Implement a Dead Letter Queue</strong></p>\n\n<p>Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.</p>\n\n<p>Sometimes, messages canâ€™t be processed because of a variety of possible issues, such as when a user comments on a story but it remains unprocessed because the original story itself is deleted by the author while the comments were being posted. In such a case, the dead-letter queue can be used to handle message processing failures.</p>\n\n<p>How do dead-letter queues work?\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p>Use-cases for dead-letter queues:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q44-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p>Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that cannot be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.</p>\n\n<p>Incorrect:</p>\n\n<p><strong>Use DeleteMessage</strong> - This API call deletes the message in the queue but does not help you find the issue.</p>\n\n<p><strong>Reduce the VisibilityTimeout</strong> - Amazon SQS uses a visibility timeout to prevent other consumers from receiving and processing the same message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. If you reduce the VisibilityTimeout, more consumers will get the failed message</p>\n\n<p><strong>Increase the VisibilityTimeout</strong> - It won't help because you don't need more time but rather an isolated place to debug.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n",
                "options": [
                    {
                        "id": 2390,
                        "content": "<p>Use DeleteMessage</p>",
                        "isValid": false
                    },
                    {
                        "id": 2391,
                        "content": "<p>Reduce the VisibilityTimeout</p>",
                        "isValid": false
                    },
                    {
                        "id": 2392,
                        "content": "<p>Increase the VisibilityTimeout</p>",
                        "isValid": false
                    },
                    {
                        "id": 2393,
                        "content": "<p>Implement a Dead Letter Queue</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 584,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.033Z",
                "updatedAt": "2023-09-07T08:39:50.033Z",
                "content": "<p>Your company wants to move away from manually managing Lambda in the AWS console and wants to upload and update them using AWS CloudFormation.</p>\n\n<p>How do you declare an AWS Lambda function in CloudFormation? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\">\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p><strong>Upload all the code as a zip to S3 and refer the object in <code>AWS::Lambda::Function</code> block</strong></p>\n\n<p>You can upload all the code as a zip to S3 and refer the object in <code>AWS::Lambda::Function</code> block.</p>\n\n<p>The AWS::Lambda::Function resource creates a Lambda function. To create a function, you need a deployment package and an execution role. The deployment package contains your function code.</p>\n\n<p><strong>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block as long as there are no third-party dependencies</strong></p>\n\n<p>The other option is to write the code inline for Node.js and Python as long as there are no dependencies for your code, besides the dependencies already provided by AWS in your Lambda Runtime (aws-sdk and cfn-response and many other AWS related libraries are preloaded via, for example, boto3 (python) in the lambda instances.)</p>\n\n<p>YAML template for creating a Lambda function:</p>\n\n<pre><code>Type: AWS::Lambda::Function\nProperties:\n  Code:\n    Code\n  DeadLetterConfig:\n    DeadLetterConfig\n  Description: String\n  Environment:\n    Environment\n  FileSystemConfigs:\n    - FileSystemConfig\n  FunctionName: String\n  Handler: String\n  KmsKeyArn: String\n  Layers:\n    - String\n  MemorySize: Integer\n  ReservedConcurrentExecutions: Integer\n  Role: String\n  Runtime: String\n  Tags:\n    - Tag\n  Timeout: Integer\n  TracingConfig:\n    TracingConfig\n  VpcConfig:\n    VpcConfig\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upload all the code to CodeCommit and refer to the CodeCommit Repository in <code>AWS::Lambda::Function</code> block</strong></p>\n\n<p><strong>Upload all the code as a folder to S3 and refer the folder in <code>AWS::Lambda::Function</code> block</strong></p>\n\n<p><strong>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block and reference the dependencies as a zip file stored in S3</strong></p>\n\n<p>These three options contradict the explanation provided earlier. So these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html</a></p>\n",
                "options": [
                    {
                        "id": 2394,
                        "content": "<p>Upload all the code as a zip to S3 and refer the object in <code>AWS::Lambda::Function</code> block</p>",
                        "isValid": true
                    },
                    {
                        "id": 2395,
                        "content": "<p>Upload all the code as a folder to S3 and refer the folder in <code>AWS::Lambda::Function</code> block</p>",
                        "isValid": false
                    },
                    {
                        "id": 2396,
                        "content": "<p>Upload all the code to CodeCommit and refer to the CodeCommit Repository in <code>AWS::Lambda::Function</code> block</p>",
                        "isValid": false
                    },
                    {
                        "id": 2397,
                        "content": "<p>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block as long as there are no third-party dependencies</p>",
                        "isValid": true
                    },
                    {
                        "id": 2398,
                        "content": "<p>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block and reference the dependencies as a zip file stored in S3</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 585,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.123Z",
                "updatedAt": "2023-09-07T08:39:50.123Z",
                "content": "<p>Your company likes to operate multiple AWS accounts so that teams have their environments. Services deployed across these accounts interact with one another, and now there's a requirement to implement X-Ray traces across all your applications deployed on EC2 instances and AWS accounts.</p>\n\n<p>As such, you would like to have a unified account to view all the traces. What should you in your X-Ray daemon set up to make this work? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your applicationâ€™s underlying components.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p><strong>Create a role in the target unified account and allow roles in each sub-account to assume the role</strong></p>\n\n<p><strong>Configure the X-Ray daemon to use an IAM instance role</strong></p>\n\n<p>The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account.</p>\n\n<p>X-Ray can also track requests flowing through applications or services across multiple AWS Regions.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q27-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/xray/faqs/\">https://aws.amazon.com/xray/faqs/</a></p>\n\n<p>You can create the necessary configurations for cross-account access via this reference documentation -\n<a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a user in the target unified account and generate access and secret keys</strong></p>\n\n<p><strong>Configure the X-Ray daemon to use access and secret keys</strong></p>\n\n<p>These two options combined together would work but wouldn't be a best-practice security-wise. Therefore these are not correct.</p>\n\n<p><strong>Enable Cross Account collection in the X-Ray console</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/xray/faqs/\">https://aws.amazon.com/xray/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html</a></p>\n",
                "options": [
                    {
                        "id": 2399,
                        "content": "<p>Create a role in the target unified account and allow roles in each sub-account to assume the role.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2400,
                        "content": "<p>Configure the X-Ray daemon to use access and secret keys</p>",
                        "isValid": false
                    },
                    {
                        "id": 2401,
                        "content": "<p>Configure the X-Ray daemon to use an IAM instance role</p>",
                        "isValid": true
                    },
                    {
                        "id": 2402,
                        "content": "<p>Enable Cross Account collection in the X-Ray console</p>",
                        "isValid": false
                    },
                    {
                        "id": 2403,
                        "content": "<p>Create a user in the target unified account and generate access and secret keys</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 586,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.194Z",
                "updatedAt": "2023-09-07T08:39:50.194Z",
                "content": "<p>You've just deployed an AWS Lambda function. The lambda function will be invoked via the API Gateway. The API Gateway will need to control access to it.</p>\n\n<p>Which of the following mechanisms is not supported for API Gateway?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p><strong>STS</strong></p>\n\n<p>The AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). However, is not supported at the time with API Gateway.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>IAM permissions with sigv4</strong> - They can be applied to an entire API or individual methods.</p>\n\n<p><strong>Lambda Authorizer</strong> - Control access to REST API methods using bearer token authentication as well as information described by headers, paths, query strings, stage variables, or context variables request parameter.</p>\n\n<p><strong>Cognito User Pools</strong> - Use Cognito User Pools to create customizable authentication and authorization solutions for your REST APIs.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n",
                "options": [
                    {
                        "id": 2404,
                        "content": "<p>Lambda Authorizer</p>",
                        "isValid": false
                    },
                    {
                        "id": 2405,
                        "content": "<p>Cognito User Pools</p>",
                        "isValid": false
                    },
                    {
                        "id": 2406,
                        "content": "<p>STS</p>",
                        "isValid": true
                    },
                    {
                        "id": 2407,
                        "content": "<p>IAM permissions with sigv4</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 587,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.257Z",
                "updatedAt": "2023-09-07T08:39:50.257Z",
                "content": "<p>You are looking to invoke an AWS Lambda function every hour (similar to a cron job) in a serverless way.</p>\n\n<p>Which event source should you use for your AWS Lambda function?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>CloudWatch Events</strong></p>\n\n<p>You can create a Lambda function and direct CloudWatch Events to execute it on a regular schedule. You can specify a fixed rate (for example, execute a Lambda function every hour or 15 minutes), or you can specify a Cron expression.</p>\n\n<p>CloudWatch Events Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q38-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p>Schedule Expressions for CloudWatch Events Rules:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q38-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3</strong></p>\n\n<p><strong>SQS</strong></p>\n\n<p><strong>Kinesis</strong></p>\n\n<p>These three AWS services don't have cron capabilities, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html</a></p>\n",
                "options": [
                    {
                        "id": 2408,
                        "content": "<p>Kinesis</p>",
                        "isValid": false
                    },
                    {
                        "id": 2409,
                        "content": "<p>Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 2410,
                        "content": "<p>CloudWatch Events</p>",
                        "isValid": true
                    },
                    {
                        "id": 2411,
                        "content": "<p>SQS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 588,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.336Z",
                "updatedAt": "2023-09-07T08:39:50.336Z",
                "content": "<p>A development team has configured an Elastic Load Balancer for host-based routing. The idea is to support multiple subdomains and different top-level domains.</p>\n\n<p>The rule *.sample.com matches which of the following?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>test.sample.com</strong> - You can use host conditions to define rules that route requests based on the hostname in the host header (also known as host-based routing). This enables you to support multiple subdomains and different top-level domains using a single load balancer.</p>\n\n<p>A hostname is not case-sensitive, can be up to 128 characters in length, and can contain any of the following characters:\n1. Aâ€“Z, aâ€“z, 0â€“9\n2. - .\n3. * (matches 0 or more characters)\n4. ? (matches exactly 1 character)</p>\n\n<p>You must include at least one \".\" character. You can include only alphabetical characters after the final \".\" character.</p>\n\n<p>The rule *.sample.com matches test.sample.com but doesn't match sample.com.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>sample.com</strong></p>\n\n<p><strong>sample.test.com</strong></p>\n\n<p><strong>SAMPLE.COM</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a></p>\n",
                "options": [
                    {
                        "id": 2412,
                        "content": "<p>sample.com</p>",
                        "isValid": false
                    },
                    {
                        "id": 2413,
                        "content": "<p>sample.test.com</p>",
                        "isValid": false
                    },
                    {
                        "id": 2414,
                        "content": "<p>test.sample.com</p>",
                        "isValid": true
                    },
                    {
                        "id": 2415,
                        "content": "<p>SAMPLE.COM</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 589,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.409Z",
                "updatedAt": "2023-09-07T08:39:50.409Z",
                "content": "<p>A company that specializes in cloud communications platform as a service allows software developers to programmatically use their services to send and receive text messages. The initial platform did not have a scalable architecture as all components were hosted on one server and should be redesigned for high availability and scalability.</p>\n\n<p>Which of the following options can be used to implement the new architecture? (select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>ALB + ECS</strong></p>\n\n<p>Amazon Elastic Container Service (ECS) is a highly scalable, high-performance container management service that supports Docker containers and allows you to easily run applications on a managed cluster of Amazon EC2 instances.</p>\n\n<p>How ECS Works:\n<img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png\">\nvia - <a href=\"https://aws.amazon.com/ecs/\">https://aws.amazon.com/ecs/</a></p>\n\n<p>Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.</p>\n\n<p>When you use ECS with a load balancer such as ALB deployed across multiple Availability Zones, it helps provide a scalable and highly available REST API.</p>\n\n<p><strong>API Gateway + Lambda</strong></p>\n\n<p>Amazon API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale. Using API Gateway, you can create an API that acts as a â€œfront doorâ€ for applications to access data, business logic, or functionality from your back-end services, such as EC2 or Lambda functions.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\">\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p>API Gateway and Lambda help achieve the same purpose integrating some capabilities such as authentication in a serverless fashion, with fully scalable and highly available architectures.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SES + S3</strong> - The combination of these services only provide email and object storage services.</p>\n\n<p><strong>CloudWatch + CloudFront</strong> - The combination of these services only provide monitoring and fast content delivery network (CDN) services.</p>\n\n<p><strong>EBS + RDS</strong> - The combination of these services only provide elastic block storage and database services.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/\">https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/\">https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/</a></p>\n",
                "options": [
                    {
                        "id": 2416,
                        "content": "<p>EBS + RDS</p>",
                        "isValid": false
                    },
                    {
                        "id": 2417,
                        "content": "<p>SES + S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 2418,
                        "content": "<p>ALB + ECS</p>",
                        "isValid": true
                    },
                    {
                        "id": 2419,
                        "content": "<p>API Gateway + Lambda</p>",
                        "isValid": true
                    },
                    {
                        "id": 2420,
                        "content": "<p>CloudWatch + CloudFront</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 590,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.486Z",
                "updatedAt": "2023-09-07T08:39:50.486Z",
                "content": "<p>A company wants to automate the creation of ECS clusters using CloudFormation. The process has worked for a while, but after creating task definitions and assigning roles, the development team discovers that the tasks for containers are not using the permissions assigned to them.</p>\n\n<p>Which ECS config must be set in <code>/etc/ecs/ecs.config</code> to allow ECS tasks to use IAM roles?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong><code>ECS_ENABLE_TASK_IAM_ROLE</code></strong></p>\n\n<p>This configuration item is used to enable IAM roles for tasks for containers with the bridge and default network modes.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>ECS_ENGINE_AUTH_DATA</code></strong> - This refers to the authentication data within a Docker configuration file, so this is not the correct option.</p>\n\n<p><strong><code>ECS_AVAILABLE_LOGGING_DRIVERS</code></strong> - The Amazon ECS container agent running on a container instance must register the logging drivers available on that instance with this variable. This configuration item refers to the logging driver.</p>\n\n<p><strong><code>ECS_CLUSTER</code></strong> - This refers to the ECS cluster that the ECS agent should check into. This is passed to the container instance at launch through Amazon EC2 user data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html</a></p>\n",
                "options": [
                    {
                        "id": 2421,
                        "content": "<p><code>ECS_ENGINE_AUTH_DATA</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2422,
                        "content": "<p><code>ECS_CLUSTER</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2423,
                        "content": "<p><code>ECS_AVAILABLE_LOGGING_DRIVERS</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2424,
                        "content": "<p><code>ECS_ENABLE_TASK_IAM_ROLE</code></p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 591,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.575Z",
                "updatedAt": "2023-09-07T08:39:50.575Z",
                "content": "<p>You are working for a technology startup building web and mobile applications. You would like to pull Docker images from the ECR repository called <code>demo</code> so you can start running local tests against the latest application version.</p>\n\n<p>Which of the following commands must you run to pull existing Docker images from ECR? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong><code>$(aws ecr get-login --no-include-email)</code></strong></p>\n\n<p><strong><code>docker pull 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></strong></p>\n\n<p>The get-login command retrieves a token that is valid for a specified registry for 12 hours, and then it prints a docker login command with that authorization token. You can execute the printed command to log in to your registry with Docker, or just run it automatically using the $() command wrapper. After you have logged in to an Amazon ECR registry with this command, you can use the Docker CLI to push and pull images from that registry until the token expires. The docker pull command is used to pull an image from the ECR registry.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>docker login -u $AWS_ACCESS_KEY_ID -p $AWS_SECRET_ACCESS_KEY</code></strong> - You cannot login to AWS ECR this way. AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are only used by the CLI and not by docker.</p>\n\n<p><strong><code>aws docker push 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></strong> - docker push here is the wrong answer, you need to use docker pull.</p>\n\n<p><strong><code>docker build -t 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></strong> - This is a docker command that is used to build Docker images from a Dockerfile.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login.html\">https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login.html</a></p>\n",
                "options": [
                    {
                        "id": 2425,
                        "content": "<p><code>docker login -u $AWS_ACCESS_KEY_ID -p $AWS_SECRET_ACCESS_KEY</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2426,
                        "content": "<p><code>$(aws ecr get-login --no-include-email)</code></p>",
                        "isValid": true
                    },
                    {
                        "id": 2427,
                        "content": "<p><code>docker pull 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></p>",
                        "isValid": true
                    },
                    {
                        "id": 2428,
                        "content": "<p><code>aws docker push 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2429,
                        "content": "<p><code>docker build -t 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 592,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.644Z",
                "updatedAt": "2023-09-07T08:39:50.644Z",
                "content": "<p>An analytics company is using Kinesis Data Streams (KDS) to process automobile health-status data from the taxis managed by a taxi ride-hailing service. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams.</p>\n\n<p>As a Developer Associate, which of the following options would you suggest for improving the performance for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Enhanced Fanout feature of Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream. You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream.</p>\n\n<p>Kinesis Data Streams Fanout\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q33-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Swap out Kinesis Data Streams with Kinesis Data Firehose</strong> -  Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Kinesis Data Firehose can only write to S3, Redshift, Elasticsearch or Splunk. You can't have applications consuming data streams from Kinesis Data Firehose, that's the job of Kinesis Data Streams. Therefore this option is not correct.</p>\n\n<p><strong>Swap out Kinesis Data Streams with SQS Standard queues</strong></p>\n\n<p><strong>Swap out Kinesis Data Streams with SQS FIFO queues</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both SQS Standard and SQS FIFO are not the right fit for the given use-case.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please understand the differences between the capabilities of Kinesis Data Streams vs SQS, as you may be asked scenario-based questions on this topic in the exam.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q33-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 2430,
                        "content": "<p>Swap out Kinesis Data Streams with SQS Standard queues</p>",
                        "isValid": false
                    },
                    {
                        "id": 2431,
                        "content": "<p>Swap out Kinesis Data Streams with Kinesis Data Firehose</p>",
                        "isValid": false
                    },
                    {
                        "id": 2432,
                        "content": "<p>Swap out Kinesis Data Streams with SQS FIFO queues</p>",
                        "isValid": false
                    },
                    {
                        "id": 2433,
                        "content": "<p>Use Enhanced Fanout feature of Kinesis Data Streams</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 593,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.712Z",
                "updatedAt": "2023-09-07T08:39:50.712Z",
                "content": "<p>A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed.</p>\n\n<p>Which of the following options represents the best solution for the given requirements?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Elastic File System (EFS) Standardâ€“IA storage class</strong> - Amazon EFS is a file storage service for use with Amazon compute (EC2, containers, serverless) and on-premises servers. Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently accessible storage for up to thousands of Amazon EC2 instances.</p>\n\n<p>The Standardâ€“IA storage class reduces storage costs for files that are not accessed every day. It does this without sacrificing the high availability, high durability, elasticity, and POSIX file system access that Amazon EFS provides. AWS recommends Standard-IA storage if you need your full dataset to be readily accessible and want to automatically save on storage costs for files that are less frequently accessed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class</strong> - Amazon S3 is an object storage service. Amazon S3 makes data available through an Internet API that can be accessed anywhere. It is not a file storage service, as is needed in the use case.</p>\n\n<p><strong>Amazon Elastic File System (EFS) Standard storage class</strong> - Amazon EFS Standard storage classes are ideal for workloads that require the highest levels of durability and availability. The EFS Standard storage class is used for frequently accessed files. It is the storage class to which customer data is initially written for Standard storage classes. The company is also looking at cutting costs by optimally storing the infrequently accessed data. Hence, EFS standard storage class is not the right solution for the given use case.</p>\n\n<p><strong>Amazon Elastic Block Store (EBS)</strong> - Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest latency access to data from a single EC2 instance. EBS volume cannot be accessed by hundreds of EC2 instances concurrently. It is not a file storage service, as is needed in the use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html\">https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html</a></p>\n",
                "options": [
                    {
                        "id": 2434,
                        "content": "<p>Amazon Elastic File System (EFS) Standardâ€“IA storage class</p>",
                        "isValid": true
                    },
                    {
                        "id": 2435,
                        "content": "<p>Amazon Elastic Block Store (EBS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 2436,
                        "content": "<p>Amazon Elastic File System (EFS) Standard storage class</p>",
                        "isValid": false
                    },
                    {
                        "id": 2437,
                        "content": "<p>Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 594,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.782Z",
                "updatedAt": "2023-09-07T08:39:50.782Z",
                "content": "<p>As a site reliability engineer, you are responsible for improving the companyâ€™s deployment by scaling and automating applications. As new application versions are ready for production you ensure that the application gets deployed to different sets of EC2 instances at different times allowing for a smooth transition.</p>\n\n<p>Using AWS CodeDeploy, which of the following options will allow you to do this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>CodeDeploy Deployment Groups</strong></p>\n\n<p>You can specify one or more deployment groups for a CodeDeploy application. The deployment group contains settings and configurations used during the deployment. Most deployment group settings depend on the compute platform used by your application. Some settings, such as rollbacks, triggers, and alarms can be configured for deployment groups for any compute platform.</p>\n\n<p>In an EC2/On-Premises deployment, a deployment group is a set of individual instances targeted for deployment. A deployment group contains individually tagged instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CodeDeploy Agent</strong> - The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The agent connects the EC2 instances to the CodeDeploy service.</p>\n\n<p><strong>CodeDeploy Hooks</strong> - Hooks are found in the AppSec file used by AWS CodeDeploy to manage deployment. Hooks correspond to lifecycle events such as ApplicationStart, ApplicationStop, etc. to which you can assign a script.</p>\n\n<p><strong>Define multiple CodeDeploy Applications</strong> - This option has been added as a distractor. Instead, you want to use deployment groups to use the same deployment and maybe separate the times when a group of instances receives the software updates.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html</a></p>\n",
                "options": [
                    {
                        "id": 2438,
                        "content": "<p>CodeDeploy Deployment Groups</p>",
                        "isValid": true
                    },
                    {
                        "id": 2439,
                        "content": "<p>Define multiple CodeDeploy Applications</p>",
                        "isValid": false
                    },
                    {
                        "id": 2440,
                        "content": "<p>CodeDeploy Hooks</p>",
                        "isValid": false
                    },
                    {
                        "id": 2441,
                        "content": "<p>CodeDeploy Agent</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 595,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.850Z",
                "updatedAt": "2023-09-07T08:39:50.850Z",
                "content": "<p>A development team has created AWS CloudFormation templates that are reusable by taking advantage of input parameters to name resources based on client names.</p>\n\n<p>You would like to save your templates on the cloud, which storage option should you choose?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>S3</strong></p>\n\n<p>If you upload a local template file, AWS CloudFormation uploads it to an Amazon Simple Storage Service (Amazon S3) bucket in your AWS account. If you don't already have an S3 bucket that was created by AWS CloudFormation, it creates a unique bucket for each region in which you upload a template file. If you already have an S3 bucket that was created by AWS CloudFormation in your AWS account, AWS CloudFormation adds the template to that bucket.</p>\n\n<p>Selecting a stack template for CloudFormation:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q56-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EBS</strong> - An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. Amazon EBS is a recommended storage option when data must be quickly accessible and requires long-term persistence. EBS cannot be used for selecting a stack template for CloudFormation.</p>\n\n<p><strong>EFS</strong> - EFS is a file storage service where you mount the file system on an Amazon EC2 Linux-based instance which is not an option for CloudFormation.</p>\n\n<p><strong>ECR</strong> - Amazon ECR eliminates the need to operate your container repositories or worry about scaling the underlying infrastructure which does not apply to CloudFormation.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html</a></p>\n",
                "options": [
                    {
                        "id": 2442,
                        "content": "<p>EFS</p>",
                        "isValid": false
                    },
                    {
                        "id": 2443,
                        "content": "<p>ECR</p>",
                        "isValid": false
                    },
                    {
                        "id": 2444,
                        "content": "<p>EBS</p>",
                        "isValid": false
                    },
                    {
                        "id": 2445,
                        "content": "<p>S3</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 596,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.923Z",
                "updatedAt": "2023-09-07T08:39:50.923Z",
                "content": "<p>You are a developer working at a cloud company that embraces serverless. You have performed your initial deployment and would like to work towards adding API Gateway stages and associate them with existing deployments. Your stages will include prod, test, and dev and will need to match a Lambda function variant that can be updated over time.</p>\n\n<p>Which of the following features must you add to achieve this? (select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>Stage Variables</strong></p>\n\n<p>Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of an API. They act like environment variables and can be used in your API setup and mapping templates. With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.</p>\n\n<p>For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage and calls a different web host (for example, beta.example.com).</p>\n\n<p><strong>Lambda Aliases</strong></p>\n\n<p>A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.</p>\n\n<p>Lambda Aliases allow you to create a \"mutable\" Lambda version that points to whatever version you want in the backend. This allows you to have a \"dev\", \"test\", prod\" Lambda alias that can remain stable over time.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Lambda Versions</strong> - Versions are immutable and cannot be updated over time. So this option is not correct.</p>\n\n<p><strong>Lambda X-Ray integration</strong> - This is good for tracing and debugging requests so it can be looked at as a good option for troubleshooting issues in the future. This is not the right fit for the given use-case.</p>\n\n<p><strong>Mapping Templates</strong> - Mapping template overrides provides you with the flexibility to perform many-to-one parameter mappings; override parameters after standard API Gateway mappings have been applied; conditionally map parameters based on body content or other parameter values; programmatically create new parameters on the fly, and override status codes returned by your integration endpoint. This is not the right fit for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n",
                "options": [
                    {
                        "id": 2446,
                        "content": "<p>Lambda X-Ray integration</p>",
                        "isValid": false
                    },
                    {
                        "id": 2447,
                        "content": "<p>Stage Variables</p>",
                        "isValid": true
                    },
                    {
                        "id": 2448,
                        "content": "<p>Mapping Templates</p>",
                        "isValid": false
                    },
                    {
                        "id": 2449,
                        "content": "<p>Lambda Versions</p>",
                        "isValid": false
                    },
                    {
                        "id": 2450,
                        "content": "<p>Lambda Aliases</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 597,
            "attributes": {
                "createdAt": "2023-09-07T08:39:50.993Z",
                "updatedAt": "2023-09-07T08:39:50.993Z",
                "content": "<p>A developer in your company has configured a build using AWS CodeBuild. The build fails and the developer needs to quickly troubleshoot the issue to see which commands or settings located in the BuildSpec file are causing an issue.</p>\n\n<p>Which approach will help them accomplish this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Run AWS CodeBuild locally using CodeBuild Agent</strong></p>\n\n<p>AWS CodeBuild is a fully managed build service. There are no servers to provision and scale, or software to install, configure, and operate.</p>\n\n<p>With the Local Build support for AWS CodeBuild, you just specify the location of your source code, choose your build settings, and CodeBuild runs build scripts for compiling, testing, and packaging your code. You can use the AWS CodeBuild agent to test and debug builds on a local machine.</p>\n\n<p>By building an application on a local machine you can:</p>\n\n<p>Test the integrity and contents of a buildspec file locally.</p>\n\n<p>Test and build an application locally before committing.</p>\n\n<p>Identify and fix errors quickly from your local development environment.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSH into the CodeBuild Docker container</strong> - It is not possible to SSH into the CodeBuild Docker container, that's why you should test and fix errors locally.</p>\n\n<p><strong>Freeze the CodeBuild during its next execution</strong> - You cannot freeze the CodeBuild process but you can stop it. Please see more details on - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/stop-build.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/stop-build.html</a></p>\n\n<p><strong>Enable detailed monitoring</strong> - Detailed monitoring is available for EC2 instances. You do not enable detailed monitoring but you can specify output logs to be captured via CloudTrail.</p>\n\n<p>AWS CodeBuild is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in CodeBuild. CloudTrail captures all API calls for CodeBuild as events, including calls from the CodeBuild console and from code calls to the CodeBuild APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an S3 bucket, including events for CodeBuild.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/announcing-local-build-support-for-aws-codebuild/\">https://aws.amazon.com/blogs/devops/announcing-local-build-support-for-aws-codebuild/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/use-codebuild-agent.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/use-codebuild-agent.html</a></p>\n",
                "options": [
                    {
                        "id": 2451,
                        "content": "<p>SSH into the CodeBuild Docker container</p>",
                        "isValid": false
                    },
                    {
                        "id": 2452,
                        "content": "<p>Freeze the CodeBuild during its next execution</p>",
                        "isValid": false
                    },
                    {
                        "id": 2453,
                        "content": "<p>Enable detailed monitoring</p>",
                        "isValid": false
                    },
                    {
                        "id": 2454,
                        "content": "<p>Run AWS CodeBuild locally using CodeBuild Agent</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 598,
            "attributes": {
                "createdAt": "2023-09-07T08:39:51.075Z",
                "updatedAt": "2023-09-07T08:39:51.075Z",
                "content": "<p>A developer has just integrated an AWS Lambda function to an Amazon API Gateway API. The integration has led to errors that the developer is unable to troubleshoot. The developer has decided to enable CloudWatch logging at the method level for the API Gateway API.</p>\n\n<p>What are the key points of consideration while configuring configuring method-level logging for the API Gateway? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>AWS Security Token Service(STS) is used by API Gateway for logging data to CloudWatch logs. Hence, AWS STS has to be enabled for the Region that you're using</strong></p>\n\n<p>API Gateway calls AWS Security Token Service to assume the IAM role, so make sure that AWS STS is enabled for the Region. If you receive an error when setting the IAM role ARN, check your AWS Security Token Service account settings to make sure that AWS STS is enabled in the Region that you're using.</p>\n\n<p><strong>To enable CloudWatch Logs for all or only some of the methods, you must also specify the ARN of an IAM role that enables API Gateway to write information to CloudWatch Logs on behalf of your user. The IAM role must also contain the following trust relationship statement</strong></p>\n\n<p>To enable CloudWatch Logs for all or only some of the methods, you must also specify the ARN of an IAM role that enables API Gateway to write information to CloudWatch Logs on behalf of your user. To do so, choose <code>Settings</code> from the APIs main navigation pane. Then enter the ARN of an IAM role in the <code>CloudWatch log role ARN</code> text field. The IAM role must also contain the trust relationship statement.</p>\n\n<p>Policy of AmazonAPIGatewayPushToCloudWatchLogs for IAM role:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q9-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html#how-to-stage-settings-console\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html#how-to-stage-settings-console</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In access logging, only $context and $input variables are supported</strong> - This statement is incorrect. In access logging, you, as an API developer, want to log who has accessed your API and how the caller accessed the API. You can create your own log group or choose an existing log group that could be managed by API Gateway. To specify the access details, you select $context variables and choose a log group as the destination. Only $context variables are supported (not $input, and so on).</p>\n\n<p><strong>You are charged for accessing method-level and stage-level CloudWatch metrics, but not for API-level metrics</strong> - This statement is incorrect. Your account is charged for accessing method-level CloudWatch metrics, but not the API-level or stage-level metrics.</p>\n\n<p><strong>API Gateway API log groups or streams can only be deleted and recreated by redeploying the API</strong> - API Gateway API log groups or streams can be deleted from the CloudWatch console. But, it is not recommended.</p>\n\n<p>Do not manually delete API Gateway API log groups or streams; let API Gateway manage these resources. Manually deleting log groups or streams may cause API requests and responses not to be logged. If that happens, you can delete the entire log group for the API and redeploy the API. This is because API Gateway creates log groups or log streams for an API stage at the time when it is deployed.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html#how-to-stage-settings-console\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html#how-to-stage-settings-console</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-logging.html#apigateway-cloudwatch-log-formats\">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-logging.html#apigateway-cloudwatch-log-formats</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/view-cloudwatch-log-events-in-cloudwatch-console.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/view-cloudwatch-log-events-in-cloudwatch-console.html</a></p>\n",
                "options": [
                    {
                        "id": 2455,
                        "content": "<p>In access logging, only $context and $input variables are supported</p>",
                        "isValid": false
                    },
                    {
                        "id": 2456,
                        "content": "<p>AWS Security Token Service(STS) is used by API Gateway for logging data to CloudWatch logs. Hence, AWS STS has to be enabled for the Region that you're using</p>",
                        "isValid": true
                    },
                    {
                        "id": 2457,
                        "content": "<p>API Gateway API log groups or streams can only be deleted and recreated by redeploying the API</p>",
                        "isValid": false
                    },
                    {
                        "id": 2458,
                        "content": "<p>You are charged for accessing method-level and stage-level CloudWatch metrics, but not for API-level metrics</p>",
                        "isValid": false
                    },
                    {
                        "id": 2459,
                        "content": "<p>To enable CloudWatch Logs for all or only some of the methods, you must also specify the ARN of an IAM role that enables API Gateway to write information to CloudWatch Logs on behalf of your user. The IAM role must also contain the following trust relationship statement</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 599,
            "attributes": {
                "createdAt": "2023-09-07T08:39:51.156Z",
                "updatedAt": "2023-09-07T08:39:51.156Z",
                "content": "<p>An e-commerce company has a fleet of EC2 based web servers running into very high CPU utilization issues. The development team has determined that serving secure traffic via HTTPS is a major contributor to the high CPU load.</p>\n\n<p>Which of the following steps can take the high CPU load off the web servers? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>\"Configure an SSL/TLS certificate on an Application Load Balancer via AWS Certificate Manager (ACM)\"</p>\n\n<p>\"Create an HTTPS listener on the Application Load Balancer with SSL termination\"</p>\n\n<p>An Application load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/images/component_architecture.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p>\n\n<p>To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. You can create an HTTPS listener, which uses encrypted connections (also known as SSL offload). This feature enables traffic encryption between your load balancer and the clients that initiate SSL or TLS sessions. As the EC2 instances are under heavy CPU load, the load balancer will use the server certificate to terminate the front-end connection and then decrypt requests from clients before sending them to the EC2 instances.</p>\n\n<p>Please review this resource to understand how to associate an ACM SSL/TLS certificate with an Application Load Balancer:\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/\">https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Create an HTTPS listener on the Application Load Balancer with SSL pass-through\" - If you use an HTTPS listener with SSL pass-through, then the EC2 instances would continue to be under heavy CPU load as they would still need to decrypt the secure traffic\nat the instance level. Hence this option is incorrect.</p>\n\n<p>\"Create an HTTP listener on the Application Load Balancer with SSL termination\"</p>\n\n<p>\"Create an HTTP listener on the Application Load Balancer with SSL pass-through\"</p>\n\n<p>You cannot have an HTTP listener for an Application Load Balancer to support SSL termination or SSL pass-through, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/\">https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/</a></p>\n",
                "options": [
                    {
                        "id": 2460,
                        "content": "<p>Create an HTTPS listener on the Application Load Balancer with SSL pass-through</p>",
                        "isValid": false
                    },
                    {
                        "id": 2461,
                        "content": "<p>Create an HTTP listener on the Application Load Balancer with SSL pass-through</p>",
                        "isValid": false
                    },
                    {
                        "id": 2462,
                        "content": "<p>Create an HTTP listener on the Application Load Balancer with SSL termination</p>",
                        "isValid": false
                    },
                    {
                        "id": 2463,
                        "content": "<p>Configure an SSL/TLS certificate on an Application Load Balancer via AWS Certificate Manager (ACM)</p>",
                        "isValid": true
                    },
                    {
                        "id": 2464,
                        "content": "<p>Create an HTTPS listener on the Application Load Balancer with SSL termination</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 600,
            "attributes": {
                "createdAt": "2023-09-07T08:39:51.225Z",
                "updatedAt": "2023-09-07T08:39:51.225Z",
                "content": "<p>A development team is configuring Kinesis Data Streams for ingesting real-time data from various appliances. The team has declared a shard capacity of one to test the configuration.</p>\n\n<p>What happens if the capacity limits of an Amazon Kinesis data stream are exceeded while the data producer adds data to the data stream?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The put data calls will be rejected with a <code>ProvisionedThroughputExceeded</code> exception</strong></p>\n\n<p>The capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception. If this is due to a temporary rise of the data streamâ€™s input data rate, retry by the data producer will eventually lead to completion of the requests. If this is due to a sustained rise of the data streamâ€™s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The put data calls will be rejected with a <code>AccessDeniedException</code> exception once the limit is reached</strong> - Access Denied error is thrown when the accessing system does not have enough permissions. Since data was getting ingested into Data Streams before reaching the capacity, this error is not possible.</p>\n\n<p><strong>Data is lost unless the partition key of the data records is changed in order to write data to a different shard in the stream</strong> - Partition key is used to segregate and route records to different shards of a data stream. A partition key is specified by your data producer while adding data to an Amazon Kinesis data stream. The use case talks about provisioning only one shard. It is not possible to set up more shards by simply changing the partition key. Hence, this choice is incorrect.</p>\n\n<p><strong>Contact AWS support to request an increase in the number of shards</strong> - This is a made-up option that acts as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 2465,
                        "content": "<p>The put data calls will be rejected with a <code>ProvisionedThroughputExceeded</code> exception</p>",
                        "isValid": true
                    },
                    {
                        "id": 2466,
                        "content": "<p>The put data calls will be rejected with a <code>AccessDeniedException</code> exception once the limit is reached</p>",
                        "isValid": false
                    },
                    {
                        "id": 2467,
                        "content": "<p>Data is lost unless the partition key of the data records is changed in order to write data to a different shard in the stream</p>",
                        "isValid": false
                    },
                    {
                        "id": 2468,
                        "content": "<p>Contact AWS support to request an increase in the number of shards</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 601,
            "attributes": {
                "createdAt": "2023-09-07T08:39:51.295Z",
                "updatedAt": "2023-09-07T08:39:51.295Z",
                "content": "<p>A media application uses Amazon CloudFront distribution to distribute static content configured on an Amazon S3 bucket. The application is used across different countries and various AWS Regions. Some regions have been experiencing latency when there is a cache miss on CloudFront.</p>\n\n<p>Which of the following configuration changes will you suggest to decrease latency and improve user performance?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Redirect requests on cache misses to the Amazon S3 bucket nearest to the user country. Create a CloudFront function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the CloudFront function with the distribution's viewer request event</strong></p>\n\n<p>With CloudFront Functions in Amazon CloudFront, you can write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. Your functions can manipulate the requests and responses that flow through CloudFront, perform basic authentication and authorization, generate HTTP responses at the edge, and more.</p>\n\n<p>When you associate a CloudFront function with a CloudFront distribution, CloudFront intercepts requests and responses at CloudFront edge locations and passes them to your function. You can invoke CloudFront functions when the following events occur:\n1. When CloudFront receives a request from a viewer (viewer request): The function executes when CloudFront receives a request from a viewer before it checks to see whether the requested object is in the CloudFront cache.</p>\n\n<ol>\n<li>Before CloudFront returns the response to the viewer (viewer response): The function executes before returning the requested file to the viewer. Note that the function executes regardless of whether the file is already in the CloudFront cache.</li>\n</ol>\n\n<p>We use the value of the CloudFront-Viewer-Country header to update the S3 bucket domain name to a bucket in a Region that is closer to the viewer. This can be useful in several ways:\n1. It reduces latencies when the Region specified is nearer to the viewer's country.\n2. It provides data sovereignty by making sure that data is served from an origin that's in the same country that the request came from.</p>\n\n<p>The example below shows how a Lambda handler is used to change response based on user country. A similar CloudFront function can be defined to direct user traffic to the nearest S3 bucket.</p>\n\n<p>Example for redirecting viewer requests to a country-specific URL:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q32-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-content-based-S3-origin-request-trigger\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-content-based-S3-origin-request-trigger</a></p>\n\n<p>Choosing between CloudFront Functions and Lambda@Edge:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q32-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a Lambda@Edge function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the Lambda@Edge function with the distribution's origin request event</strong></p>\n\n<p><strong>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a Lambda@Edge function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the Lambda@Edge function with the distribution's viewer request event</strong> -</p>\n\n<p>As discussed above, CloudFront Function is a better choice than Lambda@Edge function for this use case. Also, the Viewer request event is better suited for this requirement than the Origin request event to trigger the function.</p>\n\n<p>Viewer request event: The function executes when CloudFront receives a request from a viewer before it checks to see whether the requested object is in the CloudFront cache.</p>\n\n<p>Origin request event: The function executes only when CloudFront forwards a request to your origin. When the requested object is in the CloudFront cache, the function doesn't execute.</p>\n\n<p><strong>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a CloudFront function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the CloudFront function with the distribution's origin request event</strong> - This option is incorrect. You can invoke CloudFront functions for only two events: When CloudFront receives a request from a viewer (viewer request) and Before CloudFront returns the response to the viewer (viewer response). Origin request and Origin response events are not supported for the CloudFront function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-redirecting-examples\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-redirecting-examples</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-cloudfront-trigger-events.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-cloudfront-trigger-events.html</a></p>\n",
                "options": [
                    {
                        "id": 2469,
                        "content": "<p>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a CloudFront function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the CloudFront function with the distribution's origin request event</p>",
                        "isValid": false
                    },
                    {
                        "id": 2470,
                        "content": "<p>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a CloudFront function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the CloudFront function with the distribution's viewer request event</p>",
                        "isValid": true
                    },
                    {
                        "id": 2471,
                        "content": "<p>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a Lambda@Edge function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the Lambda@Edge function with the distribution's viewer request event</p>",
                        "isValid": false
                    },
                    {
                        "id": 2472,
                        "content": "<p>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a Lambda@Edge function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the Lambda@Edge function with the distribution's origin request event</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 602,
            "attributes": {
                "createdAt": "2023-09-07T08:39:51.370Z",
                "updatedAt": "2023-09-07T08:39:51.370Z",
                "content": "<p>An application runs on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. The company's audit requirements mandate that logging and storing of application log data must be done centrally on AWS.</p>\n\n<p>How will you configure this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use the awslogs log driver to configure the containers in your tasks to send log information to CloudWatch Logs. Add the required <code>logConfiguration</code> parameters to your task definition</strong></p>\n\n<p>Using the awslogs log driver you can configure the containers in your tasks to send log information to CloudWatch Logs. If you're using the Fargate launch type for your tasks, you need to add the required logConfiguration parameters to your task definition to turn on the awslogs log driver.</p>\n\n<p>Before your containers can send logs to CloudWatch, you must specify the awslogs log driver for containers in your task definition. The example task definition JSON that follows has a logConfiguration object specified for each container. One is for the WordPress container that sends logs to a log group called awslogs-wordpress. The other is for a MySQL container that sends logs to a log group that's called awslogs-mysql. Both containers use the awslogs-example log stream. prefix.</p>\n\n<p>Specifying a log configuration in your task definition:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q26-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html#specify-log-config\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html#specify-log-config</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the awslogs log driver to send log information to CloudWatch Logs. To turn on the awslogs log driver, your Amazon ECS container instances require at least version 1.9.0 of the container agent</strong> - This statement is incorrect. If you're using the EC2 launch type (and not Fargate) for your tasks and want to turn on the awslogs log driver, your Amazon ECS container instances require at least version 1.9.0 of the container agent.</p>\n\n<p><strong>Amazon ECS metric data is automatically sent to CloudWatch in 1-minute periods. Amazon ECS service using the Fargate launch type has CloudWatch CPU and memory utilization metrics that can be enabled from the ECS console</strong> - Indeed, Amazon ECS metric data is automatically sent to CloudWatch in 1-minute periods. Also, any Amazon ECS service using the Fargate launch type has CloudWatch CPU and memory utilization metrics automatically, so you don't need to take any manual steps. But, these are system logs and not the application logs as is needed in the current use case.</p>\n\n<p><strong>Download and install the unified CloudWatch agent on the ECS instances to collect internal system-level metrics and application logs from the instances. The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch logs and can be queried for report generation</strong> - This statement is incorrect and given only as a distractor. ECS Fargate is serverless and hence the scope of downloading and installing software on the instance does not arise.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/userguide/cloudwatch-metrics.html#available_cloudwatch_metrics\">https://docs.aws.amazon.com/AmazonECS/latest/userguide/cloudwatch-metrics.html#available_cloudwatch_metrics</a></p>\n",
                "options": [
                    {
                        "id": 2473,
                        "content": "<p>Use the awslogs log driver to configure the containers in your tasks to send log information to CloudWatch Logs. Add the required <code>logConfiguration</code> parameters to your task definition</p>",
                        "isValid": true
                    },
                    {
                        "id": 2474,
                        "content": "<p>Download and install the unified CloudWatch agent on the ECS instances to collect internal system-level metrics and application logs from the instances. The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch logs and can be queried for report generation</p>",
                        "isValid": false
                    },
                    {
                        "id": 2475,
                        "content": "<p>Use the awslogs log driver to send log information to CloudWatch Logs. To turn on the awslogs log driver, your Amazon ECS container instances require at least version 1.9.0 of the container agent</p>",
                        "isValid": false
                    },
                    {
                        "id": 2476,
                        "content": "<p>Amazon ECS metric data is automatically sent to CloudWatch in 1-minute periods. Amazon ECS service using the Fargate launch type has CloudWatch CPU and memory utilization metrics that can be enabled from the ECS console</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 603,
            "attributes": {
                "createdAt": "2023-09-07T08:39:51.442Z",
                "updatedAt": "2023-09-07T08:39:51.442Z",
                "content": "<p>The development team at an IT company has configured an Application Load Balancer (ALB) with a Lambda function A as the target but the Lambda function A is not able to process any request from the ALB. Upon investigation, the team finds that there is another Lambda function B in the AWS account that is exceeding the concurrency limits.</p>\n\n<p>How can the development team address this issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Set up reserved concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</strong></p>\n\n<p>Concurrency is the number of requests that a Lambda function is serving at any given time. If a Lambda function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.</p>\n\n<p>To ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency. When a function has reserved concurrency, no other function can use that concurrency. More importantly, reserved concurrency also limits the maximum concurrency for the function, and applies to the function as a whole, including versions and aliases.</p>\n\n<p>Please review this note to understand how reserved concurrency works:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q6-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved</a></p>\n\n<p>Therefore using reserved concurrency for Lambda function B would limit its maximum concurrency and allow Lambda function A to execute without getting throttled.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up provisioned concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</strong> - You should use provisioned concurrency to enable your function to scale without fluctuations in latency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency. Provisioned concurrency is not used to limit the maximum concurrency for a given Lambda function, so this option is incorrect.</p>\n\n<p><strong>Use an API Gateway instead of an Application Load Balancer (ALB) for Lambda function A</strong> - This has been added as a distractor as using an API Gateway for Lambda function A has no bearing on limiting the concurrency of Lambda function B, so this option is incorrect.</p>\n\n<p><strong>Use a Cloudfront Distribution instead of an Application Load Balancer (ALB) for Lambda function A</strong> - When you associate a CloudFront distribution with a Lambda function (known as Lambda@Edge), CloudFront intercepts requests and responses at CloudFront edge locations and runs the function. Again, this has no bearing on limiting the concurrency of Lambda function B, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/lambda-functions-as-targets-for-application-load-balancers/\">https://aws.amazon.com/blogs/networking-and-content-delivery/lambda-functions-as-targets-for-application-load-balancers/</a></p>\n",
                "options": [
                    {
                        "id": 2477,
                        "content": "<p>Set up reserved concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</p>",
                        "isValid": true
                    },
                    {
                        "id": 2478,
                        "content": "<p>Use an API Gateway instead of an Application Load Balancer (ALB) for Lambda function A</p>",
                        "isValid": false
                    },
                    {
                        "id": 2479,
                        "content": "<p>Set up provisioned concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</p>",
                        "isValid": false
                    },
                    {
                        "id": 2480,
                        "content": "<p>Use a Cloudfront Distribution instead of an Application Load Balancer (ALB) for Lambda function A</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 604,
            "attributes": {
                "createdAt": "2023-09-07T08:39:51.515Z",
                "updatedAt": "2023-09-07T08:39:51.515Z",
                "content": "<p>As an AWS Certified Developer Associate, you are writing a CloudFormation template in YAML. The template consists of an EC2 instance creation and one RDS resource. Once your resources are created you would like to output the connection endpoint for the RDS database.</p>\n\n<p>Which intrinsic function returns the value needed?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation provides several built-in functions that help you manage your stacks. Intrinsic functions are used in templates to assign values to properties that are not available until runtime.</p>\n\n<p><strong><code>!GetAtt</code></strong> - The Fn::GetAtt intrinsic function returns the value of an attribute from a resource in the template. This example snippet returns a string containing the DNS name of the load balancer with the logical name myELB -\nYML :   !GetAtt myELB.DNSName\nJSON :   \"Fn::GetAtt\" : [ \"myELB\" , \"DNSName\" ]</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>!Sub</code></strong> - The intrinsic function Fn::Sub substitutes variables in an input string with values that you specify. In your templates, you can use this function to construct commands or outputs that include values that aren't available until you create or update a stack.</p>\n\n<p><strong><code>!Ref</code></strong> - The intrinsic function Ref returns the value of the specified parameter or resource.</p>\n\n<p><strong><code>!FindInMap</code></strong> - The intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that is declared in the Mappings section. For example, you can use this in the Mappings section that contains a single map, RegionMap, that associates AMIs with AWS regions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html</a></p>\n",
                "options": [
                    {
                        "id": 2481,
                        "content": "<p><code>!Sub</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2482,
                        "content": "<p><code>!FindInMap</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2483,
                        "content": "<p><code>!Ref</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2484,
                        "content": "<p><code>!GetAtt</code></p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 605,
            "attributes": {
                "createdAt": "2023-09-07T08:39:51.590Z",
                "updatedAt": "2023-09-07T08:39:51.590Z",
                "content": "<p>A Company uses a large set of EBS volumes for their fleet of Amazon EC2 instances. As an AWS Certified Developer Associate, your help has been requested to understand the security features of the EBS volumes. The company does not want to build or maintain their own encryption key management infrastructure.</p>\n\n<p>Can you help them understand what works for Amazon EBS encryption? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region</strong> - You can configure your AWS account to enforce the encryption of the new EBS volumes and snapshot copies that you create. Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n\n<p><strong>A volume restored from an encrypted snapshot, or a copy of an encrypted snapshot, is always encrypted</strong> - By default, the CMK that you selected when creating a volume encrypts the snapshots that you make from the volume and the volumes that you restore from those encrypted snapshots. You cannot remove encryption from an encrypted volume or snapshot, which means that a volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q23-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can encrypt an existing unencrypted volume or snapshot by using AWS Key Management Service (KMS) AWS SDKs</strong> - This is an incorrect statement. There is no direct way to encrypt an existing unencrypted volume or snapshot. You can encrypt an unencrypted snapshot by copying and enabling encryption while copying the snapshot. To encrypt an EBS volume, you need to create a snapshot and then encrypt the snapshot as described earlier. From this new encrypted snapshot, you can then create an encrypted volume.</p>\n\n<p><strong>A snapshot of an encrypted volume can be encrypted or unencrypted</strong> - This is an incorrect statement. You cannot remove encryption from an encrypted volume or snapshot, which means that a volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted.</p>\n\n<p><strong>Encryption by default is an AZ specific setting. If you enable it for an AZ, you cannot disable it for individual volumes or snapshots in that AZ</strong> - This is an incorrect statement. Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encrypt-unencrypted\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encrypt-unencrypted</a></p>\n",
                "options": [
                    {
                        "id": 2485,
                        "content": "<p>A volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted</p>",
                        "isValid": true
                    },
                    {
                        "id": 2486,
                        "content": "<p>Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region</p>",
                        "isValid": true
                    },
                    {
                        "id": 2487,
                        "content": "<p>A snapshot of an encrypted volume can be encrypted or unencrypted</p>",
                        "isValid": false
                    },
                    {
                        "id": 2488,
                        "content": "<p>Encryption by default is an AZ specific setting. If you enable it for an AZ, you cannot disable it for individual volumes or snapshots in that AZ</p>",
                        "isValid": false
                    },
                    {
                        "id": 2489,
                        "content": "<p>You can encrypt an existing unencrypted volume or snapshot by using AWS Key Management Service (KMS) AWS SDKs</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 606,
            "attributes": {
                "createdAt": "2023-09-07T08:39:51.663Z",
                "updatedAt": "2023-09-07T08:39:51.663Z",
                "content": "<p>Your e-commerce company needs to improve its software delivery process and is moving away from the waterfall methodology. You decided that every application should be built using the best CI/CD practices and every application should be packaged and deployed as a Docker container. The Docker images should be stored in ECR and pushed with AWS CodePipeline and AWS CodeBuild.</p>\n\n<p>When you attempt to do this, the last step fails with an authorization issue. What is the most likely issue?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The IAM permissions are wrong for the CodeBuild service</strong></p>\n\n<p>You can push your Docker or Open Container Initiative (OCI) images to an Amazon ECR repository with the docker push command.</p>\n\n<p>Amazon ECR users require permission to call ecr:GetAuthorizationToken before they can authenticate to a registry and push or pull any images from any Amazon ECR repository. Amazon ECR provides several managed policies to control user access at varying levels</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The ECR repository is stale, you must delete and re-create it</strong> - You can delete a repository when you are done using it, stale is not a concept within ECR. This option has been added as a distractor.</p>\n\n<p><strong>CodeBuild cannot talk to ECR because of security group issues</strong> - A security group acts as a virtual firewall at the instance level and it is not related to pushing Docker images, so this option does not fit the given use-case.</p>\n\n<p><strong>The ECS instances are misconfigured and must contain additional data in /etc/ecs/ecs.config</strong> - The error Authorization is an indication that there is an access issue, therefore you should not look at your configuration first but rather permissions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html\">https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html</a></p>\n",
                "options": [
                    {
                        "id": 2490,
                        "content": "<p>The ECR repository is stale, you must delete and re-create it</p>",
                        "isValid": false
                    },
                    {
                        "id": 2491,
                        "content": "<p>The ECS instances are misconfigured and must contain additional data in /etc/ecs/ecs.config</p>",
                        "isValid": false
                    },
                    {
                        "id": 2492,
                        "content": "<p>The IAM permissions are wrong for the CodeBuild service</p>",
                        "isValid": true
                    },
                    {
                        "id": 2493,
                        "content": "<p>CodeBuild cannot talk to ECR because of security group issues</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 607,
            "attributes": {
                "createdAt": "2023-09-07T08:39:51.731Z",
                "updatedAt": "2023-09-07T08:39:51.731Z",
                "content": "<p>A startup manages its Cloud resources with Elastic Beanstalk. The environment consists of few Amazon EC2 instances, an Auto Scaling Group (ASG), and an Elastic Load Balancer. Even after the Load Balancer marked an EC2 instance as unhealthy, the ASG has not replaced it with a healthy instance.</p>\n\n<p>As a Developer, suggest the necessary configurations to automate the replacement of unhealthy instance.</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The health check type of your instance's Auto Scaling group, must be changed from EC2 to ELB by using a configuration file</strong> - By default, the health check configuration of your Auto Scaling group is set as an EC2 type that performs a status check of EC2 instances. To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from EC2 to ELB by using a configuration file.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Health check parameters were configured for checking the instance health alone. The instance failed because of application failure which was not configured as a parameter for health check status</strong> - This is an incorrect statement. Status checks, by definition, cover only an EC2 instance's health, and not the health of your application, server, or any Docker containers running on the instance.</p>\n\n<p><strong>Auto Scaling group doesn't automatically replace the unhealthy instances marked by the load balancer. They have to be manually replaced from AWS Console</strong> - Incorrect statement. As discussed above, if the health check type of ASG is changed from EC2 to ELB, Auto Scaling will be able to replace the unhealthy instance.</p>\n\n<p><strong>The ping path field of the Load Balancer is configured incorrectly</strong> - Ping path is a health check configuration field of Elastic Load Balancer. If the ping path is configured wrong, ELB will not be able to reach the instance and hence will consider the instance unhealthy. However, this would then apply to all instances, not just once instance. So it does not address the issue given in the use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-instance-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-instance-automation/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html</a></p>\n",
                "options": [
                    {
                        "id": 2494,
                        "content": "<p>Health check parameters were configured for checking the instance health alone. The instance failed because of application failure which was not configured as a parameter for health check status</p>",
                        "isValid": false
                    },
                    {
                        "id": 2495,
                        "content": "<p>The health check type of your instance's Auto Scaling group, must be changed from EC2 to ELB by using a configuration file</p>",
                        "isValid": true
                    },
                    {
                        "id": 2496,
                        "content": "<p>Auto Scaling group doesn't automatically replace the unhealthy instances marked by the load balancer. They have to be manually replaced from AWS Console</p>",
                        "isValid": false
                    },
                    {
                        "id": 2497,
                        "content": "<p>The ping path field of the Load Balancer is configured incorrectly</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 608,
            "attributes": {
                "createdAt": "2023-09-07T08:39:51.802Z",
                "updatedAt": "2023-09-07T08:39:51.802Z",
                "content": "<p>A company has AWS Lambda functions where each is invoked by other AWS services such as Amazon Kinesis Data Firehose, Amazon API Gateway, Amazon Simple Storage Service, or Amazon CloudWatch Events. What these Lambda functions have in common is that they process heavy workloads such as big data analysis, large file processing, and statistical computations.</p>\n\n<p>What should you do to improve the performance of your AWS Lambda functions without changing your code?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Increase the RAM assigned to your Lambda function</strong></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>In the AWS Lambda resource model, you choose the amount of memory you want for your function which allocates proportional CPU power and other resources. This means you will have access to more compute power when you choose one of the new larger settings. To configure the memory for your function, set a value between 128 MB and 10,240 MB in 1-MB increments. At 1,769 MB, a function has the equivalent of one vCPU (one vCPU-second of credits per second). You access these settings when you create a function or update its configuration. The settings are available using the AWS Management Console, AWS CLI, or SDKs.</p>\n\n<p>Therefore, by increasing the amount of memory available to the Lambda functions, you can run the compute-heavy workflows.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the instance type for your Lambda function</strong> - Instance types apply to the EC2 service and not to Lambda function as its a serverless service.</p>\n\n<p><strong>Change your Lambda function runtime to use Golang</strong> - This changes programming language which requires code changes, so this option is not correct. Besides, changing the runtime may not even address the performance issues.</p>\n\n<p><strong>Increase the Lambda function timeout</strong> - This option would increase the amount of time for which the Lambda function executes, which may help in case you have some heavy processing, but won't help with the actual performance of your Lambda function.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a></p>\n",
                "options": [
                    {
                        "id": 2498,
                        "content": "<p>Change your Lambda function runtime to use Golang</p>",
                        "isValid": false
                    },
                    {
                        "id": 2499,
                        "content": "<p>Change the instance type for your Lambda function</p>",
                        "isValid": false
                    },
                    {
                        "id": 2500,
                        "content": "<p>Increase the RAM assigned to your Lambda function</p>",
                        "isValid": true
                    },
                    {
                        "id": 2501,
                        "content": "<p>Increase the Lambda function timeout</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 609,
            "attributes": {
                "createdAt": "2023-09-07T08:39:51.869Z",
                "updatedAt": "2023-09-07T08:39:51.869Z",
                "content": "<p>A retail company manages its IT infrastructure on AWS Cloud via Elastic Beanstalk. The development team at the company is planning to deploy the next version with MINIMUM application downtime and the ability to rollback quickly in case deployment goes wrong.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend to the development team?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the new version to a separate environment via Blue/Green Deployment, and then swap Route 53 records of the two environments to redirect traffic to the new version</strong></p>\n\n<p>With deployment policies such as 'All at once', AWS Elastic Beanstalk performs an in-place update when you update your application versions and your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs (via Route 53) of the two environments to redirect traffic to the new version instantly. In case of any deployment issues, the rollback process is very quick via swapping the URLs for the two environments.</p>\n\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the new application version using 'All at once' deployment policy</strong> - Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time. So this option is not correct.</p>\n\n<p><strong>Deploy the new application version using 'Rolling' deployment policy</strong> - This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. However rollback process is via manual redeploy, so it's not as quick as the Blue/Green deployment.</p>\n\n<p><strong>Deploy the new application version using 'Rolling with additional batch' deployment policy</strong> - This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. However rollback process is via manual redeploy, so it's not as quick as the Blue/Green deployment.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n",
                "options": [
                    {
                        "id": 2502,
                        "content": "<p>Deploy the new application version using 'Rolling with additional batch' deployment policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 2503,
                        "content": "<p>Deploy the new application version using 'All at once' deployment policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 2504,
                        "content": "<p>Deploy the new application version using 'Rolling' deployment policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 2505,
                        "content": "<p>Deploy the new version to a separate environment via Blue/Green Deployment, and then swap Route 53 records of the two environments to redirect traffic to the new version</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 610,
            "attributes": {
                "createdAt": "2023-09-07T08:39:51.941Z",
                "updatedAt": "2023-09-07T08:39:51.941Z",
                "content": "<p>You are a software engineer working for an IT company and are asked to contribute to a growing internal application that includes dashboards for data visualization. You are provisioning your AWS DynamoDB table and need to perform 10 strongly consistent reads per second of 4 KB in size each.</p>\n\n<p>How many Read Capacity Units (RCUs) are needed?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Before proceeding with the calculations, please review the following:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q38-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q38-i2.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p><strong>10</strong></p>\n\n<p>One Read Capacity Unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. The total number of Read Capacity Units required depends on the item size, and whether you want an eventually consistent or strongly consistent read.</p>\n\n<p>1) Item Size / 4KB, rounding to the nearest whole number.</p>\n\n<p>So, in the above case, 4KB / 4 KB = 1 read capacity unit.</p>\n\n<p>2) 1 read capacity unit per item (since strongly consistent read) Ã— No of reads per second</p>\n\n<p>So, in the above case, 1 x 10 = 10 read capacity units.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>40</strong></p>\n\n<p><strong>20</strong></p>\n\n<p><strong>5</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n",
                "options": [
                    {
                        "id": 2506,
                        "content": "<p>10</p>",
                        "isValid": true
                    },
                    {
                        "id": 2507,
                        "content": "<p>5</p>",
                        "isValid": false
                    },
                    {
                        "id": 2508,
                        "content": "<p>20</p>",
                        "isValid": false
                    },
                    {
                        "id": 2509,
                        "content": "<p>40</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 611,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.011Z",
                "updatedAt": "2023-09-07T08:39:52.011Z",
                "content": "<p>A developer is configuring the redirect actions for an Application Load Balancer. The developer stumbled upon the following snippet of code.</p>\n\n<p>Which of the following is an example of a query string condition that the developer can use on AWS CLI?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>**</p>\n\n<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"QueryStringConfig\": {\n          \"Values\": [\n            {\n                \"Key\": \"version\",\n                \"Value\": \"v1\"\n            },\n            {\n                \"Value\": \"*example*\"\n            }\n          ]\n      }\n  }\n]\n</code></pre>\n\n<p>**</p>\n\n<p>You can use query string conditions to configure rules that route requests based on key/value pairs or values in the query string. The match evaluation is not case-sensitive. The following wildcard characters are supported: * (matches 0 or more characters) and ? (matches exactly 1 character). You can specify conditions when you create or modify a rule.</p>\n\n<p>Query parameters are often used along with the path component of the URL for applying a special logic to the resource being fetched.</p>\n\n<p>The query string component starts after the first \"?\" in a URI. Typically query strings contain key-value pairs separated by a delimiter \"&amp;\". Example: http://example.com/path/to/page?version=A&amp;gender=female</p>\n\n<p>The example condition given in the question is satisfied by requests with a query string that includes either a key/value pair of \"version=v1\" or any key set to \"example\".</p>\n\n<p>Incorrect options:</p>\n\n<p>**</p>\n\n<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"PathPatternConfig\": {\n          \"Values\": [\"/img/*\"]\n      }\n  }\n]\n</code></pre>\n\n<p>**</p>\n\n<p>**</p>\n\n<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"StringHeaderConfig\": {\n          \"Values\": [\"*.example.com\"]\n      }\n  }\n]\n</code></pre>\n\n<p>**</p>\n\n<p>These two options are malformed and are incorrect.</p>\n\n<p>**</p>\n\n<pre><code>[\n  {\n      \"Type\": \"redirect\",\n      \"RedirectConfig\": {\n          \"Protocol\": \"HTTPS\",\n          \"Port\": \"443\",\n          \"Host\": \"#{host}\",\n          \"Path\": \"/#{path}\",\n          \"Query\": \"#{query}\",\n          \"StatusCode\": \"HTTP_301\"\n      }\n  }\n]\n</code></pre>\n\n<p>** - This action redirects an HTTP request to an HTTPS request on port 443, with the same hostname, path, and query string as the HTTP request.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#query-string-conditions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#query-string-conditions</a></p>\n",
                "options": [
                    {
                        "id": 2510,
                        "content": "<pre><code>[\n  {\n      \"Type\": \"redirect\",\n      \"RedirectConfig\": {\n          \"Protocol\": \"HTTPS\",\n          \"Port\": \"443\",\n          \"Host\": \"#{host}\",\n          \"Path\": \"/#{path}\",\n          \"Query\": \"#{query}\",\n          \"StatusCode\": \"HTTP_301\"\n      }\n  }\n]\n\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 2511,
                        "content": "<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"StringHeaderConfig\": {\n          \"Values\": [\"*.example.com\"]\n      }\n  }\n]\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 2512,
                        "content": "<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"PathPatternConfig\": {\n          \"Values\": [\"/img/*\"]\n      }\n  }\n]\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 2513,
                        "content": "<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"QueryStringConfig\": {\n          \"Values\": [\n            {\n                \"Key\": \"version\",\n                \"Value\": \"v1\"\n            },\n            {\n                \"Value\": \"*example*\"\n            }\n          ]\n      }\n  }\n]\n\n</code></pre>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 612,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.081Z",
                "updatedAt": "2023-09-07T08:39:52.081Z",
                "content": "<p>A development team has inherited a web application running in the us-east-1 region with three availability zones (us-east-1a, us-east1-b, and us-east-1c) whose incoming web traffic is routed by a load balancer. When one of the EC2 instances hosting the web application crashes, the team realizes that the load balancer continues to route traffic to that instance causing intermittent issues.</p>\n\n<p>Which of the following should the development team do to minimize this problem?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Enable Health Checks</strong></p>\n\n<p>To discover the availability of your EC2 instances, a load balancer periodically sends pings, attempts connections, or sends requests to test the EC2 instances. These tests are called health checks. The status of the instances that are healthy at the time of the health check is InService. The status of any instances that are unhealthy at the time of the health check is OutOfService.</p>\n\n<p>Load Balancer Health Checks:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Stickiness</strong> - Stickiness enables the load balancer to bind a user's session to a specific instance, it cannot be used for gauging the health of an instance.</p>\n\n<p><strong>Enable Multi-AZ deployments</strong> - It's a good practice to provision instances in more than one availability zone however you still need a way to check the health status of the instances, so this option is incorrect.</p>\n\n<p><strong>Enable SSL</strong> - This option has been added as a distractor. SSL encrypts the transmission of data between a web server and a browser.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n",
                "options": [
                    {
                        "id": 2514,
                        "content": "<p>Enable SSL</p>",
                        "isValid": false
                    },
                    {
                        "id": 2515,
                        "content": "<p>Enable Stickiness</p>",
                        "isValid": false
                    },
                    {
                        "id": 2516,
                        "content": "<p>Enable Health Checks</p>",
                        "isValid": true
                    },
                    {
                        "id": 2517,
                        "content": "<p>Enable Multi AZ deployments</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 613,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.155Z",
                "updatedAt": "2023-09-07T08:39:52.155Z",
                "content": "<p>A developer while working on Amazon EC2 instances, realized that an instance was not needed and had shut it down. But another instance of the same type automatically got launched in the account.</p>\n\n<p>Which of the following options can attribute the given sequence of actions?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Instance might be part of Auto Scaling Group and hence re-launched similar instance</strong> - Auto Scaling groups can be configured to launch an instance to replace an instance that is undergoing maintenance. This could have been the reason why an instance of the same type got launched automatically. The size of an Auto Scaling group depends on the number of instances that you set as the desired capacity. If you wish to terminate an instance that is part of Auto Scaling Group, the configuration of the group should be changed to a reduced number of instances, so the automatic launch of instances does not happen when an unwanted instance is terminated.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The user did not have the right permissions to shutdown the instance. User needs root permissions to terminate an instance</strong> - This is an incorrect statement. If the user does not have enough permissions, then the action itself is unavailable for him. A user does not need root permissions to terminate an EC2 instance.</p>\n\n<p><strong>The instance could have been a part of the Application Load Balancer and hence was automatically started</strong> - Application Load Balancer is used to balance the incoming traffic requests equally among the available EC2 instances so keep the performance and availability at its best. ALBs are configured with Auto Scaling Groups, but this is not specified in the use-case. In the absence of Auto Scaling Group, ALB cannot launch instances by itself.</p>\n\n<p><strong>The instance could have been a part of Network Load Balancer and hence was automatically started</strong> - As explained above for ALB, a Network Load Balancer is not capable of launching instances by itself if it's not configured with an Auto Scaling Group.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/detach-instance-asg.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/detach-instance-asg.html</a></p>\n",
                "options": [
                    {
                        "id": 2518,
                        "content": "<p>Instance might be part of Auto Scaling Group and hence re-launched similar instance</p>",
                        "isValid": true
                    },
                    {
                        "id": 2519,
                        "content": "<p>The user did not have the right permissions to shutdown the instance. User needs root permissions to terminate an instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 2520,
                        "content": "<p>The instance could have been a part of Network Load Balancer and hence was automatically started</p>",
                        "isValid": false
                    },
                    {
                        "id": 2521,
                        "content": "<p>The instance could have been a part of Application Load Balancer and hence was automatically started</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 614,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.226Z",
                "updatedAt": "2023-09-07T08:39:52.226Z",
                "content": "<p>An investment firm wants to continuously generate time-series analytics of the stocks being purchased by its customers. The firm wants to build a live leaderboard with near-real-time analytics for these in-demand stocks.</p>\n\n<p>Which of the following represents a fully managed solution with the least cost to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Kinesis Firehose to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</strong></p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Datadog, New Relic, MongoDB, and Splunk. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt your data streams before loading, minimizing the amount of storage used and increasing security.</p>\n\n<p>Amazon Kinesis Data Analytics is the easiest way to transform and analyze streaming data in real-time with Apache Flink. Apache Flink is an open source framework and engine for processing data streams. Amazon Kinesis Data Analytics reduces the complexity of building, managing, and integrating Apache Flink applications with other AWS services.</p>\n\n<p>Amazon Kinesis Data Analytics provides built-in functions to filter, aggregate, and transform streaming data for advanced analytics. It processes streaming data with sub-second latencies, enabling you to analyze and respond to incoming data and events in real-time.</p>\n\n<p>Amazon Kinesis Data Analytics is serverless; there are no servers to manage. It runs your streaming applications without requiring you to provision or manage any infrastructure. Amazon Kinesis Data Analytics automatically scales the infrastructure up and down as required to process incoming data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Kinesis Data Streams to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Although Kinesis Data Streams supports on-demand provisioning of shards, however, the data ingestion cost along with the per hour shards cost would be more than the corresponding cost incurred while using Firehose. The use-case clearly states that the company wants a fully managed solution with the least cost, so Kinesis Firehose is a better solution.</p>\n\n<p><strong>Use Kinesis Data Streams to ingest data and Amazon Kinesis Client Library to the application logic to generate leaderboard scores and time-series analytics</strong> - The Amazon Kinesis Client Library (KCL) is a pre-built library that helps you build consumer applications for reading and processing data from an Amazon Kinesis data stream. The KCL handles complex issues such as adapting to changes in data stream volume, load balancing streaming data, coordinating distributed services, and processing data with fault-tolerance. The KCL enables you to focus on business logic while building applications.</p>\n\n<p>If you want a fully managed solution and you want to use SQL to process the data from your data stream, you should use Kinesis Data Analytics. Use KCL if you need to build a custom processing solution whose requirements are not met by Kinesis Data Analytics, and you can manage the resulting consumer application.</p>\n\n<p><strong>Use Kinesis Firehose to ingest data and Amazon Athena to generate leaderboard scores and time-series analytics</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena is used for running analytics on S3 based data. For running analytics on real-time streaming data, Kinesis Data Analytics is the right fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/data-analysis.html\">https://docs.aws.amazon.com/firehose/latest/dev/data-analysis.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/pricing/\">https://aws.amazon.com/kinesis/data-firehose/pricing/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/pricing/\">https://aws.amazon.com/kinesis/data-streams/pricing/</a></p>\n",
                "options": [
                    {
                        "id": 2522,
                        "content": "<p>Use Kinesis Data Streams to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</p>",
                        "isValid": false
                    },
                    {
                        "id": 2523,
                        "content": "<p>Use Kinesis Firehose to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</p>",
                        "isValid": true
                    },
                    {
                        "id": 2524,
                        "content": "<p>Use Kinesis Firehose to ingest data and Amazon Athena to generate leaderboard scores and time-series analytics</p>",
                        "isValid": false
                    },
                    {
                        "id": 2525,
                        "content": "<p>Use Kinesis Data Streams to ingest data and Amazon Kinesis Client Library to the application logic to generate leaderboard scores and time-series analytics</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 615,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.293Z",
                "updatedAt": "2023-09-07T08:39:52.293Z",
                "content": "<p>A developer is creating a RESTful API service using an Amazon API Gateway with AWS Lambda integration. The service must support different API versions for testing purposes.</p>\n\n<p>As a Developer Associate, which of the following would you suggest as the best way to accomplish this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the API versions as unique stages with unique endpoints and use stage variables to provide the context to identify the API versions</strong> - A stage is a named reference to a deployment, which is a snapshot of the API. You use a stage to manage and optimize a particular deployment. For example, you can configure stage settings to enable caching, customize request throttling, configure logging, define stage variables, or attach a canary release for testing.</p>\n\n<p>Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates.</p>\n\n<p>With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.</p>\n\n<p>For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example, beta.example.com). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an X-Version header to identify which version is being called and pass that header to the Lambda function</strong> - This is an incorrect option and has been added as a distractor.</p>\n\n<p><strong>Use an API Gateway Lambda authorizer to route API clients to the correct API version</strong> - A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity.</p>\n\n<p><strong>Set up an API Gateway resource policy to identify the API versions and provide context to the Lambda function</strong> - Amazon API Gateway resource policies are JSON policy documents that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. You can use API Gateway resource policies to allow your API to be securely invoked requestors. They are not meant for choosing the version of APIs.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n",
                "options": [
                    {
                        "id": 2526,
                        "content": "<p>Use an X-Version header to identify which version is being called and pass that header to the Lambda function</p>",
                        "isValid": false
                    },
                    {
                        "id": 2527,
                        "content": "<p>Deploy the API versions as unique stages with unique endpoints and use stage variables to provide the context to identify the API versions</p>",
                        "isValid": true
                    },
                    {
                        "id": 2528,
                        "content": "<p>Use an API Gateway Lambda authorizer to route API clients to the correct API version</p>",
                        "isValid": false
                    },
                    {
                        "id": 2529,
                        "content": "<p>Set up an API Gateway resource policy to identify the API versions and provide context to the Lambda function</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 616,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.364Z",
                "updatedAt": "2023-09-07T08:39:52.364Z",
                "content": "<p>A developer at a university is encrypting a large XML payload transferred over the network using AWS KMS and wants to test the application before going to production.</p>\n\n<p>What is the maximum data size supported by AWS KMS?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>4 KB</strong></p>\n\n<p>You can encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information.</p>\n\n<p>While AWS KMS does support sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>1MB</strong> - For anything over 4 KB, you may want to look at envelope encryption</p>\n\n<p><strong>10MB</strong> - For anything over 4 KB, you may want to look at envelope encryption</p>\n\n<p><strong>16KB</strong> - For anything over 4 KB, you may want to look at envelope encryption</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kms/faqs/\">https://aws.amazon.com/kms/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 2530,
                        "content": "<p>4KB</p>",
                        "isValid": true
                    },
                    {
                        "id": 2531,
                        "content": "<p>16KB</p>",
                        "isValid": false
                    },
                    {
                        "id": 2532,
                        "content": "<p>1MB</p>",
                        "isValid": false
                    },
                    {
                        "id": 2533,
                        "content": "<p>10MB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 617,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.442Z",
                "updatedAt": "2023-09-07T08:39:52.442Z",
                "content": "<p>An organization with high data volume workloads have successfully moved to DynamoDB after having many issues with traditional database systems. However, a few months into production, DynamoDB tables are consistently recording high latency.</p>\n\n<p>As a Developer Associate, which of the following would you suggest to reduce the latency? (Select two)</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup, and restore and in-memory caching for internet-scale applications.</p>\n\n<p><strong>Consider using Global tables if your application is accessed by globally distributed users</strong> - If you have globally dispersed users, consider using global tables. With global tables, you can specify the AWS Regions where you want the table to be available. This can significantly reduce latency for your users. So, reducing the distance between the client and the DynamoDB endpoint is an important performance fix to be considered.</p>\n\n<p><strong>Use eventually consistent reads in place of strongly consistent reads whenever possible</strong> - If your application doesn't require strongly consistent reads, consider using eventually consistent reads. Eventually consistent reads are cheaper and are less likely to experience high latency.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the request timeout settings, so the client gets enough time to complete the requests, thereby reducing retries on the system</strong> - This statement is incorrect. The right way is to reduce the request timeout settings. This causes the client to abandon high latency requests after the specified time period and then send a second request that usually completes much faster than the first.</p>\n\n<p><strong>Reduce connection pooling, which keeps the connections alive even when user requests are not present, thereby, blocking the services</strong> - This is not correct. When you're not making requests, consider having the client send dummy traffic to a DynamoDB table. Alternatively, you can reuse client connections or use connection pooling. All of these techniques keep internal caches warm, which helps keep latency low.</p>\n\n<p><strong>Use DynamoDB Accelerator (DAX) for businesses with heavy write-only workloads</strong> - This is not correct. If your traffic is read-heavy, consider using a caching service such as DynamoDB Accelerator (DAX). DAX is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvementâ€”from milliseconds to microsecondsâ€”even at millions of requests per second.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-high-latency/\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-high-latency/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n",
                "options": [
                    {
                        "id": 2534,
                        "content": "<p>Consider using Global tables if your application is accessed by globally distributed users</p>",
                        "isValid": true
                    },
                    {
                        "id": 2535,
                        "content": "<p>Use DynamoDB Accelerator (DAX) for businesses with heavy write-only workloads</p>",
                        "isValid": false
                    },
                    {
                        "id": 2536,
                        "content": "<p>Reduce connection pooling, which keeps the connections alive even when user requests are not present, thereby, blocking the services</p>",
                        "isValid": false
                    },
                    {
                        "id": 2537,
                        "content": "<p>Use eventually consistent reads in place of strongly consistent reads whenever possible</p>",
                        "isValid": true
                    },
                    {
                        "id": 2538,
                        "content": "<p>Increase the request timeout settings, so the client gets enough time to complete the requests, thereby reducing retries on the system</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 618,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.506Z",
                "updatedAt": "2023-09-07T08:39:52.506Z",
                "content": "<p>An e-commerce application writes log files into Amazon S3. The application also reads these log files in parallel on a near real-time basis. The development team wants to address any data discrepancies that might arise when the application overwrites an existing log file and then tries to read that specific log file.</p>\n\n<p>Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object</strong></p>\n\n<p>Amazon S3 delivers strong read-after-write consistency automatically, without changes to performance or availability, without sacrificing regional isolation for applications, and at no additional cost.</p>\n\n<p>After a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected.</p>\n\n<p>Strong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects.</p>\n\n<p>To summarize, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are strongly consistent. What you write is what you will read, and the results of a LIST will be an accurate reflection of whatâ€™s in the bucket.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data</strong></p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data</strong></p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data</strong></p>\n\n<p>These three options contradict the earlier details provided in the explanation.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 2539,
                        "content": "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data</p>",
                        "isValid": false
                    },
                    {
                        "id": 2540,
                        "content": "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data</p>",
                        "isValid": false
                    },
                    {
                        "id": 2541,
                        "content": "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data</p>",
                        "isValid": false
                    },
                    {
                        "id": 2542,
                        "content": "<p>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 619,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.583Z",
                "updatedAt": "2023-09-07T08:39:52.583Z",
                "content": "<p>As a Developer Associate, you are responsible for the data management of the AWS Kinesis streams at your company. The security team has mandated stricter security requirements by leveraging mechanisms available with the Kinesis Data Streams service that won't require code changes on your end.</p>\n\n<p>Which of the following features meet the given requirements? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>KMS encryption for data at rest</strong></p>\n\n<p><strong>Encryption in flight with HTTPS endpoint</strong></p>\n\n<p>Server-side encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it's written to the Kinesis stream storage layer and decrypted after it's retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. Also, the HTTPS protocol ensures that data inflight is encrypted as well.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-C encryption</strong> - SSE-C is functionality in Amazon S3 where S3 encrypts your data, on your behalf, using keys that you provide. This does not apply for the given use-case.</p>\n\n<p><strong>Client-Side Encryption</strong> - This involves code changes, so the option is incorrect.</p>\n\n<p><strong>Envelope Encryption</strong> - This involves code changes, so the option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html\">https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html</a></p>\n",
                "options": [
                    {
                        "id": 2543,
                        "content": "<p>Envelope Encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 2544,
                        "content": "<p>Encryption in flight with HTTPS endpoint</p>",
                        "isValid": true
                    },
                    {
                        "id": 2545,
                        "content": "<p>SSE-C encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 2546,
                        "content": "<p>Client-Side Encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 2547,
                        "content": "<p>KMS encryption for data at rest</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 620,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.649Z",
                "updatedAt": "2023-09-07T08:39:52.649Z",
                "content": "<p>As a site reliability engineer, you work on building and running large-scale, distributed, fault-tolerant systems in the cloud using automation. You have just replaced the company's Jenkins based CI/CD platform with AWS CodeBuild and would like to programmatically define your build steps.</p>\n\n<p>Which of the following options should you choose?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Define a <code>buildspec.yml</code> file in the root directory</strong></p>\n\n<p>AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.</p>\n\n<p>A build spec is a collection of build commands and related settings, in YAML format, that AWS CodeBuild uses to run a build. You can include a build spec as part of the source code or you can define a build spec when you create a build project.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q37-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Define an <code>appspec.yml</code> file in the root directory</strong> - The AppSpec file is used for deployment in the CodeDeploy service.</p>\n\n<p><strong>Define a <code>buildspec.yml</code> file in the codebuild/ directory</strong> - The file is correct but must be in the root directory.</p>\n\n<p><strong>Define an <code>appspec.yml</code> file in the codebuild/ directory</strong> - The AppSpec file is used for deployment in the CodeDeploy service.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n",
                "options": [
                    {
                        "id": 2548,
                        "content": "<p>Define a <code>buildspec.yml</code> file in the root directory</p>",
                        "isValid": true
                    },
                    {
                        "id": 2549,
                        "content": "<p>Define a <code>buildspec.yml</code> file in the codebuild/ directory</p>",
                        "isValid": false
                    },
                    {
                        "id": 2550,
                        "content": "<p>Define an <code>appspec.yml</code> file in the codebuild/ directory</p>",
                        "isValid": false
                    },
                    {
                        "id": 2551,
                        "content": "<p>Define an <code>appspec.yml</code> file in the root directory</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 621,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.718Z",
                "updatedAt": "2023-09-07T08:39:52.718Z",
                "content": "<p>As a Developer, you are working on a mobile application that utilizes Amazon Simple Queue Service (SQS) for sending messages to downstream systems for further processing. One of the requirements is that the messages should be stored in the queue for a period of 12 days.</p>\n\n<p>How will you configure this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Change the queue message retention setting</strong> - Amazon SQS automatically deletes messages that have been in a queue for more than the maximum message retention period. The default message retention period is 4 days. However, you can set the message retention period to a value from 60 seconds to 1,209,600 seconds (14 days) using the SetQueueAttributes action.</p>\n\n<p>More info here:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Long Polling for the SQS queue</strong> - Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). This feature is not useful for the current use case.</p>\n\n<p><strong>The maximum retention period of SQS messages is 7 days, therefore retention period of 12 days is not possible</strong> - This is an incorrect statement. Retention period of up to 14 days is possible.</p>\n\n<p><strong>Use a FIFO SQS queue</strong> - FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated. This is not useful for the current scenario.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n",
                "options": [
                    {
                        "id": 2552,
                        "content": "<p>The maximum retention period of SQS messages is 7 days, therefore retention period of 12 days is not possible</p>",
                        "isValid": false
                    },
                    {
                        "id": 2553,
                        "content": "<p>Use a FIFO SQS queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 2554,
                        "content": "<p>Change the queue message retention setting</p>",
                        "isValid": true
                    },
                    {
                        "id": 2555,
                        "content": "<p>Enable Long Polling for the SQS queue</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 622,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.783Z",
                "updatedAt": "2023-09-07T08:39:52.783Z",
                "content": "<p>An e-commerce company uses Amazon SQS queues to decouple their application architecture. The development team has observed message processing failures for an edge case scenario when a user places an order for a particular product ID, but the product ID is deleted, thereby causing the application code to fail.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to address such message failures?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use a dead-letter queue to handle message processing failures</strong></p>\n\n<p>Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.</p>\n\n<p>Sometimes, messages canâ€™t be processed because of a variety of possible issues, such as when a user comments on a story but it remains unprocessed because the original story itself is deleted by the author while the comments were being posted. In such a case, the dead-letter queue can be used to handle message processing failures.</p>\n\n<p>How do dead-letter queues work?\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p>Use-cases for dead-letter queues:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q31-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a temporary queue to handle message processing failures</strong> - The most common use case for temporary queues is the request-response messaging pattern (for example, processing a login request), where a requester creates a temporary queue for receiving each response message. To avoid creating an Amazon SQS queue for each response message, the Temporary Queue Client lets you create and delete multiple temporary queues without making any Amazon SQS API calls. Temporary queues cannot be used to handle message processing failures.</p>\n\n<p><strong>Use short polling to handle message processing failures</strong></p>\n\n<p><strong>Use long polling to handle message processing failures</strong></p>\n\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.\nNeither short polling nor long polling can be used to handle message processing failures.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n",
                "options": [
                    {
                        "id": 2556,
                        "content": "<p>Use short polling to handle message processing failures</p>",
                        "isValid": false
                    },
                    {
                        "id": 2557,
                        "content": "<p>Use a temporary queue to handle message processing failures</p>",
                        "isValid": false
                    },
                    {
                        "id": 2558,
                        "content": "<p>Use a dead-letter queue to handle message processing failures</p>",
                        "isValid": true
                    },
                    {
                        "id": 2559,
                        "content": "<p>Use long polling to handle message processing failures</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 623,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.857Z",
                "updatedAt": "2023-09-07T08:39:52.857Z",
                "content": "<p>A company has sensitive data stored in an Amazon S3 bucket that is encrypted using AWS Key Management Service (AWS KMS). A developer wants to enforce encryption in transit for all users who have been granted permission to use the S3 GetObject operation across multiple AWS accounts.</p>\n\n<p>Which of the following represents the best solution for this use case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Configure a resource-based policy on the S3 bucket to deny access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></strong></p>\n\n<p>If you want to prevent potential attackers from manipulating network traffic, you can use HTTPS (TLS) to only allow encrypted connections while restricting HTTP requests from accessing your bucket. To determine whether the request is HTTP or HTTPS, use the aws:SecureTransport global condition key in your S3 bucket policy. The aws:SecureTransport condition key checks whether a request was sent by using HTTP.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q16-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a resource-based policy on the S3 bucket to allow access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></strong> - This option contradicts the explanation provided above.</p>\n\n<p><strong>Configure a resource-based policy on the KMS key to allow access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></strong></p>\n\n<p><strong>Configure a resource-based policy on the KMS key to deny access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></strong></p>\n\n<p>Since the use case is about granting permission to use the S3 GetObject operations with encryption in transit, you cannot use a Resource-based policy for KMS. So, both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/\">https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html</a></p>\n",
                "options": [
                    {
                        "id": 2560,
                        "content": "<p>Configure a resource-based policy on the KMS key to deny access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2561,
                        "content": "<p>Configure a resource-based policy on the S3 bucket to deny access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></p>",
                        "isValid": true
                    },
                    {
                        "id": 2562,
                        "content": "<p>Configure a resource-based policy on the S3 bucket to allow access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2563,
                        "content": "<p>Configure a resource-based policy on the KMS key to allow access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 624,
            "attributes": {
                "createdAt": "2023-09-07T08:39:52.932Z",
                "updatedAt": "2023-09-07T08:39:52.932Z",
                "content": "<p>Your team-mate has configured an Amazon S3 event notification for an S3 bucket that holds sensitive audit data of a firm. As the Team Lead, you are receiving the SNS notifications for every event in this bucket. After validating the event data, you realized that few events are missing.</p>\n\n<p>What could be the reason for this behavior and how to avoid this in the future?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent</strong> - Amazon S3 event notifications are designed to be delivered at least once. Typically, event notifications are delivered in seconds but can sometimes take a minute or longer.</p>\n\n<p>If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent. If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket. With versioning, every successful write will create a new version of your object and will also send event notification.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Someone could have created a new notification configuration and that has overridden your existing configuration</strong> - It is possible that the configuration can be overridden. But, in the current scenario, the team lead is receiving notifications for most of the events, which nullifies the claim that the configuration is overridden.</p>\n\n<p><strong>Versioning is enabled on the S3 bucket and event notifications are getting fired for only one version</strong> - This is an incorrect statement. If you want to ensure that an event notification is sent for every successful write, you should enable versioning on your bucket. With versioning, every successful write will create a new version of your object and will also send event notification.</p>\n\n<p><strong>Your notification action is writing to the same bucket that triggers the notification</strong> - If your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. But it will not result in missing events.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n",
                "options": [
                    {
                        "id": 2564,
                        "content": "<p>Versioning is enabled on the S3 bucket and event notifications are getting fired for only one version</p>",
                        "isValid": false
                    },
                    {
                        "id": 2565,
                        "content": "<p>Your notification action is writing to the same bucket that triggers the notification</p>",
                        "isValid": false
                    },
                    {
                        "id": 2566,
                        "content": "<p>If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent</p>",
                        "isValid": true
                    },
                    {
                        "id": 2567,
                        "content": "<p>Someone could have created a new notification configuration and that has overridden your existing configuration</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 625,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.001Z",
                "updatedAt": "2023-09-07T08:39:53.001Z",
                "content": "<p>An AWS CodePipeline was configured to be triggered by Amazon CloudWatch Events. Recently the pipeline failed and upon investigation, the Team Lead noticed that the source was changed from AWS CodeCommit to Amazon Simple Storage Service (S3). The Team Lead has requested you to find the user who had made the changes.</p>\n\n<p>Which service will help you solve this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS CloudTrail</strong></p>\n\n<p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides an event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services.</p>\n\n<p>AWS CloudTrail increases visibility into your user and resource activity by recording AWS Management Console actions and API calls. You can identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred.</p>\n\n<p>How CloudTrail works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q25-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon CloudWatch</strong> - Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. CloudWatch can collect numbers and respond to AWS service-related events, but it does not help in user activity logging.</p>\n\n<p><strong>AWS X-Ray</strong> - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray is a very important tool in troubleshooting but is not useful in logging user activity.</p>\n\n<p><strong>Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. This does not log User activity at the account level.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/\">https://aws.amazon.com/cloudwatch/</a></p>\n",
                "options": [
                    {
                        "id": 2568,
                        "content": "<p>Amazon CloudWatch</p>",
                        "isValid": false
                    },
                    {
                        "id": 2569,
                        "content": "<p>AWS X-Ray</p>",
                        "isValid": false
                    },
                    {
                        "id": 2570,
                        "content": "<p>AWS CloudTrail</p>",
                        "isValid": true
                    },
                    {
                        "id": 2571,
                        "content": "<p>Amazon Inspector</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 626,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.077Z",
                "updatedAt": "2023-09-07T08:39:53.077Z",
                "content": "<p>The development team at a health-care company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon RDS as the database tier for its flagship application.</p>\n\n<p>Which of the following would you identify as correct for RDS Multi-AZ? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary, and finally performing maintenance on the old primary, which becomes the new standby</strong></p>\n\n<p>Running a DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event because Amazon RDS applies operating system updates by following these steps:</p>\n\n<p>Perform maintenance on the standby.</p>\n\n<p>Promote the standby to primary.</p>\n\n<p>Perform maintenance on the old primary, which becomes the new standby.</p>\n\n<p>When you modify the database engine for your DB instance in a Multi-AZ deployment, then Amazon RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade.</p>\n\n<p><strong>Amazon RDS automatically initiates a failover to the standby, in case the primary database fails for any reason</strong> - You also benefit from enhanced database availability when running your DB instance as a Multi-AZ deployment. If an Availability Zone failure or DB instance failure occurs, your availability impact is limited to the time automatic failover takes to complete.</p>\n\n<p>Another implied benefit of running your DB instance as a Multi-AZ deployment is that DB instance failover is automatic and requires no administration. In an Amazon RDS context, this means you are not required to monitor DB instance events and initiate manual DB instance recovery in the event of an Availability Zone failure or DB instance failure.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB</strong> - The availability benefits of Multi-AZ also extend to planned maintenance. For example, with automated backups, I/O activity is no longer suspended on your primary during your preferred backup window, since backups are taken from the standby.</p>\n\n<p><strong>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</strong> - A Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. As such, the feature uses synchronous replication between primary and standby. AWS implementation makes sure the primary and the standby are constantly in sync, but precludes using the standby for read or write operations.</p>\n\n<p><strong>Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</strong> - When you create your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous â€œstandbyâ€ replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across the Availability Zone to the standby in order to keep both in sync and protect your latest database updates against DB instance failure.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n",
                "options": [
                    {
                        "id": 2572,
                        "content": "<p>For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB</p>",
                        "isValid": false
                    },
                    {
                        "id": 2573,
                        "content": "<p>Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason</p>",
                        "isValid": true
                    },
                    {
                        "id": 2574,
                        "content": "<p>RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby</p>",
                        "isValid": true
                    },
                    {
                        "id": 2575,
                        "content": "<p>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</p>",
                        "isValid": false
                    },
                    {
                        "id": 2576,
                        "content": "<p>Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 627,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.149Z",
                "updatedAt": "2023-09-07T08:39:53.149Z",
                "content": "<p>Your organization has developers that merge code changes regularly to an AWS CodeCommit repository. Your pipeline has AWS CodeCommit as the source and you would like to configure a rule that reacts to changes in CodeCommit.</p>\n\n<p>Which of the following options do you choose for this type of integration?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use CloudWatch Event Rules</strong></p>\n\n<p>Amazon CloudWatch Events is a web service that monitors your AWS resources and the applications you run on AWS. You can use Amazon CloudWatch Events to detect and react to changes in the state of a pipeline, stage, or action. Then, based on rules you create, CloudWatch Events invokes one or more target actions when a pipeline, stage, or action enters the state you specify in a rule.\nExamples of Amazon CloudWatch Events rules and targets:</p>\n\n<ol>\n<li><p>A rule that sends a notification when the instance state changes, where an EC2 instance is the event source, and Amazon SNS is the event target.</p></li>\n<li><p>A rule that sends a notification when the build phase changes, where a CodeBuild configuration is the event source, and Amazon SNS is the event target.</p></li>\n<li><p>A rule that detects pipeline changes and invokes an AWS Lambda function.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudTrail Event rules with Amazon Simple Email Service (SES)</strong> - This is an incorrect statement. There is no such thing as CloudTrail Event Rule.</p>\n\n<p><strong>Use Lambda function with Amazon Simple Notification Service (SNS)</strong> - Lambda functions can be triggered by the use of CloudWatch Event Rules as discussed above. AWS CodePipeline does not trigger Lambda functions directly.</p>\n\n<p><strong>Use Lambda Event Rules</strong> - This is an incorrect statement. There is no such thing as Lambda Event Rule.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n",
                "options": [
                    {
                        "id": 2577,
                        "content": "<p>Use CloudWatch Event Rules</p>",
                        "isValid": true
                    },
                    {
                        "id": 2578,
                        "content": "<p>Use CloudTrail Event rules with Amazon Simple Email Service (SES)</p>",
                        "isValid": false
                    },
                    {
                        "id": 2579,
                        "content": "<p>Use Lambda function with Amazon Simple Notification Service (SNS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 2580,
                        "content": "<p>Use Lambda Event Rules</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 628,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.226Z",
                "updatedAt": "2023-09-07T08:39:53.226Z",
                "content": "<p>A development team has been using Amazon S3 service as an object store. With Amazon S3 turning strongly consistent, the team wants to understand the impact of this change on its data storage practices.</p>\n\n<p>As a developer associate, can you identify the key characteristics of the strongly consistent data model followed by S3? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list</strong> - Bucket configurations have an eventual consistency model. If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list.</p>\n\n<p><strong>A process deletes an existing object and immediately tries to read it. Amazon S3 will not return any data as the object has been deleted</strong> - Amazon S3 provides strong read-after-write consistency for PUTs and DELETEs of objects in your Amazon S3 bucket in all AWS Regions. This applies to both writes to new objects as well as PUTs that overwrite existing objects and DELETEs.</p>\n\n<p>Amazon S3 data consistency model:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A process deletes an existing object and immediately lists keys within its bucket. The object could still be visible for few more minutes till the change propagates</strong></p>\n\n<p><strong>A process deletes an existing object and immediately tries to read it. Amazon S3 can return data as the object deletion has not yet propagated completely</strong> -</p>\n\n<p>These two options highlight an eventually consistent behavior. Amazon S3 is now strongly consistent and will not return any data as the object has been deleted. So both these options are incorrect.</p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Amazon S3 might return the old data</strong> - Amazon S3 will return the new data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel</a></p>\n",
                "options": [
                    {
                        "id": 2581,
                        "content": "<p>A process deletes an existing object and immediately lists keys within its bucket. The object could still be visible for few more minutes till the change propagates</p>",
                        "isValid": false
                    },
                    {
                        "id": 2582,
                        "content": "<p>A process deletes an existing object and immediately tries to read it. Amazon S3 will not return any data as the object has been deleted</p>",
                        "isValid": true
                    },
                    {
                        "id": 2583,
                        "content": "<p>If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list</p>",
                        "isValid": true
                    },
                    {
                        "id": 2584,
                        "content": "<p>A process deletes an existing object and immediately tries to read it. Amazon S3 can return data as the object deletion has not yet propagated completely</p>",
                        "isValid": false
                    },
                    {
                        "id": 2585,
                        "content": "<p>A process replaces an existing object and immediately tries to read it. Amazon S3 might return the old data</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 629,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.296Z",
                "updatedAt": "2023-09-07T08:39:53.296Z",
                "content": "<p>A photo-sharing application manages its EC2 server fleet running behind an Application Load Balancer and the traffic is fronted by a CloudFront distribution. The development team wants to decouple the user authentication process for the application so that the application servers can just focus on the business logic.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to address this use-case with minimal development effort?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito Authentication via Cognito User Pools for your Application Load Balancer</strong></p>\n\n<p>Application Load Balancer can be used to securely authenticate users for accessing your applications. This enables you to offload the work of authenticating users to your load balancer so that your applications can focus on their business logic. You can use Cognito User Pools to authenticate users through well-known social IdPs, such as Amazon, Facebook, or Google, through the user pools supported by Amazon Cognito or through corporate identities, using SAML, LDAP, or Microsoft AD, through the user pools supported by Amazon Cognito. You configure user authentication by creating an authenticate action for one or more listener rules. The authenticate-cognito and authenticate-oidc action types are supported only with HTTPS listeners.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a></p>\n\n<p>Please make sure that you adhere to the following configurations while using CloudFront distribution in front of your Application Load Balancer:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a></p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Cognito Authentication via Cognito Identity Pools for your Application Load Balancer</strong> - There is no such thing as using Cognito Authentication via Cognito Identity Pools for managing user authentication for the application. Application-specific user authentication can be provided via Cognito User Pools. Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token.</p>\n\n<p><strong>Use Cognito Authentication via Cognito User Pools for your CloudFront distribution</strong> - You cannot directly integrate Cognito User Pools with CloudFront distribution as you have to create a separate Lambda@Edge function to accomplish the authentication via Cognito User Pools. This involves additional development effort, so this option is not the best fit for the given use-case.</p>\n\n<p><strong>Use Cognito Authentication via Cognito Identity Pools for your CloudFront distribution</strong> - You cannot use Cognito Identity Pools for managing user authentication, so this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/\">https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/</a></p>\n",
                "options": [
                    {
                        "id": 2586,
                        "content": "<p>Use Cognito Authentication via Cognito Identity Pools for your CloudFront distribution</p>",
                        "isValid": false
                    },
                    {
                        "id": 2587,
                        "content": "<p>Use Cognito Authentication via Cognito User Pools for your CloudFront distribution</p>",
                        "isValid": false
                    },
                    {
                        "id": 2588,
                        "content": "<p>Use Cognito Authentication via Cognito User Pools for your Application Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 2589,
                        "content": "<p>Use Cognito Authentication via Cognito Identity Pools for your Application Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 630,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.370Z",
                "updatedAt": "2023-09-07T08:39:53.370Z",
                "content": "<p>Your application sends messages to an Amazon Simple Queue Service (SQS) queue frequently, which are then polled by another application that specifies which message to retrieve.</p>\n\n<p>Which of the following options describe the maximum number of messages that can be retrieved at one time?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>10</strong></p>\n\n<p>After you send messages to a queue, you can receive and delete them. When you request messages from a queue, you can't specify which messages to retrieve. Instead, you specify the maximum number of messages (up to 10) that you want to retrieve.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>5</strong></p>\n\n<p><strong>20</strong></p>\n\n<p><strong>100</strong></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-receive-delete-message.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-receive-delete-message.html</a></p>\n",
                "options": [
                    {
                        "id": 2590,
                        "content": "<p>20</p>",
                        "isValid": false
                    },
                    {
                        "id": 2591,
                        "content": "<p>10</p>",
                        "isValid": true
                    },
                    {
                        "id": 2592,
                        "content": "<p>5</p>",
                        "isValid": false
                    },
                    {
                        "id": 2593,
                        "content": "<p>100</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 631,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.446Z",
                "updatedAt": "2023-09-07T08:39:53.446Z",
                "content": "<p>A video streaming application uses Amazon CloudFront for its data distribution. The development team has decided to use CloudFront with origin failover for high availability.</p>\n\n<p>Which of the following options are correct while configuring CloudFront with Origin Groups? (Select two)</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p><strong>CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin</strong></p>\n\n<p>CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin. CloudFront only sends requests to the secondary origin after a request to the primary origin fails.</p>\n\n<p><strong>CloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, HEAD or OPTIONS</strong></p>\n\n<p>CloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, HEAD, or OPTIONS. CloudFront does not failover when the viewer sends a different HTTP method (for example POST, PUT, and so on).</p>\n\n<p>How origin failover works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When thereâ€™s a cache hit, CloudFront routes the request to the primary origin in the origin group</strong> - When thereâ€™s a cache miss, CloudFront routes the request to the primary origin in the origin group. When thereâ€™s a cache hit, CloudFront returns the requested file.</p>\n\n<p><strong>To set up origin failover, you must have a distribution with at least three origins</strong> - Two origins are enough to set up an origin failover.</p>\n\n<p><strong>In the Origin Group of your distribution, all the origins are defined as primary for automatic failover in case an origin fails</strong> - To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Only one origin can be set as primary.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n",
                "options": [
                    {
                        "id": 2594,
                        "content": "<p>To set up origin failover, you must have a distribution with at least three origins</p>",
                        "isValid": false
                    },
                    {
                        "id": 2595,
                        "content": "<p>In the Origin Group of your distribution, all the origins are defined as primary for automatic failover in case an origin fails</p>",
                        "isValid": false
                    },
                    {
                        "id": 2596,
                        "content": "<p>CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin</p>",
                        "isValid": true
                    },
                    {
                        "id": 2597,
                        "content": "<p>When thereâ€™s a cache hit, CloudFront routes the request to the primary origin in the origin group</p>",
                        "isValid": false
                    },
                    {
                        "id": 2598,
                        "content": "<p>CloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, HEAD or OPTIONS</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 632,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.515Z",
                "updatedAt": "2023-09-07T08:39:53.515Z",
                "content": "<p>A developer is configuring Amazon ECS container instances to send log information to CloudWatch Logs. For the container instances to be able to send log data to CloudWatch Logs, an IAM policy needs to be created that will allow the container instances to use the CloudWatch Logs APIs.</p>\n\n<p>Which policy is the right fit for the given requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Before your container instances can send log data to CloudWatch Logs, you must create an IAM policy to allow your container instances to use the CloudWatch Logs APIs, and then you must attach that policy to <code>ecsInstanceRole</code>.</p>\n\n<p>This policy has one statement that grants permissions to create log groups and log streams, to upload log events to log streams, and to list details about log streams.</p>\n\n<p>The wildcard character (<em>) at the end of the Resource value means that the statement allows permission for the logs:CreateLogGroup, logs:CreateLogStream, logs:PutLogEvents, and logs:DescribeLogStreams actions on any log group. To limit this permission to a specific log group, replace the wildcard character (</em>) in the resource ARN with the specific log group ARN</p>\n\n<p>Incorrect options:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<ul>\n<li>Permission to list details of the log stream needs to be attached to this policy.</li>\n</ul>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents,\n                \"ecs:DescribeServices\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:&lt;ARN of the Log Group&gt;\"\n            ]\n        }\n    ]\n}```\n\n- ecs:DescribeServices permission is not needed, but logs:DescribeLogStreams permissions are needed for the policy to perform as expected.\n\n</code></pre>\n\n<p>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:DescribeLogGroups\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:<em>:</em>:*\"\n            ]\n        }\n    ]\n}\n```</p>\n\n<ul>\n<li>logs:DescribeLogGroups is an erroneous permission here.</li>\n</ul>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/iam-identity-based-access-control-cwl.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/iam-identity-based-access-control-cwl.html</a></p>\n",
                "options": [
                    {
                        "id": 2599,
                        "content": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:*\"\n            ]\n        }\n    ]\n}\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 2600,
                        "content": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:DescribeLogGroups\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:*\"\n            ]\n        }\n    ]\n}\n</code></pre>",
                        "isValid": false
                    },
                    {
                        "id": 2601,
                        "content": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:*\"\n            ]\n        }\n    ]\n}\n</code></pre>",
                        "isValid": true
                    },
                    {
                        "id": 2602,
                        "content": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents,\n                \"ecs:DescribeServices\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:&lt;ARN of the Log Group&gt;\"\n            ]\n        }\n    ]\n}\n</code></pre>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 633,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.589Z",
                "updatedAt": "2023-09-07T08:39:53.589Z",
                "content": "<p>A developer is configuring an Amazon EC2 Auto Scaling Group that has to launch both Spot and On-Demand instances based on the requirement. Also, the CodeDeploy agent has to be automatically installed on these EC2 instances. All the EC2 instances are running on the Amazon Linux operating system.</p>\n\n<p>What is the most operationally efficient way to configure this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use launch templates to configure the EC2 Auto Scaling Group for On-Demand and spot instances. When you create a launch template use the User data field to add a configuration script that runs when the instance starts. This shell script can, in turn, install the CodeDeploy agent</strong></p>\n\n<p>A launch template specifies instance configuration information that includes the ID of the Amazon Machine Image (AMI), the instance type, a key pair, security groups, and other parameters used to launch EC2 instances. Defining a launch template instead of a launch configuration allows you to have multiple versions of a launch template.</p>\n\n<p>When you create a launch template, you can use the User data field to add a configuration script that runs when the instance starts. This shell script installs the CodeDeploy agent for all AWS Regions and supported Amazon Linux and Ubuntu distributions. You can configure CodeDeploy to auto-update on boot by setting the AUTOUPDATE variable to true.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use launch configurations to configure the EC2 Auto Scaling Group for On-Demand and spot instances. Add the shell script to the <code>Launch configuration</code> tab on the AWS console. This shell script will install the CodeDeploy agent</strong> - This statement is incorrect. Not all Amazon EC2 Auto Scaling features are available when you use launch configurations. For example, you cannot create an Auto Scaling group that launches both Spot and On-Demand Instances or that specifies multiple instance types. You must use a launch template to configure these features. AWS strongly recommends not using launch configurations anymore.</p>\n\n<p><strong>Use AWS Systems Manager for installing and updating the CodeDeploy agent automatically for Spot and On-Demand instances</strong> - AWS Systems Manager also uses launch templates to automatically deploy CodeDeploy agent. However, for this use case, SSM is not operationally efficient since SSM agent needs to be installed first on all EC2 instances and then you can install the CodeDeploy agent.</p>\n\n<p><strong>Configure AWS Resource Access Manager(RAM) to schedule the automatic installation of CodeDeploy agent on the EC2 instances. RAM automatic schedules work on only Linux machines and not on Windows operating systems</strong> - AWS RAM helps you securely share your resources across AWS accounts, within your organization or organizational units (OUs), and with IAM roles and users for supported resource types. You can use AWS RAM to share resources with other AWS accounts. This eliminates the need to provision and manage resources in every account. RAM cannot be used to install the CodeDeploy agent, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/codedeploy-agent-launch-template/\">https://aws.amazon.com/premiumsupport/knowledge-center/codedeploy-agent-launch-template/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-configurations.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-configurations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent-operations-install-ssm.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent-operations-install-ssm.html</a></p>\n",
                "options": [
                    {
                        "id": 2603,
                        "content": "<p>Use launch templates to configure the EC2 Auto Scaling Group for On-Demand and spot instances. When you create a launch template use the User data field to add a configuration script that runs when the instance starts. This shell script can, in turn, install the CodeDeploy agent</p>",
                        "isValid": true
                    },
                    {
                        "id": 2604,
                        "content": "<p>Use AWS Systems Manager for installing and updating the CodeDeploy agent automatically for Spot and On-Demand instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 2605,
                        "content": "<p>Configure AWS Resource Access Manager(RAM) to schedule the automatic install of CodeDeploy agent on the EC2 instances. RAM automatic schedules work on only Linux machines and not on Windows operating systems</p>",
                        "isValid": false
                    },
                    {
                        "id": 2606,
                        "content": "<p>Use launch configurations to configure the EC2 Auto Scaling Group for On-Demand and spot instances. Add the shell script to the <code>Launch configuration</code> tab on the AWS console. This shell script will install the CodeDeploy agent</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 634,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.666Z",
                "updatedAt": "2023-09-07T08:39:53.666Z",
                "content": "<p>A video encoding application running on an EC2 instance takes about 20 seconds on average to process each raw footage file. The application picks the new job messages from an SQS queue. The development team needs to account for the use-case when the video encoding process takes longer than usual so that the same raw footage is not processed by multiple consumers.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to address this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use ChangeMessageVisibility action to extend a message's visibility timeout</strong></p>\n\n<p>Amazon SQS uses a visibility timeout to prevent other consumers from receiving and processing the same message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n\n<p>For example, you have a message with a visibility timeout of 5 minutes. After 3 minutes, you call ChangeMessageVisibility with a timeout of 10 minutes. You can continue to call ChangeMessageVisibility to extend the visibility timeout to the maximum allowed time. If you try to extend the visibility timeout beyond the maximum, your request is rejected. So, for the given use-case, the application can set the initial visibility timeout to 1 minute and then continue to update the ChangeMessageVisibility value if required.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q5-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use DelaySeconds action to delay a message's visibility timeout</strong> - Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. To set delay seconds on individual messages, rather than on an entire queue, use message timers to allow Amazon SQS to use the message timer's DelaySeconds value instead of the delay queue's DelaySeconds value. You cannot use DelaySeconds to alter the visibility of a message which has been picked for processing.</p>\n\n<p><strong>Use WaitTimeSeconds action to short poll and extend a message's visibility timeout</strong></p>\n\n<p><strong>Use WaitTimeSeconds action to long poll and extend a message's visibility timeout</strong></p>\n\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. Both these options have been added as distractors as WaitTimeSeconds (via short polling or long polling) cannot be used to influence the message's visibility.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html</a></p>\n",
                "options": [
                    {
                        "id": 2607,
                        "content": "<p>Use WaitTimeSeconds action to long poll and extend a message's visibility timeout</p>",
                        "isValid": false
                    },
                    {
                        "id": 2608,
                        "content": "<p>Use WaitTimeSeconds action to short poll and extend a message's visibility timeout</p>",
                        "isValid": false
                    },
                    {
                        "id": 2609,
                        "content": "<p>Use ChangeMessageVisibility action to extend a message's visibility timeout</p>",
                        "isValid": true
                    },
                    {
                        "id": 2610,
                        "content": "<p>Use DelaySeconds action to delay a message's visibility timeout</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 635,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.735Z",
                "updatedAt": "2023-09-07T08:39:53.735Z",
                "content": "<p>A multi-national company maintains separate AWS accounts for different verticals in their organization. The project manager of a team wants to migrate the Elastic Beanstalk environment from Team A's AWS account into Team B's AWS account. As a Developer, you have been roped in to help him in this process.</p>\n\n<p>Which of the following will you suggest?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a saved configuration in Team A's account and download it to your local machine. Make the account-specific parameter changes and upload to the S3 bucket in Team B's account. From Elastic Beanstalk console, create an application from 'Saved Configurations</strong> - You must use saved configurations to migrate an Elastic Beanstalk environment between AWS accounts.\nYou can save your environment's configuration as an object in Amazon Simple Storage Service (Amazon S3) that can be applied to other environments during environment creation, or applied to a running environment. Saved configurations are YAML formatted templates that define an environment's platform version, tier, configuration option settings, and tags.</p>\n\n<p>Download the saved configuration to your local machine. Change your account-specific parameters in the downloaded configuration file, and then save the changes. For example, change the key pair name, subnet ID, or application name (such as application-b-name). Upload the saved configuration from your local machine to an S3 bucket in Team B's account. From this account, create a new Beanstalk application by choosing 'Saved Configurations' from the navigation panel.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a saved configuration in Team A's account and configure it to Export. Now, log into Team B's account and choose the Import option. Here, you need to specify the name of the saved configuration and allow the system to create the new application. This takes a little time based on the Regions the two accounts belong to</strong> - There is no direct Export and Import\noption for migrating Elastic Beanstalk configurations.</p>\n\n<p><strong>It is not possible to migrate Elastic Beanstalk environment from one AWS account to the other</strong> - This is an incorrect statement.</p>\n\n<p><strong>Create an export configuration from the Elastic Beanstalk console from Team A's account. This configuration has to be shared with the IAM Role of Team B's account. The import option of the Team B's account will show the saved configuration, that can be used to create a new Beanstalk application</strong> - This contradicts the explanation provided earlier.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-migration-accounts/\">https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-migration-accounts/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-savedconfig.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-savedconfig.html</a></p>\n",
                "options": [
                    {
                        "id": 2611,
                        "content": "<p>It is not possible to migrate Elastic Beanstalk environment from one AWS account to the other</p>",
                        "isValid": false
                    },
                    {
                        "id": 2612,
                        "content": "<p>Create an export configuration from the Elastic Beanstalk console from Team A's account. This configuration has to be shared with the IAM Role of Team B's account. The import option of Team B's account will show the saved configuration, that can be used to create a new Beanstalk application</p>",
                        "isValid": false
                    },
                    {
                        "id": 2613,
                        "content": "<p>Create a saved configuration in Team A's account and configure it to Export. Now, log into Team B's account and choose the Import option. Here, you need to specify the name of the saved configuration and allow the system to create the new application. This takes a little time based on the Regions the two accounts belong to</p>",
                        "isValid": false
                    },
                    {
                        "id": 2614,
                        "content": "<p>Create a saved configuration in Team A's account and download it to your local machine. Make the account-specific parameter changes and upload to the S3 bucket in Team B's account. From Elastic Beanstalk console, create an application from 'Saved Configurations'</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 636,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.804Z",
                "updatedAt": "2023-09-07T08:39:53.804Z",
                "content": "<p>Your company has a load balancer in a VPC configured to be internet facing. The public DNS name assigned to the load balancer is <code>myDns-1234567890.us-east-1.elb.amazonaws.com</code>. When your client applications first load they capture the load balancer DNS name and then resolve the IP address for the load balancer so that they can directly reference the underlying IP.</p>\n\n<p>It is observed that the client applications work well but unexpectedly stop working after a while. What is the reason for this?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>The load balancer is highly available and its public IP may change. The DNS name is constant</strong></p>\n\n<p>When your load balancer is created, it receives a public DNS name that clients can use to send requests. The DNS servers resolve the DNS name of your load balancer to the public IP addresses of the load balancer nodes for your load balancer. Never resolve the IP of a load balancer as it can change with time. You should always use the DNS name.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Your security groups are not stable</strong> - You security groups to allow your load balancer to work with registered instances. It is stable if set correctly. If your application is working and stops after a while, the issue is not with the security groups.</p>\n\n<p><strong>You need to enable stickiness</strong> - This enables the load balancer to bind a user's session to a specific instance, so this has no impact on the issue described in the given use-case.</p>\n\n<p><strong>You need to disable multi-AZ deployments</strong> - This has been added as a distractor and this has no bearing on the use-case. The change is happening with the IP of the load balancer.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html</a></p>\n",
                "options": [
                    {
                        "id": 2615,
                        "content": "<p>The load balancer is highly available and its public IP may change. The DNS name is constant</p>",
                        "isValid": true
                    },
                    {
                        "id": 2616,
                        "content": "<p>You need to enable stickiness</p>",
                        "isValid": false
                    },
                    {
                        "id": 2617,
                        "content": "<p>You need to disable multi-AZ deployments</p>",
                        "isValid": false
                    },
                    {
                        "id": 2618,
                        "content": "<p>Your security groups are not stable</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 637,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.878Z",
                "updatedAt": "2023-09-07T08:39:53.878Z",
                "content": "<p>The development team at an e-commerce company wants to run a serverless data store service on two docker containers that share resources.</p>\n\n<p>Which of the following ECS configurations can be used to facilitate this use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Put the two containers into a single task definition using a Fargate Launch Type</strong></p>\n\n<p>Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control over your infrastructure, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.</p>\n\n<p><img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png\">\nvia - <a href=\"https://aws.amazon.com/ecs/\">https://aws.amazon.com/ecs/</a></p>\n\n<p>As the development team is looking for a serverless data store service, therefore the two containers should be launched into a single task definition using a Fargate Launch Type. Using a single task definition allows the two containers to share resources. Please see these use-cases for Fargate Launch type when you should put multiple containers into the same task definition:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html</a></p>\n\n<p>For a deep-dive on understanding how Amazon ECS manages CPU and memory resources, please review this excellent blog-\n<a href=\"https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/\">https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Put the two containers into two separate task definitions using a Fargate Launch Type</strong> - This option contradicts the details provided in the explanation above, so this option is ruled out.</p>\n\n<p><strong>Put the two containers into two separate task definitions using an EC2 Launch Type</strong></p>\n\n<p><strong>Put the two containers into a single task definition using an EC2 Launch Type</strong></p>\n\n<p>As the development team is looking for a serverless data store service, therefore EC2 Launch Type is ruled out. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/\">https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/</a></p>\n",
                "options": [
                    {
                        "id": 2619,
                        "content": "<p>Put the two containers into two separate task definitions using an EC2 Launch Type</p>",
                        "isValid": false
                    },
                    {
                        "id": 2620,
                        "content": "<p>Put the two containers into a single task definition using an EC2 Launch Type</p>",
                        "isValid": false
                    },
                    {
                        "id": 2621,
                        "content": "<p>Put the two containers into two separate task definitions using a Fargate Launch Type</p>",
                        "isValid": false
                    },
                    {
                        "id": 2622,
                        "content": "<p>Put the two containers into a single task definition using a Fargate Launch Type</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 638,
            "attributes": {
                "createdAt": "2023-09-07T08:39:53.944Z",
                "updatedAt": "2023-09-07T08:39:53.944Z",
                "content": "<p>A company stores confidential data on an Amazon Simple Storage Service (S3) bucket. New regulatory guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage S3 encryption keys.</p>\n\n<p>Which of the following options should you use?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>SSE-S3</strong></p>\n\n<p>Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-C</strong> - You manage the encryption keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects.</p>\n\n<p><strong>Client-Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p><strong>SSE-KMS</strong> - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p>\n",
                "options": [
                    {
                        "id": 2623,
                        "content": "<p>Client Side Encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 2624,
                        "content": "<p>SSE-C</p>",
                        "isValid": false
                    },
                    {
                        "id": 2625,
                        "content": "<p>SSE-S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 2626,
                        "content": "<p>SSE-KMS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 639,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.021Z",
                "updatedAt": "2023-09-07T08:39:54.021Z",
                "content": "<p>A company has configured an Auto Scaling group with health checks. The configuration is set to the desired capacity value of 3 and maximum capacity value of 3. The EC2 instances of your Auto Scaling group are configured to scale when CPU utilization is at 60 percent and is now running at 80 percent utilization.</p>\n\n<p>Which of the following will take place?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>System will keep running as is</strong></p>\n\n<p>You are already running at max capacity. After you have created your Auto Scaling group, the Auto Scaling group starts by launching enough EC2 instances to meet its minimum capacity (or its desired capacity, if specified). If there are no other scaling conditions attached to the Auto Scaling group, the Auto Scaling group maintains this number of running instances even if an instance becomes unhealthy.</p>\n\n<p>Setting Capacity Limits for Your Auto Scaling Group:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The desired capacity will go up to 4 and the maximum capacity will stay at 3</strong> - The desired capacity cannot go over the maximum capacity.</p>\n\n<p><strong>The desired capacity will go up to 4 and the maximum capacity will also go up to 4</strong> - The maximum capacity cannot change on its own just because the desired capacity has been set to a higher value. You will have to make those changes to the maximum capacity manually.</p>\n\n<p><strong>System will trigger CloudWatch alarms to AWS support</strong> - This option has been added as a distractor. You already have alarms configured based on rules but AWS support will not intervene for you.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a></p>\n",
                "options": [
                    {
                        "id": 2627,
                        "content": "<p>The desired capacity will go up to 4 and the maximum capacity will stay at 3</p>",
                        "isValid": false
                    },
                    {
                        "id": 2628,
                        "content": "<p>The desired capacity will go up to 4 and the maximum capacity will also go up to 4</p>",
                        "isValid": false
                    },
                    {
                        "id": 2629,
                        "content": "<p>System will keep running as is</p>",
                        "isValid": true
                    },
                    {
                        "id": 2630,
                        "content": "<p>System will trigger CloudWatch alarms to AWS support</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 640,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.094Z",
                "updatedAt": "2023-09-07T08:39:54.094Z",
                "content": "<p>A company uses microservices-based infrastructure to process the API calls from clients, perform request filtering and cache requests using the AWS API Gateway. Users report receiving 501 error code and you have been contacted to find out what is failing.</p>\n\n<p>Which service will you choose to help you troubleshoot?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Use X-Ray service</strong> - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your applicationâ€™s underlying components. You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.</p>\n\n<p>X-Ray Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p>AWS X-Ray creates a map of services used by your application with trace data that you can use to drill into specific services or issues. This provides a view of connections between services in your application and aggregated data for each service, including average latency and failure rates. You can create dependency trees, perform cross-availability zone or region call detections, and more.</p>\n\n<p>X-Ray Service maps:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/xray/features/\">https://aws.amazon.com/xray/features/</a></p>\n\n<p>X-Ray Traces:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/xray/features/\">https://aws.amazon.com/xray/features/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudTrail service</strong> - With CloudTrail, you can get a history of AWS API calls for your account - including API calls made via the AWS Management Console, AWS SDKs, command-line tools, and higher-level AWS services (such as AWS CloudFormation). This is a very useful service for general monitoring and tracking. But, it will not give a detailed analysis of the outcome of microservices or drill into specific issues. For the current use case, X-Ray offers a better solution.</p>\n\n<p><strong>Use API Gateway service</strong> - Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API Gateway will not be able to drill into the flow between different microservices or their issues.</p>\n\n<p><strong>Use CloudWatch service</strong> - Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. CloudWatch can collect numbers and respond to AWS service-related events, but it can't help you debug microservices specific issues on AWS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/features/\">https://aws.amazon.com/cloudwatch/features/</a></p>\n",
                "options": [
                    {
                        "id": 2631,
                        "content": "<p>Use API Gateway service</p>",
                        "isValid": false
                    },
                    {
                        "id": 2632,
                        "content": "<p>Use CloudWatch service</p>",
                        "isValid": false
                    },
                    {
                        "id": 2633,
                        "content": "<p>Use X-Ray service</p>",
                        "isValid": true
                    },
                    {
                        "id": 2634,
                        "content": "<p>Use CloudTrail service</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 641,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.167Z",
                "updatedAt": "2023-09-07T08:39:54.167Z",
                "content": "<p>A development team has noticed that one of the EC2 instances has been wrongly configured with the 'DeleteOnTermination' attribute set to True for its root EBS volume.</p>\n\n<p>As a developer associate, can you suggest a way to disable this flag while the instance is still running?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>When an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume and is set to False for all other volume types.</p>\n\n<p><strong>Set the <code>DeleteOnTermination</code> attribute to False using the command line</strong> - If the instance is already running, you can set <code>DeleteOnTermination</code> to False using the command line.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume</strong> - You can set the <code>DeleteOnTermination</code> attribute to False when you launch a new instance. It is not possible to update this attribute of a running instance from the AWS console.</p>\n\n<p><strong>Set the <code>DisableApiTermination</code> attribute of the instance using the API</strong> - By default, you can terminate your instance using the Amazon EC2 console, command-line interface, or API. To prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination protection for the instance. The <code>DisableApiTermination</code> attribute controls whether the instance can be terminated using the console, CLI, or API. This option cannot be used to control the delete status for the EBS volume when the instance terminates.</p>\n\n<p><strong>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</strong> - This statement is wrong and given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/\">https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance</a></p>\n",
                "options": [
                    {
                        "id": 2635,
                        "content": "<p>Set the <code>DisableApiTermination</code> attribute of the instance using the API</p>",
                        "isValid": false
                    },
                    {
                        "id": 2636,
                        "content": "<p>Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume</p>",
                        "isValid": false
                    },
                    {
                        "id": 2637,
                        "content": "<p>Set the <code>DeleteOnTermination</code> attribute to False using the command line</p>",
                        "isValid": true
                    },
                    {
                        "id": 2638,
                        "content": "<p>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 642,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.239Z",
                "updatedAt": "2023-09-07T08:39:54.239Z",
                "content": "<p>A multi-national company runs its technology operations on AWS Cloud. As part of their storage solution, they use a large number of EBS volumes, with AWS Config and CloudTrail activated. A manager has tried to find the user name that created an EBS volume by searching CloudTrail events logs but wasn't successful.</p>\n\n<p>As a Developer Associate, which of the following would you recommend as the correct solution?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon EC2 launch</strong> - AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon Elastic Compute Cloud (Amazon EC2) launch.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS CloudTrail event logs for 'ManageVolume' aren't available for EBS volumes created during an Amazon EC2 launch</strong> - Event 'ManageVolume' is a made-up option and has been added as a distractor.</p>\n\n<p><strong>Amazon EBS CloudWatch metrics are disabled</strong> - Amazon Elastic Block Store (Amazon EBS) sends data points to CloudWatch for several metrics. Data is only reported to CloudWatch when the volume is attached to an instance. CloudWatch metrics are useful in tracking the status or life cycle changes of an EBS volume, they are not useful in knowing about the metadata of EBS volumes.</p>\n\n<p><strong>EBS volume status checks are disabled</strong> - Volume status checks enable you to better understand, track and manage potential inconsistencies in the data on an Amazon EBS volume. They are designed to provide you with the information that you need to determine whether your Amazon EBS volumes are impaired, and to help you control how a potentially inconsistent volume is handled. Our current use case requires us to pull data about EBS volume metadata, which is not possible with this feature.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/find-ebs-user-config-cloudtrail/\">https://aws.amazon.com/premiumsupport/knowledge-center/find-ebs-user-config-cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cloudwatch_ebs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cloudwatch_ebs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html</a></p>\n",
                "options": [
                    {
                        "id": 2639,
                        "content": "<p>EBS volume status checks are disabled</p>",
                        "isValid": false
                    },
                    {
                        "id": 2640,
                        "content": "<p>Amazon EBS CloudWatch metrics are disabled</p>",
                        "isValid": false
                    },
                    {
                        "id": 2641,
                        "content": "<p>AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon EC2 launch</p>",
                        "isValid": true
                    },
                    {
                        "id": 2642,
                        "content": "<p>AWS CloudTrail event logs for 'ManageVolume' aren't available for EBS volumes created during an Amazon EC2 launch</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 643,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.315Z",
                "updatedAt": "2023-09-07T08:39:54.315Z",
                "content": "<p>A new recruit is trying to understand the nuances of EC2 Auto Scaling. As an AWS Certified Developer Associate, you have been asked to mentor the new recruit.</p>\n\n<p>Can you identify and explain the correct statements about Auto Scaling to the new recruit? (Select two).</p>",
                "answerExplanation": "<p>Correct options:</p>\n\n<p>Amazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application.</p>\n\n<p><strong>Amazon EC2 Auto Scaling cannot add a volume to an existing instance if the existing volume is approaching capacity</strong> - A volume is attached to a new instance when it is added. Amazon EC2 Auto Scaling doesn't automatically add a volume when the existing one is approaching capacity. You can use the EC2 API to add a volume to an existing instance.</p>\n\n<p><strong>Amazon EC2 Auto Scaling works with both Application Load Balancers and Network Load Balancers</strong> - Amazon EC2 Auto Scaling works with Application Load Balancers and Network Load Balancers including their health check feature.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EC2 Auto Scaling groups are regional constructs. They span across Availability Zones and AWS regions</strong> - This is an incorrect statement. EC2 Auto Scaling groups are regional constructs. They can span Availability Zones, but not AWS regions.</p>\n\n<p><strong>Every time you create an Auto Scaling group from an existing instance, it creates a new AMI (Amazon Machine Image)</strong> - This is an incorrect statement. When you create an Auto Scaling group from an existing instance, it does not create a new AMI.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q17-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html</a></p>\n\n<p><strong>You cannot use Amazon EC2 Auto Scaling for health checks (to replace unhealthy instances) if you are not using Elastic Load Balancing (ELB)</strong> - This is an incorrect statement. You don't have to use ELB to use Auto Scaling. You can use the EC2 health check to identify and replace unhealthy instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/autoscaling/faqs/\">https://aws.amazon.com/ec2/autoscaling/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html</a></p>\n",
                "options": [
                    {
                        "id": 2643,
                        "content": "<p>Amazon EC2 Auto Scaling works with both Application Load Balancers and Network Load Balancers</p>",
                        "isValid": true
                    },
                    {
                        "id": 2644,
                        "content": "<p>Every time you create an Auto Scaling group from an existing instance, it creates a new AMI (Amazon Machine Image)</p>",
                        "isValid": false
                    },
                    {
                        "id": 2645,
                        "content": "<p>You cannot use Amazon EC2 Auto Scaling for health checks (to replace unhealthy instances) if you are not using Elastic Load Balancing (ELB)</p>",
                        "isValid": false
                    },
                    {
                        "id": 2646,
                        "content": "<p>EC2 Auto Scaling groups are regional constructs. They span across Availability Zones and AWS regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 2647,
                        "content": "<p>Amazon EC2 Auto Scaling cannot add a volume to an existing instance if the existing volume is approaching capacity</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 644,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.381Z",
                "updatedAt": "2023-09-07T08:39:54.381Z",
                "content": "<p>You are a DynamoDB developer for an aerospace company that requires you to write 6 objects per second of 4.5KB in size each.</p>\n\n<p>What write capacity unit is needed for your project?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>Before proceeding with the calculations, please review the following:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q58-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q58-i2.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p><strong>30</strong></p>\n\n<p>A write capacity unit represents one write per second, for an item up to 1 KB in size.</p>\n\n<p>Item sizes for writes are rounded up to the next 1 KB multiple. For example, writing a 500-byte item consumes the same throughput as writing a 1 KB item. So, for the given use-case, each object is of size 4.5 KB, which will be rounded up to 5KB.</p>\n\n<p>Therefore, for 6 objects, you need 6x5 = 30 WCUs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>24</strong></p>\n\n<p><strong>15</strong></p>\n\n<p><strong>46</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n",
                "options": [
                    {
                        "id": 2648,
                        "content": "<p>24</p>",
                        "isValid": false
                    },
                    {
                        "id": 2649,
                        "content": "<p>46</p>",
                        "isValid": false
                    },
                    {
                        "id": 2650,
                        "content": "<p>30</p>",
                        "isValid": true
                    },
                    {
                        "id": 2651,
                        "content": "<p>15</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 645,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.452Z",
                "updatedAt": "2023-09-07T08:39:54.452Z",
                "content": "<p>A company wants to implement authentication for its new RESTful API service that uses Amazon API Gateway. To authenticate the calls, each request must include HTTP headers with a client ID and user ID. These credentials must be compared to the authentication data in a DynamoDB table.</p>\n\n<p>As an AWS Certified Developer Associate, which of the following would you recommend for implementing this authentication in API Gateway? </p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Develop an AWS Lambda authorizer that references the DynamoDB authentication table</strong> - A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API.</p>\n\n<p>A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity.</p>\n\n<p>When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output.</p>\n\n<p>There are two types of Lambda authorizers:</p>\n\n<ol>\n<li><p>A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.</p></li>\n<li><p>A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, state variables, and $context variables.</p></li>\n</ol>\n\n<p>API Gateway Lambda authorization workflow:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q55-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an API Gateway Model that requires the credentials, then grant API Gateway access to the authentication table in DynamoDB</strong> - In API Gateway, a model defines the data structure of a payload. In API Gateway models are defined using the JSON schema draft 4. Models are not mandatory.</p>\n\n<p><strong>Update the API Gateway integration requests to require the credentials, then grant API Gateway access to the authentication table in DynamoDB</strong> - After setting up an API method, you must integrate it with an endpoint in the backend. A backend endpoint is also referred to as an integration endpoint and can be a Lambda function, an HTTP webpage, or an AWS service action.</p>\n\n<p>An integration request is an HTTP request that API Gateway submits to the backend, passing along the client-submitted request data, and transforming the data, if necessary. The HTTP method (or verb) and URI of the integration request are dictated by the backend (that is, the integration endpoint). They can be the same as or different from the method request's HTTP method and URI, respectively.</p>\n\n<p><strong>Authorize using Amazon Cognito that will reference the authentication table of DynamoDB</strong> - As an alternative to using IAM roles and policies or Lambda authorizers (formerly known as custom authorizers), you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway.</p>\n\n<p>To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user in to the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-integration-settings.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-integration-settings.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html</a></p>\n",
                "options": [
                    {
                        "id": 2652,
                        "content": "<p>Develop an AWS Lambda authorizer that references the authentication data in the DynamoDB table</p>",
                        "isValid": true
                    },
                    {
                        "id": 2653,
                        "content": "<p>Update the API Gateway integration requests to require the credentials, then grant API Gateway access to the authentication table in DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 2654,
                        "content": "<p>Authorize using Amazon Cognito that will reference the authentication table of DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 2655,
                        "content": "<p>Set up an API Gateway Model that requires the credentials, then grant API Gateway access to the authentication table in DynamoDB</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 646,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.521Z",
                "updatedAt": "2023-09-07T08:39:54.521Z",
                "content": "<p>The development team at an IT company uses CloudFormation to manage its AWS infrastructure. The team has created a network stack containing a VPC with subnets and a web application stack with EC2 instances and an RDS instance. The team wants to reference the VPC created in the network stack into its web application stack.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</strong></p>\n\n<p>AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p>You can create a cross-stack reference to export resources from one AWS CloudFormation stack to another. For example, you might have a network stack with a VPC and subnets and a separate public web application stack. To use the security group and subnet from the network stack, you can create a cross-stack reference that allows the web application stack to reference resource outputs from the network stack. With a cross-stack reference, owners of the web application stacks don't need to create or maintain networking rules or assets.</p>\n\n<p>To create a cross-stack reference, use the Export output field to flag the value of a resource output for export. Then, use the Fn::ImportValue intrinsic function to import the value.</p>\n\n<p>You cannot use the Ref intrinsic function to import the value.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</strong></p>\n\n<p><strong>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</strong></p>\n\n<p><strong>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these options are not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html</a></p>\n",
                "options": [
                    {
                        "id": 2656,
                        "content": "<p>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</p>",
                        "isValid": false
                    },
                    {
                        "id": 2657,
                        "content": "<p>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</p>",
                        "isValid": false
                    },
                    {
                        "id": 2658,
                        "content": "<p>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</p>",
                        "isValid": true
                    },
                    {
                        "id": 2659,
                        "content": "<p>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 647,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.593Z",
                "updatedAt": "2023-09-07T08:39:54.593Z",
                "content": "<p>You're a developer maintaining a web application written in .NET. The application makes references to public objects in a public S3 accessible bucket using a public URL. While doing a code review your colleague advises that the approach is not a best practice because some of the objects contain private data. After the administrator makes the S3 bucket private you can no longer access the S3 objects but you would like to create an application that will enable people to access some objects as needed with a time policy constraint.</p>\n\n<p>Which of the following options will give access to the objects?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>\"Using pre-signed URL\"</p>\n\n<p>All objects by default are private, with object owner having permission to access the objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The pre-signed URLs are valid only for the specified duration.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q36-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n\n<p>Incorrect:</p>\n\n<p>\"Using bucket policy\" - You can use this policy to limit users from a source IP address however for time-based constraints you are better off using a pre-signed URL.</p>\n\n<p>\"Using Routing Policy\" - This concept applies to DNS in Route 53, so this option is ruled out.</p>\n\n<p>\"Using IAM policy\" - You can use IAM policy to grant access toa specific bucket however for time-based constraints you are better off using a pre-signed URL.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n",
                "options": [
                    {
                        "id": 2660,
                        "content": "<p>Using IAM policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 2661,
                        "content": "<p>Using pre-signed URL</p>",
                        "isValid": true
                    },
                    {
                        "id": 2662,
                        "content": "<p>Using Routing Policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 2663,
                        "content": "<p>Using bucket policy</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 648,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.662Z",
                "updatedAt": "2023-09-07T08:39:54.662Z",
                "content": "<p>Your web application front end consists of 5 EC2 instances behind an Application Load Balancer. You have configured your web application to capture the IP address of the client making requests. When viewing the data captured you notice that every IP address being captured is the same, which also happens to be the IP address of the Application Load Balancer.</p>\n\n<p>What should you do to identify the true IP address of the client?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Look into the X-Forwarded-For header in the backend</strong></p>\n\n<p>The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Modify the front-end of the website so that the users send their IP in the requests</strong> - When a user makes a request the IP address is sent with the request to the server and the load balancer intercepts it. There is no need to modify the application.</p>\n\n<p><strong>Look into the X-Forwarded-Proto header in the backend</strong> - The X-Forwarded-Proto request header helps you identify the protocol (HTTP or HTTPS) that a client used to connect to your load balancer.</p>\n\n<p><strong>Look into the client's cookie</strong> - For this, we would need to modify the client-side logic and server-side logic, which would not be efficient.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html</a></p>\n",
                "options": [
                    {
                        "id": 2664,
                        "content": "<p>Look into the client's cookie</p>",
                        "isValid": false
                    },
                    {
                        "id": 2665,
                        "content": "<p>Modify the front-end of the website so that the users send their IP in the requests</p>",
                        "isValid": false
                    },
                    {
                        "id": 2666,
                        "content": "<p>Look into the X-Forwarded-Proto header in the backend</p>",
                        "isValid": false
                    },
                    {
                        "id": 2667,
                        "content": "<p>Look into the X-Forwarded-For header in the backend</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 649,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.735Z",
                "updatedAt": "2023-09-07T08:39:54.735Z",
                "content": "<p>Your team has just signed up an year-long contract with a client maintaining a three-tier web application, that needs to be moved to AWS Cloud. The application has steady traffic throughout the day and needs to be on a reliable system with no down-time or access issues. The solution needs to be cost-optimal for this startup.</p>\n\n<p>Which of the following options should you choose?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EC2 Reserved Instances</strong> - Reserved instances can provide a capacity reservation, offering additional confidence in your ability to launch the number of instances you have reserved when you need them. You save money going with Reserved instances vs on-demand especially in a year's worth of time.</p>\n\n<p>Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account. These On-Demand Instances must match certain attributes, such as instance type and Region, to benefit from the billing discount. So, there is no performance difference between an On-Demand instance or a Reserved instance.</p>\n\n<p>How RIs work:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q60-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon EC2 Spot Instances</strong> - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot instances are useful if your applications can be interrupted, like data analysis, batch jobs, background processing, and optional tasks. Spot instances can be pulled down anytime without prior notice. Hence, not the right choice for the current scenario.</p>\n\n<p><strong>Amazon EC2 On-Demand Instances</strong> - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycleâ€”you decide when to launch, stop, hibernate, start, reboot, or terminate it. But, On-Demand instances cost a lot more than Reserved instances. Here, in our use case, we already know that the systems are required for a complete year, so making use of Reserved Instances discount makes a lot more sense.</p>\n\n<p><strong>On-premise EC2 instance</strong> - On-premise implies the client has to maintain the physical machines, their capacity provisioning and maintenance. Not an option when the client is planning to move to AWS Cloud.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/pricing/reserved-instances/\">https://aws.amazon.com/ec2/pricing/reserved-instances/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html</a></p>\n",
                "options": [
                    {
                        "id": 2668,
                        "content": "<p>Amazon EC2 Reserved Instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 2669,
                        "content": "<p>On-premise EC2 instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 2670,
                        "content": "<p>Amazon EC2 Spot Instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 2671,
                        "content": "<p>Amazon EC2 On Demand Instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 650,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.804Z",
                "updatedAt": "2023-09-07T08:39:54.804Z",
                "content": "<p>A developer has just completed configuring the Application Load Balancer for the EC2 instances. Just as he started testing his configuration, he realized that he has missed assigning target groups to his ALB.</p>\n\n<p>Which error code should he expect in his debug logs?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>HTTP 503</strong> - HTTP 503 indicates 'Service unavailable' error. This error in ALB is an indicator of the target groups for the load balancer having no registered targets.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>HTTP 500</strong> - HTTP 500 indicates 'Internal server' error. There are several reasons for their error: A client submitted a request without an HTTP protocol, and the load balancer was unable to generate a redirect URL, there was an error executing the web ACL rules.</p>\n\n<p><strong>HTTP 504</strong> - HTTP 504 is 'Gateway timeout' error. Several reasons for this error, to quote a few: The load balancer failed to establish a connection to the target before the connection timeout expired, The load balancer established a connection to the target but the target did not respond before the idle timeout period elapsed.</p>\n\n<p><strong>HTTP 403</strong> - HTTP 403 is 'Forbidden' error. You configured an AWS WAF web access control list (web ACL) to monitor requests to your Application Load Balancer and it blocked a request.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html</a></p>\n",
                "options": [
                    {
                        "id": 2672,
                        "content": "<p>HTTP 504</p>",
                        "isValid": false
                    },
                    {
                        "id": 2673,
                        "content": "<p>HTTP 500</p>",
                        "isValid": false
                    },
                    {
                        "id": 2674,
                        "content": "<p>HTTP 403</p>",
                        "isValid": false
                    },
                    {
                        "id": 2675,
                        "content": "<p>HTTP 503</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 651,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.877Z",
                "updatedAt": "2023-09-07T08:39:54.877Z",
                "content": "<p>You're a developer for 'Movie Gallery', a company that just migrated to the cloud. A database must be created using NoSQL technology to hold movies that are listed for public viewing. You are taking an important step in designing the database with DynamoDB and need to choose the appropriate partition key.</p>\n\n<p>Which of the following unique attributes satisfies this requirement?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p>DynamoDB stores data as groups of attributes, known as items. Items are similar to rows or records in other database systems. DynamoDB stores and retrieves each item based on the primary key value, which must be unique. Items are distributed across 10-GB storage units, called partitions (physical storage internal to DynamoDB).</p>\n\n<p>DynamoDB uses the partition keyâ€™s value as an input to an internal hash function. The output from the hash function determines the partition in which the item is stored. Each itemâ€™s location is determined by the hash value of its partition key.</p>\n\n<p>Please see these details for the DynamoDB Partition Keys:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q11-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q11-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a></p>\n\n<p><strong><code>movie_id</code></strong></p>\n\n<p>The <code>movie_id</code> attribute has high-cardinality across the entire collection of the movie database, hence it is the most suitable candidate for the partition key in this use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>producer_name</code></strong>  - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)</p>\n\n<p><strong><code>lead_actor_name</code></strong> - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)</p>\n\n<p><strong><code>movie_language</code></strong> - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a></p>\n",
                "options": [
                    {
                        "id": 2676,
                        "content": "<p><code>movie_id</code></p>",
                        "isValid": true
                    },
                    {
                        "id": 2677,
                        "content": "<p><code>lead_actor_name</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2678,
                        "content": "<p><code>movie_language</code></p>",
                        "isValid": false
                    },
                    {
                        "id": 2679,
                        "content": "<p><code>producer_name</code></p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 652,
            "attributes": {
                "createdAt": "2023-09-07T08:39:54.944Z",
                "updatedAt": "2023-09-07T08:39:54.944Z",
                "content": "<p>The development team at a retail organization wants to allow a Lambda function in its AWS Account A to access a DynamoDB table in another AWS Account B.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>",
                "answerExplanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM role in account B with access to DynamoDB. Modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Update the Lambda function code to add the AssumeRole API call</strong></p>\n\n<p>You can give a Lambda function created in one account (\"account A\") permissions to assume a role from another account (\"account B\") to access resources such as DynamoDB or S3 bucket. You need to create an execution role in Account A that gives the Lambda function permission to do its work. Then you need to create a role in account B that the Lambda function in account A assumes to gain access to the cross-account DynamoDB table. Make sure that you modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Finally, update the Lambda function code to add the AssumeRole API call.</p>\n\n<p>Sample use-case to configure a Lambda function to assume a role from another AWS account:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q1-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a clone of the Lambda function in AWS Account B so that it can access the DynamoDB table in the same account</strong> - Creating a clone of the Lambda function is a distractor as this does not solve the use-case outlined in the problem statement.</p>\n\n<p><strong>Add a resource policy to the DynamoDB table in AWS Account B to give access to the Lambda function in Account A</strong> - You cannot attach a resource policy to a DynamoDB table, so this option is incorrect.</p>\n\n<p><strong>Create an IAM role in Account B with access to DynamoDB. Modify the trust policy of the execution role in Account A to allow the execution role of Lambda to assume the IAM role in Account B. Update the Lambda function code to add the AssumeRole API call</strong> - As mentioned in the explanation above, you need to modify the trust policy of the IAM role in Account B so that it allows the execution role of Lambda function in account A to assume the IAM role in Account B.</p>\n\n<p>Reference:\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/</a></p>\n",
                "options": [
                    {
                        "id": 2680,
                        "content": "<p>Create an IAM role in Account B with access to DynamoDB. Modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Update the Lambda function code to add the AssumeRole API call</p>",
                        "isValid": true
                    },
                    {
                        "id": 2681,
                        "content": "<p>Add a resource policy to the DynamoDB table in AWS Account B to give access to the Lambda function in Account A</p>",
                        "isValid": false
                    },
                    {
                        "id": 2682,
                        "content": "<p>Create a clone of the Lambda function in AWS Account B so that it can access the DynamoDB table in the same account</p>",
                        "isValid": false
                    },
                    {
                        "id": 2683,
                        "content": "<p>Create an IAM role in Account B with access to DynamoDB. Modify the trust policy of the execution role in Account A to allow the execution role of Lambda to assume the IAM role in Account B. Update the Lambda function code to add the AssumeRole API call</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 653,
            "attributes": {
                "createdAt": "2023-09-07T08:51:17.608Z",
                "updatedAt": "2023-09-07T08:51:17.608Z",
                "content": "<p>An application uses an Amazon RDS database. The company requires that the performance of database reads is improved, and they want to add a caching layer in front of the database. The cached data must be encrypted, and the solution must be highly available.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>Amazon ElastiCache is an in-memory database cache that can be used in front of Amazon RDS. The key to answering this question is to know the differences between ElastiCache Memcached and ElastiCache Redis. To support both encryption and high availability we must use ElastiCache Redis with cluster mode enabled.</p><p>You can see the differences between the different engines and configuration options for ElastiCache in the table below:</p><p><br></p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-29_11-24-04-6f7ba820e70f1c88c4d37dbf4a94517b.jpg\"><p><strong>CORRECT: </strong>\"Amazon ElastiCache for Redis in cluster mode\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Amazon ElastiCache for Memcached\" is incorrect.</p><p>The Memcached engine does not support encryption or high availability.</p><p><strong>INCORRECT:</strong> \"Amazon CloudFront with multiple origins\" is incorrect.</p><p>You cannot configure an Amazon RDS as an origin for Amazon RDS. Also, what would the second origin be anyway? Thereâ€™s only one database!</p><p><strong>INCORRECT:</strong> \"Amazon DynamoDB Accelerator (DAX)\" is incorrect.</p><p>DynamoDB DAX can be used to increase the performance of DynamoDB tables and offload read requests. It cannot be used in front of an Amazon RDS database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis-RedisCluster.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis-RedisCluster.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 2684,
                        "content": "<p>Amazon ElastiCache for Memcached.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2685,
                        "content": "<p>Amazon CloudFront with multiple origins.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2686,
                        "content": "<p>Amazon DynamoDB Accelerator (DAX).</p>",
                        "isValid": false
                    },
                    {
                        "id": 2687,
                        "content": "<p>Amazon ElastiCache for Redis in cluster mode.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 654,
            "attributes": {
                "createdAt": "2023-09-07T08:51:17.688Z",
                "updatedAt": "2023-09-07T08:51:17.688Z",
                "content": "<p>An application deployed on AWS Elastic Beanstalk experienced increased error rates during deployments of new application versions, resulting in service degradation for users. The Development team believes that this is because of the reduction in capacity during the deployment steps. The team would like to change the deployment policy configuration of the environment to an option that maintains full capacity during deployment while using the existing instances.</p><p>Which deployment policy will meet these requirements while using the existing instances?</p>",
                "answerExplanation": "<p>AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.</p><p>All at once:</p><p>Â· Deploys the new version to all instances simultaneously.</p><p>Rolling:</p><p>Â· Update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy (downtime for 1 bucket at a time).</p><p>Rolling with additional batch:</p><p>Â· Like Rolling but launches new instances in a batch ensuring that there is full availability.</p><p>Immutable:</p><p>Â· Launches new instances in a new ASG and deploys the version update to these instances before swapping traffic to these instances once healthy.</p><p>Â· Zero downtime.</p><p>Blue / Green deployment:</p><p>Â· Zero downtime and release facility.</p><p>Â· Create a new â€œstageâ€ environment and deploy updates there.</p><p>The rolling with additional batch launches a new batch to ensure capacity is not reduced and then updates the existing instances. Therefore, this is the best option to use for these requirements.</p><p><strong>CORRECT: </strong>â€œRolling with additional batchâ€ is the correct answer.</p><p><strong>INCORRECT:</strong> â€œRollingâ€ is incorrect as this will only use the existing instances without introducing an extra batch and therefore this will reduce the capacity of the application while the updates are taking place.</p><p><strong>INCORRECT:</strong> â€œAll at onceâ€ is incorrect as this will run the updates on all instances at the same time causing a total outage.</p><p><strong>INCORRECT:</strong> â€œImmutableâ€ is incorrect as this installs the updates on new instances, not existing instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
                "options": [
                    {
                        "id": 2688,
                        "content": "<p>All at once</p>",
                        "isValid": false
                    },
                    {
                        "id": 2689,
                        "content": "<p>Rolling</p>",
                        "isValid": false
                    },
                    {
                        "id": 2690,
                        "content": "<p>Rolling with additional batch</p>",
                        "isValid": true
                    },
                    {
                        "id": 2691,
                        "content": "<p>Immutable</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 655,
            "attributes": {
                "createdAt": "2023-09-07T08:51:17.772Z",
                "updatedAt": "2023-09-07T08:51:17.772Z",
                "content": "<p>A company is using an AWS Step Functions state machine. When testing the state machine errors were experienced in the Step Functions task state machine. To troubleshoot the issue a developer requires that the state input be included along with the error message in the state output.</p><p>Which coding practice can preserve both the original input and the error for the state?</p>",
                "answerExplanation": "<p>A Step Functions execution receives a JSON text as input and passes that input to the first state in the workflow. Individual states receive JSON as input and usually pass JSON as output to the next state.</p><p>In the Amazon States Language, these fields filter and control the flow of JSON from state to state:</p><p> â€¢ InputPath</p><p> â€¢ OutputPath</p><p> â€¢ ResultPath</p><p> â€¢ Parameters</p><p> â€¢ ResultSelector</p><p>Use ResultPath to combine a task result with task input, or to select one of these. The path you provide to ResultPath controls what information passes to the output. Use ResultPath in a Catch to include the error with the original input, instead of replacing it. The following code is an example of this tactic:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-30_04-15-24-31b57043970155ff3ce455619308a672.jpg\"></p><p><strong>CORRECT: </strong>\"Use ResultPath in a Catch statement to include the original input with the error\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use InputPath in a Catch statement to include the original input with the error\" is incorrect.</p><p>You can use InputPath to select a portion of the state input.</p><p><strong>INCORRECT:</strong> \"Use ErrorEquals in a Retry statement to include the original input with the error\" is incorrect.</p><p>A retry is used to attempt to retry the process that caused the error based on the retry policy described by ErrorEquals.</p><p><strong>INCORRECT:</strong> \"Use OutputPath in a Retry statement to include the original input with the error\" is incorrect.</p><p>OutputPath enables you to select a portion of the state output to pass to the next state. This enables you to filter out unwanted information and pass only the portion of JSON that you care about.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/input-output-resultpath.html\">https://docs.aws.amazon.com/step-functions/latest/dg/input-output-resultpath.html</a></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-handling-error-conditions.html\">https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-handling-error-conditions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 2692,
                        "content": "<p>Use OutputPath in a Retry statement to include the original input with the error.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2693,
                        "content": "<p>Use ResultPath in a Catch statement to include the original input with the error.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2694,
                        "content": "<p>Use InputPath in a Catch statement to include the original input with the error.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2695,
                        "content": "<p>Use ErrorEquals in a Retry statement to include the original input with the error.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 656,
            "attributes": {
                "createdAt": "2023-09-07T08:51:17.852Z",
                "updatedAt": "2023-09-07T08:51:17.852Z",
                "content": "<p>A Developer will be launching several Docker containers on a new Amazon ECS cluster using the EC2 Launch Type. The containers will all run a web service on port 80.</p><p>What is the EASIEST way the Developer can configure the task definition to ensure the web services run correctly and there are no port conflicts on the host instances?</p>",
                "answerExplanation": "<p>Port mappings allow containers to access ports on the host container instance to send or receive traffic. Port mappings are specified as part of the container definition. The container port is the port number on the container that is bound to the user-specified or automatically assigned host port. The host port is the port number on the container instance to reserve for your container.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-14-21-2e05425ee95cbc437d856fb9b53d6aa2.jpg\"></p><p>As we cannot have multiple services bound to the same host port, we need to ensure that each container port mapping uses a different host port. The easiest way to do this is to set the host port number to 0 and ECS will automatically assign an available port. We also need to assign port 80 to the container port so that the web service is able to run.</p><p><strong>CORRECT: </strong>\"Specify port 80 for the container port and port 0 for the host port\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Specify port 80 for the container port and a unique port number for the host port\" is incorrect as this is more difficult to manage as you have to manually assign the port number.</p><p><strong>INCORRECT:</strong> \"Specify a unique port number for the container port and port 80 for the host port\" is incorrect as the web service on the container needs to run on pot 80 and you can only bind one container to port 80 on the host so this would not allow more than one container to work.</p><p><strong>INCORRECT:</strong> \"Leave both the container port and host port configuration blank\" is incorrect as this would mean that ECS would dynamically assign both the container and host port. As the web service must run on port 80 this would not work correctly.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html\">https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 2696,
                        "content": "<p>Specify port 80 for the container port and a unique port number for the host port</p>",
                        "isValid": false
                    },
                    {
                        "id": 2697,
                        "content": "<p>Specify a unique port number for the container port and port 80 for the host port</p>",
                        "isValid": false
                    },
                    {
                        "id": 2698,
                        "content": "<p>Leave both the container port and host port configuration blank</p>",
                        "isValid": false
                    },
                    {
                        "id": 2699,
                        "content": "<p>Specify port 80 for the container port and port 0 for the host port</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 657,
            "attributes": {
                "createdAt": "2023-09-07T08:51:17.928Z",
                "updatedAt": "2023-09-07T08:51:17.928Z",
                "content": "<p>An organization is launching a new service that will use an IoT device. How can secure communication protocols be established over the internet to ensure the security of the IoT devices during the launch?</p>",
                "answerExplanation": "<p>AWS Certificate Manager (ACM) is used to provision X.509 certificates for TLS/SSL secured communications. It can be used to create certificates for use with many AWS services and applications. It is compatible with IoT devices and applications such as IoT Core and IoT Greengrass.</p><p><strong>CORRECT: </strong>\"Use AWS Certificate Manager (ACM) to provide TLS secured communications to IoT devices and deploy X.509 certificates in the IoT environment\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Private Certificate Authority (CA) to provide TLS secured communications to the IoT devices and deploy X.509 certificates in the IoT environment\" is incorrect. AWS Private Certificate Authority cannot be used over the internet.</p><p><strong>INCORRECT:</strong> \"Use IoT Greengrass to enable TLS secured communications to AWS from the IoT devices by issuing X.509 certificates\" is incorrect. AWS IoT Greengrass is not a certificate authority.</p><p><strong>INCORRECT:</strong> \"Use IoT Core to provide TLS secured communications to AWS from the IoT devices by issuing X.509 certificates\" is incorrect. AWS IoT Core is not a certificate authority.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/iot/latest/developerguide/device-certs-your-own.html\">https://docs.aws.amazon.com/iot/latest/developerguide/device-certs-your-own.html</a></p>",
                "options": [
                    {
                        "id": 2700,
                        "content": "<p>Use AWS Private Certificate Authority (CA) to provide TLS secured communications to the IoT devices and deploy X.509 certificates in the IoT environment.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2701,
                        "content": "<p>Use IoT Greengrass to enable TLS secured communications to AWS from the IoT devices by issuing X.509 certificates.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2702,
                        "content": "<p>Use AWS Certificate Manager (ACM) to provide TLS secured communications to IoT devices and deploy X.509 certificates in the IoT environment.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2703,
                        "content": "<p>Use IoT Core to provide TLS secured communications to AWS from the IoT devices by issuing X.509 certificates.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 658,
            "attributes": {
                "createdAt": "2023-09-07T08:51:18.006Z",
                "updatedAt": "2023-09-07T08:51:18.006Z",
                "content": "<p>A review of Amazon CloudWatch metrics shows that there are a high number of reads taking place on a primary database built on Amazon Aurora with MySQL. What can a developer do to improve the read scaling of the database? (Select TWO.)</p>",
                "answerExplanation": "<p>Aurora Replicas can help improve read scaling because it synchronously updates data with the primary database (within 100 ms). Aurora Replicas are created in the same DB cluster within a Region. With Aurora MySQL you can also enable binlog replication to another Aurora DB cluster which can be in the same or a different Region.</p><p><strong>CORRECT: </strong>\"Create Aurora Replicas in same cluster as the primary database instance\" is the correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create a separate Aurora MySQL cluster and configure binlog replication\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a duplicate Aurora database cluster to process read requests\" is incorrect. A duplicate Aurora database cluster would be a separate database with read and write capability and would not help with read scaling.</p><p><strong>INCORRECT:</strong> \"Create a duplicate Aurora primary database to process read requests\" is incorrect. A duplicate Aurora primary database would be for read and write requests and would not help with read scaling.</p><p><strong>INCORRECT:</strong> \"Creating read replicas of Aurora in a S3 global bucket as the primary read source\" is incorrect. S3 is an object storage service. It cannot be used to host databases.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html#Aurora.Managing.Performance.ReadScaling\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html#Aurora.Managing.Performance.ReadScaling</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
                "options": [
                    {
                        "id": 2704,
                        "content": "<p>Create Aurora Replicas in same cluster as the primary database instance.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2705,
                        "content": "<p>Create Aurora Replicas in a global S3 bucket as the primary read source.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2706,
                        "content": "<p>Create a duplicate Aurora database cluster to process read requests.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2707,
                        "content": "<p>Create a duplicate Aurora primary database to process read requests.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2708,
                        "content": "<p>Create a separate Aurora MySQL cluster and configure binlog replication.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 659,
            "attributes": {
                "createdAt": "2023-09-07T08:51:18.085Z",
                "updatedAt": "2023-09-07T08:51:18.085Z",
                "content": "<p>A developer is setting up the primary key for an Amazon DynamoDB table that logs a company's purchases from various suppliers. Each transaction is recorded with these attributes: supplierId, transactionTime, item, and unitCost.</p><p>Which primary key configuration will be valid in this case?</p>",
                "answerExplanation": "<p>A composite primary key with supplierId as the partition key and transactionTime as the sort key ensures each entry is unique. It is extremely unlikely for the same supplier to make two separate deliveries at the exact same millisecond.</p><p><strong>CORRECT: </strong>\"A composite primary key with supplierId as the partition key and transactionTime as the sort key\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"A simple primary key with supplierId serving as the partition key\" is incorrect.</p><p>This simple primary key (only partition key) will not ensure uniqueness because multiple purchases can be made from the same supplier.</p><p><strong>INCORRECT:</strong> \"A composite primary key with supplierId as the partition key and item as the sort key\" is incorrect.</p><p>Using supplierId and item as a composite key would not guarantee uniqueness, as a supplier can deliver the same item multiple times.</p><p><strong>INCORRECT:</strong> \"A composite primary key with supplierId as the partition key and unitCost as the sort key\" is incorrect.</p><p>A composite key using supplierId and unitCost would not ensure uniqueness either, as the same supplier can supply multiple items with the same unit cost.</p><p><strong>References:</strong></p><p><a href=\"https://repost.aws/knowledge-center/dynamodb-create-composite-key\">https://repost.aws/knowledge-center/dynamodb-create-composite-key</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 2709,
                        "content": "<p>A composite primary key with supplierId as the partition key and transactionTime as the sort key.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2710,
                        "content": "<p>A composite primary key with supplierId as the partition key and unitCost as the sort key.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2711,
                        "content": "<p>A composite primary key with supplierId as the partition key and item as the sort key.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2712,
                        "content": "<p>A simple primary key with supplierId serving as the partition key.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 660,
            "attributes": {
                "createdAt": "2023-09-07T08:51:18.157Z",
                "updatedAt": "2023-09-07T08:51:18.157Z",
                "content": "<p>A company has an application that logs all information to Amazon S3. Whenever there is a new log file, an AWS Lambda function is invoked to process the log files. The code works, gathering all of the necessary information. However, when checking the Lambda function logs, duplicate entries with the same request ID are found.</p><p>What is the BEST explanation for the duplicate entries?</p>",
                "answerExplanation": "<p>From the AWS documentation:</p><p>â€œWhen an error occurs, your function may be invoked multiple times. Retry behavior varies by error type, client, event source, and invocation type. For example, if you invoke a function asynchronously and it returns an error, Lambda executes the function up to two more times. For more information, see Retry Behavior.</p><p>For asynchronous invocation, Lambda adds events to a queue before sending them to your function. If your function does not have enough capacity to keep up with the queue, events may be lost. Occasionally, your function may receive the same event multiple times, even if no error occurs. To retain events that were not processed, configure your function with a dead-letter queue.â€</p><p>Therefore, the most likely explanation is that the function failed, and Lambda retried the invocation.</p><p><strong>CORRECT: </strong>\"The Lambda function failed, and the Lambda service retried the invocation with a delay\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The S3 bucket name was specified incorrectly\" is incorrect. If this was the case all attempts would fail but this is not the case.</p><p><strong>INCORRECT:</strong> \"There was an S3 outage, which caused duplicate entries of the same log file\" is incorrect. There cannot be duplicate log files in Amazon S3 as every object must be unique within a bucket. Therefore, if the same log file was uploaded twice it would just overwrite the previous version of the file. Also, if a separate request was made to Lambda it would have a different request ID.</p><p><strong>INCORRECT:</strong> \"The application stopped intermittently and then resumed\" is incorrect. The issue is duplicate entries of the same request ID.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html\">https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 2713,
                        "content": "<p>The S3 bucket name was specified incorrectly</p>",
                        "isValid": false
                    },
                    {
                        "id": 2714,
                        "content": "<p>The application stopped intermittently and then resumed</p>",
                        "isValid": false
                    },
                    {
                        "id": 2715,
                        "content": "<p>The Lambda function failed, and the Lambda service retried the invocation with a delay</p>",
                        "isValid": true
                    },
                    {
                        "id": 2716,
                        "content": "<p>There was an S3 outage, which caused duplicate entries of the same log file</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 661,
            "attributes": {
                "createdAt": "2023-09-07T08:51:18.230Z",
                "updatedAt": "2023-09-07T08:51:18.230Z",
                "content": "<p>A Developer has used a third-party tool to build, bundle, and package a software package on-premises. The software package is stored in a local file system and must be deployed to Amazon EC2 instances.</p><p>How can the application be deployed onto the EC2 instances?</p>",
                "answerExplanation": "<p>AWS CodeDeploy can deploy software packages using an archive that has been uploaded to an Amazon S3 bucket. The archive file will typically be a .zip file containing the code and files required to deploy the software package.</p><p><strong>CORRECT: </strong>\"Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS CodeDeploy and point it to the local file system to deploy the software package\" is incorrect. You cannot point CodeDeploy to a local file system running on-premises.</p><p><strong>INCORRECT:</strong> \"Create a repository using AWS CodeCommit to automatically trigger a deployment to the EC2 instances\" is incorrect. CodeCommit is a source control system. In this case the source code has already been package using a third-party tool.</p><p><strong>INCORRECT:</strong> \"Use AWS CodeBuild to commit the package and automatically deploy the software package\" is incorrect. CodeBuild does not commit packages (CodeCommit does) or deploy the software. It is a build service.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/tutorials-windows-upload-application.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/tutorials-windows-upload-application.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 2717,
                        "content": "<p>Use AWS CodeDeploy and point it to the local file system to deploy the software package.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2718,
                        "content": "<p>Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2719,
                        "content": "<p>Use AWS CodeBuild to commit the package and automatically deploy the software package.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2720,
                        "content": "<p>Create a repository using AWS CodeCommit to automatically trigger a deployment to the EC2 instances.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 662,
            "attributes": {
                "createdAt": "2023-09-07T08:51:18.318Z",
                "updatedAt": "2023-09-07T08:51:18.318Z",
                "content": "<p>A Developer has created a task definition that includes the following JSON code:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"placementStrategy\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L1\"><span class=\"pun\">{</span></li><li class=\"L2\"><span class=\"str\">\"field\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"attribute:ecs.availability-zone\"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"type\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"spread\"</span></li><li class=\"L4\"><span class=\"pun\">},</span></li><li class=\"L5\"><span class=\"pun\">{</span></li><li class=\"L6\"><span class=\"str\">\"field\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"instanceId\"</span><span class=\"pun\">,</span></li><li class=\"L7\"><span class=\"str\">\"type\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"spread\"</span></li><li class=\"L8\"><span class=\"pun\">}</span></li><li class=\"L9\"><span class=\"pun\">]</span></li></ol></pre></div></div><p>What is the effect of this task placement strategy?</p>",
                "answerExplanation": "<p>A <em>task placement strategy</em> is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.</p><p>Amazon ECS supports the following task placement strategies:</p><p><code>binpack</code></p><p>Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.</p><p><code>random</code></p><p>Place tasks randomly.</p><p><code>spread</code></p><p>Place tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone.</p><p>You can specify task placement strategies with the following actions: <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CreateService.html\">CreateService</a>, <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_UpdateService.html\">UpdateService</a>, and <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RunTask.html\">RunTask</a>. You can also use multiple strategies together as in the example JSON code provided with the question.</p><p><strong>CORRECT: </strong>\"It distributes tasks evenly across Availability Zones and then distributes tasks evenly across the instances within each Availability Zone\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"It distributes tasks evenly across Availability Zones and then bin packs tasks based on memory within each Availability Zone\" is incorrect as it does not use the binpack strategy.</p><p><strong>INCORRECT:</strong> \"It distributes tasks evenly across Availability Zones and then distributes tasks evenly across distinct instances within each Availability Zone\" is incorrect as it does not spread tasks across distinct instances (use a task placement constraint).</p><p><strong>INCORRECT:</strong> \"It distributes tasks evenly across Availability Zones and then distributes tasks randomly across instances within each Availability Zone\" is incorrect as it does not use the random strategy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 2721,
                        "content": "<p>It distributes tasks evenly across Availability Zones and then distributes tasks randomly across instances within each Availability Zone</p>",
                        "isValid": false
                    },
                    {
                        "id": 2722,
                        "content": "<p>It distributes tasks evenly across Availability Zones and then distributes tasks evenly across the instances within each Availability Zone</p>",
                        "isValid": true
                    },
                    {
                        "id": 2723,
                        "content": "<p>It distributes tasks evenly across Availability Zones and then distributes tasks evenly across distinct instances within each Availability Zone</p>",
                        "isValid": false
                    },
                    {
                        "id": 2724,
                        "content": "<p>It distributes tasks evenly across Availability Zones and then bin packs tasks based on memory within each Availability Zone</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 663,
            "attributes": {
                "createdAt": "2023-09-07T08:51:18.422Z",
                "updatedAt": "2023-09-07T08:51:18.422Z",
                "content": "<p>An application on-premises uses Linux servers and a relational database using PostgreSQL. The company will be migrating the application to AWS and require a managed service that will take care of capacity provisioning, load balancing, and auto-scaling.</p><p>Which combination of services should the Developer use? (Select TWO.)</p>",
                "answerExplanation": "<p>The company require a managed service therefore the Developer should choose to use Elastic Beanstalk for the compute layer and Amazon RDS with the PostgreSQL engine for the database layer.</p><p>AWS Elastic Beanstalk will handle all capacity provisioning, load balancing, and auto-scaling for the web front-end and Amazon RDS provides push-button scaling for the backend.</p><p><strong>CORRECT: </strong>\"AWS Elastic Beanstalk\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Amazon RDS with PostrgreSQL\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 with Auto Scaling\" is incorrect as though these services will be used to provide the automatic scalability required for the solution, they still need to be managed. The questions asks for a managed solution and Elastic Beanstalk will manage this for you. Also, there is no mention of a load balancer so connections cannot be distributed to instances.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 with PostgreSQL\" is incorrect as the question asks for a managed service and therefore the database should be run on Amazon RDS.</p><p><strong>INCORRECT:</strong> \"AWS Lambda with CloudWatch Events\" is incorrect as there is no mention of refactoring application code to run on AWS Lambda.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p><p><a href=\"https://aws.amazon.com/rds/postgresql/\">https://aws.amazon.com/rds/postgresql/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 2725,
                        "content": "<p>AWS Lambda with CloudWatch Events</p>",
                        "isValid": false
                    },
                    {
                        "id": 2726,
                        "content": "<p>Amazon EC2 with Auto Scaling</p>",
                        "isValid": false
                    },
                    {
                        "id": 2727,
                        "content": "<p>Amazon EC2 with PostgreSQL</p>",
                        "isValid": false
                    },
                    {
                        "id": 2728,
                        "content": "<p>Amazon RDS with PostrgreSQL</p>",
                        "isValid": true
                    },
                    {
                        "id": 2729,
                        "content": "<p>AWS Elastic Beanstalk</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 664,
            "attributes": {
                "createdAt": "2023-09-07T08:51:18.516Z",
                "updatedAt": "2023-09-07T08:51:18.516Z",
                "content": "<p>A Developer is creating a web application that will be used by employees working from home. The company uses a SAML directory on-premises for storing user information. The Developer must integrate with the SAML directory and authorize each employee to access only their own data when using the application.</p><p>Which approach should the Developer take?</p>",
                "answerExplanation": "<p>Amazon Cognito leverages IAM roles to generate temporary credentials for your application's users. Access to permissions is controlled by a role's trust relationships.</p><p>In this example the Developer must limit access to specific identities in the SAML directory. The Developer can create a trust policy with an IAM condition key that limits access to a specific set of app users by checking the value of cognito-identity.amazonaws.com:sub:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-06-09_11-30-30-25305d386d0978ee6835d38458666dfe.jpg\"></p><p><strong>CORRECT: </strong>\"Use an Amazon Cognito identity pool, federate with the SAML provider, and use a trust policy with an IAM condition key to limit employee access\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon Cognito user pools, federate with the SAML provider, and use user pool groups with an IAM policy\" is incorrect. A user pool can be used to authenticate but the identity pool is used to provide authorized access to AWS services.</p><p><strong>INCORRECT:</strong> \"Create the application within an Amazon VPC and use a VPC endpoint with a trust policy to grant access to the employees\" is incorrect. You cannot provide access to an on-premises SAML directory using a VPC endpoint.</p><p><strong>INCORRECT:</strong> \"Create a unique IAM role for each employee and have each employee assume the role to access the application so they can access their personal data only\" is incorrect. This is not an integration into the SAML directory and would be very difficult to manage.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/role-trust-and-permissions.html\">https://docs.aws.amazon.com/cognito/latest/Developerguide/role-trust-and-permissions.html</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/iam-roles.html\">https://docs.aws.amazon.com/cognito/latest/Developerguide/iam-roles.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 2730,
                        "content": "<p>Create the application within an Amazon VPC and use a VPC endpoint with a trust policy to grant access to the employees.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2731,
                        "content": "<p>Use Amazon Cognito user pools, federate with the SAML provider, and use user pool groups with an IAM policy.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2732,
                        "content": "<p>Create a unique IAM role for each employee and have each employee assume the role to access the application so they can access their personal data only.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2733,
                        "content": "<p>Use an Amazon Cognito identity pool, federate with the SAML provider, and use a trust policy with an IAM condition key to limit employee access.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 665,
            "attributes": {
                "createdAt": "2023-09-07T08:51:18.600Z",
                "updatedAt": "2023-09-07T08:51:18.600Z",
                "content": "<p>A customer requires a serverless application with an API which mobile clients will use. The API will have both and AWS Lambda function and an Amazon DynamoDB table as data sources. Responses that are sent to the mobile clients must contain data that is aggregated from both of these data sources.</p><p>The developer must minimize the number of API endpoints and must minimize the number of API calls that are required to retrieve the necessary data.</p><p>Which solution should the developer use to meet these requirements?</p>",
                "answerExplanation": "<p>GraphQL APIs built with AWS AppSync give front-end developers the ability to query multiple databases, microservices, and APIs from a single GraphQL endpoint. This would not be possible with a REST API running on API Gateway which would have a single target for each API endpoint.</p><p>The example diagram below depicts a solution that includes AWS AppSync in front of mobile clients. The services then connect to Lambda and DynamoDB via API Gateway and AppSync.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-29_11-26-10-97d7c66e5420d18322a48e08eb041f85.jpg\"><p><strong>CORRECT: </strong>\"GraphQL API on AWS AppSync\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"REST API on Amazon API Gateway\" is incorrect.</p><p>A REST API is not suitable as the question asks to reduce the number of API endpoints. With a REST API there is a single target such as a Lambda per API endpoint so more endpoints would be required.</p><p><strong>INCORRECT:</strong> \"GraphQL API on an Amazon EC2 instance\" is incorrect.</p><p>This would not be a serverless solution and the question states that the solution must be serverless.</p><p><strong>INCORRECT:</strong> \"REST API on AWS Elastic Beanstalk\" is incorrect.</p><p>As explained above.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mobile/appsync-microservices/\">https://aws.amazon.com/blogs/mobile/appsync-microservices/</a></p>",
                "options": [
                    {
                        "id": 2734,
                        "content": "<p>REST API on AWS Elastic Beanstalk</p>",
                        "isValid": false
                    },
                    {
                        "id": 2735,
                        "content": "<p>GraphQL API on AWS AppSync</p>",
                        "isValid": true
                    },
                    {
                        "id": 2736,
                        "content": "<p>GraphQL API on an Amazon EC2 instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 2737,
                        "content": "<p>REST API on Amazon API Gateway</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 666,
            "attributes": {
                "createdAt": "2023-09-07T08:51:18.674Z",
                "updatedAt": "2023-09-07T08:51:18.674Z",
                "content": "<p>A programmer has designed an AWS Lambda function that retrieves resources within a VPC. This Lambda function fetches new messages from an Amazon SQS queue. It then computes a cumulative average of the numerical data within the messages. After the initial Lambda function tests, the programmer discovered the computed cumulative average was not consistent.</p><p>What should the programmer do to ascertain the accuracy of the cumulative average computed by the function?</p>",
                "answerExplanation": "<p>Setting the provisioned concurrency of the function to 1 ensures that only one instance of the function will be operating at any given time, helping to avoid potential conflicts where multiple instances of the function could be trying to update the average concurrently.</p><p>Storing the computed cumulative average in Amazon ElastiCache enables the function to maintain and retrieve the cumulative average values across different invocations. This offers a consistent and accurate cumulative average calculation.</p><p><strong>CORRECT: </strong>\"Set the provisioned concurrency of the function to 1. Compute the cumulative average within the function. Retain the computed cumulative average in Amazon ElastiCache\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Set the reserved concurrency of the function to 1. Compute the cumulative average within the function. Retain the computed cumulative average in Amazon ElastiCache\" is incorrect.</p><p>Reserved concurrency does not guarantee that only one function instance will be executing at a time.</p><p><strong>INCORRECT:</strong> \"Adjust the function to retain the data in Amazon ElastiCache. At function initialization, utilize the earlier data from the cache to compute the cumulative average\" is incorrect.</p><p>Reserved concurrency does not guarantee that only one function instance will be executing at a time.</p><p><strong>INCORRECT:</strong> \"Adjust the function to retain the data in the function's layers. At function initialization, utilize the previously retained data to compute the cumulative average\" is incorrect.</p><p>AWS Lambda function layers are used to manage code and resources that may be shared across multiple functions, they are not designed to hold state or data between function invocations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 2738,
                        "content": "<p>Adjust the function to retain the data in the function's layers. At function initialization, utilize the previously retained data to compute the cumulative average.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2739,
                        "content": "<p>Set the reserved concurrency of the function to 1. Compute the cumulative average within the function. Retain the computed cumulative average in Amazon ElastiCache.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2740,
                        "content": "<p>Set the provisioned concurrency of the function to 1. Compute the cumulative average within the function. Retain the computed cumulative average in Amazon ElastiCache.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2741,
                        "content": "<p>Adjust the function to retain the data in Amazon ElastiCache. At function initialization, utilize the earlier data from the cache to compute the cumulative average.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 667,
            "attributes": {
                "createdAt": "2023-09-07T08:51:18.754Z",
                "updatedAt": "2023-09-07T08:51:18.754Z",
                "content": "<p>A developer must identify the public IP addresses of clients connecting to Amazon EC2 instances behind a public Application Load Balancer (ALB). The EC2 instances run an HTTP server that logs all requests to a log file.</p><p>How can the developer ensure the client public IP addresses are captured in the log files on the EC2 instances?</p>",
                "answerExplanation": "<p>The X-Forwarded-For request header is automatically added and helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer.</p><p>Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header.</p><p>The HTTP server may need to be configured to include the x-forwarded-for request header in the log files. Once this is done, the logs will contain the public IP addresses of the clients.</p><p><strong>CORRECT: </strong>\"Configure the HTTP server to add the x-forwarded-for request header to the logs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the HTTP server to add the x-forwarded-proto request header to the logs\" is incorrect.</p><p>This request header identifies the protocol (HTTP or HTTPS).</p><p><strong>INCORRECT:</strong> \"Install the AWS X-Ray daemon on the EC2 instances and configure request logging\" is incorrect.</p><p>X-Ray is used for tracing applications; it will not help identify the public IP addresses of clients.</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch Logs agent on the EC2 instances and configure logging\" is incorrect.</p><p>The Amazon CloudWatch Logs agent will send application and system logs to CloudWatch Logs. This does not help to capture the client IP addresses of connections.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/x-forwarded-headers.html#x-forwarded-for\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/x-forwarded-headers.html#x-forwarded-for</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 2742,
                        "content": "<p>Configure the HTTP server to add the x-forwarded-for request header to the logs.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2743,
                        "content": "<p>Install the AWS X-Ray daemon on the EC2 instances and configure request logging.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2744,
                        "content": "<p>Install the Amazon CloudWatch Logs agent on the EC2 instances and configure logging.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2745,
                        "content": "<p>Configure the HTTP server to add the x-forwarded-proto request header to the logs.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 668,
            "attributes": {
                "createdAt": "2023-09-07T08:51:18.825Z",
                "updatedAt": "2023-09-07T08:51:18.825Z",
                "content": "<p>An application needs to generate SMS text messages and emails for a large number of subscribers. Which AWS service can be used to send these messages to customers?</p>",
                "answerExplanation": "<p>Amazon Simple Notification Service (Amazon SNS) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clientsâ€”publishers and subscribersâ€”also referred to as producers and consumers.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_05-48-02-778d257dde32ce0bc682c9b269c19d94.png\"></p><p>Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel.</p><p>Subscribers (that is, web servers, email addresses, Amazon SQS queues, AWS Lambda functions) consume or receive the message or notification over one of the supported protocols (that is, Amazon SQS, HTTP/S, email, SMS, Lambda) when they are subscribed to the topic.</p><p><strong>CORRECT: </strong>\"Amazon SNS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon SES\" is incorrect as this service only sends email, not SMS text messages.</p><p><strong>INCORRECT:</strong> \"Amazon SQS\" is incorrect as this is a hosted message queue for decoupling application components.</p><p><strong>INCORRECT:</strong> \"Amazon SWF\" is incorrect as the Simple Workflow Service is used for orchestrating multi-step workflows.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/welcome.html\">https://docs.aws.amazon.com/sns/latest/dg/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 2746,
                        "content": "<p>Amazon SNS</p>",
                        "isValid": true
                    },
                    {
                        "id": 2747,
                        "content": "<p>Amazon SQS</p>",
                        "isValid": false
                    },
                    {
                        "id": 2748,
                        "content": "<p>Amazon SWF</p>",
                        "isValid": false
                    },
                    {
                        "id": 2749,
                        "content": "<p>Amazon SES</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 669,
            "attributes": {
                "createdAt": "2023-09-07T08:51:18.901Z",
                "updatedAt": "2023-09-07T08:51:18.901Z",
                "content": "<p>An organization handles data that requires high availability in its relational database. The main headquarters for the organization is in Virginia with smaller offices located in California. The main headquarters uses the data more frequently than the smaller offices. How should the developer configure their databases to meet high availability standards?</p>",
                "answerExplanation": "<p>Aurora is a relational database that provides high availability by allowing customers to create up to 15 database replications in different Availability Zones. It also allows you to specify which Aurora replica can be promoted to the primary database should the primary database become unavailable. Selecting the AZ that is closest to the main headquarters should not negatively impact the smaller offices but changing the primary database to California could negatively impact the main headquarters.</p><p><strong>CORRECT: </strong>\"Create an Aurora database with the primary database in Virginia and specify the failover to the Aurora replica in another AZ in Virginia\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Aurora database with the primary database in Virginia and specify the failover to the Aurora replica in another AZ in California\" is incorrect. It could create some latency issues for the main headquarters in Virginia.</p><p><strong>INCORRECT:</strong> \"Create a DynamoDB database with the primary database in Virginia and specify the failover to the DynamoDB replica in another AZ in Virginia\" is incorrect. DynamoDB is not a relational database.</p><p><strong>INCORRECT:</strong> \"Create an Athena database with the primary database in Virginia and specify the failover to the Athena replica in another AZ in Virginia\" is incorrect. Athena analyzes data but is not a database service.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
                "options": [
                    {
                        "id": 2750,
                        "content": "<p>Create a DynamoDB database with the primary database in Virginia and specify the failover to the DynamoDB replica in another AZ in Virginia.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2751,
                        "content": "<p>Create an Athena database with the primary database in Virginia and specify the failover to the Athena replica in another AZ in Virginia.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2752,
                        "content": "<p>Create an Aurora database with the primary database in Virginia and specify the failover to the Aurora replica in another AZ in California.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2753,
                        "content": "<p>Create an Aurora database with the primary database in Virginia and specify the failover to the Aurora replica in another AZ in Virginia.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 670,
            "attributes": {
                "createdAt": "2023-09-07T08:51:18.984Z",
                "updatedAt": "2023-09-07T08:51:18.984Z",
                "content": "<p>A Developer is building a three-tier web application that must be able to handle a minimum of 10,000 requests per minute. The requirements state that the web tier should be completely stateless while the application maintains session state data for users.</p><p>How can the session state data be maintained externally, whilst keeping latency at the LOWEST possible value?</p>",
                "answerExplanation": "<p>It is common to use key/value stores for storing session state data. The two options presented in the answers are Amazon DynamoDB and Amazon ElastiCache Redis. Of these two, ElastiCache will provide the lowest latency as it is an in-memory database.</p><p>Therefore, the best answer is to create an Amazon ElastiCache Redis cluster, then implement session handling at the application level to leverage the cluster for session data storage</p><p><strong>CORRECT: </strong>\"Create an Amazon ElastiCache Redis cluster, then implement session handling at the application level to leverage the cluster for session data storage\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon DynamoDB table, then implement session handling at the application level to leverage the table for session data storage\" is incorrect as though this is a good solution for storing session state data, the latency will not be as low as with ElastiCache.</p><p><strong>INCORRECT:</strong> \"Create an Amazon RedShift instance, then implement session handling at the application level to leverage a database inside the RedShift database instance for session data storage\" is incorrect. RedShift is a data warehouse that is used for OLAP use cases, not for storing session state data.</p><p><strong>INCORRECT:</strong> \"Implement a shared Amazon EFS file system solution across the underlying Amazon EC2 instances, then implement session handling at the application level to leverage the EFS file system for session data storage\" is incorrect. For session state data a key/value store such as DynamoDB or ElastiCache will provide better performance.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/caching/session-management/\">https://aws.amazon.com/caching/session-management/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 2754,
                        "content": "<p>Create an Amazon ElastiCache Redis cluster, then implement session handling at the application level to leverage the cluster for session data storage</p>",
                        "isValid": true
                    },
                    {
                        "id": 2755,
                        "content": "<p>Create an Amazon RedShift instance, then implement session handling at the application level to leverage a database inside the RedShift database instance for session data storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 2756,
                        "content": "<p>Create an Amazon DynamoDB table, then implement session handling at the application level to leverage the table for session data storage</p>",
                        "isValid": false
                    },
                    {
                        "id": 2757,
                        "content": "<p>Implement a shared Amazon EFS file system solution across the underlying Amazon EC2 instances, then implement session handling at the application level to leverage the EFS file system for session data storage</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 671,
            "attributes": {
                "createdAt": "2023-09-07T08:51:19.063Z",
                "updatedAt": "2023-09-07T08:51:19.063Z",
                "content": "<p>A developer is writing an application for a company. The program needs to access and read the file named \"secret-data.xlsx\" located in the root directory of an Amazon S3 bucket named \"DATA-BUCKET\". The company's security policies mandate the enforcement of the principle of least privilege for the IAM policy associated with the application.</p><p>Which IAM policy statement will comply with these security stipulations?</p>",
                "answerExplanation": "<p>This statement provides the minimal permission necessary for the application to read the specific \"secret-data.xlsx\" file from the specified S3 bucket.</p><p><strong>CORRECT: </strong>\"{\"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET/secret-data.xlsx\"}\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"{\"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET/*\"}\" is incorrect.</p><p>This policy statement gives permission for all S3 actions on all objects in the bucket, which goes against the principle of least privilege.</p><p><strong>INCORRECT:</strong> \"{\"Effect\": \"Allow\", \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET\"}\" is incorrect.</p><p>This policy permits the application to list all objects in the bucket, but it doesn't grant read permission for \"secret-data.xlsx\".</p><p><strong>INCORRECT:</strong> \"{\"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET/*\"}\" is incorrect.</p><p>This statement provides read access for all objects in the bucket, not just the \"secret-data.xlsx\" file, contradicting the principle of least privilege.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 2758,
                        "content": "<p>{\"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET/*\"}</p>",
                        "isValid": false
                    },
                    {
                        "id": 2759,
                        "content": "<p>{\"Effect\": \"Allow\", \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET\"}</p>",
                        "isValid": false
                    },
                    {
                        "id": 2760,
                        "content": "<p>{\"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET/*\"}</p>",
                        "isValid": false
                    },
                    {
                        "id": 2761,
                        "content": "<p>{\"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET/secret-data.xlsx\"}</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 672,
            "attributes": {
                "createdAt": "2023-09-07T08:51:19.133Z",
                "updatedAt": "2023-09-07T08:51:19.133Z",
                "content": "<p>A Developer is writing an imaging microservice on AWS Lambda. The service is dependent on several libraries that are not available in the Lambda runtime environment.</p>",
                "answerExplanation": "<p>A deployment package is a ZIP archive that contains your function code and dependencies. You need to create a deployment package if you use the Lambda API to manage functions, or if you need to include libraries and dependencies other than the AWS SDK.</p><p>You can upload the package directly to Lambda, or you can use an Amazon S3 bucket, and then upload it to Lambda. If the deployment package is larger than 50 MB, you must use Amazon S3.</p><p><strong>CORRECT: </strong>\"Create a ZIP file with the source code and all dependent libraries\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a ZIP file with the source code and a script that installs the dependent libraries at runtime\" is incorrect as the Developer should not run a script at runtime as this will cause latency. Instead, the Developer should include the dependent libraries in the ZIP package.</p><p><strong>INCORRECT:</strong> \"Create a ZIP file with the source code and an appspec.yml file. Add the libraries to the appspec.yml file and upload to Amazon S3. Deploy using CloudFormation\" is incorrect. The appspec.yml file is used with CodeDeploy, you cannot add libraries into it, and it is not deployed using CloudFormation.</p><p><strong>INCORRECT:</strong> \"Create a ZIP file with the source code and a buildspec.yml file that installs the dependent libraries on AWS Lambda\" is incorrect as the buildspec.yml file is used with CodeBuild for compiling source code and running tests. It cannot be used to install dependent libraries within Lambda.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/python-package.html\">https://docs.aws.amazon.com/lambda/latest/dg/python-package.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 2762,
                        "content": "<p>Create a ZIP file with the source code and an appspec.yml file. Add the libraries to the appspec.yml file and upload to Amazon S3. Deploy using CloudFormation</p>",
                        "isValid": false
                    },
                    {
                        "id": 2763,
                        "content": "<p>Create a ZIP file with the source code and a script that installs the dependent libraries at runtime</p>",
                        "isValid": false
                    },
                    {
                        "id": 2764,
                        "content": "<p>Create a ZIP file with the source code and all dependent libraries</p>",
                        "isValid": true
                    },
                    {
                        "id": 2765,
                        "content": "<p>Create a ZIP file with the source code and a buildspec.yml file that installs the dependent libraries on AWS Lambda</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 673,
            "attributes": {
                "createdAt": "2023-09-07T08:51:19.218Z",
                "updatedAt": "2023-09-07T08:51:19.218Z",
                "content": "<p>A developer has deployed an application on AWS Lambda. The application uses Python and must generate and then upload a file to an Amazon S3 bucket. The developer must implement the upload functionality with the least possible change to the application code.</p><p>Which solution BEST meets these requirements?</p>",
                "answerExplanation": "<p>The best practice for Lambda development is to bundle all dependencies used by your Lambda function, including the AWS SDK. However, since this question specifically requests that the least possible changes are made to the application code, the developer can instead use the SDK for Python that is installed in the Lambda environment to upload the file to Amazon S3.</p><p><strong>CORRECT: </strong>\"Use the AWS SDK for Python that is installed in the Lambda execution environment\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Include the AWS SDK for Python in the Lambda function code\" is incorrect.</p><p>This is the best practice for deployment. However, in this case the developer must minimize changes to code and including the SDK as a dependency in the code would require potential updates to existing Python code.</p><p><strong>INCORRECT:</strong> \"Make an HTTP request directly to the S3 API to upload the file\" is incorrect.</p><p>AWS supports uploads to S3 using the console, AWS SDKs, REST API, and the AWS CLI.</p><p><strong>INCORRECT:</strong> \"Use the AWS CLI that is installed in the Lambda execution environment\" is incorrect.</p><p>The AWS CLI is not installed in the Lambda execution environment.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/upcoming-changes-to-the-python-sdk-in-aws-lambda/\">https://aws.amazon.com/blogs/compute/upcoming-changes-to-the-python-sdk-in-aws-lambda/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 2766,
                        "content": "<p>Make an HTTP request directly to the S3 API to upload the file.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2767,
                        "content": "<p>Use the AWS SDK for Python that is installed in the Lambda execution environment.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2768,
                        "content": "<p>Use the AWS CLI that is installed in the Lambda execution environment.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2769,
                        "content": "<p>Include the AWS SDK for Python in the Lambda function code.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 674,
            "attributes": {
                "createdAt": "2023-09-07T08:51:19.298Z",
                "updatedAt": "2023-09-07T08:51:19.298Z",
                "content": "<p>An application uses an Amazon DynamoDB table that is 50 GB in size and provisioned with 10,000 read capacity units (RCUs) per second. The table must be scanned during non-peak hours when normal traffic consumes around 5,000 RCUs. The Developer must scan the whole table in the shortest possible time whilst ensuring the normal workload is not affected.</p><p>How would the Developer optimize this scan cost-effectively?</p>",
                "answerExplanation": "<p>To make the most of the tableâ€™s provisioned throughput, the Developer can use the Parallel Scan API operation so that the scan is distributed across the tableâ€™s partitions. This will help to optimize the scan to complete in the fastest possible time. However, the Developer will also need to apply rate limiting to ensure that the scan does not affect normal workloads.</p><p><strong>CORRECT: </strong>\"Use the Parallel Scan API operation and limit the rate\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use sequential scans and apply a FilterExpression\" is incorrect. A FilterExpression is a string that contains conditions that DynamoDB applies after the Scan operation, but before the data is returned to you. This will not assist with speeding up the scan or preventing it from affecting normal workloads.</p><p><strong>INCORRECT:</strong> \"Increase read capacity units during the scan operation\" is incorrect. There are already more RCUs provisioned than are needed during the non-peak hours. The key here is to use what is available for cost-effectiveness whilst ensuing normal workloads are not affected.</p><p><strong>INCORRECT:</strong> \"Use sequential scans and set the ConsistentRead parameter to false\" is incorrect. This setting would turn off consistent reads making the scan eventually consistent. This will not satisfy the requirements of the question.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/\">https://aws.amazon.com/blogs/Developer/rate-limited-scans-in-amazon-dynamodb/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 2770,
                        "content": "<p>Use sequential scans and set the ConsistentRead parameter to false.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2771,
                        "content": "<p>Increase read capacity units during the scan operation.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2772,
                        "content": "<p>Use sequential scans and apply a FilterExpression.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2773,
                        "content": "<p>Use the Parallel Scan API operation and limit the rate.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 675,
            "attributes": {
                "createdAt": "2023-09-07T08:51:19.372Z",
                "updatedAt": "2023-09-07T08:51:19.372Z",
                "content": "<p>A company is deploying a microservices application on AWS Fargate using Amazon ECS. The application has environment variables that must be passed to a container for the application to initialize.</p><p>How should the environment variables be passed to the container?</p>",
                "answerExplanation": "<p>When you register a task definition, you must specify a list of container definitions that are passed to the Docker daemon on a container instance.</p><p>The developer should use advanced container definition parameters and define environment variables to pass to the container.</p><p><strong>CORRECT: </strong>\"Use advanced container definition parameters and define environment variables under the environment parameter within the task definition\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use advanced container definition parameters and define environment variables under the environment parameter within the service definition\" is incorrect.</p><p>The task definition is the correct place to define the environment variables to pass to the container.</p><p><strong>INCORRECT:</strong> \"Use standard container definition parameters and define environment variables under the secrets parameter within the task definition\" is incorrect.</p><p>Advanced container definition parameters must be used to pass the environment variables to the container. The environment parameter should also be used.</p><p><strong>INCORRECT:</strong> \"Use standard container definition parameters and define environment variables under the WorkingDirectory parameter within the service definition\" is incorrect.</p><p>Advanced container definition parameters must be used to pass the environment variables to the container. The environment parameter should also be used.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/userguide/task_definition_parameters.html#container_definition_environment\">https://docs.aws.amazon.com/AmazonECS/latest/userguide/task_definition_parameters.html#container_definition_environment</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 2774,
                        "content": "<p>Use standard container definition parameters and define environment variables under the WorkingDirectory parameter within the service definition.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2775,
                        "content": "<p>Use advanced container definition parameters and define environment variables under the environment parameter within the service definition.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2776,
                        "content": "<p>Use standard container definition parameters and define environment variables under the secrets parameter within the task definition.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2777,
                        "content": "<p>Use advanced container definition parameters and define environment variables under the environment parameter within the task definition.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 676,
            "attributes": {
                "createdAt": "2023-09-07T08:51:19.454Z",
                "updatedAt": "2023-09-07T08:51:19.454Z",
                "content": "<p>A company has a large Amazon DynamoDB table which they scan periodically so they can analyze several attributes. The scans are consuming a lot of provisioned throughput. What technique can a Developer use to minimize the impact of the scan on the table's provisioned throughput?</p>",
                "answerExplanation": "<p>In general, Scan operations are less efficient than other operations in DynamoDB. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set.</p><p>If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation.</p><p>The following diagram illustrates the impact of a sudden spike of capacity unit usage by Query and Scan operations, and its impact on your other requests against the same table.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-28-17-ac3a660ebbeeb2f3ede6dce4e9b8aa76.jpg\"></p><p>Instead of using a large Scan operation, you can use the following techniques to minimize the impact of a scan on a table's provisioned throughput.</p><p><strong>Reduce page size</strong></p><p>Because a Scan operation reads an entire page (by default, 1 MB), you can reduce the impact of the scan operation by setting a smaller page size. The Scan operation provides a <em>Limit</em> parameter that you can use to set the page size for your request. Each Query or Scan request that has a smaller page size uses fewer read operations and creates a \"pause\" between each request.</p><p><strong>Isolate scan operations</strong></p><p>DynamoDB is designed for easy scalability. As a result, an application can create tables for distinct purposes, possibly even duplicating content across several tables. You want to perform scans on a table that is not taking \"mission-critical\" traffic. Some applications handle this load by rotating traffic hourly between two tablesâ€”one for critical traffic, and one for bookkeeping. Other applications can do this by performing every write on two tables: a \"mission-critical\" table, and a \"shadow\" table.</p><p>Therefore, the best option to reduce the impact of the scan on the table's provisioned throughput is to set a smaller page size for the scan.</p><p><strong>CORRECT: </strong>\"Set a smaller page size for the scan\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use parallel scans\" is incorrect as this will return results faster but place more burden on the tableâ€™s provisioned throughput.</p><p><strong>INCORRECT:</strong> \"Define a range key on the table\" is incorrect. A range key is a composite key that includes the hash key and another attribute. This is of limited use in this scenario as the table is being scanned to analyze multiple attributes.</p><p><strong>INCORRECT:</strong> \"Prewarm the table by updating all items\" is incorrect as updating all items would incur significant costs in terms of provisioned throughput and would not be advantageous.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 2778,
                        "content": "<p>Set a smaller page size for the scan</p>",
                        "isValid": true
                    },
                    {
                        "id": 2779,
                        "content": "<p>Prewarm the table by updating all items</p>",
                        "isValid": false
                    },
                    {
                        "id": 2780,
                        "content": "<p>Define a range key on the table</p>",
                        "isValid": false
                    },
                    {
                        "id": 2781,
                        "content": "<p>Use parallel scans</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 677,
            "attributes": {
                "createdAt": "2023-09-07T08:51:19.530Z",
                "updatedAt": "2023-09-07T08:51:19.530Z",
                "content": "<p>A Developer needs to scan a full DynamoDB 50GB table within non-peak hours. About half of the strongly consistent RCUs are typically used during non-peak hours and the scan duration must be minimized.</p><p>How can the Developer optimize the scan execution time without impacting production workloads?</p>",
                "answerExplanation": "<p>Performing a scan on a table consumes a lot of RCUs. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set. To reduce the amount of RCUs used by the scan so it doesnâ€™t affect production workloads whilst minimizing the execution time, there are a couple of recommendations the Developer can follow.</p><p>Firstly, the <em>Limit</em> parameter can be used to reduce the page size. The Scan operation provides a <em>Limit</em> parameter that you can use to set the page size for your request. Each Query or Scan request that has a smaller page size uses fewer read operations and creates a \"pause\" between each request.</p><p>Secondly, the Developer can configure parallel scans. With parallel scans the Developer can maximize usage of the available throughput and have the scans distributed across the tableâ€™s partitions.</p><p>A parallel scan can be the right choice if the following conditions are met:</p><p>The table size is 20 GB or larger.</p><p>The table's provisioned read throughput is not being fully used.</p><p>Sequential Scan operations are too slow.</p><p>Therefore, to optimize the scan operation the Developer should use parallel scans while limiting the rate as this will ensure that the scan operation does not affect the performance of production workloads and still have it complete in the minimum time.</p><p><strong>CORRECT: </strong>\"Use parallel scans while limiting the rate\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use sequential scans\" is incorrect as this is slower than parallel scans and the Developer needs to minimize scan execution time.</p><p><strong>INCORRECT:</strong> \"Increase the RCUs during the scan operation\" is incorrect as the table is only using half of the RCUs during non-peak hours so there are RCUs available. You could increase RCUs and perform the scan faster, but this would be more expensive. The better solution is to use parallel scans with the limit parameter.</p><p><strong>INCORRECT:</strong> \"Change to eventually consistent RCUs during the scan operation\" is incorrect as this does not provide a solution for preventing impact to the production workloads. The limit parameter should be used to ensure the tables RCUs are not fully used.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html#QueryAndScanGuidelines.ParallelScan\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html#QueryAndScanGuidelines.ParallelScan</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/certification-training/aws-developer-associate/aws-database/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 2782,
                        "content": "<p>Change to eventually consistent RCUs during the scan operation</p>",
                        "isValid": false
                    },
                    {
                        "id": 2783,
                        "content": "<p>Increase the RCUs during the scan operation</p>",
                        "isValid": false
                    },
                    {
                        "id": 2784,
                        "content": "<p>Use sequential scans</p>",
                        "isValid": false
                    },
                    {
                        "id": 2785,
                        "content": "<p>Use parallel scans while limiting the rate</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 678,
            "attributes": {
                "createdAt": "2023-09-07T08:51:19.612Z",
                "updatedAt": "2023-09-07T08:51:19.612Z",
                "content": "<p>A startup is developing a prototype for a news aggregator application. This application will display the latest news for a specific industry and provide a RESTful API endpoint that clients can invoke. Where feasible, the application should leverage AWS's caching features to reduce the load on the backend service. The backend of the application is expected to handle a modest amount of traffic, primarily during testing periods.</p><p>Which method would be the most cost-effective for the developer to implement this REST endpoint?</p>",
                "answerExplanation": "<p>AWS API Gateway, combined with AWS Lambda, offers a serverless solution where costs are directly related to usage. API Gateway also provides a caching feature which can help reduce the load on the backend.</p><p><strong>CORRECT: </strong>\"Utilize AWS API Gateway with an AWS Lambda function as the backend and enable caching in API Gateway\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Launch an Amazon EC2 instance, host the application on it, and employ Amazon ElastiCache for caching purposes\" is incorrect.</p><p>Using an Amazon EC2 instance would incur constant costs, regardless of whether the instance is in use or idle. Additionally, employing AWS ElastiCache could increase the costs unnecessarily, especially for a prototype application.</p><p><strong>INCORRECT:</strong> \"Construct an Amazon ECS service for the application, using Amazon ElastiCache for caching\" is incorrect.</p><p>Amazon ECS is overkill for a simple prototype application like this. It requires more resources and management than a serverless solution.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Elastic Beanstalk application and use Amazon DynamoDB for caching services\" is incorrect.</p><p>While AWS Elastic Beanstalk simplifies application deployment, it does not inherently provide caching capabilities. Amazon DynamoDB is a database service, not a caching service, making it unsuitable for this requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 2786,
                        "content": "<p>Launch an Amazon EC2 instance, host the application on it, and employ Amazon ElastiCache for caching purposes.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2787,
                        "content": "<p>Construct an Amazon ECS service for the application, using Amazon ElastiCache for caching.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2788,
                        "content": "<p>Utilize AWS API Gateway with an AWS Lambda function as the backend and enable caching in API Gateway.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2789,
                        "content": "<p>Deploy an AWS Elastic Beanstalk application and use Amazon DynamoDB for caching services.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 679,
            "attributes": {
                "createdAt": "2023-09-07T08:51:19.694Z",
                "updatedAt": "2023-09-07T08:51:19.694Z",
                "content": "<p>An organization is selling memorabilia that is illegal in specific countries. How can a developer restrict access to the website to countries where the memorabilia are legal?</p>",
                "answerExplanation": "<p>AWS WAF can be used to set up a WEB ACL that can be used to block statements that originate from a specific country.</p><p><strong>CORRECT: </strong>\"Create a Web ACL in AWS WAF with a rule that matches the specified countries and blocks access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a Web ACL in AWS Shield with a rule that matches the specified countries and blocks access\" is incorrect. AWS Shield is used to protect from DDoS attacks.</p><p><strong>INCORRECT:</strong> \"Create a Web ACL in AWS WAF with a rule that matches the specified countries and triggers an SNS notification\" is incorrect. This will not block access to specific countries.</p><p><strong>INCORRECT:</strong> \" Create a Web ACL in AWS Shield with a rule that matches the specified countries and triggers an SNS notification\" is incorrect. This will not block access to specific countries.</p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/waf-allow-block-country-geolocation/\">https://aws.amazon.com/premiumsupport/knowledge-center/waf-allow-block-country-geolocation/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>",
                "options": [
                    {
                        "id": 2790,
                        "content": "<p>Create a Web ACL in AWS WAF with a rule that matches the specified countries and blocks access.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2791,
                        "content": "<p>Create a Web ACL in AWS Shield with a rule that matches the specified countries and blocks access.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2792,
                        "content": "<p>Create a Web ACL in AWS Shield with a rule that matches the specified countries and triggers an SNS notification.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2793,
                        "content": "<p>Create a Web ACL in AWS WAF with a rule that matches the specified countries and triggers an SNS notification.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 680,
            "attributes": {
                "createdAt": "2023-09-07T08:51:19.772Z",
                "updatedAt": "2023-09-07T08:51:19.772Z",
                "content": "<p>A developer is updating an Amazon ECS app that uses an ALB with two target groups and a single listener. The developer has an AppSpec file in an S3 bucket and an AWS CodeDeploy deployment group tied to the ALB and AppSpec file. The developer needs to use an AWS Lambda function for update validation before deployment.</p><p>Which solution meets these requirements?</p>",
                "answerExplanation": "<p>The AppSpec file allows a developer to specify scripts to be run at different lifecycle hooks. The BeforeAllowTraffic lifecycle event occurs before the updated task set is moved to the target group that is receiving live traffic. So, any validation before production deployment should be configured at this lifecycle event. The listener is configured at the ALB level, not at the deployment group.</p><p><strong>CORRECT: </strong>\"Add a listener to the ALB. Update the AppSpec file to link the Lambda function to the BeforeAllowTraffic lifecycle hook\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add a listener to the ALB. Update the AppSpec file to link the Lambda function to the AfterAllowTraffic lifecycle hook\" is incorrect.</p><p>This is incorrect because AfterAllowTraffic lifecycle event occurs after the updated task set is moved to the target group that is receiving live traffic. Validation should be performed before the updated task set receives live traffic, not after.</p><p><strong>INCORRECT:</strong> \"Attach a listener to the deployment group. Update the AppSpec file to link the Lambda function to the BeforeAllowTraffic lifecycle hook\" is incorrect.</p><p>This is incorrect because the listener is not attached to the deployment group. It is configured at the ALB level.</p><p><strong>INCORRECT:</strong> \"Attach a listener to the deployment group. Update the AppSpec file to link the Lambda function to the AfterAllowTraffic lifecycle hook\" is incorrect.</p><p>Listeners are added to the ALB, not the deployment group, and validation should occur before the updated task set receives live traffic.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 2794,
                        "content": "<p>Add a listener to the ALB. Update the AppSpec file to link the Lambda function to the AfterAllowTraffic lifecycle hook.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2795,
                        "content": "<p>Add a listener to the ALB. Update the AppSpec file to link the Lambda function to the BeforeAllowTraffic lifecycle hook.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2796,
                        "content": "<p>Attach a listener to the deployment group. Update the AppSpec file to link the Lambda function to the AfterAllowTraffic lifecycle hook.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2797,
                        "content": "<p>Attach a listener to the deployment group. Update the AppSpec file to link the Lambda function to the BeforeAllowTraffic lifecycle hook.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 681,
            "attributes": {
                "createdAt": "2023-09-07T08:51:19.852Z",
                "updatedAt": "2023-09-07T08:51:19.852Z",
                "content": "<p>A Developer has recently created an application that uses an AWS Lambda function, an Amazon DynamoDB table, and also sends notifications using Amazon SNS. The application is not working as expected and the Developer needs to analyze what is happening across all components of the application.</p><p>What is the BEST way to analyze the issue?</p>",
                "answerExplanation": "<p>AWS X-Ray makes it easy for developers to analyze the behavior of their production, distributed applications with end-to-end tracing capabilities. You can use X-Ray to identify performance bottlenecks, edge case errors, and other hard to detect issues.</p><p>AWS X-Ray provides an end-to-end, cross-service view of requests made to your application. It gives you an application-centric view of requests flowing through your application by aggregating the data gathered from individual services in your application into a single unit called a trace. You can use this trace to follow the path of an individual request as it passes through each service or tier in your application so that you can pinpoint where issues are occurring.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_13-05-35-b3f0f59b4ce5e42177cf09e46934c48e.jpg\"></p><p>AWS X-Ray will assist the developer with visually analyzing the end-to-end view of connectivity between the application components and how they are performing using a Service Map. X-Ray also provides aggregated data about the application.</p><p><strong>CORRECT: </strong>\"Enable X-Ray tracing for the Lambda function\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Events rule\" is incorrect as this feature of CloudWatch is used to trigger actions based on changes in the state of AWS services.</p><p><strong>INCORRECT:</strong> \"Assess the application with Amazon Inspector\" is incorrect. Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.</p><p><strong>INCORRECT:</strong> \"Monitor the application with AWS Trusted Advisor\" is incorrect. <strong>AWS Trusted Advisor</strong> is an online tool that provides you real time guidance to help you provision your resources following AWS best practices.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/xray/features/\">https://aws.amazon.com/xray/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 2798,
                        "content": "<p>Create an Amazon CloudWatch Events rule</p>",
                        "isValid": false
                    },
                    {
                        "id": 2799,
                        "content": "<p>Monitor the application with AWS Trusted Advisor</p>",
                        "isValid": false
                    },
                    {
                        "id": 2800,
                        "content": "<p>Enable X-Ray tracing for the Lambda function</p>",
                        "isValid": true
                    },
                    {
                        "id": 2801,
                        "content": "<p>Assess the application with Amazon Inspector</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 682,
            "attributes": {
                "createdAt": "2023-09-07T08:51:19.928Z",
                "updatedAt": "2023-09-07T08:51:19.928Z",
                "content": "<p>A critical application is hosted on AWS, exposed by an HTTP API through Amazon API Gateway. The API is integrated with an AWS Lambda function and the application data is housed in an Amazon RDS for PostgreSQL DB instance, featuring 2 vCPUs and 16 GB of RAM.</p><p>The company has been receiving customer complaints about occasional HTTP 500 Internal Server Error responses from some API calls during unpredictable peak usage times. Amazon CloudWatch Logs has recorded \"connection limit exceeded\" errors. The company wants to ensure resilience in the application, with no unscheduled downtime for the database.</p><p>Which solution would best fit these requirements?</p>",
                "answerExplanation": "<p>Amazon RDS Proxy is designed to improve application scalability and resilience by pooling and sharing database connections, reducing the CPU and memory overhead on the database. It handles the burst in connections seamlessly and improves the application's ability to scale.</p><p><strong>CORRECT: </strong>\"Use Amazon RDS Proxy and update the Lambda function to connect to the proxy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Double the RAM and vCPUs of the RDS instance\" is incorrect.</p><p>Merely augmenting the number of vCPUs and RAM for the RDS instance might not resolve the issue as it doesn't directly tackle the problem of too many connections.</p><p><strong>INCORRECT:</strong> \"Implement auto-scaling for the RDS instance based on connection count\" is incorrect.</p><p>Auto-scaling in response to connection count is not a feature provided by RDS.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda to create a connection pool for the RDS instance\" is incorrect.</p><p>The best solution is to use RDS proxy which is designed for creating a pool of connections. Lambda may not be the best solution for this problem as it could be costly and has a limitation in execution time.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/\">https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 2802,
                        "content": "<p>Use AWS Lambda to create a connection pool for the RDS instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2803,
                        "content": "<p>Use Amazon RDS Proxy and update the Lambda function to connect to the proxy.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2804,
                        "content": "<p>Implement auto-scaling for the RDS instance based on connection count.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2805,
                        "content": "<p>Double the RAM and vCPUs of the RDS instance.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 683,
            "attributes": {
                "createdAt": "2023-09-07T08:51:20.003Z",
                "updatedAt": "2023-09-07T08:51:20.003Z",
                "content": "<p>A developer is working on an application that must save hundreds of sensitive files. The application needs to encrypt each file using a unique key before storing it.</p><p>What should the developer do to implement this in the application?</p>",
                "answerExplanation": "<p>The AWS KMS GenerateDataKey API returns a plaintext version of the key and a copy of the key encrypted under a KMS key. The application can use the plaintext key to encrypt data, and then discard it from memory as soon as possible to reduce potential exposure.</p><p><strong>CORRECT: </strong>\"Use the AWS KMS GenerateDataKey API to acquire a data key, use the data key to encrypt the data, and store both the encrypted data key and the data\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Utilize the AWS Key Management Service (KMS) Encrypt API to secure the data, storing both the encrypted data key and the actual data\" is incorrect.</p><p>The AWS KMS Encrypt API could be used, but it doesn't generate a unique key for each file. It encrypts data under a specified KMS key, which isn't the requirement here.</p><p><strong>INCORRECT:</strong> \"Use a crypto library to generate a unique encryption key for the application, employ the encryption key to secure the data, and store the encrypted data\" is incorrect.</p><p>Although a cryptography library can be used to generate a key and encrypt the data, it puts the onus of secure key management on the application. It's a less secure and more complex solution compared to using KMS.</p><p><strong>INCORRECT:</strong> \"Upload the data to an Amazon S3 bucket employing server-side encryption with a key from AWS KMS\" is incorrect.</p><p>The requirement is to encrypt data within the application prior to storage. Uploading to an S3 bucket using server-side encryption doesn't fulfill this requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
                "options": [
                    {
                        "id": 2806,
                        "content": "<p>Upload the data to an Amazon S3 bucket employing server-side encryption with a key from AWS KMS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2807,
                        "content": "<p>Use a crypto library to generate a unique encryption key for the application, employ the encryption key to secure the data, and store the encrypted data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2808,
                        "content": "<p>Use the AWS KMS GenerateDataKey API to acquire a data key, use the data key to encrypt the data, and store both the encrypted data key and the data.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2809,
                        "content": "<p>Utilize the AWS Key Management Service (KMS) Encrypt API to secure the data, storing both the encrypted data key and the actual data.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 684,
            "attributes": {
                "createdAt": "2023-09-07T08:51:20.074Z",
                "updatedAt": "2023-09-07T08:51:20.074Z",
                "content": "<p>A company runs a legacy application that uses an XML-based SOAP interface. The company needs to expose the functionality of the service to external customers and plans to use Amazon API Gateway.</p><p>How can a Developer configure the integration?</p>",
                "answerExplanation": "<p>In API Gateway, an API's method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, the backend may return an integration response payload different from the method response payload, as expected by the frontend.</p><p>API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.</p><p><strong>CORRECT: </strong>\"Create a RESTful API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a RESTful API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through an Application Load Balancer\" is incorrect. The API Gateway cannot process the XML SOAP data and cannot pass it through an ALB.</p><p><strong>INCORRECT:</strong> \"Create a SOAP API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using AWS Lambda\" is incorrect. API Gateway does not support SOAP APIs.</p><p><strong>INCORRECT:</strong> \"Create a SOAP API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through a Network Load Balancer\" is incorrect. API Gateway does not support SOAP APIs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/request-response-data-mappings.html\">https://docs.aws.amazon.com/apigateway/latest/Developerguide/request-response-data-mappings.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/certification-training/aws-developer-associate/aws-networking-and-content-delivery/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 2810,
                        "content": "<p>Create a SOAP API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using AWS Lambda.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2811,
                        "content": "<p>Create a RESTful API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through an Application Load Balancer.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2812,
                        "content": "<p>Create a RESTful API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2813,
                        "content": "<p>Create a SOAP API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through a Network Load Balancer.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 685,
            "attributes": {
                "createdAt": "2023-09-07T08:51:20.152Z",
                "updatedAt": "2023-09-07T08:51:20.152Z",
                "content": "<p>A Developer is creating a serverless application that uses an Amazon DynamoDB table. The application must make idempotent, all-or-nothing operations for multiple groups of write actions.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>TransactWriteItems is a synchronous and idempotent write operation that groups up to 25 write actions in a single all-or-nothing operation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same Region. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions are completed atomically so that either all of them succeed or none of them succeeds.</p><p>A TransactWriteItems operation differs from a BatchWriteItem operation in that all the actions it contains must be completed successfully, or no changes are made at all. With a BatchWriteItem operation, it is possible that only some of the actions in the batch succeed while the others do not.</p><p><strong>CORRECT: </strong>\"Update the items in the table using the TransactWriteltems operation to group the changes\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Update the items in the table using the BatchWriteltem operation and configure idempotency at the table level\" is incorrect. As explained above, the TransactWriteItems operation must be used.</p><p><strong>INCORRECT:</strong> \"Enable DynamoDB streams and capture new images. Update the items in the table using the BatchWriteltem\" is incorrect. DynamoDB streams will not assist with making idempotent write operations.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SQS FIFO queue and use the SendMessageBatch operation to group the changes\" is incorrect. Amazon SQS should not be used as it does not assist and this solution is supposed to use a DynamoDB table</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_TransactWriteItems.html\">https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_TransactWriteItems.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 2814,
                        "content": "<p>Update the items in the table using the BatchWriteltem operation and configure idempotency at the table level.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2815,
                        "content": "<p>Create an Amazon SQS FIFO queue and use the SendMessageBatch operation to group the changes.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2816,
                        "content": "<p>Enable DynamoDB streams and capture new images. Update the items in the table using the BatchWriteltem.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2817,
                        "content": "<p>Update the items in the table using the TransactWriteltems operation to group the changes.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 686,
            "attributes": {
                "createdAt": "2023-09-07T08:51:20.227Z",
                "updatedAt": "2023-09-07T08:51:20.227Z",
                "content": "<p>A web application runs on a fleet of Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). A developer needs a store for session data so it can be reliably served across multiple requests.</p><p>Where is the best place to store the session data?</p>",
                "answerExplanation": "<p>ElastiCache is a good solution for storing session state data as it has very low latency and high performance. DynamoDB is often used for the same purpose. In this case the session data can be written to the ElastiCache cluster and can then be easily retrieved from subsequent sessions on the same or a different EC2 instance. This decouples the data from the individual instance so if an instance fails, the data is not lost.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-29_11-54-07-18affbc17863d347c7b505ea988c337d.jpg\"><p><strong>CORRECT: </strong>\"Write the data to an Amazon ElastiCache cluster\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Write the data to a shared Amazon EBS volume\" is incorrect.</p><p>You cannot share an EBS volume except under specific circumstances and even then, you must share the volume from an EC2 instance which could fail.</p><p><strong>INCORRECT:</strong> \"Write the data to the root of the filesystem\" is incorrect.</p><p>This will result in data loss if the instance fails.</p><p><strong>INCORRECT:</strong> \"Write the data to the local instance store volumes\" is incorrect.</p><p>This will result in data loss if the instance fails.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/caching/session-management/\">https://aws.amazon.com/caching/session-management/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 2818,
                        "content": "<p>Write the data to an Amazon ElastiCache cluster.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2819,
                        "content": "<p>Write the data to the root of the filesystem.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2820,
                        "content": "<p>Write the data to a shared Amazon EBS volume.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2821,
                        "content": "<p>Write the data to the local instance store volumes.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 687,
            "attributes": {
                "createdAt": "2023-09-07T08:51:20.309Z",
                "updatedAt": "2023-09-07T08:51:20.309Z",
                "content": "<p>A company is deploying a static website hosted from an Amazon S3 bucket. The website must support encryption in-transit for website visitors.</p><p>Which combination of actions must the Developer take to meet this requirement? (Select TWO.)</p>",
                "answerExplanation": "<p>Amazon S3 static websites use the HTTP protocol only and you cannot enable HTTPS. To enable HTTPS connections to your S3 static website, use an Amazon CloudFront distribution that is configured with an SSL/TLS certificate. This will ensure that connections between clients and the CloudFront distribution are encrypted in-transit as per the requirements.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudFront distribution. Set the S3 bucket as an origin\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Configure an Amazon CloudFront distribution with an SSL/TLS certificate\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF WebACL with a secure listener\" is incorrect. You cannot configure a secure listener on a WebACL.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon CloudFront distribution with an AWS WAF WebACL\" is incorrect. This will not enable encrypted connections.</p><p><strong>INCORRECT:</strong> \"Configure the S3 bucket with an SSL/TLS certificate\" is incorrect. You cannot manually add SSL/TLS certificates to Amazon S3, and it is not possible to directly configure an S3 bucket that is configured as a static website to accept encrypted connections.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 2822,
                        "content": "<p>Configure the S3 bucket with an SSL/TLS certificate.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2823,
                        "content": "<p>Configure an Amazon CloudFront distribution with an SSL/TLS certificate.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2824,
                        "content": "<p>Create an AWS WAF WebACL with a secure listener.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2825,
                        "content": "<p>Configure an Amazon CloudFront distribution with an AWS WAF WebACL.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2826,
                        "content": "<p>Create an Amazon CloudFront distribution. Set the S3 bucket as an origin.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 688,
            "attributes": {
                "createdAt": "2023-09-07T08:51:20.396Z",
                "updatedAt": "2023-09-07T08:51:20.396Z",
                "content": "<p>An e-commerce web application that shares session state on-premises is being migrated to AWS. The application must be fault tolerant, natively highly scalable, and any service interruption should not affect the user experience.</p><p>What is the best option to store the session state?</p>",
                "answerExplanation": "<p>There are various ways to manage user sessions including storing those sessions locally to the node responding to the HTTP request or designating a layer in your architecture which can store those sessions in a scalable and robust manner. Common approaches used include utilizing Sticky sessions or using a Distributed Cache for your session management.</p><p>In this scenario, a distributed cache is suitable for storing session state data. ElastiCache can perform this role and with the Redis engine replication is also supported. Therefore, the solution is fault-tolerant and natively highly scalable.</p><p><strong>CORRECT: </strong>\"Store the session state in Amazon ElastiCache\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Store the session state in Amazon CloudFront\" is incorrect as CloudFront is not suitable for storing session state data, it is used for caching content for better global performance.</p><p><strong>INCORRECT:</strong> \"Store the session state in Amazon S3\" is incorrect as though you can store session data in Amazon S3 and replicate the data to another bucket, this would result in a service interruption if the S3 bucket was not accessible.</p><p><strong>INCORRECT:</strong> \"Enable session stickiness using elastic load balancers\" is incorrect as this feature directs sessions from a specific client to a specific EC2 instances. Therefore, if the instance fails the user must be redirected to another EC2 instance and the session state data would be lost.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/caching/session-management/\">https://aws.amazon.com/caching/session-management/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 2827,
                        "content": "<p>Enable session stickiness using elastic load balancers</p>",
                        "isValid": false
                    },
                    {
                        "id": 2828,
                        "content": "<p>Store the session state in Amazon ElastiCache</p>",
                        "isValid": true
                    },
                    {
                        "id": 2829,
                        "content": "<p>Store the session state in Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 2830,
                        "content": "<p>Store the session state in Amazon CloudFront</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 689,
            "attributes": {
                "createdAt": "2023-09-07T08:51:20.473Z",
                "updatedAt": "2023-09-07T08:51:20.473Z",
                "content": "<p>An application asynchronously invokes an AWS Lambda function. The application has recently been experiencing occasional errors that result in failed invocations. A developer wants to store the messages that resulted in failed invocations such that the application can automatically retry processing them.</p><p>What should the developer do to accomplish this goal with the LEAST operational overhead?</p>",
                "answerExplanation": "<p>Amazon SQS supports <em>dead-letter queues</em> (DLQ), which other queues (<em>source queues</em>) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate unconsumed messages to determine why their processing doesn't succeed.</p><p>The <em>redrive policy</em> specifies the <em>source queue</em>, the <em>dead-letter queue</em>, and the conditions under which Amazon SQS moves messages from the former to the latter if the consumer of the source queue fails to process a message a specified number of times.</p><p>You can set your DLQ as an event source to the Lambda function to drain your DLQ. This will ensure that all failed invocations are automatically retried.</p><p><strong>CORRECT: </strong>\"Configure a redrive policy on an Amazon SQS queue. Set the dead-letter queue as an event source to the Lambda function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure logging to an Amazon CloudWatch Logs group. Configure Lambda to read failed invocation events from the log group\" is incorrect.</p><p>The information in the logs may not be sufficient for processing the event. This is not an automated or ideal solution.</p><p><strong>INCORRECT:</strong> \"Configure Amazon EventBridge to send the messages to Amazon SNS to initiate the Lambda function again\" is incorrect.</p><p>Amazon EventBridge can be configured as a failure destination and can send to SNS. SNS can also be configured with Lambda as a target. However, this solution requires more operational overhead compared to using a DLQ.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon S3 bucket as a destination for failed invocations. Configure event notifications to trigger the Lambda function to process the events\" is incorrect.</p><p>S3 is not a supported failure destination. Supported destinations are Amazon SNS, Amazon SQS, and Amazon EventBridge.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/\">https://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/</a></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rule-dlq.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rule-dlq.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 2831,
                        "content": "<p>Configure logging to an Amazon CloudWatch Logs group. Configure Lambda to read failed invocation events from the log group.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2832,
                        "content": "<p>Configure an Amazon S3 bucket as a destination for failed invocations. Configure event notifications to trigger the Lambda function to process the events.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2833,
                        "content": "<p>Configure a redrive policy on an Amazon SQS queue. Set the dead-letter queue as an event source to the Lambda function.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2834,
                        "content": "<p>Configure Amazon EventBridge to send the messages to Amazon SNS to initiate the Lambda function again.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 690,
            "attributes": {
                "createdAt": "2023-09-07T08:51:20.560Z",
                "updatedAt": "2023-09-07T08:51:20.560Z",
                "content": "<p>A developer is partitioning data using Athena to improve performance when performing queries. What are two things the analyst can do that would counter any benefit of using partitions? (Select TWO.)</p>",
                "answerExplanation": "<p>There is a cost associated with partitioning data. A higher number of partitions can also increase the overhead from retrieving and processing the partition metadata. Multiple smaller files can counter the benefit of using partitioning. If your data is heavily skewed to one partition value, and most queries use that value, then the overhead may wipe out the initial benefit.</p><p><strong>CORRECT: </strong>\"Segmenting data too finely\" is a correct answer (as explained above.)</p><p><strong>CORRECT:</strong> \"Skewing data heavily to one partition value\" is a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Storing the data in S3\" is incorrect. Data must be stored in S3 buckets.</p><p><strong>INCORRECT:</strong> \"Creating partitions directly from data source \" is incorrect. Athena can pull data directly from the S3 source.</p><p><strong>INCORRECT:</strong> \"Using a Hive-style partition format\" is incorrect. Athena is compatible with Hive-style partition formats.</p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
                "options": [
                    {
                        "id": 2835,
                        "content": "<p>Skewing data heavily to one partition value.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2836,
                        "content": "<p>Using a Hive-style partition format.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2837,
                        "content": "<p>Creating partitions directly from data source.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2838,
                        "content": "<p>Storing the data in S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2839,
                        "content": "<p>Segmenting data too finely.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 691,
            "attributes": {
                "createdAt": "2023-09-07T08:51:20.633Z",
                "updatedAt": "2023-09-07T08:51:20.633Z",
                "content": "<p>A Developer is designing a fault-tolerant application that will use Amazon EC2 instances and an Elastic Load Balancer. The Developer needs to ensure that if an EC2 instance fails session data is not lost. How can this be achieved?</p>",
                "answerExplanation": "<p>For this scenario the key requirement is to ensure the data is not lost. Therefore, the data must be stored in a durable data store outside of the EC2 instances. Amazon DynamoDB is a suitable solution for storing session data. DynamoDB has a session handling capability for multiple languages as in the below example for PHP:</p><p>â€œThe <strong>DynamoDB Session Handler</strong> is a custom session handler for PHP that allows developers to use Amazon DynamoDB as a session store. Using DynamoDB for session storage alleviates issues that occur with session handling in a distributed web application by moving sessions off of the local file system and into a shared location. DynamoDB is fast, scalable, easy to setup, and handles replication of your data automatically.â€</p><p>Therefore, the best answer is to use DynamoDB to store the session data.</p><p><strong>CORRECT: </strong>\"Use Amazon DynamoDB to perform scalable session handling\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable Sticky Sessions on the Elastic Load Balancer\" is incorrect. Sticky sessions attempts to direct a user that has reconnected to the application to the same EC2 instance that they connected to previously. However, this does not ensure that the session data is going to be available.</p><p><strong>INCORRECT:</strong> \"Use an EC2 Auto Scaling group to automatically launch new instances\" is incorrect as this does not provide a solution for storing the session data.</p><p><strong>INCORRECT:</strong> \"Use Amazon SQS to save session data\" is incorrect as Amazon SQS is not suitable for storing session data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html\">https://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 2840,
                        "content": "<p>Use an EC2 Auto Scaling group to automatically launch new instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 2841,
                        "content": "<p>Use Amazon DynamoDB to perform scalable session handling</p>",
                        "isValid": true
                    },
                    {
                        "id": 2842,
                        "content": "<p>Enable Sticky Sessions on the Elastic Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 2843,
                        "content": "<p>Use Amazon SQS to save session data</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 692,
            "attributes": {
                "createdAt": "2023-09-07T08:51:20.719Z",
                "updatedAt": "2023-09-07T08:51:20.719Z",
                "content": "<p>A Development team is involved with migrating an on-premises MySQL database to Amazon RDS. The database usage is very read-heavy. The Development team wants re-factor the application code to achieve optimum read performance for queries.</p><p>How can this objective be met?</p>",
                "answerExplanation": "<p>Amazon RDS uses the MariaDB, MySQL, Oracle, and PostgreSQL DB engines' built-in replication functionality to create a special type of DB instance called a Read Replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the Read Replica.</p><p>You can reduce the load on your source DB instance by routing read queries from your applications to the Read Replica. Using Read Replicas, you can elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.</p><p>In the image below a primary Amazon RDS database server allows reads and writes while a Read Replica can be used for running read-only workloads such as BI/reporting. This reduces the load on the primary database.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-36-41-a8756f8d4c1a46eefffad5b79b56ed16.jpg\"></p><p>It is necessary to add logic to your code to direct read traffic to the Read Replica and write traffic to the primary database. Therefore, in this scenario the Development team will need to â€œAdd a connection string to use an Amazon RDS read replica for read queriesâ€.</p><p><strong>CORRECT: </strong>\"Add a connection string to use an Amazon RDS read replica for read queries\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Add database retries to the code and vertically scale the Amazon RDS database\" is incorrect as this is not a good way to scale reads as you will likely hit a ceiling at some point in terms of cost or instance type. Scaling reads can be better implemented with horizontal scaling using a Read Replica.</p><p><strong>INCORRECT:</strong> \"Use Amazon RDS with a multi-AZ deployment\" is incorrect as this creates a standby copy of the database in another AZ that can be failed over to in a failure scenario. This is used for DR not (at least not primarily) used for scaling performance. It is possible for certain RDS engines to use a multi-AZ standby as a read replica however the requirements in this solution do not warrant this configuration.</p><p><strong>INCORRECT:</strong> \"Add a connection string to use a read replica on an Amazon EC2 instance\" is incorrect as Read Replicas are something you create on Amazon RDS, not on an EC2 instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 2844,
                        "content": "<p>Add a connection string to use a read replica on an Amazon EC2 instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 2845,
                        "content": "<p>Use Amazon RDS with a multi-AZ deployment</p>",
                        "isValid": false
                    },
                    {
                        "id": 2846,
                        "content": "<p>Add a connection string to use an Amazon RDS read replica for read queries</p>",
                        "isValid": true
                    },
                    {
                        "id": 2847,
                        "content": "<p>Add database retries to the code and vertically scale the Amazon RDS database</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 693,
            "attributes": {
                "createdAt": "2023-09-07T08:51:20.799Z",
                "updatedAt": "2023-09-07T08:51:20.799Z",
                "content": "<p>To reduce the cost of API actions performed on an Amazon SQS queue, a Developer has decided to implement long polling. Which of the following modifications should the Developer make to the API actions?</p>",
                "answerExplanation": "<p>The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses short polling, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response. You can use long polling to reduce your costs while allowing your consumers to receive messages as soon as they arrive in the queue. </p><p>When you consume messages from a queue using short polling, Amazon SQS samples a subset of its servers (based on a weighted random distribution) and returns messages from only those servers. Thus, a particular <code>ReceiveMessage</code> request might not return all of your messages. However, if you have fewer than 1,000 messages in your queue, a subsequent request will return your messages. If you keep consuming from your queues, Amazon SQS samples all of its servers, and you receive all of your messages.</p><p>The following diagram shows the short-polling behavior of messages returned from a standard queue after one of your system components makes a receive request. Amazon SQS samples several of its servers (in gray) and returns messages A, C, D, and B from these servers. Message E isn't returned for this request but is returned for a subsequent request.</p><p><br></p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-06-05_17-51-10-8ff4de7426893136b68645f9f1211ff8.jpg\"><p><br></p><p>When the wait time for the <code>ReceiveMessage</code> API action is greater than 0, <em>long polling</em> is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a <code>ReceiveMessage</code> request) and false empty responses (when messages are available but aren't included in a response).</p><p>Long polling occurs when the <code>WaitTimeSeconds</code> parameter of a <code>ReceiveMessage</code> request is set to a value greater than 0 in one of two ways:</p><ul><li><p>The <code>ReceiveMessage</code> call sets <code>WaitTimeSeconds</code> to a value greater than 0.</p></li><li><p>The <code>ReceiveMessage</code> call doesnâ€™t set <code>WaitTimeSeconds</code>, but the queue attribute <code>ReceiveMessageWaitTimeSeconds</code> is set to a value greater than 0.</p></li></ul><p><strong>Therefore, the Developer should </strong>set the <code>ReceiveMessage</code> API with a <code>WaitTimeSeconds</code> of 20.</p><p><strong>CORRECT: </strong>\"Set the <code>ReceiveMessage</code> API with a <code>WaitTimeSeconds</code> of 20\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Set the <code>SetQueueAttributes</code> API with a <code>DelaySeconds</code> of 20\" is incorrect as this would be used to configure a delay queue where the delivery of messages in the queue is delayed.</p><p><strong>INCORRECT:</strong> \"Set the <code>ReceiveMessage</code> API with a <code>VisibilityTimeout</code> of 30\" is incorrect as this would configure the visibility timeout which is the length of time a message that has been received is invisible.</p><p><strong>INCORRECT:</strong> \"Set the <code>SetQueueAttributes</code> with a <code>MessageRetentionPeriod</code> of 60\" is incorrect as this would configure how long messages are retained in the queue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html</a></p><p><br></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 2848,
                        "content": "<p>Set the <code>ReceiveMessage</code> API with a <code>VisibilityTimeout</code> of 30</p>",
                        "isValid": false
                    },
                    {
                        "id": 2849,
                        "content": "<p>Set the <code>SetQueueAttributes</code> API with a <code>DelaySeconds</code> of 20</p>",
                        "isValid": false
                    },
                    {
                        "id": 2850,
                        "content": "<p>Set the <code>SetQueueAttributes</code> with a <code>MessageRetentionPeriod</code> of 60</p>",
                        "isValid": false
                    },
                    {
                        "id": 2851,
                        "content": "<p>Set the <code>ReceiveMessage</code> API with a <code>WaitTimeSeconds</code> of 20</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 694,
            "attributes": {
                "createdAt": "2023-09-07T08:51:20.881Z",
                "updatedAt": "2023-09-07T08:51:20.881Z",
                "content": "<p>An application serves customers in several different geographical regions. Information about the location users connect from is written to logs stored in Amazon CloudWatch Logs. The company needs to publish an Amazon CloudWatch custom metric that tracks connections for each location.</p><p>Which approach will meet these requirements?</p>",
                "answerExplanation": "<p>You can search and filter the log data coming into CloudWatch Logs by creating one or more <em>metric filters</em>. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p><p>When you create a metric from a log filter, you can also choose to assign dimensions and a unit to the metric. In this case, the company can assign a dimension that uses the location information.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch metric filter to extract metrics from the log files with location as a dimension\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch Logs Insights query to extract the location information from the logs and to create a custom metric with location as a dimension\" is incorrect. You cannot create a custom metric through CloudWatch Logs Insights.</p><p><strong>INCORRECT:</strong> \"Configure a CloudWatch Events rule that creates a custom metric from the CloudWatch Logs group\" is incorrect. You cannot create a custom metric using a CloudWatch Events rule.</p><p><strong>INCORRECT:</strong> \"Stream data to an Amazon Elasticsearch cluster in near-real time and export a custom metric\" is incorrect. This is not a valid way of creating a custom metric in CloudWatch.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 2852,
                        "content": "<p>Stream data to an Amazon Elasticsearch cluster in near-real time and export a custom metric.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2853,
                        "content": "<p>Create a CloudWatch Logs Insights query to extract the location information from the logs and to create a custom metric with location as a dimension.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2854,
                        "content": "<p>Configure a CloudWatch Events rule that creates a custom metric from the CloudWatch Logs group.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2855,
                        "content": "<p>Create a CloudWatch metric filter to extract metrics from the log files with location as a dimension.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 695,
            "attributes": {
                "createdAt": "2023-09-07T08:51:20.960Z",
                "updatedAt": "2023-09-07T08:51:20.960Z",
                "content": "<p>An ecommerce company manages a storefront that uses an Amazon API Gateway API which exposes an AWS Lambda function. The Lambda functions processes orders and stores the orders in an Amazon RDS for MySQL database. The number of transactions increases sporadically during marketing campaigns, and then goes close to zero during quite times.</p><p>How can a developer increase the elasticity of the system MOST cost-effectively?</p>",
                "answerExplanation": "<p>The most efficient solution would be to use Aurora Auto Scaling and configure the scaling events to happen based on target metric. The metric to use is <strong>Average connections of Aurora Replicas</strong> which will create a policy based on the average number of connections to Aurora Replicas.</p><p>This will ensure that the Aurora replicas scale based on actual numbers of connections to the replicas which will vary based on how busy the storefront is and how many transactions are being processed.</p><p><strong>CORRECT: </strong>\"Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on average connections of Aurora Replicas\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on average CPU utilization\" is incorrect.</p><p>The better metric to use for this situation would be the number of connections to Aurora Replicas as that is the metric that has the closest correlation to the number of transactions being executed.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic. Publish transactions to the topic configure an SQS queue as a destination. Configure Lambda to process transactions from the queue\" is incorrect.</p><p>This is highly inefficient. There is no need for an SNS topic in this situation.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SQS queue. Publish transactions to the queue and set the queue to invoke the Lambda function. Set the reserved concurrency of the Lambda function to be equal to the max number of database connections\" is incorrect.</p><p>This would be less cost effective as you would be paying for the reserved concurrency at all times.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
                "options": [
                    {
                        "id": 2856,
                        "content": "<p>Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on average CPU utilization.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2857,
                        "content": "<p>Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on average connections of Aurora Replicas.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2858,
                        "content": "<p>Create an Amazon SQS queue. Publish transactions to the queue and set the queue to invoke the Lambda function. Set the reserved concurrency of the Lambda function to be equal to the max number of database connections.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2859,
                        "content": "<p>Create an Amazon SNS topic. Publish transactions to the topic configure an SQS queue as a destination. Configure Lambda to process transactions from the queue.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 696,
            "attributes": {
                "createdAt": "2023-09-07T08:51:21.069Z",
                "updatedAt": "2023-09-07T08:51:21.069Z",
                "content": "<p>A business is providing its clients read-only permissions to items within an Amazon S3 bucket, utilizing IAM permissions to limit access to this S3 bucket. Clients are only permitted to access their specific files. Regulatory compliance necessitates the enforcement of in-transit encryption during communication with Amazon S3.</p><p>What solution will fulfill these criteria?</p>",
                "answerExplanation": "<p>By adding a condition to the S3 bucket policy that requires aws:SecureTransport, you are mandating that all interactions with the bucket must be encrypted in transit using SSL/TLS.</p><p><strong>CORRECT: </strong>\"Update the S3 bucket policy to include a condition that requires aws:SecureTransport for all actions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Activate Amazon S3 server-side encryption to enforce encryption during transit\" is incorrect.</p><p>Server-side encryption for S3 secures the data at rest, not during transit.</p><p><strong>INCORRECT:</strong> \"Assign IAM roles enforcing SSL/TLS encryption to each customer\" is incorrect.</p><p>Although IAM roles are utilized to manage access to AWS resources, they do not inherently mandate SSL/TLS encryption for interactions with the resources.</p><p><strong>INCORRECT:</strong> \"Enable the Amazon S3 Transfer Acceleration feature to ensure encryption during transit\" is incorrect.</p><p>While Amazon S3 Transfer Acceleration does use SSL/TLS, it is designed to expedite the transfer of data over long distances between a user and an S3 bucket, not to mandate encryption in transit for all interactions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-use-case-2\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-use-case-2</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 2860,
                        "content": "<p>Assign IAM roles enforcing SSL/TLS encryption to each customer.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2861,
                        "content": "<p>Enable the Amazon S3 Transfer Acceleration feature to ensure encryption during transit.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2862,
                        "content": "<p>Update the S3 bucket policy to include a condition that requires aws:SecureTransport for all actions.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2863,
                        "content": "<p>Activate Amazon S3 server-side encryption to enforce encryption during transit.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 697,
            "attributes": {
                "createdAt": "2023-09-07T08:51:21.153Z",
                "updatedAt": "2023-09-07T08:51:21.153Z",
                "content": "<p>An organization is hosting a website on an Amazon EC2 instance in a public subnet. The website should allow public access for HTTPS traffic on TCP port 443 but should only accept SSH traffic on TCP port 22 from a corporate address range accessible over a VPN.</p><p>Which security group configuration will support both requirements?</p>",
                "answerExplanation": "<p>Allowing traffic from 0.0.0.0/0 to port 443 will allow any traffic from the internet to access the website. Limiting the IP address to 192.168.0.0/16 for port 22 will only allow local organizational traffic.</p><p><strong>CORRECT</strong>: \"Allow traffic to port 443 from 0.0.0.0/0 and allow traffic to port 22 from 192.168.0.0/16\" is the correct answer (as explained above.)</p><p><strong>INCORRECT</strong>: â€œAllow traffic to port 22 from 0.0.0.0/0 and allow traffic to port 443 from 192.168.0.0/16\" is incorrect. This will allow traffic from the Internet to port 22 and allow traffic to port 443 from the corporate address block only (192.168.0.0/16).</p><p><strong>INCORRECT</strong>: \"Allow traffic to both port 443 and port 22 from the VPC CIDR block\" is incorrect. This would not satisfy either requirement as internet-based users will not be able to access the website and corporate users will not be able to manage the instance via SSH.</p><p><strong>INCORRECT</strong>: \"Allow traffic to both port 443 and port 22 from 0.0.0.0/0 and 192.168.0.0/16\" is incorrect. This does not satisfy the requirement to restrict access to port 22 to only the corporate address block.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 2864,
                        "content": "<p>Allow traffic to port 22 from 0.0.0.0/0 and allow traffic to port 443 from 192.168.0.0/16.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2865,
                        "content": "<p>Allow traffic to both port 443 and port 22 from the VPC CIDR block.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2866,
                        "content": "<p>Allow traffic to both port 443 and port 22 from 0.0.0.0/0 and 192.168.0.0/16.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2867,
                        "content": "<p>Allow traffic to port 443 from 0.0.0.0/0 and allow traffic to port 22 from 192.168.0.0/16.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 698,
            "attributes": {
                "createdAt": "2023-09-07T08:51:21.233Z",
                "updatedAt": "2023-09-07T08:51:21.233Z",
                "content": "<p>A developer is looking to verify that redirects are performing as expected. What is the most efficient way that the developer can access the web logs and perform an analysis on them?</p>",
                "answerExplanation": "<p>Logs can be stored in an S3 bucket to be retained for review and inspection. Athena is an interactive query service that can work directly with S3 and run ad-hoc SQL queries.</p><p><strong>CORRECT: </strong>\"Store the logs in a S3 bucket and use Athena to run SQL queries\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the logs in EBS and use Athena to run SQL queries\" is incorrect. As explained above log records are stored in S3 and Athena is compatible to perform queries directly in S3 buckets.</p><p><strong>INCORRECT:</strong> \"Store the logs in an Instance Store and use Athena to run SQL queries\" is incorrect. Instance stores are ephemeral and temporarily store data for EC2 instances. They cannot be used with Athena.</p><p><strong>INCORRECT:</strong> \"Store the logs in EFS and use Athena to run SQL queries\" is incorrect. As explained above log records are stored in S3 and Athena is compatible to perform queries directly in S3 buckets.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/what-is.html\">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
                "options": [
                    {
                        "id": 2868,
                        "content": "<p>Store the logs in EBS and use Athena to run SQL queries.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2869,
                        "content": "<p>Store the logs in a S3 bucket and use Athena to run SQL queries.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2870,
                        "content": "<p>Store the logs in EFS and use Athena to run SQL queries.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2871,
                        "content": "<p>Store the logs in an Instance Store and use Athena to run SQL queries.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 699,
            "attributes": {
                "createdAt": "2023-09-07T08:51:21.314Z",
                "updatedAt": "2023-09-07T08:51:21.314Z",
                "content": "<p>A developer is responsible for a business critical application that uses Amazon DynamoDB as its main data repository. This DynamoDB table holds millions of records and handles high volumes of requests. The developer must implement near-real time processing on the records as soon as they are inserted or modified in the DynamoDB table.</p><p>What's the most efficient way to introduce this capability with MINIMUM modification to the existing application code?</p>",
                "answerExplanation": "<p>AWS Lambda can be triggered by DynamoDB Streams to automatically process changes to the DynamoDB table, ensuring near-real-time processing without substantial changes to the application code.</p><p><strong>CORRECT: </strong>\"Use AWS Lambda triggered by DynamoDB Streams to process the documents\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon SQS to queue incoming data and process it using Amazon EC2 instances\" is incorrect.</p><p>Amazon SQS would require significant changes to the application code to integrate the queuing and processing mechanism. Moreover, it would not guarantee near-real-time processing.</p><p><strong>INCORRECT:</strong> \"Modify the application code to add processing logic after each DynamoDB write operation\" is incorrect.</p><p>Modifying the application code to include processing logic after each write operation would entail significant changes to the code and may also slow down write operations.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon Kinesis Data Stream to process updates from the DynamoDB table\" is incorrect.</p><p>Amazon Kinesis Data Streams can be used to capture the changes, but consumers are required to perform the processing, and this is not mentioned in the solution. The simplest solution is to use DynamoDB streams with Lambda.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 2872,
                        "content": "<p>Modify the application code to add processing logic after each DynamoDB write operation.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2873,
                        "content": "<p>Set up an Amazon Kinesis Data Stream to process updates from the DynamoDB table.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2874,
                        "content": "<p>Use AWS Lambda triggered by DynamoDB Streams to process the documents.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2875,
                        "content": "<p>Use Amazon SQS to queue incoming data and process it using Amazon EC2 instances.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 700,
            "attributes": {
                "createdAt": "2023-09-07T08:51:21.384Z",
                "updatedAt": "2023-09-07T08:51:21.384Z",
                "content": "<p>A Development team would use a GitHub repository and would like to migrate their application code to AWS CodeCommit.<br>What needs to be created before they can migrate a cloned repository to CodeCommit over HTTPS?</p>",
                "answerExplanation": "<p>AWS CodeCommit is a managed version control service that hosts private Git repositories in the AWS cloud. To use CodeCommit, you configure your Git client to communicate with CodeCommit repositories. As part of this configuration, you provide IAM credentials that CodeCommit can use to authenticate you. IAM supports CodeCommit with three types of credentials:</p><p>Git credentials, an IAM -generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.</p><p>SSH keys, a locally generated public-private key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.</p><p>AWS access keys, which you can use with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.</p><p>In this scenario the Development team need to connect to CodeCommit using HTTPS so they need either AWS access keys to use the AWS CLI or Git credentials generated by IAM. Access keys are not offered as an answer choice so the best answer is that they need to create a set of Git credentials generated with IAM</p><p><strong>CORRECT: </strong>\"A set of Git credentials generated with IAM\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"A GitHub secure authentication token\" is incorrect as they need to authenticate to AWS CodeCommit, not GitHub (they have already accessed and cloned the repository).</p><p><strong>INCORRECT:</strong> \"A public and private SSH key file\" is incorrect as these are used to communicate with CodeCommit repositories using SSH, not HTTPS.</p><p><strong>INCORRECT:</strong> \"An Amazon EC2 IAM role with CodeCommit permissions\" is incorrect as you need the Git credentials generated through IAM to connect to CodeCommit.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 2876,
                        "content": "<p>A set of Git credentials generated with IAM</p>",
                        "isValid": true
                    },
                    {
                        "id": 2877,
                        "content": "<p>A GitHub secure authentication token</p>",
                        "isValid": false
                    },
                    {
                        "id": 2878,
                        "content": "<p>An Amazon EC2 IAM role with CodeCommit permissions</p>",
                        "isValid": false
                    },
                    {
                        "id": 2879,
                        "content": "<p>A public and private SSH key file</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 701,
            "attributes": {
                "createdAt": "2023-09-07T08:51:21.463Z",
                "updatedAt": "2023-09-07T08:51:21.463Z",
                "content": "<p>A Developer is deploying an application in a microservices architecture on Amazon ECS. The Developer needs to choose the best task placement strategy to MINIMIZE the number of instances that are used. Which task placement strategy should be used?</p>",
                "answerExplanation": "<p>A <em>task placement strategy</em> is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.</p><p>Amazon ECS supports the following task placement strategies:</p><p><strong>binpack</strong> - Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.</p><p><strong>random</strong> - Place tasks randomly.</p><p><strong>spread</strong> - Place tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone. Service tasks are spread based on the tasks from that service. Standalone tasks are spread based on the tasks from the same task group.</p><p>The <strong>binpack</strong> task placement strategy is the most suitable for this scenario as it minimizes the number of instances used which is a requirement for this solution.</p><p><strong>CORRECT: </strong>\"binpack\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"random\" is incorrect as this would assign tasks randomly to EC2 instances which would not result in minimizing the number of instances used.</p><p><strong>INCORRECT:</strong> \"spread\" is incorrect as this would spread the tasks based on a specified value. This is not used for minimizing the number of instances used.</p><p><strong>INCORRECT:</strong> \"weighted\" is incorrect as this is not an ECS task placement strategy. Weighted is associated with Amazon Route 53 routing policies.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 2880,
                        "content": "<p>binpack</p>",
                        "isValid": true
                    },
                    {
                        "id": 2881,
                        "content": "<p>random</p>",
                        "isValid": false
                    },
                    {
                        "id": 2882,
                        "content": "<p>spread</p>",
                        "isValid": false
                    },
                    {
                        "id": 2883,
                        "content": "<p>weighted</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 702,
            "attributes": {
                "createdAt": "2023-09-07T08:51:21.534Z",
                "updatedAt": "2023-09-07T08:51:21.534Z",
                "content": "<p>A developer is updating an Amazon Aurora MySQL database to allow more clients to connect. What database parameter needs to be updated to support a higher number of client connections?</p>",
                "answerExplanation": "<p>The maximum number of connections allowed to an Aurora MySQL DB instance is determined by the max_connections parameter in the instance-level parameter group for the DB instance.</p><p>You can increase the maximum number of connections to your Aurora MySQL DB instance by scaling the instance up to a DB instance class with more memory, or by setting a larger value for the max_connections parameter in the DB parameter group for your instance, up to 16,000.</p><p><strong>CORRECT: </strong>\"max_connections\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"max_allowed_packet\" is incorrect. This parameter sets the maximum size of one packet or any generated or intermediate string.</p><p><strong>INCORRECT:</strong> \"max_join_size\" is incorrect. This option is used to set a limit on the maximum number of row accesses.</p><p><strong>INCORRECT:</strong> \"max_user_connections\" is incorrect. This option limits the number of simultaneous connections that the user can make.</p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-max-connections/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-max-connections/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
                "options": [
                    {
                        "id": 2884,
                        "content": "<p>max_join_size</p>",
                        "isValid": false
                    },
                    {
                        "id": 2885,
                        "content": "<p>max_allowed_packet</p>",
                        "isValid": false
                    },
                    {
                        "id": 2886,
                        "content": "<p>max_connections</p>",
                        "isValid": true
                    },
                    {
                        "id": 2887,
                        "content": "<p>max_user_connections</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 703,
            "attributes": {
                "createdAt": "2023-09-07T08:51:21.619Z",
                "updatedAt": "2023-09-07T08:51:21.619Z",
                "content": "<p>A business operates a web app on Amazon EC2 instances utilizing a bespoke Amazon Machine Image (AMI). They employ AWS CloudFormation for deploying their app, which is currently active in the us-east-1 Region. However, their goal is to extend the deployment to the us-west-1 Region.</p><p>During an initial attempt to create an AWS CloudFormation stack in us-west-1, the action fails, and an error message indicates that the AMI ID does not exist. A developer is tasked with addressing this error through a method that minimizes operational complexity.</p><p>Which action should the developer take?</p>",
                "answerExplanation": "<p>This is the best option as it allows the developer to use the same AMI in a different region with minimal effort and maintenance.</p><p><strong>CORRECT: </strong>\"Copy the AMI from the us-east-1 Region to the us-west-1 Region and use the new AMI ID in the CloudFormation template\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a new AMI in the us-west-1 Region and update the CloudFormation template with the new AMI ID\" is incorrect.</p><p>This is incorrect as creating a new AMI would be operationally complex and time-consuming.</p><p><strong>INCORRECT:</strong> \"Modify the CloudFormation template to refer to the AMI in us-east-1 Region\" is incorrect.</p><p>AMIs are regional resources and cannot be used directly in other regions.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda to create an AMI in the us-west-1 Region during stack creation\" is incorrect.</p><p>This process would add unnecessary complexity and the new AMI would not be identical to the original one.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 2888,
                        "content": "<p>Create a new AMI in the us-west-1 Region and update the CloudFormation template with the new AMI ID.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2889,
                        "content": "<p>Copy the AMI from the us-east-1 Region to the us-west-1 Region and use the new AMI ID in the CloudFormation template.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2890,
                        "content": "<p>Modify the CloudFormation template to refer to the AMI in us-east-1 Region.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2891,
                        "content": "<p>Use AWS Lambda to create an AMI in the us-west-1 Region during stack creation.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 704,
            "attributes": {
                "createdAt": "2023-09-07T08:51:21.705Z",
                "updatedAt": "2023-09-07T08:51:21.705Z",
                "content": "<p>A serverless application uses an AWS Lambda function to process Amazon S3 events. The Lambda function executes 20 times per second and takes 20 seconds to complete each execution.</p><p>How many concurrent executions will the Lambda function require?</p>",
                "answerExplanation": "<p>Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.</p><p>To calculate the concurrency requirements for the Lambda function simply multiply the number of executions per second (20) by the time it takes to complete the execution (20).</p><p>Therefore, for this scenario, the calculation is 20 x 20 = 400.</p><p><strong>CORRECT: </strong>\"400\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"5\" is incorrect. Please use the formula above to calculate concurrency requirements.</p><p><strong>INCORRECT:</strong> \"40\" is incorrect. Please use the formula above to calculate concurrency requirements.</p><p><strong>INCORRECT:</strong> \"20\" is incorrect. Please use the formula above to calculate concurrency requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 2892,
                        "content": "<p>5</p>",
                        "isValid": false
                    },
                    {
                        "id": 2893,
                        "content": "<p>400</p>",
                        "isValid": true
                    },
                    {
                        "id": 2894,
                        "content": "<p>20</p>",
                        "isValid": false
                    },
                    {
                        "id": 2895,
                        "content": "<p>40</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 705,
            "attributes": {
                "createdAt": "2023-09-07T08:51:21.805Z",
                "updatedAt": "2023-09-07T08:51:21.805Z",
                "content": "<p>A Developer is creating a new web application that will be deployed using AWS Elastic Beanstalk from the AWS Management Console. The Developer is about to create a source bundle which will be uploaded using the console.</p><p>Which of the following are valid requirements for creating the source bundle? (Select TWO.)</p>",
                "answerExplanation": "<p>When you use the AWS Elastic Beanstalk console to deploy a new application or an application version, you'll need to upload a source bundle. Your source bundle must meet the following requirements:</p><p>&nbsp; â€¢&nbsp; Consist of a single ZIP file or WAR file (you can include multiple WAR files inside your ZIP file)</p><p>&nbsp; â€¢&nbsp; Not exceed 512 MB</p><p>&nbsp; â€¢&nbsp; Not include a parent folder or top-level directory (subdirectories are fine)</p><p>If you want to deploy a worker application that processes periodic background tasks, your application source bundle must also include a cron.yaml file, but in other cases it is not required.</p><p><strong>CORRECT: </strong>\"Must not include a parent folder or top-level directory\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Must not exceed 512 MB\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Must include the cron.yaml file\" is incorrect. As mentioned above, this is not required in all cases.</p><p><strong>INCORRECT:</strong> \"Must include a parent folder or top-level directory\" is incorrect. A parent folder or top-level directory must NOT be included.</p><p><strong>INCORRECT:</strong> \"Must consist of one or more ZIP files\" is incorrect. You bundle into a single ZIP or WAR file.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
                "options": [
                    {
                        "id": 2896,
                        "content": "<p>Must not include a parent folder or top-level directory.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2897,
                        "content": "<p>Must include a parent folder or top-level directory.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2898,
                        "content": "<p>Must not exceed 512 MB.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2899,
                        "content": "<p>Must consist of one or more ZIP files.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2900,
                        "content": "<p>Must include the cron.yaml file.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 706,
            "attributes": {
                "createdAt": "2023-09-07T08:51:21.877Z",
                "updatedAt": "2023-09-07T08:51:21.877Z",
                "content": "<p>A critical application runs on an Amazon EC2 instance. A Developer has configured a custom Amazon CloudWatch metric that monitors application availability with a data granularity of 1 second. The Developer must be notified within 30 seconds if the application experiences any issues.</p><p>What should the Developer do to meet this requirement?</p>",
                "answerExplanation": "<p>If you set an alarm on a high-resolution metric, you can specify a high-resolution alarm with a period of 10 seconds or 30 seconds, or you can set a regular alarm with a period of any multiple of 60 seconds. There is a higher charge for high-resolution alarms.</p><p>Amazon SNS can then be used to send notifications based on the CloudWatch alarm.</p><p><strong>CORRECT: </strong>\"Configure a high-resolution CloudWatch alarm and use Amazon SNS to send the alert\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Specify an Amazon SNS topic for alarms when issuing the put-metric-data AWS CLI command\" is incorrect. You cannot specify an SNS topic with this CLI command.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Logs Insights and trigger an Amazon Eventbridge rule to send a notification\" is incorrect. Logs Insights cannot be used for alarms or alerting based on custom CloudWatch metrics.</p><p><strong>INCORRECT:</strong> \"Use a default CloudWatch metric, configure an alarm, and use Amazon SNS to send the alert\" is incorrect. There is no default metric that would monitor the application uptime and the resolution would be lower.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#high-resolution-alarms\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#high-resolution-alarms</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 2901,
                        "content": "<p>Specify an Amazon SNS topic for alarms when issuing the put-metric-data AWS CLI command.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2902,
                        "content": "<p>Configure a high-resolution CloudWatch alarm and use Amazon SNS to send the alert.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2903,
                        "content": "<p>Use a default CloudWatch metric, configure an alarm, and use Amazon SNS to send the alert.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2904,
                        "content": "<p>Use Amazon CloudWatch Logs Insights and trigger an Amazon Eventbridge rule to send a notification.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 707,
            "attributes": {
                "createdAt": "2023-09-07T08:51:21.953Z",
                "updatedAt": "2023-09-07T08:51:21.953Z",
                "content": "<p>A company has created a set of APIs using Amazon API Gateway and exposed them to partner companies. The APIs have caching enabled for all stages. The partners require a method of invalidating the cache that they can build into their applications.</p><p>What can the partners use to invalidate the API cache?</p>",
                "answerExplanation": "<p>You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.</p><p>When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled.</p><p>A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the <code>Cache-Control: max-age=0</code> header.</p><p>The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.</p><p>To grant permission for a client, attach a policy of the following format to an IAM execution role for the user.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-31-17-29b0e457a0843d462a0ea3fbe56e5bb8.jpg\"></p><p>This policy allows the API Gateway execution service to invalidate the cache for requests on the specified resource (or resources).</p><p>Therefore, as described above the solution is to get the partners to pass the HTTP header <code>Cache-Control: max-age=0</code>.</p><p><strong>CORRECT: </strong>\"They can pass the HTTP header <code>Cache-Control: max-age=0</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"They can use the query string parameter <code>INVALIDATE_CACHE</code>\" is incorrect. This is not a valid method of invalidating the cache with API Gateway.</p><p><strong>INCORRECT:</strong> \"They must wait for the TTL to expire\" is incorrect as this is not true, you do not need to wait as you can pass the HTTP header <code>Cache-Control: max-age=0</code> whenever you need to in order to invalidate the cache.</p><p><strong>INCORRECT:</strong> \"They can invoke an AWS API endpoint which invalidates the cache\" is incorrect. This is not a valid method of invalidating the cache with API Gateway.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 2905,
                        "content": "<p>They must wait for the TTL to expire</p>",
                        "isValid": false
                    },
                    {
                        "id": 2906,
                        "content": "<p>They can pass the HTTP header <code>Cache-Control: max-age=0</code> </p>",
                        "isValid": true
                    },
                    {
                        "id": 2907,
                        "content": "<p>They can invoke an AWS API endpoint which invalidates the cache</p>",
                        "isValid": false
                    },
                    {
                        "id": 2908,
                        "content": "<p>They can use the query string parameter <code>INVALIDATE_CACHE</code> </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 708,
            "attributes": {
                "createdAt": "2023-09-07T08:51:22.026Z",
                "updatedAt": "2023-09-07T08:51:22.026Z",
                "content": "<p>A company is migrating to the AWS Cloud and needs to build a managed Public Key Infrastructure (PKI) using AWS services. The solution must support the following features:</p><p>&nbsp; &nbsp; &nbsp;- IAM integration.</p><p>&nbsp; &nbsp; &nbsp;- Auditing with AWS CloudTrail.</p><p>&nbsp; &nbsp; &nbsp;- Private certificates.</p><p>&nbsp; &nbsp; &nbsp;- Subordinate certificate authorities (CAs).</p><p>Which solution should the company use to meet these requirements?</p>",
                "answerExplanation": "<p>An AWS Private CA hierarchy provides strong security and restrictive access controls for the most-trusted root CA at the top of the trust chain, while allowing more permissive access and bulk certificate issuance for subordinate CAs lower on the chain.</p><p>With AWS Private CA, you can create private certificates to identify resources and protect data. You can create versatile certificate and CA configurations to identify and protect your resources, including servers, applications, users, devices, and containers.</p><p>The service offers direct integration with AWS IAM, and you can control access to AWS Private CA with IAM policies.</p><p><strong>CORRECT: </strong>\"AWS Private Certificate Authority\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS Certificate Manager\" is incorrect. This service is used for issuing SSL/TLS certificates and is not suitable as a private CA for building a PKI with subordinate CAs.</p><p><strong>INCORRECT:</strong> \"AWS Key Management Service\" is incorrect. This service is used to create and manage the encryption keys used for encrypting data at rest.</p><p><strong>INCORRECT:</strong> \"AWS Secrets Manager\" is incorrect. This service is used for storing secret information such as database connections strings and passwords with API access.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/private-ca/features/\">https://aws.amazon.com/private-ca/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certificate-manager/\">https://digitalcloud.training/aws-certificate-manager/</a></p>",
                "options": [
                    {
                        "id": 2909,
                        "content": "<p>AWS Key Management Service.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2910,
                        "content": "<p>AWS Secrets Manager.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2911,
                        "content": "<p>AWS Certificate Manager.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2912,
                        "content": "<p>AWS Private Certificate Authority.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 709,
            "attributes": {
                "createdAt": "2023-09-07T08:51:22.101Z",
                "updatedAt": "2023-09-07T08:51:22.101Z",
                "content": "<p>A developer is running queries on Hive-compatible partitions in Athena using DDL but is facing time out issues. What is the most effective and efficient way to prevent this from continuing to happen?</p>",
                "answerExplanation": "<p>The MSCK REPAIR TABLE command scans Amazon S3 for Hive compatible partitions that were added to the file system after the table was created. It compares the partitions in the table metadata and the partitions in S3. If new partitions are present in the S3 location that you specified when you created the table, it adds those partitions to the metadata and to the Athena table. MSK REPAIR TABLE can work better than DDL if have more than a few thousand partitions and DDL is facing timeout issues.</p><p><strong>CORRECT: </strong>\"Use the MSCK REPAIR TABLE command to update the metadata in the catalog\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the ALTER TABLE ADD PARTITION command to update the column names\" is incorrect. This DDL command is used to add one or more partition columns.</p><p><strong>INCORRECT:</strong> \"Export the data into DynamoDB to perform queries in a more flexible schema\" is incorrect. DynamoDB is a NoSQL table.</p><p><strong>INCORRECT:</strong> \"Export the data into a JSON document to clean any errors and upload the cleaned data into S3\" is incorrect. This is not an efficient or effective way to reduce DDL time out issues.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/msck-repair-table.html\">https://docs.aws.amazon.com/athena/latest/ug/msck-repair-table.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
                "options": [
                    {
                        "id": 2913,
                        "content": "<p>Use the ALTER TABLE ADD PARTITION command to update the column names.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2914,
                        "content": "<p>Use the MSCK REPAIR TABLE command to update the metadata in the catalog.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2915,
                        "content": "<p>Export the data into a JSON document to clean any errors and upload the cleaned data into S3.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2916,
                        "content": "<p>Export the data into DynamoDB to perform queries in a more flexible schema.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 710,
            "attributes": {
                "createdAt": "2023-09-07T08:51:22.191Z",
                "updatedAt": "2023-09-07T08:51:22.191Z",
                "content": "<p>A developer is using AWS CodeBuild to build an application into a Docker image. The buildspec file is used to run the application build. The developer needs to push the Docker image to an Amazon ECR repository only upon the successful completion of each build.</p>",
                "answerExplanation": "<p>The post_build phase is an optional sequence. It represents the commands, if any, that CodeBuild runs after the build. For example, you might use Maven to package the build artifacts into a JAR or WAR file, or you might push a Docker image into Amazon ECR. Then you might send a build notification through Amazon SNS.</p><p>Here is an example of a buildspec file with a post_build phase that pushes a Docker image to Amazon ECR:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-29_11-50-10-9f3a3ef04f3fab72a2d99c8c431ed141.jpg\"><p><strong>CORRECT: </strong>\"Add a post_build phase to the buildspec file that uses the commands block to push the Docker image\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add a post_build phase to the buildspec file that uses the finally block to push the Docker image\" is incorrect.</p><p>Commands specified in a finally block are run after commands in the commands block. The commands in a finally block are run even if a command in the commands block fails. This would not be ideal as this would push the image to ECR even if commands in previous sequences failed.</p><p><strong>INCORRECT:</strong> \"Add an install phase to the buildspec file that uses the commands block to push the Docker image\" is incorrect.</p><p>These are commands that are run during installation. The develop would want to push the image only after all installations have succeeded. Therefore, the post_build phase should be used.</p><p><strong>INCORRECT:</strong> \"Add a post_build phase to the buildspec file that uses the artifacts sequence to find the build artifacts and push to Amazon ECR\" is incorrect.</p><p>The artifacts sequence is not required if you are building and pushing a Docker image to Amazon ECR, or you are running unit tests on your source code, but not building it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 2917,
                        "content": "<p>Add a post_build phase to the buildspec file that uses the commands block to push the Docker image.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2918,
                        "content": "<p>Add an install phase to the buildspec file that uses the commands block to push the Docker image.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2919,
                        "content": "<p>Add a post_build phase to the buildspec file that uses the finally block to push the Docker image.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2920,
                        "content": "<p>Add a post_build phase to the buildspec file that uses the artifacts sequence to find the build artifacts and push to Amazon ECR.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 711,
            "attributes": {
                "createdAt": "2023-09-07T08:51:22.267Z",
                "updatedAt": "2023-09-07T08:51:22.267Z",
                "content": "<p>A company has implemented AWS CodePipeline to automate its release pipelines. The Development team is writing an AWS Lambda function that will send notifications for state changes of each of the actions in the stages.</p><p>Which steps must be taken to associate the Lambda function with the event source?</p>",
                "answerExplanation": "<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/WhatIsCloudWatchEvents.html\">Amazon CloudWatch Events</a> help you to respond to state changes in your AWS resources. When your resources change state, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your AWS Lambda function to take action.</p><p>AWS CodePipeline can be configured as an event source in CloudWatch Events and can then send notifications using as service such as Amazon SNS.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_13-17-20-fee89385a439135b342353d0489097e2.jpg\"></p><p>Therefore, the best answer is to create an Amazon CloudWatch Events rule that uses CodePipeline as an event source.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a trigger that invokes the Lambda function from the Lambda console by selecting CodePipeline as the event source\" is incorrect as CodePipeline cannot be configured as a trigger for Lambda.</p><p><strong>INCORRECT:</strong> \"Create an event trigger and specify the Lambda function from the CodePipeline console\" is incorrect as CodePipeline cannot be configured as a trigger for Lambda.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm that monitors status changes in CodePipeline and triggers the Lambda function\" is incorrect as CloudWatch Events is used for monitoring state changes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 2921,
                        "content": "<p>Create an event trigger and specify the Lambda function from the CodePipeline console</p>",
                        "isValid": false
                    },
                    {
                        "id": 2922,
                        "content": "<p>Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source</p>",
                        "isValid": true
                    },
                    {
                        "id": 2923,
                        "content": "<p>Create an Amazon CloudWatch alarm that monitors status changes in CodePipeline and triggers the Lambda function</p>",
                        "isValid": false
                    },
                    {
                        "id": 2924,
                        "content": "<p>Create a trigger that invokes the Lambda function from the Lambda console by selecting CodePipeline as the event source</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 712,
            "attributes": {
                "createdAt": "2023-09-07T08:51:22.340Z",
                "updatedAt": "2023-09-07T08:51:22.340Z",
                "content": "<p>A Developer is setting up a code update to Amazon ECS using AWS CodeDeploy. The Developer needs to complete the code update quickly. Which of the following deployment types should the Developer use?</p>",
                "answerExplanation": "<p>CodeDeploy provides two deployment type options â€“ in-place and blue/green. Note that AWS Lambda and Amazon ECS deployments cannot use an in-place deployment type.</p><p>The Blue/green deployment type on an Amazon ECS compute platform works like this:</p><p>Traffic is shifted from the task set with the original version of an application in an Amazon ECS service to a replacement task set in the same service.</p><p>You can set the traffic shifting to linear or canary through the deployment configuration.</p><p>The protocol and port of a specified load balancer listener is used to reroute production traffic.</p><p>During a deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run.</p><p><strong>CORRECT: </strong>\"Blue/green\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Canary\" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments.</p><p><strong>INCORRECT:</strong> \"Linear\" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments.</p><p><strong>INCORRECT:</strong> \"In-place\" is incorrect as AWS Lambda and Amazon ECS deployments cannot use an in-place deployment type.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 2925,
                        "content": "<p>Blue/green</p>",
                        "isValid": true
                    },
                    {
                        "id": 2926,
                        "content": "<p>Linear</p>",
                        "isValid": false
                    },
                    {
                        "id": 2927,
                        "content": "<p>In-place</p>",
                        "isValid": false
                    },
                    {
                        "id": 2928,
                        "content": "<p>Canary</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 713,
            "attributes": {
                "createdAt": "2023-09-07T08:51:22.410Z",
                "updatedAt": "2023-09-07T08:51:22.410Z",
                "content": "<p>A team of Developers require read-only access to an Amazon DynamoDB table. The Developers have been added to a group. What should an administrator do to provide the team with access whilst following the principal of least privilege? </p>",
                "answerExplanation": "<p>The key requirement is to provide read-only access to the team for a specific DynamoDB table. Therefore, the AWS managed policy cannot be used as it will provide access to all DynamoDB tables in the account which does not follow the principal of least privilege.</p><p>Therefore, a customer managed policy should be created that provides read-only access and specifies the ARN of the table. For instance, the resource element might include the following ARN:</p><p><code>arn:aws:dynamodb:us-west-1:515148227241:table/exampletable</code></p><p>This will lock down access to the specific DynamoDB table, following the principal of least privilege.</p><p><strong>CORRECT: </strong>\"Create a customer managed policy with read only access to DynamoDB and specify the ARN of the table for the â€œResourceâ€ element. Attach the policy to the group\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Assign the <code>AmazonDynamoDBReadOnlyAccess</code> AWS managed policy to the group\" is incorrect as this will provide read-only access to all DynamoDB tables in the account.</p><p><strong>INCORRECT:</strong> \"Assign the <code>AWSLambdaDynamoDBExecutionRole</code> AWS managed policy to the group\" is incorrect as this is a role used with AWS Lambda.</p><p><strong>INCORRECT:</strong> \"Create a customer managed policy with read/write access to DynamoDB for all resources. Attach the policy to the group\" is incorrect as read-only access should be provided, not read/write.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 2929,
                        "content": "<p>Assign the AmazonDynamoDBReadOnlyAccess AWS managed policy to the group</p>",
                        "isValid": false
                    },
                    {
                        "id": 2930,
                        "content": "<p>Create a customer managed policy with read only access to DynamoDB and specify the ARN of the table for the â€œResourceâ€ element. Attach the policy to the group</p>",
                        "isValid": true
                    },
                    {
                        "id": 2931,
                        "content": "<p>Assign the <code>AWSLambdaDynamoDBExecutionRole</code> AWS managed policy to the group</p>",
                        "isValid": false
                    },
                    {
                        "id": 2932,
                        "content": "<p>Create a customer managed policy with read/write access to DynamoDB for all resources. Attach the policy to the group</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 714,
            "attributes": {
                "createdAt": "2023-09-07T08:51:22.487Z",
                "updatedAt": "2023-09-07T08:51:22.487Z",
                "content": "<p>A company runs many microservices applications that use Docker containers. The company are planning to migrate the containers to Amazon ECS. The workloads are highly variable and therefore the company prefers to be charged per running task.</p><p>Which solution is the BEST fit for the companyâ€™s requirements?</p>",
                "answerExplanation": "<p>The key requirement is that the company should be charged per running task. Therefore, the best answer is to use Amazon ECS with the Fargate launch type as with this model AWS charge you for running tasks rather than running container instances.</p><p>The Fargate launch type allows you to run your containerized applications without the need to provision and manage the backend infrastructure. You just register your task definition and Fargate launches the container for you. The Fargate Launch Type is a serverless infrastructure managed by AWS.</p><p><strong>CORRECT: </strong>\"Amazon ECS with the Fargate launch type\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon ECS with the EC2 launch type\" is incorrect as with this launch type you pay for running container instances (EC2 instances).</p><p><strong>INCORRECT:</strong> \"An Amazon ECS Service with Auto Scaling\" is incorrect as this does not specify the launch type. You can run an ECS Service on the Fargate or EC2 launch types.</p><p><strong>INCORRECT:</strong> \"An Amazon ECS Cluster with Auto Scaling\" is incorrect as this does not specify the launch type. You can run an ECS Cluster on the Fargate or EC2 launch types.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 2933,
                        "content": "<p>An Amazon ECS Cluster with Auto Scaling</p>",
                        "isValid": false
                    },
                    {
                        "id": 2934,
                        "content": "<p>An Amazon ECS Service with Auto Scaling</p>",
                        "isValid": false
                    },
                    {
                        "id": 2935,
                        "content": "<p>Amazon ECS with the EC2 launch type</p>",
                        "isValid": false
                    },
                    {
                        "id": 2936,
                        "content": "<p>Amazon ECS with the Fargate launch type</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 715,
            "attributes": {
                "createdAt": "2023-09-07T08:51:22.559Z",
                "updatedAt": "2023-09-07T08:51:22.559Z",
                "content": "<p>A Developer is deploying an AWS Lambda update using AWS CodeDeploy. In the appspec.yaml file, which of the following is a valid structure for the order of hooks that should be specified?</p>",
                "answerExplanation": "<p>The content in the 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment. The 'hooks' section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts.</p><p>The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.</p><p>The following code snippet shows a valid example of the structure of hooks for an AWS Lambda deployment:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-33-57-404e61aa9de4be6bc1dcd44c189c04c1.jpg\"></p><p>Therefore, in this scenario a valid structure for the order of hooks that should be specified in the appspec.yml file is: BeforeAllowTraffic &gt; AfterAllowTraffic</p><p><strong>CORRECT: </strong>\"BeforeAllowTraffic &gt; AfterAllowTraffic\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"BeforeInstall &gt; AfterInstall &gt; ApplicationStart &gt; ValidateService\" is incorrect as this would be valid for Amazon EC2.</p><p><strong>INCORRECT:</strong> \"BeforeInstall &gt; AfterInstall &gt; AfterAllowTestTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic\" is incorrect as this would be valid for Amazon ECS.</p><p><strong>INCORRECT:</strong> \"BeforeBlockTraffic &gt; AfterBlockTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic\" is incorrect as this is a partial listing of hooks for Amazon EC2 but is incomplete.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 2937,
                        "content": "<p>BeforeBlockTraffic &gt; AfterBlockTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
                        "isValid": false
                    },
                    {
                        "id": 2938,
                        "content": "<p>BeforeInstall &gt; AfterInstall &gt; ApplicationStart &gt; ValidateService</p>",
                        "isValid": false
                    },
                    {
                        "id": 2939,
                        "content": "<p>BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
                        "isValid": true
                    },
                    {
                        "id": 2940,
                        "content": "<p>BeforeInstall &gt; AfterInstall &gt; AfterAllowTestTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 716,
            "attributes": {
                "createdAt": "2023-09-07T08:51:22.642Z",
                "updatedAt": "2023-09-07T08:51:22.642Z",
                "content": "<p>A Developer is writing code to run in a cron job on an Amazon EC2 instance that sends status information about the application to Amazon CloudWatch.</p><p>Which method should the Developer use?</p>",
                "answerExplanation": "<p>The put-metric-data command publishes metric data points to Amazon CloudWatch. CloudWatch associates the data points with the specified metric. If the specified metric does not exist, CloudWatch creates the metric.</p><p><strong>CORRECT: </strong>\"Use the AWS CLI put-metric-data command\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use the AWS CLI put-metric-alarm command\" is incorrect. This command creates or updates an alarm and associates it with the specified metric, metric math expression, or anomaly detection model.</p><p><strong>INCORRECT:</strong> \"Use the unified CloudWatch agent to publish custom metrics\" is incorrect. It is not necessary to use the unified CloudWatch agent. In this case the Developer can use the AWS CLI with the cron job.</p><p><strong>INCORRECT:</strong> \"Use the CloudWatch console with detailed monitoring\" is incorrect. You cannot collect custom metric data using the CloudWatch console with detailed monitoring. Detailed monitoring sends data at 1-minute rather than 5-minute frequencies but will not collect custom data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/put-metric-data.html\">https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/put-metric-data.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 2941,
                        "content": "<p>Use the AWS CLI put-metric-alarm command.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2942,
                        "content": "<p>Use the unified CloudWatch agent to publish custom metrics.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2943,
                        "content": "<p>Use the AWS CLI put-metric-data command.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2944,
                        "content": "<p>Use the CloudWatch console with detailed monitoring.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 717,
            "attributes": {
                "createdAt": "2023-09-07T08:51:22.712Z",
                "updatedAt": "2023-09-07T08:51:22.712Z",
                "content": "<p>A Development team wants to run their container workloads on Amazon ECS. Each application container needs to share data with another container to collect logs and metrics.</p><p>What should the Development team do to meet these requirements?</p>",
                "answerExplanation": "<p>Amazon ECS tasks support Docker volumes. To use data volumes, you must specify the volume and mount point configurations in your task definition. Docker volumes are supported for the EC2 launch type only.</p><p>To configure a Docker volume, in the task definition volumes section, define a data volume with name and DockerVolumeConfiguration values. In the containerDefinitions section, define multiple containers with mountPoints values that reference the name of the defined volume and the containerPath value to mount the volume at on the container.</p><p>The containers should both be specified in the same task definition. Therefore, the Development team should create one task definition, specify both containers in the definition and then mount a shared volume between those two containers</p><p><strong>CORRECT: </strong>\"Create one task definition. Specify both containers in the definition. Mount a shared volume between those two containers\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create two pod specifications. Make one to include the application container and the other to include the other container. Link the two pods together\" is incorrect as pods are a concept associated with the Elastic Kubernetes Service (EKS).</p><p><strong>INCORRECT:</strong> \"Create two task definitions. Make one to include the application container and the other to include the other container. Mount a shared volume between the two tasks\" is incorrect as a single task definition should be created with both containers.</p><p><strong>INCORRECT:</strong> \"Create a single pod specification. Include both containers in the specification. Mount a persistent volume to both containers\" is incorrect as pods are a concept associated with the Elastic Kubernetes Service (EKS).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-volumes.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-volumes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 2945,
                        "content": "<p>Create two task definitions. Make one to include the application container and the other to include the other container. Mount a shared volume between the two tasks</p>",
                        "isValid": false
                    },
                    {
                        "id": 2946,
                        "content": "<p>Create two pod specifications. Make one to include the application container and the other to include the other container. Link the two pods together</p>",
                        "isValid": false
                    },
                    {
                        "id": 2947,
                        "content": "<p>Create one task definition. Specify both containers in the definition. Mount a shared volume between those two containers</p>",
                        "isValid": true
                    },
                    {
                        "id": 2948,
                        "content": "<p>Create a single pod specification. Include both containers in the specification. Mount a persistent volume to both containers</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 718,
            "attributes": {
                "createdAt": "2023-09-07T08:51:22.794Z",
                "updatedAt": "2023-09-07T08:51:22.794Z",
                "content": "<p>A developer is planning on using AWS Cloud9 to build a new application. The developer wants to spend minimal time configuring resources. What solution should the developer choose?</p>",
                "answerExplanation": "<p>An AWS Cloud9 Elastic Compute Cloud (EC2) Environment has many of the configuration and management taken care of by AWS. This includes creating and managing the instance lifecycle, setting up the Command Line Interface (CLI), and access to packages that are already installed and configured like Git, Node.js, and Python.</p><p><strong>CORRECT</strong>: \"Create an AWS Cloud9 Elastic Compute Cloud (EC2) environment that will provision many of the resources and manage the lifecycle\" is the correct answer (as explained above.)</p><p><strong>INCORRECT</strong>: \"Create an SSH environment that will provision many of the resources and manage the lifecycle\" is incorrect.</p><p>An AWS Cloud9 SSH environment does not provide instance lifecycle management and will require the developer to manually configure or add many features such as the Command Line Interface (CLI) and popular packages like Git, Node.js and Python.</p><p><strong>INCORRECT</strong>: \"Use AWS Toolkit to create an EC2 environment that will provision many of the resources and manage the lifecycle\" is incorrect.</p><p>AWS Toolkit is an extension of the AWS Cloud9 integrated development environment (IDE). It is not used to create an EC2 environment.</p><p><strong>INCORRECT</strong>: \"Use AWS Toolkit to create an SSH environment that will provision many of the resources and manage the lifecycle\" is incorrect.</p><p>AWS Toolkit is an extension of the AWS Cloud9 integrated development environment (IDE). It is not used to create an SSH environment.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cloud9/latest/user-guide/ec2-env-versus-ssh-env.html\">https://docs.aws.amazon.com/cloud9/latest/user-guide/ec2-env-versus-ssh-env.html</a></p>",
                "options": [
                    {
                        "id": 2949,
                        "content": "<p>Create an AWS Cloud9 Elastic Compute Cloud (EC2) environment that will provision many of the resources and manage the lifecycle.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2950,
                        "content": "<p>Use AWS Toolkit to create an Elastic Compute Cloud (EC2) environment that will provision many of the resources and manage the lifecycle.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2951,
                        "content": "<p>Create an SSH environment that will provision many of the resources and manage the lifecycle.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2952,
                        "content": "<p>Use AWS Toolkit to create an SSH environment that will provision many of the resources and manage the lifecycle.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 719,
            "attributes": {
                "createdAt": "2023-09-07T08:51:22.866Z",
                "updatedAt": "2023-09-07T08:51:22.866Z",
                "content": "<p>A Developer is creating a serverless application that will process sensitive data. The AWS Lambda function must encrypt all data that is written to /tmp storage at rest.</p><p>How should the Developer encrypt this data?</p>",
                "answerExplanation": "<p>On a per-function basis, you can configure Lambda to use an encryption key that you create and manage in AWS Key Management Service. These are referred to as <em>customer managed</em> customer master keys (CMKs) or customer managed keys. If you don't configure a customer managed key, Lambda uses an AWS managed CMK named aws/lambda, which Lambda creates in your account.</p><p>The CMK can be used to generate a data encryption key that can be used for encrypting all data uploaded to Lambda or generated by Lambda.</p><p><strong>CORRECT: </strong>\"Configure Lambda to use an AWS KMS <em>customer managed</em> customer master key (CMK). Use the CMK to generate a data key and encrypt all data prior to writing to /tmp storage\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Attach the Lambda function to a VPC and encrypt Amazon EBS volumes at rest using the AWS managed CMK. Mount the EBS volume to /tmp\" is incorrect. You cannot attach an EBS volume to a Lambda function.</p><p><strong>INCORRECT:</strong> \"Enable default encryption on an Amazon S3 bucket using an AWS KMS <em>customer managed</em> customer master key (CMK). Mount the S3 bucket to /tmp\" is incorrect. You cannot mount an S3 bucket to a Lambda function.</p><p><strong>INCORRECT:</strong> \"Enable secure connections over HTTPS for the AWS Lambda API endpoints using Transport Layer Security (TLS)\" is incorrect. The Lambda API endpoints are always encrypted using TLS and this is encryption in-transit not encryption at-rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/security-dataprotection.html\">https://docs.aws.amazon.com/lambda/latest/dg/security-dataprotection.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 2953,
                        "content": "<p>Configure Lambda to use an AWS KMS <em>customer managed</em> customer master key (CMK). Use the CMK to generate a data key and encrypt all data prior to writing to /tmp storage.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2954,
                        "content": "<p>Attach the Lambda function to a VPC and encrypt Amazon EBS volumes at rest using the AWS managed CMK. Mount the EBS volume to /tmp.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2955,
                        "content": "<p>Enable secure connections over HTTPS for the AWS Lambda API endpoints using Transport Layer Security (TLS).</p>",
                        "isValid": false
                    },
                    {
                        "id": 2956,
                        "content": "<p>Enable default encryption on an Amazon S3 bucket using an AWS KMS <em>customer managed</em> customer master key (CMK). Mount the S3 bucket to /tmp.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 720,
            "attributes": {
                "createdAt": "2023-09-07T08:51:22.934Z",
                "updatedAt": "2023-09-07T08:51:22.934Z",
                "content": "<p>A serverless application uses an IAM role to authenticate and authorize access to an Amazon DynamoDB table. A Developer is troubleshooting access issues affecting the application. The Developer has access to the IAM role that the application is using.</p><p>Which of the following commands will help the Developer to test the role permissions using the AWS CLI?</p>",
                "answerExplanation": "<p>The AWS CLI â€œaws sts assume roleâ€ command will enable the Developer to assume the role and gain temporary security credentials. The Developer can then use those security credentials to troubleshoot access issues that are affecting the application.</p><p><strong>CORRECT: </strong>\"aws sts assume-role\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"aws sts get-session-token\" is incorrect. This is used to get temporary credentials for an AWS account or IAM user. It can subsequently be used to call the assume-role API.</p><p><strong>INCORRECT:</strong> \"aws iam get-role-policy\" is incorrect. This command retrieves the specified inline policy document that is embedded with the specified IAM role.</p><p><strong>INCORRECT:</strong> \"aws dynamodb describe-endpoints\" is incorrect. This command returns the regional endpoint information.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/sts/assume-role.html\">https://docs.aws.amazon.com/cli/latest/reference/sts/assume-role.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 2957,
                        "content": "<p>aws sts assume-role</p>",
                        "isValid": true
                    },
                    {
                        "id": 2958,
                        "content": "<p>aws iam get-role-policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 2959,
                        "content": "<p>aws dynamodb describe-endpoints</p>",
                        "isValid": false
                    },
                    {
                        "id": 2960,
                        "content": "<p>aws sts get-session-token</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 721,
            "attributes": {
                "createdAt": "2023-09-07T08:51:23.010Z",
                "updatedAt": "2023-09-07T08:51:23.010Z",
                "content": "<p>A developer received the following error message during an AWS CloudFormation deployment:</p><p>DELETE_FAILED (The following resource(s) failed to delete: (sg-11223344).)</p><p>Which action should the developer take to resolve this error?</p>",
                "answerExplanation": "<p>The stack may be stuck in the DELETE_FAILED state because the dependent object (security group), can't be deleted. This can be for many reasons, for example, the security group could have an ENI attached thatâ€™s not part of the CloudFormation stack.</p><p>To delete the stack you must choose to delete the stack in the console and then select to retain the resource(s) that failed to delete. This can also be achieved from the AWS CLI:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-30_04-35-14-3cedd43a9f1f0a220452f4b399842a88.jpg\"><p><strong>CORRECT: </strong>\"Modify the CloudFormation template to retain the security group resource. Then manually delete the resource after deployment\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add a DependsOn attribute to the sg-11223344 resource in the CloudFormation template. Then delete the stack\" is incorrect.</p><p>This creates a dependency for stack creation. It does not assist with resolving the issue that is preventing the stack from deleting successfully.</p><p><strong>INCORRECT:</strong> \"Manually delete the security group. Then execute a change set to force deletion of the CloudFormation stack\" is incorrect.</p><p>You can manually delete the security group. However, you would not then use a change set to continue with the deletion. You would instead simply choose to delete the stack from the console or the CLI.</p><p><strong>INCORRECT:</strong> \"Update the logical ID of the security group resource with the security groups ARN. Then delete the stack\" is incorrect.</p><p>The issue has nothing to do with logical IDs or ARNs. The resource cannot be deleted by CloudFormation so the developer simply needs to choose to retain the resource before continuing with the stack deletion process.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>",
                "options": [
                    {
                        "id": 2961,
                        "content": "<p>Modify the CloudFormation template to retain the security group resource. Then manually delete the resource after deployment.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2962,
                        "content": "<p>Update the logical ID of the security group resource with the security groups ARN. Then delete the stack.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2963,
                        "content": "<p>Manually delete the security group. Then execute a change set to force deletion of the CloudFormation stack.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2964,
                        "content": "<p>Add a DependsOn attribute to the sg-11223344 resource in the CloudFormation template. Then delete the stack.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 722,
            "attributes": {
                "createdAt": "2023-09-07T08:51:23.086Z",
                "updatedAt": "2023-09-07T08:51:23.086Z",
                "content": "<p>An organization wants to ensure that there is minimal latency for their customers on both the east and west coast of the United States. What can they do to provide optimal performance with minimal latency?</p>",
                "answerExplanation": "<p><strong>CORRECT: </strong>\"Use Amazon Route 53 with latency-based routing by creating latency records in multiple AWS Regions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Route 53 to direct the traffic based on health checks\" is incorrect. Health checks just show that the resource is healthy, and do not ensure that the best performing endpoint is chosen. Reducing latency is the best option here.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail to monitor traffic and identify where requests are coming from and deploy the application to those Regions\" is incorrect. CloudTrail is used to monitor API calls.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch to monitor traffic spikes and identify how to scale horizontally\" is incorrect. CloudWatch does monitor spikes in usage, but it is not used to improve latency to end users.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
                "options": [
                    {
                        "id": 2965,
                        "content": "<p>Use Amazon Route 53 to direct the traffic based on health checks.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2966,
                        "content": "<p>Use AWS CloudTrail to monitor traffic and identify where requests are coming from and deploy the application to those Regions.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2967,
                        "content": "<p>Use Amazon CloudWatch to monitor traffic spikes and identify how to scale horizontally.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2968,
                        "content": "<p>Use Amazon Route 53 with latency-based routing by creating latency records in multiple AWS Regions.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 723,
            "attributes": {
                "createdAt": "2023-09-07T08:51:23.173Z",
                "updatedAt": "2023-09-07T08:51:23.173Z",
                "content": "<p>An application running on Amazon EC2 generates a large number of small files (1KB each) containing personally identifiable information that must be converted to ciphertext. The data will be stored on a proprietary network-attached file system. What is the SAFEST way to encrypt the data using AWS KMS?</p>",
                "answerExplanation": "<p>With AWS KMS you can encrypt files directly with a customer master key (CMK). A CMK can encrypt up to 4KB (4096 bytes) of data in a single encrypt, decrypt, or reencrypt operation. As CMKs cannot be exported from KMS this is a very safe way to encrypt small amounts of data.</p><p><em>Customer managed CMKs</em> are CMKs in your AWS account that you create, own, and manage. You have full control over these CMKs, including establishing and maintaining their <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/control-access.html\">key policies, IAM policies, and grants</a>, <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/enabling-keys.html\">enabling and disabling</a> them, <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\">rotating their cryptographic material</a>, <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/tagging-keys.html\">adding tags</a>, <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/programming-aliases.html\">creating aliases</a> that refer to the CMK, and <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html\">scheduling the CMKs for deletion</a>.</p><p><em>AWS managed CMKs</em> are CMKs in your account that are created, managed, and used on your behalf by an AWS service that is integrated with AWS KMS. Some AWS services support only an AWS managed CMK. In this example the Amazon EC2 instance is saving files on a proprietary network-attached file system and this will not have support for AWS managed CMKs.</p><p><em>Data keys</em> are encryption keys that you can use to encrypt data, including large amounts of data and other data encryption keys. You can use AWS KMS CMKs to generate, encrypt, and decrypt data keys. However, AWS KMS does not store, manage, or track your data keys, or perform cryptographic operations with data keys. You must use and manage data keys outside of AWS KMS â€“ this is potentially less secure as you need to manage the security of these keys.</p><p><strong>CORRECT: </strong>\"Encrypt the data directly with a customer managed customer master key\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a data encryption key from a customer master key and encrypt the data with the data encryption key\" is incorrect as this is not the most secure option here as you need to secure the data encryption key outside of KMS. It is also unwarranted as you can use a CMK directly to encrypt files up to 4KB in size.</p><p><strong>INCORRECT:</strong> \"Create a data encryption key from a customer master key and encrypt the data with the customer master key\" is incorrect as the creation of the data encryption key is of no use here. It does not necessarily pose a security risk as the data key hasnâ€™t been used (and you can use the CMK to encrypt the data), however this is not the correct process to follow.</p><p><strong>INCORRECT:</strong> \"Encrypt the data directly with an AWS managed customer master key\" is incorrect as the network-attached file system is proprietary and therefore will not be supported by AWS managed CMKs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
                "options": [
                    {
                        "id": 2969,
                        "content": "<p>Encrypt the data directly with an AWS managed customer master key</p>",
                        "isValid": false
                    },
                    {
                        "id": 2970,
                        "content": "<p>Encrypt the data directly with a customer managed customer master key</p>",
                        "isValid": true
                    },
                    {
                        "id": 2971,
                        "content": "<p>Create a data encryption key from a customer master key and encrypt the data with the customer master key</p>",
                        "isValid": false
                    },
                    {
                        "id": 2972,
                        "content": "<p>Create a data encryption key from a customer master key and encrypt the data with the data encryption key</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 724,
            "attributes": {
                "createdAt": "2023-09-07T08:51:23.267Z",
                "updatedAt": "2023-09-07T08:51:23.267Z",
                "content": "<p>An organization has an account for each environment: Production, Testing, Development. A Developer with an IAM user in the Development account needs to launch resources in the Production and Testing accounts. What is the MOST efficient way to provide access</p>",
                "answerExplanation": "<p>You can grant your IAM usersâ€™ permission to switch to roles within your AWS account or to roles defined in other AWS accounts that you own. This is known as cross-account access.</p><p>In the image below a user in the Development account needs to access an S3 bucket in the Production account:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_06-08-34-c04908e5b40ad6142024fee3a875a0b2.png\"></p><p>The user is able to assume the role in the Production account and access the S3 bucket. This is more efficient than providing the user with multiple accounts. In this scenario the user requests to switch to the role through either the console or the API/CLI.</p><p><strong>CORRECT: </strong>\"Create a role with the required permissions in the Production and Testing accounts and have the Developer assume that role\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a separate IAM user in each account and have the Developer login separately to each account\" is incorrect as this is not the most efficient method of providing access. Cross-account access is preferred .</p><p><strong>INCORRECT:</strong> \"Create an IAM group in the Production and Testing accounts and add the Developerâ€™s user from the Development account to the groups\" is incorrect as you cannot add an IAM user from another AWS account to a group.</p><p><strong>INCORRECT:</strong> \"Create an IAM permissions policy in the Production and Testing accounts and reference the IAM user in the Development account\" is incorrect as you cannot reference an IAM user from another AWS account in a permissions policy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 2973,
                        "content": "<p>Create an IAM group in the Production and Testing accounts and add the Developerâ€™s user from the Development account to the groups</p>",
                        "isValid": false
                    },
                    {
                        "id": 2974,
                        "content": "<p>Create an IAM permissions policy in the Production and Testing accounts and reference the IAM user in the Development account</p>",
                        "isValid": false
                    },
                    {
                        "id": 2975,
                        "content": "<p>Create a role with the required permissions in the Production and Testing accounts and have the Developer assume that role</p>",
                        "isValid": true
                    },
                    {
                        "id": 2976,
                        "content": "<p>Create a separate IAM user in each account and have the Developer login separately to each account</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 725,
            "attributes": {
                "createdAt": "2023-09-07T08:51:23.341Z",
                "updatedAt": "2023-09-07T08:51:23.341Z",
                "content": "<p>A company has deployed a new web application that uses Amazon Cognito for authentication. The company wants to allow sign-in from any source but wants to automatically block all sign-in attempts if the risk level is elevated.</p><p>Which Amazon Cognito feature will meet these requirements?</p>",
                "answerExplanation": "<p>With adaptive authentication, you can configure your user pool to block suspicious sign-ins or add second factor authentication in response to an increased risk level.</p><p>For each sign-in attempt, Amazon Cognito generates a risk score for how likely the sign-in request is to be from a compromised source. This risk score is based on many factors, including whether it detects a new device, user location, or IP address.</p><p><strong>For each risk level, you can choose from the following options:</strong></p><p> â€¢&nbsp; Allow - Users can sign in without an additional factor.</p><p> â€¢&nbsp; Optional MFA - Users who have a second factor configured must complete a second factor challenge to sign in.</p><p> â€¢&nbsp; Require MFA - Users who have a second factor configured must complete a second factor challenge to sign in. Amazon Cognito blocks sign-in for users who don't have a second factor configured.</p><p> â€¢&nbsp; Block - Amazon Cognito blocks all sign-in attempts at the designated risk level.</p><p>In this case the company should use adaptive authentication and configure Cognito to block sign-in attempts at the specific risk level they feel is appropriate.</p><p><strong>CORRECT: </strong>\"Adaptive authentication\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Advanced security metrics\" is incorrect.</p><p>Amazon Cognito publishes sign-in attempts, their risk levels, and failed challenges to Amazon CloudWatch. These are known as advanced security metrics. This information is useful for analysis, but adaptive authentication is required to automatically block sign-in attempts.</p><p><strong>INCORRECT:</strong> \"Multi-factor authentication (MFA)\" is incorrect.</p><p>This is not a method of blocking. In this case adaptive authentication with a block response should be configured.</p><p><strong>INCORRECT:</strong> \"Case sensitive user pools\" is incorrect.</p><p>This has nothing to do with responding to security threats. This is a configuration that determines whether Cognito considers the case of email addresses and usernames.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pool-settings-adaptive-authentication.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pool-settings-adaptive-authentication.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 2977,
                        "content": "<p>Multi-factor authentication (MFA).</p>",
                        "isValid": false
                    },
                    {
                        "id": 2978,
                        "content": "<p>Adaptive authentication.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2979,
                        "content": "<p>Case sensitive user pools.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2980,
                        "content": "<p>Advanced security metrics.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 726,
            "attributes": {
                "createdAt": "2023-09-07T08:51:23.414Z",
                "updatedAt": "2023-09-07T08:51:23.414Z",
                "content": "<p>A developer wants to use a cron job to schedule key rotation for an Amazon RDS database. What steps should the developer take to complete this task?</p>",
                "answerExplanation": "<p><em>Rotation</em> is the process of periodically updating a secret. When you rotate a secret, you update the credentials in both the secret and the database or service. In Secrets Manager, you can set up automatic rotation for your secrets.</p><p>Some services offer <em>managed rotation</em>, where the service configures and manages rotation for you. With managed rotation, you don't use an AWS Lambda function to update the secret and the credentials in the database. Amazon RDS can be used with managed rotation.</p><p><strong>CORRECT: </strong>\"Use AWS Secrets Manager to create a managed rotation under the Rotation Schedule and schedule the time for the automatic rotation using a cron expression\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Identity and Access Management (IAM) to create a managed rotation under the Rotation Schedule and schedule the time for the automatic rotation using a cron expression\" is incorrect. IAM is an identity and management services to help assign permissions to users, groups and roles. It does not manage secrets such as passwords or access keys.</p><p><strong>INCORRECT:</strong> \"Use AWS Relational Database Service to create a managed rotation under the Rotation Schedule and schedule the time for the automatic rotation using a cron expression\" is incorrect. RDS is a used to simplify the set-up of relational databases. It is not used for managing secrets.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Parameter Store to create a managed rotation under the Rotation Schedule and schedule the time for the automatic rotation using a cron expression\" is incorrect. The SSM Parameter store does not use cron jobs to manage rotations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_schedule.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_schedule.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-secrets-manager/\">https://digitalcloud.training/aws-secrets-manager/</a></p>",
                "options": [
                    {
                        "id": 2981,
                        "content": "<p>Use AWS Systems Manager (SSM) Parameter Store to create a managed rotation under the Rotation Schedule and schedule the time for the automatic rotation using a cron expression.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2982,
                        "content": "<p>Use AWS Relational Database Service to create a managed rotation under the Rotation Schedule and schedule the time for the automatic rotation using a cron expression.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2983,
                        "content": "<p>Use AWS Secrets Manager to create a managed rotation under the Rotation Schedule and schedule the time for the automatic rotation using a cron expression.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2984,
                        "content": "<p>Use AWS Identity and Access Management (IAM) to create a managed rotation under the Rotation Schedule and schedule the time for the automatic rotation using a cron expression.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 727,
            "attributes": {
                "createdAt": "2023-09-07T08:51:23.485Z",
                "updatedAt": "2023-09-07T08:51:23.485Z",
                "content": "<p>A company is creating an application that must support Security Assertion Markup Language (SAML) and authentication with social identity providers. The application must also be authorized to access data in Amazon S3 buckets and Amazon DynamoDB tables.</p><p>Which AWS service or feature will meet these requirements with the LEAST amount of additional coding?</p>",
                "answerExplanation": "<p>Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services.</p><p>Amazon Cognito identity pools support the following identity providers:</p><p> â€¢ Public providers: Amazon, Facebook, Google, Apple</p><p> â€¢ Amazon Cognito user pools</p><p> â€¢ Open ID Connect providers (identity pools)</p><p> â€¢ SAML identity providers (identity pools)</p><p> â€¢ Developer authenticated identities (identity pools)</p><p>Identity pools are well suited to use cases where you need to authenticate users through one of the above IdPs and then authorize access to AWS services such as Amazon S3 and DynamoDB.</p><p><strong>CORRECT: </strong>\"Amazon Cognito identity pools\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Amazon Cognito user pools\" is incorrect.</p><p>You can use a user pool for authentication but you would then need to use the identity pool for authorization to AWS services. Therefore, this option would require more additional coding.</p><p><strong>INCORRECT:</strong> \"AWS AppSync GraphQL API\" is incorrect.</p><p>There is no need to implement an API for this use case. The developer simply needs a solution for authorization and access control.</p><p><strong>INCORRECT:</strong> \"Amazon API Gateway REST API\" is incorrect.</p><p>There is no need to implement an API for this use case. The developer simply needs a solution for authorization and access control.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/\">https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 2985,
                        "content": "<p>Amazon Cognito identity pools.</p>",
                        "isValid": true
                    },
                    {
                        "id": 2986,
                        "content": "<p>Amazon API Gateway REST API.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2987,
                        "content": "<p>Amazon Cognito user pools.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2988,
                        "content": "<p>AWS AppSync GraphQL API.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 728,
            "attributes": {
                "createdAt": "2023-09-07T08:51:23.561Z",
                "updatedAt": "2023-09-07T08:51:23.561Z",
                "content": "<p>A company is developing a new online game that will run on top of Amazon ECS. Four distinct Amazon ECS services will be part of the architecture, each requiring specific permissions to various AWS services. The company wants to optimize the use of the underlying Amazon EC2 instances by bin packing the containers based on memory reservation.</p><p>Which configuration would allow the Development team to meet these requirements MOST securely</p>",
                "answerExplanation": "<p>With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task. Applications must sign their AWS API requests with AWS credentials, and this feature provides a strategy for managing credentials for your applications to use, similar to the way that Amazon EC2 instance profiles provide credentials to EC2 instances.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_06-21-36-8e63335980c5cab31bb81697b00df2f2.png\"></p><p>Instead of creating and distributing your AWS credentials to the containers or using the EC2 instanceâ€™s role, you can associate an IAM role with an ECS task definition or RunTask API operation. The applications in the taskâ€™s containers can then use the AWS SDK or CLI to make API requests to authorized AWS services.</p><p>In this case each service requires access to different AWS services so following the principal of least privilege it is best to assign as a separate role to each task definition.</p><p><strong>CORRECT: </strong>\"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS task definition to reference the associated IAM role\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances\" is incorrect. It is a best practice to use IAM roles for tasks instead of assigning the roles to the container instances.</p><p><strong>INCORRECT:</strong> \"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS service to reference the associated IAM role\" is incorrect as the reference should be made within the task definition.</p><p><strong>INCORRECT:</strong> \"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then, create an IAM group and configure the ECS cluster to reference that group\" is incorrect as the reference should be made within the task definition.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 2989,
                        "content": "<p>Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS service to reference the associated IAM role</p>",
                        "isValid": false
                    },
                    {
                        "id": 2990,
                        "content": "<p>Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then, create an IAM group and configure the ECS cluster to reference that group</p>",
                        "isValid": false
                    },
                    {
                        "id": 2991,
                        "content": "<p>Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS task definition to reference the associated IAM role</p>",
                        "isValid": true
                    },
                    {
                        "id": 2992,
                        "content": "<p>Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 729,
            "attributes": {
                "createdAt": "2023-09-07T08:51:23.641Z",
                "updatedAt": "2023-09-07T08:51:23.641Z",
                "content": "<p>An application runs on a fleet of Amazon EC2 instances in an Auto Scaling group. The application stores data in an Amazon DynamoDB table and all instances make updates to the table. When querying data, EC2 instances sometimes retrieve stale data. The Developer needs to update the application to ensure the most up-to-date data is retrieved for all queries.</p><p>How can the Developer accomplish this?</p>",
                "answerExplanation": "<p>DynamoDB supports <em>eventually consistent</em> and <em>strongly consistent</em> reads. When using eventually consistent reads the response might not reflect the results of a recently completed write operation. The response might include some stale data.</p><p>When using strongly consistent reads DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful.</p><p>DynamoDB uses eventually consistent reads unless you specify otherwise. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation.</p><p><strong>CORRECT: </strong>\"Set the ConsistentRead parameter to true when calling GetItem\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Cache the database writes using Amazon DynamoDB Accelerator\" is incorrect. DynamoDB DAX caches items from DynamoDB to improve read performance but will not ensure the latest data is retrieved.</p><p><strong>INCORRECT:</strong> \"Use the TransactWriteItems API when issuing PutItem actions\" is incorrect. This operation is used to group transactions in an all-or-nothing update.</p><p><strong>INCORRECT:</strong> \"Use the UpdateGlobalTable API to create a global secondary index\" is incorrect. A GSI does not assist in any way in this solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\">https://docs.aws.amazon.com/amazondynamodb/latest/Developerguide/HowItWorks.ReadConsistency.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 2993,
                        "content": "<p>Cache the database writes using Amazon DynamoDB Accelerator.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2994,
                        "content": "<p>Use the UpdateGlobalTable API to create a global secondary index.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2995,
                        "content": "<p>Use the TransactWriteItems API when issuing PutItem actions.</p>",
                        "isValid": false
                    },
                    {
                        "id": 2996,
                        "content": "<p>Set the ConsistentRead parameter to true when calling GetItem.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 730,
            "attributes": {
                "createdAt": "2023-09-07T08:51:23.711Z",
                "updatedAt": "2023-09-07T08:51:23.711Z",
                "content": "<p>A Developer is creating a REST service using Amazon API Gateway with AWS Lambda integration. The service adds data to a spreadsheet and the data is sent as query string parameters in the method request.</p><p>How should the Developer convert the query string parameters to arguments for the Lambda function?</p>",
                "answerExplanation": "<p>Standard API Gateway <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html\">parameter and response code mapping templates</a> allow you to map parameters one-to-one and map a family of integration response status codes (matched by a regular expression) to a single response status code.</p><p>Mapping template overrides provides you with the flexibility to perform many-to-one parameter mappings; override parameters after standard API Gateway mappings have been applied; conditionally map parameters based on body content or other parameter values; programmatically create new parameters on the fly; and override status codes returned by your integration endpoint.</p><p>Any type of request parameter, response header, or response status code may be overridden.</p><p>Following are example uses for a mapping template override:</p><p>To create a new header (or overwrite an existing header) as a concatenation of two parameters</p><p>To override the response code to a success or failure code based on the contents of the body</p><p>To conditionally remap a parameter based on its contents or the contents of some other parameter</p><p>To iterate over the contents of a json body and remap key value pairs to headers or query strings</p><p>Therefore, the Developer can convert the query string parameters by creating a mapping template.</p><p><strong>CORRECT: </strong>\"Create a mapping template\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable request validation\" is incorrect as this is used to configure API Gateway to perform basic validation of an API request before proceeding with the integration request.</p><p><strong>INCORRECT:</strong> \"Include the Amazon Resource Name (ARN) of the Lambda function\" is incorrect as that doesnâ€™t assist with converting the query string parameters.</p><p><strong>INCORRECT:</strong> \"Change the integration type\" is incorrect as to perform a conversion the Lambda integration does not need to have a different integration type such as Lambda proxy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 2997,
                        "content": "<p>Enable request validation</p>",
                        "isValid": false
                    },
                    {
                        "id": 2998,
                        "content": "<p>Change the integration type</p>",
                        "isValid": false
                    },
                    {
                        "id": 2999,
                        "content": "<p>Create a mapping template</p>",
                        "isValid": true
                    },
                    {
                        "id": 3000,
                        "content": "<p>Include the Amazon Resource Name (ARN) of the Lambda function</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 731,
            "attributes": {
                "createdAt": "2023-09-07T08:51:23.805Z",
                "updatedAt": "2023-09-07T08:51:23.805Z",
                "content": "<p>A Developer is deploying an application using Docker containers running on the Amazon Elastic Container Service (ECS). The Developer is testing application latency and wants to capture trace information between the microservices.</p><p>Which solution will meet these requirements?</p>",
                "answerExplanation": "<p>In Amazon ECS, create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file to allow your application to communicate with the daemon container.</p><p><strong>CORRECT: </strong>\"Create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to the Amazon ECS cluster.\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch agent on the container image. Use the CloudWatch SDK to publish custom metrics from each of the microservices\" is incorrect. The CloudWatch agent does not capture trace information between Docker containers.</p><p><strong>INCORRECT:</strong> \"Install the AWS X-Ray daemon on each of the Amazon ECS instances\" is incorrect. The X-Ray daemon must be installed on the Docker containers, not the ECS hosts.</p><p><strong>INCORRECT:</strong> \"Install the AWS X-Ray daemon locally on an Amazon EC2 instance and instrument the Amazon ECS microservices using the X-Ray SDK\" is incorrect. You cannot trace Docker microservices from an Amazon EC2 instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3001,
                        "content": "<p>Install the Amazon CloudWatch agent on the container image. Use the CloudWatch SDK to publish custom metrics from each of the microservices.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3002,
                        "content": "<p>Create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to the Amazon ECS cluster.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3003,
                        "content": "<p>Install the AWS X-Ray daemon on each of the Amazon ECS instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3004,
                        "content": "<p>Install the AWS X-Ray daemon locally on an Amazon EC2 instance and instrument the Amazon ECS microservices using the X-Ray SDK.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 732,
            "attributes": {
                "createdAt": "2023-09-07T08:51:23.889Z",
                "updatedAt": "2023-09-07T08:51:23.889Z",
                "content": "<p>A developer needs to create a serverless application that uses an event-driven architecture.</p><p>How can the developer configure the application to automatically receive and process events?</p>",
                "answerExplanation": "<p>You can use a Lambda function to process Amazon Simple Notification Service (Amazon SNS) notifications. Amazon SNS supports Lambda functions as a target for messages sent to a topic. You can subscribe your function to topics in the same account or in other AWS accounts.</p><p>When an event is submitted to the SNS topic it will be sent to AWS Lambda which can then process the event. This is an example of a simple event-driven architecture.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-30_04-48-58-2a40f82996630ba43c79a0d366709ab4.jpg\"><p><strong>CORRECT: </strong>\"Create an Amazon SNS topic and an AWS Lambda function. Subscribe the Lambda function to the SNS topic and submit events to the SNS topic\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon SQS queue and publish events to the queue. Configure an Amazon EC2 instance to poll the queue and consume the messages\" is incorrect.</p><p>Amazon EC2 is not a serverless service so cannot if the application must be serverless.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SQS topic and an Amazon EC2 instance. Subscribe the instance to the SNS topic and submit events to the topic\" is incorrect.</p><p>EC2 is not serverless and cannot be subscribed to an SNS topic.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic and an AWS Lambda function. Configure an HTTP endpoint on the Lambda function and subscribe the HTTP endpoint to the SNS topic. Submit events to the SNS topic\" is incorrect.</p><p>An HTTP endpoint is not required for the serverless application. Lambda can be subscribed directly to an SNS topic.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3005,
                        "content": "<p>Create an Amazon SNS topic and an AWS Lambda function. Subscribe the Lambda function to the SNS topic and submit events to the SNS topic.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3006,
                        "content": "<p>Create an Amazon SQS queue and publish events to the queue. Configure an Amazon EC2 instance to poll the queue and consume the messages.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3007,
                        "content": "<p>Create an Amazon SQS topic and an Amazon EC2 instance. Subscribe the instance to the SNS topic and submit events to the topic.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3008,
                        "content": "<p>Create an Amazon SNS topic and an AWS Lambda function. Configure an HTTP endpoint on the Lambda function and subscribe the HTTP endpoint to the SNS topic. Submit events to the SNS topic.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 733,
            "attributes": {
                "createdAt": "2023-09-07T08:51:23.967Z",
                "updatedAt": "2023-09-07T08:51:23.967Z",
                "content": "<p>A Developer is creating a serverless application. The application looks up information about a customer using a separate Lambda function for each item such as address and phone number. The Developer has created branches in AWS Step Functions for each lookup function.</p><p>How can the Developer optimize the performance, so the lookups complete faster?</p>",
                "answerExplanation": "<p>The Parallel state (\"Type\": \"Parallel\") can be used to create parallel branches of execution in your AWS Step Functions state machine. This will improve the performance of the application by ensuring that all information lookups occur in parallel.</p><p><strong>CORRECT: </strong>\"Use a Parallel state to iterate over all the branches parallel\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a Choice state to lookup the specific information required\" is incorrect. This is used to add additional logic but is not required and is unlikely to improve performance.</p><p><strong>INCORRECT:</strong> \"Use a Wait state to reduce the wait time for function execution\" is incorrect. The Wait state delays the state machine from continuing for a specified time.</p><p><strong>INCORRECT:</strong> \"Use a Map state to iterate over all the items\" is incorrect. The Map state executes the same steps for multiple entries of an array in the state input.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-parallel-state.html\">https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-parallel-state.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3009,
                        "content": "<p>Use a Map state to iterate over all the items.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3010,
                        "content": "<p>Use a Choice state to lookup the specific information required.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3011,
                        "content": "<p>Use a Wait state to reduce the wait time for function execution.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3012,
                        "content": "<p>Use a Parallel state to iterate over all the branches parallel.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 734,
            "attributes": {
                "createdAt": "2023-09-07T08:51:24.042Z",
                "updatedAt": "2023-09-07T08:51:24.042Z",
                "content": "<p>A static website that serves a collection of images runs from an Amazon S3 bucket in the us-east-1 region. The website is gaining in popularity and is now being viewed around the world. How can a Developer improve the performance of the website for global users?</p>",
                "answerExplanation": "<p>CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. CloudFront is a good choice for distribution of frequently accessed static content that benefits from edge deliveryâ€”like popular website images, videos, media files or software downloads.</p><p><strong>CORRECT: </strong>\"Use Amazon CloudFront to cache the website content\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon ElastiCache to cache the website content\" is incorrect as ElastiCache is used for caching the contents of databases, not S3 buckets.</p><p><strong>INCORRECT:</strong> \"Use cross region replication to replicate the bucket to several global regions\" is incorrect as though this would get the content closer to users it would not provide a mechanism for connecting to those copies. This could be achieved using Route 53 latency based routing however it would be easier to use CloudFront.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 Transfer Acceleration to improve the performance of the website\" is incorrect as this service is used for improving the performance of uploads to Amazon S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 3013,
                        "content": "<p>Use Amazon S3 Transfer Acceleration to improve the performance of the website</p>",
                        "isValid": false
                    },
                    {
                        "id": 3014,
                        "content": "<p>Use Amazon CloudFront to cache the website content</p>",
                        "isValid": true
                    },
                    {
                        "id": 3015,
                        "content": "<p>Use cross region replication to replicate the bucket to several global regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 3016,
                        "content": "<p>Use Amazon ElastiCache to cache the website content</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 735,
            "attributes": {
                "createdAt": "2023-09-07T08:51:24.125Z",
                "updatedAt": "2023-09-07T08:51:24.125Z",
                "content": "<p>Twelve months ago, an organization requested a public certificate for their domain via AWS Certificate Manager (ACM). It was validated using DNS validation. What will ACM do prior to expiration? (Select TWO.)</p>",
                "answerExplanation": "<p>An AWS ACM certificate that was validated using DNS validation will automatically renew if the certificate is still being using by an AWS service 60 days prior to its expiration and has an ACM-provided CNAME that is accessible via public DNS. If the certificate is not being used or if the CNAME is not correct, ACM will not automatically validate the DNS and will send notifications starting at 45 days prior to the expiration date.</p><p><strong>CORRECT: </strong>â€œIf the certificate is being used by an AWS service and ACM-provided CNAME records are accessible via the public DNS, ACM will consider the domain name validated and auto renew the certificate\" is the correct answer (as explained above.)</p><p><strong>CORRECT:</strong> \"If the domain is not validated, it will send Health events or EventBridge events to notify the domain owner prior to expiration\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"If the certificate is being used by an AWS service and ACM-provided CNAME records are accessible via the public DNS, ACM will consider the domain name validated and send Health events or EventBridge events to notify the owner to renew the certificate\" is incorrect. If criteria are met, DNS can be automatically validated and auto renewed without further action.</p><p><strong>INCORRECT:</strong> \"If the certificate was validated through DNS validation with a valid CNAME record provided by ACM and is currently being used by an AWS service, it will use SNS to send notifications of pending expiration\" is incorrect. If criteria are met, certificate is auto renewed with no further action or notifications required.</p><p><strong>INCORRECT:</strong> \"If the certificate was issued by a third party, AWS ACM will send a request to the third party to verify the domain owner\" is incorrect. The AWS ACM will not send a request. A new certificate will need to be created using AWS ACM or imported.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/acm/latest/userguide/dns-renewal-validation.html\">https://docs.aws.amazon.com/acm/latest/userguide/dns-renewal-validation.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certificate-manager/\">https://digitalcloud.training/aws-certificate-manager/</a></p>",
                "options": [
                    {
                        "id": 3017,
                        "content": "<p>If the domain is not validated, it will send Health events or EventBridge events to notify the domain owner prior to expiration.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3018,
                        "content": "<p>If the certificate was issued by a third party, AWS ACM will send a request to the third party to verify the domain owner.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3019,
                        "content": "<p>If the certificate is being used by an AWS service and ACM-provided CNAME records are accessible via the public DNS, ACM will consider the domain name validated and send Health events or EventBridge events to notify the owner to renew the certificate.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3020,
                        "content": "<p>If the certificate was validated through DNS validation with a valid CNAME record provided by ACM and is currently being used by an AWS service, it will use SNS to send notifications of pending expiration.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3021,
                        "content": "<p>If the certificate is being used by an AWS service and ACM-provided CNAME records are accessible via the public DNS, ACM will consider the domain name validated and auto renew the certificate.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 736,
            "attributes": {
                "createdAt": "2023-09-07T08:51:24.206Z",
                "updatedAt": "2023-09-07T08:51:24.206Z",
                "content": "<p>A new application will be hosted on the domain name dctlabs.com using an Amazon API Gateway REST API front end. The Developer needs to configure the API with a path to dctlabs.com/products that will be accessed using the HTTP GET verb. How MUST the Developer configure the API? (Select TWO</p>",
                "answerExplanation": "<p>An API Gateway REST API is a collection of HTTP resources and methods that are integrated with backend HTTP endpoints, Lambda functions, or other AWS services. You can deploy this collection in one or more stages. Typically, API resources are organized in a resource tree according to the application logic. Each API resource can expose one or more API methods that have unique HTTP verbs supported by API Gateway.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_06-14-47-f88043be97e72b975a068fc68a6df375.png\"></p><p>As you can see from the image above, the Developer would need to create a resource which in this case would be /products. The Developer would then create a GET method within the resource.</p><p><strong>CORRECT: </strong>\"Create a /products resource\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Create a GET method\" is a correct answer.</p><p><strong>INCORRECT:</strong> \"Create a /products method\" is incorrect as a resource should be created.</p><p><strong>INCORRECT:</strong> \"Create a GET resource\" is incorrect as a method should be created.</p><p><strong>INCORRECT:</strong> \"Create a /GET method\" is incorrect as a method is not preceded by a slash.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-basic-concept.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-basic-concept.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 3022,
                        "content": "<p>Create a /GET method</p>",
                        "isValid": false
                    },
                    {
                        "id": 3023,
                        "content": "<p>Create a /products method</p>",
                        "isValid": false
                    },
                    {
                        "id": 3024,
                        "content": "<p>Create a GET resource</p>",
                        "isValid": false
                    },
                    {
                        "id": 3025,
                        "content": "<p>Create a /products resource</p>",
                        "isValid": true
                    },
                    {
                        "id": 3026,
                        "content": "<p>Create a GET method</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 737,
            "attributes": {
                "createdAt": "2023-09-07T08:51:24.276Z",
                "updatedAt": "2023-09-07T08:51:24.276Z",
                "content": "<p>An application will be hosted on the AWS Cloud. Developers will be using an Agile software development methodology with regular updates deployed through a continuous integration and delivery (CI/CD) model. Which AWS service can assist the Developers with automating the build, test, and deploy phases of the release process every time there is a code change?</p>",
                "answerExplanation": "<p>AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously.</p><p>Specifically, you can:</p><p><strong>Automate your release processes</strong>: CodePipeline fully automates your release process from end to end, starting from your source repository through build, test, and deployment. You can prevent changes from moving through a pipeline by including a manual approval action in any stage except a Source stage. You can release when you want, in the way you want, on the systems of your choice, across one instance or multiple instances.</p><p><strong>Establish a consistent release process</strong>: Define a consistent set of steps for every code change. CodePipeline runs each stage of your release according to your criteria.</p><p><strong>Speed up delivery while improving quality</strong>: You can automate your release process to allow your developers to test and release code incrementally and speed up the release of new features to your customers.</p><p><strong>Use your favorite tools</strong>: You can incorporate your existing source, build, and deployment tools into your pipeline.</p><p><strong>View progress at a glance</strong>: You can review real-time status of your pipelines, check the details of any alerts, retry failed actions, view details about the source revisions used in the latest pipeline execution in each stage, and manually rerun any pipeline.</p><p><strong>View pipeline history details</strong>: You can view details about executions of a pipeline, including start and end times, run duration, and execution IDs.</p><p>Therefore, AWS CodePipeline is the perfect tool for the Developerâ€™s requirements.</p><p><strong>CORRECT: </strong>\"AWS CodePipeline\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CloudFormation\" is incorrect as CloudFormation is not triggered by changes in a source code repository. You must create change sets for deploying updates.</p><p><strong>INCORRECT:</strong> \"AWS Elastic Beanstalk\" is incorrect as this is a platform service that can be used to deploy code to managed runtimes such as Nodejs. It does not update automatically based on changes to source code. You must update that environment when you need to release new code.</p><p><strong>INCORRECT:</strong> \"AWS CodeBuild\" is incorrect as CodeBuild is used for compiling code, running unit tests and creating the deployment package. It does not manage the deployment of the code.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-what-can-I-do.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-what-can-I-do.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3027,
                        "content": "<p>AWS Elastic Beanstalk</p>",
                        "isValid": false
                    },
                    {
                        "id": 3028,
                        "content": "<p>AWS CloudFormation</p>",
                        "isValid": false
                    },
                    {
                        "id": 3029,
                        "content": "<p>AWS CodePipeline</p>",
                        "isValid": true
                    },
                    {
                        "id": 3030,
                        "content": "<p>AWS CodeBuild</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 738,
            "attributes": {
                "createdAt": "2023-09-07T08:51:24.354Z",
                "updatedAt": "2023-09-07T08:51:24.354Z",
                "content": "<p>A company operates a multimedia sharing service on AWS. The service is hosted on Amazon EC2 instances in an Auto Scaling group, serving as the target for an Application Load Balancer (ALB).</p><p>The multimedia files are stored in an Amazon S3 bucket. The company is developing a feature for system request testing, which will redirect these requests to a separate target group hosting a test version of the application.</p><p>How can this be accomplished most efficiently?</p>",
                "answerExplanation": "<p>The most efficient solution is to use the ALB's content-based routing feature with a cookie-based strategy. This way, test requests identified by a specific cookie can be directed to the new target group hosting the test variant of the application, without any significant changes to the existing setup.</p><p><strong>CORRECT: </strong>\"Use ALB content-based routing. Create a separate target group for the test version of the application and route requests identified by a specific cookie\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Replicate the existing application and host it on a distinct EC2 instance. Manually route test requests to this instance\" is incorrect.</p><p>This approach doesn't leverage the ALB's content-based routing and adds unnecessary manual work</p><p><strong>INCORRECT:</strong> \"Create a new S3 bucket to host the test variant of the application. Redirect all test requests to this new bucket\" is incorrect.</p><p>This method doesn't make use of the ALB and EC2 instances for application hosting, thus inefficient</p><p><strong>INCORRECT:</strong> \"Create a separate AWS Lambda function to handle the test requests and direct them to the new target group\" is incorrect.</p><p>This is inefficient as it adds unnecessary complexity and circumvents the routing features of the ALB</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 3031,
                        "content": "<p>Use ALB content-based routing. Create a separate target group for the test version of the application and route requests identified by a specific cookie.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3032,
                        "content": "<p>Create a separate AWS Lambda function to handle the test requests and direct them to the new target group.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3033,
                        "content": "<p>Create a new S3 bucket to host the test variant of the application. Redirect all test requests to this new bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3034,
                        "content": "<p>Replicate the existing application and host it on a distinct EC2 instance. Manually route test requests to this instance.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 739,
            "attributes": {
                "createdAt": "2023-09-07T08:51:24.443Z",
                "updatedAt": "2023-09-07T08:51:24.443Z",
                "content": "<p>An organization has a new containerized application that needs to be launched quickly. The team lead has stated to choose the option that would reduce the burden of infrastructure maintenance for the team.</p><p>Which solution best meets this requirement?</p>",
                "answerExplanation": "<p>Copilot can run tasks and services on serverless infrastructures such as AWS Fargate. This can provide the support of containers using ECS and serverless services like Fargate does not require customers to manage.</p><p><strong>CORRECT</strong>: â€œUse AWS Copilot to provide a template that supports a quick launch of Amazon Elastic Container Service (ECS) applications on AWS Fargateâ€ is the correct answer (as explained above.)</p><p><strong>INCORRECT</strong>: \"Use AWS Copilot to provide a template that supports a quick launch of Amazon Elastic Container Service (ECS) applications on Amazon Elastic Compute Cloud (EC2)\" is incorrect.</p><p>EC2 is not a serverless service and would require additional maintenance and support from the development team.</p><p><strong>INCORRECT</strong>: \"Use AWS Fargate to provide a template that supports a quick launch of Amazon Elastic Container Service (ECS) applications on Amazon Elastic Compute Cloud (EC2)\" is incorrect.</p><p>EC2 is not a serverless service and would require additional maintenance and support from the development team. Fargate is a serverless compute engine. It would not provide a template to support a quick launch.</p><p><strong>INCORRECT</strong>: \"Use Amazon Elastic Container Service (ECS) to provide a template that supports a quick launch of AWS Fargate applications on Amazon Elastic Compute Cloud (EC2)\" is incorrect.</p><p>EC2 is not a serverless service and would require additional maintenance and support from the development team. ECS is a container orchestration service that would not provide a template.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-a-clustered-application-to-amazon-ecs-by-using-aws-copilot.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-a-clustered-application-to-amazon-ecs-by-using-aws-copilot.html</a></p>",
                "options": [
                    {
                        "id": 3035,
                        "content": "<p>Use Amazon Elastic Container Service (ECS) to provide a template that supports a quick launch of AWS Fargate applications on Amazon Elastic Compute Cloud (EC2).</p>",
                        "isValid": false
                    },
                    {
                        "id": 3036,
                        "content": "<p>Use AWS Copilot to provide a template that supports a quick launch of Amazon Elastic Container Service (ECS) applications on Amazon Elastic Compute Cloud (EC2).</p>",
                        "isValid": false
                    },
                    {
                        "id": 3037,
                        "content": "<p>Use AWS Fargate to provide a template that supports a quick launch of Amazon Elastic Container Service (ECS) applications on Amazon Elastic Compute Cloud (EC2).</p>",
                        "isValid": false
                    },
                    {
                        "id": 3038,
                        "content": "<p>Use AWS Copilot to provide a template that supports a quick launch of Amazon Elastic Container Service (ECS) applications on AWS Fargate.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 740,
            "attributes": {
                "createdAt": "2023-09-07T08:51:24.530Z",
                "updatedAt": "2023-09-07T08:51:24.530Z",
                "content": "<p>A development team is working with Amazon MemoryDB for Redis. The team lead wants to limit the default access of the service-linked role and implement a custom-managed policy instead.</p><p>What needs to be done prior to making this change?</p>",
                "answerExplanation": "<p>Service-linked roles are tied to the resource with predefined permissions. The default policy associated Amazon MemoryDB for Redis is â€œAmazonMemoryDBFullAccessâ€. This comes pre-provisioned with permission that the service requires to create a service link that is used to create resources and access other AWS resources and services.</p><p>You might decide not to use the default policy and instead to use a custom-managed policy. In this case, make sure that you have either permissions to call iam:createServiceLinkedRole or that you have created the MemoryDB service-linked role.</p><p><strong>CORRECT</strong>: \"The team lead would need to have permission to call `iam:createServiceLinkedRole`\" is the correct answer (as explained above.)</p><p><strong>INCORRECT</strong>: \"The team lead would have to login as the root user to change the permission setting\" is incorrect.</p><p>The team lead may not be the root user. Logging in and accessing the root user should be limited to the tasks that are necessary and limited to the root user. A Cloud Administrator, for example, could update the IAM roles needed for the team lead to make the appropriate service-linked policy changes.</p><p><strong>INCORRECT</strong>: \"The team lead would need permission from an AWS Technical Account Manager\" is incorrect.</p><p>There are some changes to an account or services that an organization would need to contact their technical account manager to change. An update to the IAM policy or changing a service-linked role is not one of them.</p><p><strong>INCORRECT</strong>: \"The team lead would edit the default policy named `AmazonMemoryDBFullAccess`\" is incorrect.</p><p>AWS managed policies cannot be edited. A custom service policy would need to create and attached with the appropriate IAM role.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/memorydb/latest/devguide/set-up.html\">https://docs.aws.amazon.com/memorydb/latest/devguide/set-up.html</a></p>",
                "options": [
                    {
                        "id": 3039,
                        "content": "<p>The team lead would edit the default policy named `AmazonMemoryDBFullAccess`.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3040,
                        "content": "<p>The team lead would need to have permission to call `iam:createServiceLinkedRole`.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3041,
                        "content": "<p>The team lead would have to login as the root user to change the permission setting.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3042,
                        "content": "<p>The team lead would need permission from an AWS Technical Account Manager.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 741,
            "attributes": {
                "createdAt": "2023-09-07T08:51:24.605Z",
                "updatedAt": "2023-09-07T08:51:24.605Z",
                "content": "<p>An organization is modernizing a software solution by migrating its database operations from Amazon EC2 instances to a serverless architecture. The software utilizes an Amazon RDS for PostgreSQL database and operates within a single VPC on AWS. Both the software and the database are currently deployed on a private subnet in the VPC. The organization needs to enable AWS Lambda functions to interact with the PostgreSQL database.</p><p>Which solution would securely satisfy these needs?</p>",
                "answerExplanation": "<p>Placing the Lambda functions within the same VPC as the RDS instance and ensuring they have an appropriate security group rule to access the RDS instance would allow the Lambda functions to interact with the PostgreSQL database without exposing the database to the public internet, keeping the operations secure and in line with the existing infrastructure.</p><p><strong>CORRECT: </strong>\"Place the Lambda functions within the same VPC as the RDS instance and ensure they have an appropriate security group rule to access the RDS instance\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the Lambda functions to connect directly to the RDS instance by using its public IP address\" is incorrect.</p><p>Configuring the Lambda functions to connect directly to the RDS instance using its public IP address is insecure and goes against best practices for keeping resources in private subnets isolated.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS AppRunner service in the VPC, which will host the Lambda functions and connect them to the RDS instance\" is incorrect.</p><p>AWS App Runner is primarily designed for building, deploying, and scaling containerized applications quickly, and is not suited to hosting Lambda functions or establishing connections with an RDS instance.</p><p><strong>INCORRECT:</strong> \"Establish an AWS Direct Connect link between the Lambda functions and the RDS instance\" is incorrect.</p><p>AWS Direct Connect is used for establishing a dedicated network connection from your premises to AWS and would not be applicable or necessary for Lambda functions trying to connect to an RDS instance within the same AWS environment.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3043,
                        "content": "<p>Establish an AWS Direct Connect link between the Lambda functions and the RDS instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3044,
                        "content": "<p>Deploy an AWS AppRunner service in the VPC, which will host the Lambda functions and connect them to the RDS instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3045,
                        "content": "<p>Place the Lambda functions within the same VPC as the RDS instance and ensure they have an appropriate security group rule to access the RDS instance.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3046,
                        "content": "<p>Configure the Lambda functions to connect directly to the RDS instance by using its public IP address.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 742,
            "attributes": {
                "createdAt": "2023-09-07T08:51:24.676Z",
                "updatedAt": "2023-09-07T08:51:24.676Z",
                "content": "<p>A company has a production application deployed using AWS Elastic Beanstalk. A new version of the application must be installed, and the company cannot tolerate any website downtime. If the application update fails, rollback should be fast and easy.</p><p>What deployment method should be used?</p>",
                "answerExplanation": "<p>The immutable deployment type launches new instances in a new ASG and deploys the version update to these instances before swapping traffic to these instances once healthy. There is zero downtime and a quick rollback in case of failures.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-30_04-53-37-af2590b49ad3dfd3ae78903b878f4da4.jpg\"><p><strong>CORRECT: </strong>\"Immutable\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Rolling\" is incorrect.</p><p>With a rolling update a few instances are updated at a time (batch), and then the deployment moves onto the next batch once the first batch is healthy. Each batch is taken out of service during deployment leading to downtime. If the update fails, you need to perform an additional rolling update to roll back the changes. This would not be ideal for this production environment.</p><p><strong>INCORRECT:</strong> \"All at once\" is incorrect.</p><p>This would take all instances down at the same time. This is not suitable for a production environment.</p><p><strong>INCORRECT:</strong> \"Incremental\" is incorrect.</p><p>This is not actually a type of deployment model in Elastic Beanstalk.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
                "options": [
                    {
                        "id": 3047,
                        "content": "<p>All at once</p>",
                        "isValid": false
                    },
                    {
                        "id": 3048,
                        "content": "<p>Rolling</p>",
                        "isValid": false
                    },
                    {
                        "id": 3049,
                        "content": "<p>Incremental</p>",
                        "isValid": false
                    },
                    {
                        "id": 3050,
                        "content": "<p>Immutable</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 743,
            "attributes": {
                "createdAt": "2023-09-07T08:51:24.749Z",
                "updatedAt": "2023-09-07T08:51:24.749Z",
                "content": "<p>An application stores data in Amazon RDS and uses Amazon ElastiCache to improve read performance. The developer has configured ElastiCache to update the cache immediately after any writes to the primary database.</p><p>What will be the result of this approach to caching?</p>",
                "answerExplanation": "<p>The ElastiCache deployment is using a write-through caching strategy. The write-through strategy adds data or updates data in the cache whenever data is written to the database. This means data in the cache is never stale. There is a write penalty, but not a read penalty (in terms of latency added).</p><p>However, with a write-through strategy, most data are never read so the cache can become large and expensive. Adding a TTL to records can assist with this.</p><p><strong>CORRECT: </strong>\"The cache will become large and expensive because the infrequently requested data is also written to the cache\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Caching will slow performance of the read queries because the cache is updated when the cache cannot find the requested data\" is incorrect.</p><p>This is true of a lazy loading strategy. With a write-through strategy there is a write penalty, but not a read penalty.</p><p><strong>INCORRECT:</strong> \"Load on the RDS database instance will increase because the cache is updated for every database update\" is incorrect.</p><p>There is still only one write to the RDS database instance.</p><p><strong>INCORRECT:</strong> \"There is a cache miss penalty because the cache is updated only after a cache miss, resulting in response latency\" is incorrect.</p><p>This is a description of a lazy loading strategy, not a write-through strategy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 3051,
                        "content": "<p>Caching will slow performance of the read queries because the cache is updated when the cache cannot find the requested data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3052,
                        "content": "<p>There is a cache miss penalty because the cache is updated only after a cache miss, resulting in response latency.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3053,
                        "content": "<p>Load on the RDS database instance will increase because the cache is updated for every database update.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3054,
                        "content": "<p>The cache will become large and expensive because the infrequently requested data is also written to the cache.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 744,
            "attributes": {
                "createdAt": "2023-09-07T08:51:24.826Z",
                "updatedAt": "2023-09-07T08:51:24.826Z",
                "content": "<p>A developer needs a long-term mountable storage solution for an Amazon Elastic Compute Cloud (EC2) instance using a compute optimized C6i instance type that meets heavily regulated compliance standards on data encryption for data at rest and in transit. What solution would provide this?</p>",
                "answerExplanation": "<p>Amazon EBS is permanent, mountable storage solution for Amazon EC2 instances. Amazon EBS volumes are not encrypted by default but can be encrypted for all current generation instance types and specific previous generation instance types. Amazon EBS volumes are not encrypted by default without additional configuration.</p><p><strong>CORRECT: </strong>\"Attach an Amazon Elastic Block Store (EBS) volume to the instance and enable encryption during creation\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Attach an Amazon Elastic Block Store (EBS) volume to the instance. Encryption is enabled automatically\" is incorrect.</p><p>Amazon EBS volumes can be configured to be encrypted upon creation or can be configured to encrypt new instances by default. AWS does not have Amazon EBS volumes encrypted by default.</p><p><strong>INCORRECT:</strong> \"Attach an AWS Simple Storage Service (S3) bucket to the instance and enable encryption during creation\" is incorrect.</p><p>Amazon S3 provides a block storage solution. It is not mountable to instances. In January 2023, AWS announced that new objects in AWS S3 would be encrypted by default.</p><p><strong>INCORRECT:</strong> Attach an AWS Simple Storage Service (S3) bucket to the instance. Encryption is enabled automatically\" is incorrect.</p><p>Amazon S3 provide a block storage solution. It is not mountable to instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
                "options": [
                    {
                        "id": 3055,
                        "content": "<p>Attach an AWS Simple Storage Service (S3) bucket to the instance. Encryption is enabled automatically.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3056,
                        "content": "<p>Attach an Amazon Elastic Block Store (EBS) volume to the instance. Encryption is enabled automatically.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3057,
                        "content": "<p>Attach an AWS Simple Storage Service (S3) bucket to the instance and enable encryption during creation.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3058,
                        "content": "<p>Attach an Amazon Elastic Block Store (EBS) volume to the instance and enable encryption during creation.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 745,
            "attributes": {
                "createdAt": "2023-09-07T08:51:24.901Z",
                "updatedAt": "2023-09-07T08:51:24.901Z",
                "content": "<p>A new AWS Lambda function processes data and sends it to another service. The data is around 1 MB in size. A developer has been asked to update the function so it encrypts the data before sending it on to the other service.</p><p>Which API call is required to perform the encryption?</p>",
                "answerExplanation": "<p>To create a data key, call the GenerateDataKey operation. AWS KMS generates the data key. Then it encrypts a copy of the data key under a symmetric encryption KMS key that you specify. The operation returns a plaintext copy of the data key and the copy of the data key encrypted under the KMS key. The following image shows this operation.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-30_07-04-37-8dfdaca6937b17733985dbbd7ae4d9f2.jpg\"><p>AWS KMS cannot use a data key to encrypt data. But you can use the data key outside of AWS KMS, such as by using OpenSSL or a cryptographic library like the AWS Encryption SDK.</p><p>After using the plaintext data key to encrypt data, remove it from memory as soon as possible. You can safely store the encrypted data key with the encrypted data, so it is available to decrypt the data.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-30_07-04-38-91457dce5546a2f8371cda0931b9552d.jpg\"><p>In this case, the Lambda function can use the encryption keys generated to encrypt the data before sending it to the other service. The GenerateDataKey API is the correct API action to use.</p><p><strong>CORRECT: </strong>\"Issue the AWS KMS GenerateDataKey API to return an encryption key\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Issue the AWS KMS GenerateDataKeyWithoutPlainText API to return an encryption key\" is incorrect.</p><p>This API action returns only an encrypted data key. When you need to use the data key, ask AWS KMS to decrypt it. The correct API for directly encrypting larger amounts of data is the GenerateDataKey API.</p><p><strong>INCORRECT:</strong> \"Pass the data directly to AWS KMS and issue the Encrypt API for encryption\" is incorrect.</p><p>AWS KMS can only encrypt data up to 4096 bytes. Therefore, the data must be encrypted outside of KMS and a data key must be generated for this purpose.</p><p><strong>INCORRECT:</strong> \"Pass the data directly to AWS KMS and issue the ReEncrypt API for encryption\" is incorrect.</p><p>The reencrypt API decrypts data before reencrypting it within AWS KMS. In this case the data is not currently encrypted and the operations cannot take place within KMS as the data is larger than the KMS maximum of 4096 bytes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</a></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#data-keys\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#data-keys</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
                "options": [
                    {
                        "id": 3059,
                        "content": "<p>Pass the data directly to AWS KMS and issue the Encrypt API for encryption.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3060,
                        "content": "<p>Pass the data directly to AWS KMS and issue the ReEncrypt API for encryption.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3061,
                        "content": "<p>Issue the AWS KMS GenerateDataKeyWithoutPlainText API to return an encryption key.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3062,
                        "content": "<p>Issue the AWS KMS GenerateDataKey API to return an encryption key.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 746,
            "attributes": {
                "createdAt": "2023-09-07T08:51:24.972Z",
                "updatedAt": "2023-09-07T08:51:24.972Z",
                "content": "<p>A developer is implementing Amazon ElastiCache in an application that needs to reflect real-time data on dashboards. The data is sourced from a database and needs to be stored in the cache.</p><p>What would be the ideal caching mechanism for this requirement?</p>",
                "answerExplanation": "<p>The write-through caching strategy ensures that data is written into the cache every time it's updated in the database. This makes it an ideal strategy for scenarios where real-time data is required on dashboards as it reduces the chances of a cache miss, and any read request will have the latest data.</p><p><strong>CORRECT: </strong>\"A write-through cache\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"A lazy-loading cache\" is incorrect.</p><p>Lazy-loading strategy only caches the data when a request for that data is made. This might cause a delay in populating real-time dashboards during the first request for data.</p><p><strong>INCORRECT:</strong> \"A write-behind cache\" is incorrect.</p><p>Write-behind caching is not supported by Amazon ElastiCache. This strategy can lead to data inconsistency between the cache and the database.</p><p><strong>INCORRECT:</strong> \"A read-through cache\" is incorrect.</p><p>Read-through caches are not supported by Amazon ElastiCache.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Strategies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 3063,
                        "content": "<p>A read-through cache</p>",
                        "isValid": false
                    },
                    {
                        "id": 3064,
                        "content": "<p>A lazy-loading cache</p>",
                        "isValid": false
                    },
                    {
                        "id": 3065,
                        "content": "<p>A write-behind cache</p>",
                        "isValid": false
                    },
                    {
                        "id": 3066,
                        "content": "<p>A write-through cache</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 747,
            "attributes": {
                "createdAt": "2023-09-07T08:51:25.047Z",
                "updatedAt": "2023-09-07T08:51:25.047Z",
                "content": "<p>A start-up organization is launching a new website. Which statement correctly describes how to set up the domain, routing, and health checks in AWS?</p>",
                "answerExplanation": "<p>Route 53 can be used as a DNS to register a domain name, route the internet traffic, and perform health checks on resources. If being used for all three tasks, the order of register domain, route the traffic, and perform health checks must be sequential. ACM and Shield are not needed for this task.</p><p><strong>CORRECT: </strong>\"Use Route 53 to register a domain name, route the internet traffic to domain, and specify the values to perform health checks on resources\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Route 53 to register a domain name and AWS Certificate Manager to perform routing to the domain and Shield to perform health checks on resources\" is incorrect. Route 53 can do all the tasks. ACM and Shield cannot perform these tasks.</p><p><strong>INCORRECT:</strong> \"Use Route 53 to specify the IP address to perform health checks, register a domain name, and route the internet traffic to domain\" is incorrect. Route 53 can be used to perform these tasks but must done in a specific order starting with registering a domain.</p><p><strong>INCORRECT:</strong> \"Use Route 53 to register a domain name and perform routing to the domain and Shield to perform health checks on resources\" is incorrect. Route 53 can do all these tasks. Shield does not perform health checks on resources.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
                "options": [
                    {
                        "id": 3067,
                        "content": "<p>Use Route 53 to register a domain name and AWS Certificate Manager to perform routing to the domain and Shield to perform health checks on resources.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3068,
                        "content": "<p>Use Route 53 to specify the IP address to perform health checks, register a domain name, and route the internet traffic to domain.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3069,
                        "content": "<p>Use Route 53 to register a domain name and perform routing to the domain and Shield to perform health checks on resources.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3070,
                        "content": "<p>Use Route 53 to register a domain name, route the internet traffic to domain, and specify the values to perform health checks on resources.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 748,
            "attributes": {
                "createdAt": "2023-09-07T08:51:25.128Z",
                "updatedAt": "2023-09-07T08:51:25.128Z",
                "content": "<p>A company has an application that provides access to objects in Amazon S3 based on the type of user. The user types are registered user and guest user. The company has 30,000 users. Information is read from an S3 bucket depending on the user type.</p><p>Which approaches are recommended to provide access to both user types MOST efficiently? (Select TWO.)</p>",
                "answerExplanation": "<p>Amazon Cognito can be used with identity pools. A Cognito identity pool supports both authenticated and unauthenticated identities. Authenticated identities belong to users who are authenticated by any supported identity provider. Unauthenticated identities typically belong to guest users.</p><p>The most secure way of using the IAM service for this solution would be to use separate roles. IAM roles can be securely assumed based on the type of user. Each role can be configured with different permission sets as applicable to registered and guest users.</p><p><strong>CORRECT: </strong>\"Use Amazon Cognito to provide access using authenticated and unauthenticated roles\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use the AWS IAM service and let the application assume different roles depending on the type of user\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store separate access keys in the application code for registered users and guest users to provide access to the objects\" is incorrect.</p><p>This is highly insecure. You should avoid embedding access keys in application code and use IAM roles instead.</p><p><strong>INCORRECT:</strong> \"Create a new IAM user for each user and grant access to the S3 objects\" is incorrect.</p><p>This would be a lot of users and is an inefficient solution.</p><p><strong>INCORRECT:</strong> \"Use S3 bucket policies to restrict read access to specific IAM users\" is incorrect.</p><p>This would also be highly complex with so many users and would need constant updating when users need to be added or removed.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 3071,
                        "content": "<p>Use the AWS IAM service and let the application assume different roles depending on the type of user.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3072,
                        "content": "<p>Create a new IAM user for each user and grant access to the S3 objects.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3073,
                        "content": "<p>Use Amazon Cognito to provide access using authenticated and unauthenticated roles.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3074,
                        "content": "<p>Store separate access keys in the application code for registered users and guest users to provide access to the objects.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3075,
                        "content": "<p>Use S3 bucket policies to restrict read access to specific IAM users.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 749,
            "attributes": {
                "createdAt": "2023-09-07T08:51:25.204Z",
                "updatedAt": "2023-09-07T08:51:25.204Z",
                "content": "<p>A company uses Amazon DynamoDB to store sensitive data that must be encrypted. The company security policy mandates that data must be encrypted before it is submitted to DynamoDB</p><p>How can a Developer meet these requirements?</p>",
                "answerExplanation": "<p>In addition to encryption at rest, which is a <em>server-side encryption</em> feature, AWS provides the Amazon DynamoDB Encryption Client. This <em>client-side encryption</em> library enables you to protect your table data before submitting it to DynamoDB. With server-side encryption, your data is encrypted in transit over an HTTPS connection, decrypted at the DynamoDB endpoint, and then re-encrypted before being stored in DynamoDB. Client-side encryption provides end-to-end protection for your data from its source to storage in DynamoDB.</p><p><strong>CORRECT: </strong>\"Use the DynamoDB Encryption Client to enable end-to-end protection using client-side encryption\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use the UpdateTable operation to switch to a customer managed customer master key (CMK)\" is incorrect. This will not ensure data is encrypted before it is submitted to DynamoDB; to meet this requirement, client-side encryption must be used.</p><p><strong>INCORRECT:</strong> \"Use the UpdateTable operation to switch to an AWS managed customer master key (CMK)\" is incorrect. is will not ensure data is encrypted before it is submitted to DynamoDB; to meet this requirement, client-side encryption must be used.</p><p><strong>INCORRECT:</strong> \"Use AWS Certificate Manager (ACM) to create one certificate for each DynamoDB table\" is incorrect. ACM is used to create SSL/TLS certificates and you cannot attach these to a DynamoDB table.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-dynamodb.html\">https://docs.aws.amazon.com/kms/latest/Developerguide/services-dynamodb.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3076,
                        "content": "<p>Use the UpdateTable operation to switch to a customer managed customer master key (CMK).</p>",
                        "isValid": false
                    },
                    {
                        "id": 3077,
                        "content": "<p>Use the UpdateTable operation to switch to an AWS managed customer master key (CMK).</p>",
                        "isValid": false
                    },
                    {
                        "id": 3078,
                        "content": "<p>Use the DynamoDB Encryption Client to enable end-to-end protection using client-side encryption.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3079,
                        "content": "<p>Use AWS Certificate Manager (ACM) to create one certificate for each DynamoDB table.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 750,
            "attributes": {
                "createdAt": "2023-09-07T08:51:25.277Z",
                "updatedAt": "2023-09-07T08:51:25.277Z",
                "content": "<p>A Developer has deployed an application that runs on an Auto Scaling group of Amazon EC2 instances. The application data is stored in an Amazon DynamoDB table and records are constantly updated by all instances. An instance sometimes retrieves old data. The Developer wants to correct this by making sure the reads are strongly consistent.</p><p>How can the Developer accomplish this?</p>",
                "answerExplanation": "<p>When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful.</p><p>The GetItem operation returns a set of attributes for the item with the given primary key. If there is no matching item, GetItem does not return any data and there will be no Item element in the response.</p><p>GetItem provides an eventually consistent read by default. If your application requires a strongly consistent read, set ConsistentRead to true. Although a strongly consistent read might take more time than an eventually consistent read, it always returns the last updated value.</p><p>Therefore, the Developer should set ConsistentRead to true when calling GetItem.</p><p><strong>CORRECT: </strong>\"Set ConsistentRead to true when calling GetItem\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a new DynamoDB Accelerator (DAX) table\" is incorrect as DAX is not used to enable strongly consistent reads. DAX is used for improving read performance as it caches data in an in-memory cache.</p><p><strong>INCORRECT:</strong> \"Set consistency to strong when calling UpdateTable\" is incorrect as you cannot use this API action to configure consistency at a table level.</p><p><strong>INCORRECT:</strong> \"Use the GetShardIterator command\" is incorrect as this is not related to DynamoDB, it is related to Amazon Kinesis.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_GetItem.html\">https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_GetItem.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3080,
                        "content": "<p>Set consistency to strong when calling <code>UpdateTable</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3081,
                        "content": "<p>Create a new DynamoDB Accelerator (DAX) table</p>",
                        "isValid": false
                    },
                    {
                        "id": 3082,
                        "content": "<p>Use the <code>GetShardIterator</code> command</p>",
                        "isValid": false
                    },
                    {
                        "id": 3083,
                        "content": "<p>Set <code>ConsistentRead</code> to true when calling <code>GetItem</code> </p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 751,
            "attributes": {
                "createdAt": "2023-09-07T08:51:25.358Z",
                "updatedAt": "2023-09-07T08:51:25.358Z",
                "content": "<p>An AWS Lambda function is being developed in a VPC. When a file is added to an Amazon S3 bucket, this Lambda function is triggered, processes the file, and logs the results into a file. These result and log files need to be accessible by other AWS services and on-premises resources.</p><p>What should the developer use to meet these requirements?</p>",
                "answerExplanation": "<p>The correct solution is Amazon EFS as it provides a scalable, fully managed elastic NFS file system for AWS Cloud and on-premises resources. EFS can scale to petabytes without disrupting applications, making it suitable for Lambda functions to read/write large amounts of data.</p><p>Furthermore, AWS Lambda can now mount an Amazon EFS file system, making it easy for Lambda functions to read and write large amounts of data or to share data across a fleet of functions.</p><p><strong>CORRECT: </strong>\"Keep the result and log files in Amazon Elastic File System (EFS) accessible by Lambda functions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the result and log files in Amazon S3 and append the new log entries to existing objects\" is incorrect.</p><p>Amazon S3 does not support appending data to existing S3 objects.</p><p><strong>INCORRECT:</strong> \"Use Amazon DynamoDB to store the files and enable DynamoDB Streams to send notifications of changes\" is incorrect.</p><p>While Amazon DynamoDB can store data and Streams can notify services of changes, it is not designed for the type of file storage and sharing described in this scenario.</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to consolidate and catalog all result and log files and append log entries\" is incorrect.</p><p>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for users to prepare and load their data for analytics, but it is not used for direct file storage and sharing.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
                "options": [
                    {
                        "id": 3084,
                        "content": "<p>Use Amazon DynamoDB to store the files and enable DynamoDB Streams to send notifications of changes.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3085,
                        "content": "<p>Store the result and log files in Amazon S3 and append the new log entries to existing objects.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3086,
                        "content": "<p>Use AWS Glue to consolidate and catalog all result and log files and append log entries.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3087,
                        "content": "<p>Keep the result and log files in Amazon Elastic File System (EFS) accessible by Lambda functions.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 752,
            "attributes": {
                "createdAt": "2023-09-07T08:51:25.435Z",
                "updatedAt": "2023-09-07T08:51:25.435Z",
                "content": "<p>An engineer is tasked with creating an AWS CloudFormation template that is capable of automatically determining the AWS Region where it is being deployed.</p><p>What is the MOST efficient method to ascertain the Region during deployment of the template?</p>",
                "answerExplanation": "<p>In AWS CloudFormation, the AWS::Region pseudo parameter automatically resolves to the Region where the stack is being created or updated. This makes it the most operationally efficient way to determine the Region in which the template is being deployed, as it doesn't require any additional input, processing, or external service calls.</p><p><strong>CORRECT: </strong>\"Utilize the AWS::Region pseudo parameter\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Request the Region as an input parameter during CloudFormation deployment\" is incorrect.</p><p>This method is not operationally efficient as it relies on manual input from the user each time the template is deployed. It also increases the risk of human error or inconsistencies, especially in multi-region deployments.</p><p><strong>INCORRECT:</strong> \"Extract the Region from the AWS::StackId pseudo parameter by applying the Fn::Split intrinsic function\" is incorrect.</p><p>Although this method technically works because the stack ID contains the region, it's not the most efficient approach. This requires additional processing to extract the region from the stack ID, which makes it less efficient than directly using the AWS::Region pseudo parameter.</p><p><strong>INCORRECT:</strong> \"Dynamically fetch the Region by referencing the corresponding entry in AWS Systems Manager Parameter Store\" is incorrect.</p><p>This method requires an external service and assumes that the region has been correctly stored as a parameter. This adds additional complexity and dependency on another service, making it less efficient than using the built-in AWS::Region pseudo parameter.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>",
                "options": [
                    {
                        "id": 3088,
                        "content": "<p>Request the Region as an input parameter during CloudFormation deployment.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3089,
                        "content": "<p>Dynamically fetch the Region by referencing the corresponding entry in AWS Systems Manager Parameter Store.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3090,
                        "content": "<p>Utilize the AWS::Region pseudo parameter.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3091,
                        "content": "<p>Extract the Region from the AWS::StackId pseudo parameter by applying the Fn::Split intrinsic function.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 753,
            "attributes": {
                "createdAt": "2023-09-07T08:51:25.505Z",
                "updatedAt": "2023-09-07T08:51:25.505Z",
                "content": "<p>A developer is working on a serverless application using AWS Cloud Development Kit (AWS CDK) and intends to set up several AWS Lambda functions and Amazon API Gateway APIs during the creation of an AWS CloudFormation stack. The developer's local machine is equipped with AWS Serverless Application Model (AWS SAM) and the AWS CDK.</p><p>What is the best way for the developer to locally test a particular Lambda function?</p>",
                "answerExplanation": "<p>AWS SAM CLI is a developer tool that allows you to locally test and debug your AWS Lambda functions defined by AWS Serverless Application Model (SAM) templates. Using the <strong>sam local invoke</strong> command with the specific Lambda function as an argument, you can execute the function locally.</p><p><strong>CORRECT: </strong>\"Use the sam local invoke command specifying the specific Lambda function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Test the Lambda function through the AWS Management Console\" is incorrect.</p><p>The AWS Management Console allows you to create and manage AWS Lambda functions but doesn't provide a feature to test functions locally.</p><p><strong>INCORRECT:</strong> \"Use the AWS CDK command line to locally run the Lambda function\" is incorrect.</p><p>AWS CDK is a software development framework to define cloud infrastructure in code and provision it through AWS CloudFormation, but it doesn't allow local execution of Lambda functions.</p><p><strong>INCORRECT:</strong> \"Directly execute the Lambda function on the local machine\" is incorrect.</p><p>AWS Lambda functions cannot be executed directly on a local machine without a local testing tool like AWS SAM CLI.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-using-invoke.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-using-invoke.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-sam/\">https://digitalcloud.training/aws-sam/</a></p>",
                "options": [
                    {
                        "id": 3092,
                        "content": "<p>Directly execute the Lambda function on the local machine.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3093,
                        "content": "<p>Test the Lambda function through the AWS Management Console.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3094,
                        "content": "<p>Use the AWS CDK command line to locally run the Lambda function.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3095,
                        "content": "<p>Use the sam local invoke command specifying the specific Lambda function.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 754,
            "attributes": {
                "createdAt": "2023-09-07T08:51:25.578Z",
                "updatedAt": "2023-09-07T08:51:25.578Z",
                "content": "<p>An organization needs to create a central file storage solution that scales on demand and can be used by multiple Amazon Elastic Compute Cloud (EC2) instances and AWS Lambda functions. Which storage solution will meet these requirements?</p>",
                "answerExplanation": "<p>Amazon Elastic File System (EFS) provides a scalable storage solution that does not require provisioning. It can scale up and down based on files stored. It can be used as a file source for multiple computing services such as Amazon Elastic Compute Cloud (EC2), Amazon Elastic Container Service (ECS), AWS Lambda.</p><p><strong>CORRECT</strong>: \"Create an Amazon Elastic File System (EFS) file system\" is the correct answer (as explained above.)</p><p><strong>INCORRECT</strong>: \"Create an Amazon ElastiCache cluster\" is incorrect.</p><p>Amazon ElastiCache is a database caching service; it cannot be used for file storage.</p><p><strong>INCORRECT</strong>: \"Create an Amazon EBS Multi-Attach volume\" is incorrect.</p><p>Amazon EBS Multi-Attach Provisioned IOPS SSD can be used to attach a volume to multiple EC2 instances but not to other computing services such as Lambda.</p><p><strong>INCORRECT</strong>: \"Create an AWS Storage Gateway volume gateway\" is incorrect.</p><p>Amazon Storage Gateway is used for hybrid infrastructure set-ups that allows on-premises infrastructure to use AWS cloud storage.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html\">https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
                "options": [
                    {
                        "id": 3096,
                        "content": "<p>Create an Amazon Elastic File System (EFS) file system.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3097,
                        "content": "<p>Create an AWS Storage Gateway volume gateway.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3098,
                        "content": "<p>Create an Amazon EBS Multi-Attach volume.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3099,
                        "content": "<p>Create an Amazon ElastiCache cluster.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 755,
            "attributes": {
                "createdAt": "2023-09-07T08:51:25.663Z",
                "updatedAt": "2023-09-07T08:51:25.663Z",
                "content": "<p>A company has launched a web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The connection between clients and the ALB should use the HTTPS protocol. A developer uses AWS Certificate Manager (ACM) to issue an X.509 certificate.</p><p>What steps must the developer take to secure the connection?</p>",
                "answerExplanation": "<p>To secure the connection between clients and the ALB the developer must simply attach the certificate to the ALB. This can be performed using the AWS Management Console. There is no need to add the certificate to each EC2 instance. If end-to-end encryption is required, self-signed certificates or certificates issued by a private CA can be used on the EC2 instances (not required by the solution requirements).</p><p><strong>CORRECT: </strong>\"Configure the ALB to use the X.509 certificate by using the AWS Management Console\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure each EC2 instance to use the X.509 certificate by using the AWS Management Console\" is incorrect. This is not required as explained above.</p><p><strong>INCORRECT:</strong> \"Export the root key of the X.509 certificate to an Amazon S3 bucket. Configure the ALB to use the X.509 certificate from the S3 bucket\" is incorrect. This is the wrong procedure, the certificate can simply be attached to the ALB.</p><p><strong>INCORRECT:</strong> \"Export the root key of the X.509 certificate to an Amazon S3 bucket. Configure each EC2 instance to use the same X.509 certificate from the S3 bucket\" is incorrect. As explained above, this is not required and is the wrong procedure.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/\">https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certificate-manager/\">https://digitalcloud.training/aws-certificate-manager/</a></p>",
                "options": [
                    {
                        "id": 3100,
                        "content": "<p>Configure each EC2 instance to use the X.509 certificate by using the AWS Management Console.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3101,
                        "content": "<p>Export the root key of the X.509 certificate to an Amazon S3 bucket. Configure each EC2 instance to use the same X.509 certificate from the S3 bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3102,
                        "content": "<p>Configure the ALB to use the X.509 certificate by using the AWS Management Console.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3103,
                        "content": "<p>Export the root key of the X.509 certificate to an Amazon S3 bucket. Configure the ALB to use the X.509 certificate from the S3 bucket.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 756,
            "attributes": {
                "createdAt": "2023-09-07T08:51:25.738Z",
                "updatedAt": "2023-09-07T08:51:25.738Z",
                "content": "<p>An organization has applied a public certificate to a domain name using AWS Certificate Manager (ACM). What will the browser show to indicate that the site is trusted and is connected via SSL/TLS?</p>",
                "answerExplanation": "<p>ACM certificates are trusted by all major browsers. When connected by SSL/TLS sites using ACM certificates, this trust is indicated by the lock icon in browser address bar.</p><p><strong>CORRECT: </strong>\"Lock icon on the address bar\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"DNS validation\" is incorrect. DNS validation is used to validate domain ownership and is completed when the certificate is requested.</p><p><strong>INCORRECT:</strong> \"RSA key algorithm\" is incorrect. The RSA key algorithm is selected when requesting the certificate. It is not shown in the browser.</p><p><strong>INCORRECT:</strong> \"ECDSA key algorithm\" is incorrect. The RSA key algorithm is selected when requesting the certificate. It is not shown in the browser.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/acm/latest/userguide/acm-certificate.html\">https://docs.aws.amazon.com/acm/latest/userguide/acm-certificate.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certificate-manager/\">https://digitalcloud.training/aws-certificate-manager/</a></p>",
                "options": [
                    {
                        "id": 3104,
                        "content": "<p>DNS validation.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3105,
                        "content": "<p>ECDSA key algorithm.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3106,
                        "content": "<p>Lock icon on the address bar.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3107,
                        "content": "<p>RSA key algorithm.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 757,
            "attributes": {
                "createdAt": "2023-09-07T08:51:25.808Z",
                "updatedAt": "2023-09-07T08:51:25.808Z",
                "content": "<p>A Developer is building a WebSocket API using Amazon API Gateway. The payload sent to this API is JSON that includes an action key which can have multiple values. The Developer must integrate with different routes based on the value of the action key of the incoming JSON payload.</p><p>How can the Developer accomplish this task with the LEAST amount of configuration?</p>",
                "answerExplanation": "<p>In your WebSocket API, incoming JSON messages are directed to backend integrations based on routes that you configure. (Non-JSON messages are directed to a $default route that you configure.)</p><p>A <em>route</em> includes a <em>route key</em>, which is the value that is expected once a <em>route selection expression</em> is evaluated. The routeSelectionExpression is an attribute defined at the API level. It specifies a JSON property that is expected to be present in the message payload.</p><p>For example, if your JSON messages contain an action property and you want to perform different actions based on this property, your route selection expression might be ${request.body.action}. Your routing table would specify which action to perform by matching the value of the action property against the custom route key values that you have defined in the table.</p><p><strong>CORRECT: </strong>\"Set the value of the route selection expression to $request.body.action\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a separate stage for each possible value of the action key\" is incorrect. There is no need to create separate stages, the action key can be used for routing as described above.</p><p><strong>INCORRECT:</strong> \"Create a mapping template to map the action key to an integration request\" is incorrect. Mapping templates are not used for routing to different integrations, they are used for transforming data.</p><p><strong>INCORRECT:</strong> \"Set the value of the route selection expression to $default\" is incorrect. The $default route is used for routing non-JSON messages.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/websocket-api-develop-routes.html\">https://docs.aws.amazon.com/apigateway/latest/Developerguide/websocket-api-develop-routes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 3108,
                        "content": "<p>Set the value of the route selection expression to $request.body.action.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3109,
                        "content": "<p>Set the value of the route selection expression to $default.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3110,
                        "content": "<p>Create a mapping template to map the action key to an integration request.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3111,
                        "content": "<p>Create a separate stage for each possible value of the action key.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 758,
            "attributes": {
                "createdAt": "2023-09-07T08:51:25.885Z",
                "updatedAt": "2023-09-07T08:51:25.885Z",
                "content": "<p>A company is deploying a serverless application on AWS, using an AWS Lambda function for handling customer orders. This function utilizes an external HTTP API for processing payments, which occasionally times out. The requirement is to notify the support team via an existing Amazon SNS topic when the error rate of this API surpasses 10% per hour.</p><p>How can this be achieved?</p>",
                "answerExplanation": "<p>Amazon CloudWatch Metrics can monitor and track API error rates and can be configured to trigger an alert when a particular threshold is surpassed. By linking this to the existing Amazon SNS topic, the support team can be notified in near real-time when the payment processing external API error rate exceeds 10% of the total number of transactions in an hour.</p><p><strong>CORRECT: </strong>\"Implement Amazon CloudWatch Metrics with a custom threshold and link it to the existing SNS topic\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS X-Ray to monitor the error rate and configure it to send alerts through the SNS topic\" is incorrect.</p><p>AWS X-Ray is more suited for tracing applications to debug and analyze individual user paths, rather than for real-time alerting on API error rates.</p><p><strong>INCORRECT:</strong> \"Configure the AWS Lambda function to directly send alerts to the SNS topic when there is a timeout\" is incorrect.</p><p>Modifying the Lambda function to directly send alerts may increase the complexity of the function and may not be as reliable or timely as using a dedicated monitoring service like CloudWatch.</p><p><strong>INCORRECT:</strong> \"Enable AWS CloudTrail to monitor the Lambda function and send alerts to the SNS topic when the error rate exceeds 10%\" is incorrect.</p><p>AWS CloudTrail is a service that provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services. It is not designed to monitor error rates of APIs or Lambda functions and trigger alerts based on those.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3112,
                        "content": "<p>Enable AWS CloudTrail to monitor the Lambda function and send alerts to the SNS topic when the error rate exceeds 10%.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3113,
                        "content": "<p>Implement Amazon CloudWatch Metrics with a custom threshold and link it to the existing SNS topic.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3114,
                        "content": "<p>Configure the AWS Lambda function to directly send alerts to the SNS topic when there is a timeout.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3115,
                        "content": "<p>Use AWS X-Ray to monitor the error rate and configure it to send alerts through the SNS topic.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 759,
            "attributes": {
                "createdAt": "2023-09-07T08:51:25.958Z",
                "updatedAt": "2023-09-07T08:51:25.958Z",
                "content": "<p>In an organization, AWS CloudFormation templates are used to deploy all Amazon RDS DB instances as part of AWS CodePipeline CI/CD automation. As part of the deployment process, the main password for the DB instance needs to be auto generated.</p><p>What approach can be taken to fulfill these prerequisites with the minimum possible development effort?</p>",
                "answerExplanation": "<p>This answer is correct because it leverages AWS Secrets Manager, a service designed specifically for secure storage and retrieval of secrets.</p><p>It does this without the added complexity and potential security concerns of using a custom Lambda function or CodeBuild action.</p><p>By using the secretsmanager dynamic reference in the CloudFormation template, you can retrieve the secret value directly, reducing the risk of exposure.</p><p><strong>CORRECT: </strong>\"Use AWS Secrets Manager via the AWS SDK to generate a secure string. Use a dynamic reference to create the DB instance with the secret value\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS KMS to generate a random encryption key, then use this key as the password for the DB instance\" is incorrect.</p><p>This is incorrect because AWS KMS is used for creating and managing cryptographic keys and controlling their use across a wide range of AWS services and in your applications, not for generating random strings.</p><p><strong>INCORRECT:</strong> \"Utilize AWS::EC2::KeyPair to generate a secure string and assign it as the password for the DB instance\" is incorrect.</p><p>This approach is incorrect as the AWS::EC2::KeyPair is used for SSH login information for EC2 instances, not for generating passwords for RDS DB instances.</p><p><strong>INCORRECT:</strong> \"Write custom AWS Lambda function code that, when triggered, creates an encrypted string, and uses it as the password for the DB instance\" is incorrect.</p><p>This approach introduces unnecessary complexity and development effort.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-secrets-manager/\">https://digitalcloud.training/aws-secrets-manager/</a></p>",
                "options": [
                    {
                        "id": 3116,
                        "content": "<p>Use AWS KMS to generate a random encryption key, then use this key as the password for the DB instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3117,
                        "content": "<p>Utilize AWS::EC2::KeyPair to generate a secure string and assign it as the password for the DB instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3118,
                        "content": "<p>Write custom AWS Lambda function code that, when triggered, creates an encrypted string, and uses it as the password for the DB instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3119,
                        "content": "<p>Use AWS Secrets Manager via the AWS SDK to generate a secure string. Use a dynamic reference to create the DB instance with the secret value.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 760,
            "attributes": {
                "createdAt": "2023-09-07T08:51:26.032Z",
                "updatedAt": "2023-09-07T08:51:26.032Z",
                "content": "<p>A junior developer has been assigned the project of making updates to a current application using AWS Cloud Development Kit (CDK).</p><p>How can the developer revert to a previous version if there is an error?</p>",
                "answerExplanation": "<p>Synthesis is the process of converting AWS CDK stacks to AWS CloudFormation templates and assets. Changes should only be during the deployment phase and after AWS CloudFormation template has been created. This will allow it to roll back the change if there are any problems.</p><p><strong>CORRECT:</strong> \"Only make changes in the deployment phase, and never during synthesis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Once CloudFormation has been installed, it can be used to revert to previous versions\" is incorrect. The CDK Toolkit already provides the ability to convert CDK stacks into CloudFormation templates. There is no need to install CloudFormation.</p><p><strong>INCORRECT:</strong> \"Once CloudFormation has been added to the application, it can be used to manage the scope\" is incorrect. The CDK Toolkit already provides the ability to convert CDK stacks into CloudFormation templates. It is not used to manage the scope.</p><p><strong>INCORRECT:</strong> \"Once synthesis is complete, it can be used to track drift changes\" is incorrect. CloudFormation can detect drift changes if changes are made outside of CloudFormation management. Synthesis is the process of converting AWS CDK stacks to AWS CloudFormation templates.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cdk/v2/guide/best-practices.html\">https://docs.aws.amazon.com/cdk/v2/guide/best-practices.html</a></p>",
                "options": [
                    {
                        "id": 3120,
                        "content": "<p>Only make changes in the deployment phase, and never during synthesis.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3121,
                        "content": "<p>Once CloudFormation has been installed, it can be used to track changes in application stacks.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3122,
                        "content": "<p>Once CloudFormation has been added to the application, it can be used to manage the scope.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3123,
                        "content": "<p>Once CloudFormation has been installed, it can be used to revert to previous versions.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 761,
            "attributes": {
                "createdAt": "2023-09-07T08:51:26.107Z",
                "updatedAt": "2023-09-07T08:51:26.107Z",
                "content": "<p>A developer is using AWS AppConfig to deploy an application. The application's data needs to be encrypted when at rest. Which statement describes how this requirement is met?</p>",
                "answerExplanation": "<p>AWS AppConfig will automatically encrypt data at rest using AWS owned keys and AWS Key Management Service (KMS). This layer cannot be disabled or altered by the customer. The customer can add a second layer of encryption protection that they can control and manage using customer managed keys.</p><p><strong>CORRECT</strong>: \"Data is automatically encrypted using AWS owned keys. This cannot be disabled. The developer can add an additional layer of encryption using customer managed keys\" is the correct answer (as explained above.)</p><p><strong>INCORRECT</strong>: \"Data is automatically encrypted using AWS owned keys and AWS Key Management Service (KMS). The developer can disable this layer and instead add their own customer managed key\" is incorrect. The default layer of encryption cannot be disabled. The developer can add a second layer of a customer managed key.</p><p><strong>INCORRECT</strong>: \"Data is stored in S3 buckets unencrypted by default. The developer can use AWS Key Management Service (KMS) to apply encryption keys to the data at rest\" is incorrect. Data is encrypted by default using AWS owned keys. The developer can use customer managed keys to add a second layer of encryption.</p><p><strong>INCORRECT</strong>: \"Data is unencrypted by default. The developer can choose to use AWS managed keys or customer managed keys to encrypt the data\" is incorrect. Data is encrypted by default using AWS managed keys and AWS Key Management Service (KMS). A second layer can be added using customer managed keys.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/appconfig/latest/userguide/appconfig-security.html\">https://docs.aws.amazon.com/appconfig/latest/userguide/appconfig-security.html</a></p>",
                "options": [
                    {
                        "id": 3124,
                        "content": "<p>Data is stored in S3 buckets unencrypted by default. The developer can use AWS Key Management Service (KMS) to apply encryption keys to the data at rest.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3125,
                        "content": "<p>Data is automatically encrypted using AWS owned keys and AWS Key Management Service (KMS). The developer can disable this layer and instead add their own customer managed key.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3126,
                        "content": "<p>Data is unencrypted by default. The developer can choose to use AWS managed keys or customer managed keys to encrypt the data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3127,
                        "content": "<p>Data is automatically encrypted using AWS owned keys. This cannot be disabled. The developer can add an additional layer of encryption using customer managed keys.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 762,
            "attributes": {
                "createdAt": "2023-09-07T08:51:26.194Z",
                "updatedAt": "2023-09-07T08:51:26.194Z",
                "content": "<p>Customers who use a REST API have reported performance issues. A Developer needs to measure the time between when API Gateway receives a request from a client and when it returns a response to the client.</p><p>Which metric should the Developer monitor?</p>",
                "answerExplanation": "<p>The Latency metric measures the time between when API Gateway receives a request from a client and when it returns a response to the client. The latency includes the integration latency and other API Gateway overhead.</p><p><strong>CORRECT: </strong>\"Latency\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"IntegrationLatency\" is incorrect. This measures the time between when API Gateway relays a request to the backend and when it receives a response from the backend.</p><p><strong>INCORRECT:</strong> \"CacheHitCount\" is incorrect. This measures the number of requests served from the API cache in a given period.</p><p><strong>INCORRECT:</strong> \"5XXError\" is incorrect. This measures the number of server-side errors captured in a given period.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-metrics-and-dimensions.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-metrics-and-dimensions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3128,
                        "content": "<p>CacheHitCount</p>",
                        "isValid": false
                    },
                    {
                        "id": 3129,
                        "content": "<p>5XXError</p>",
                        "isValid": false
                    },
                    {
                        "id": 3130,
                        "content": "<p>IntegrationLatency</p>",
                        "isValid": false
                    },
                    {
                        "id": 3131,
                        "content": "<p>Latency</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 763,
            "attributes": {
                "createdAt": "2023-09-07T08:51:26.271Z",
                "updatedAt": "2023-09-07T08:51:26.271Z",
                "content": "<p>A development team using AWS Amplify is testing new features for an application. How can the team test the features with the current production code without impacting the last published application version?</p>",
                "answerExplanation": "<p>Features testing on an application should not be done on production code. AWS Amplify supports connecting branches from the production code environment that will clone a repository so that features can be developed, tested and rolled back, if necessary, without impacting the latest version of the published application.</p><p><strong>CORRECT</strong>: â€œConnect a new branch that will deploy a version named https://dev.applicationId.amplifyapp.com to test new featuresâ€ is the correct answer (as explained above.)</p><p><strong>INCORRECT</strong>: \"Connect a new branch that will deploy a version named https://main.applicationId.amplifyapp.com to test new features\" is incorrect.</p><p>The â€œmainâ€ branch indicated in the URL indicates this is the main production environment and not the one to test features on.</p><p><strong>INCORRECT</strong>: â€œConnect the production environment into AWS Cloud9 and test the features after publishing them\" is incorrect.</p><p>AWS Cloud9 is an IDE that can be used to develop new features if the development chooses to use it. Other IDEs can also be used and integrated in the development process. Once features have passed all the checks, it can be pushed.</p><p><strong>INCORRECT</strong>: \"Connect the production environment into AWS Cloud9 and test the features after committing them\" is incorrect.</p><p>AWS Cloud9 is an IDE that can be used to develop new features if the development chooses to use it. Other IDEs can also be used and integrated in the development process. Committing the features captures a snapshot of the staged changes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amplify/latest/userguide/multi-environments.html\">https://docs.aws.amazon.com/amplify/latest/userguide/multi-environments.html</a></p>",
                "options": [
                    {
                        "id": 3132,
                        "content": "<p>Connect a new branch that will deploy a version named https://main.applicationId.amplifyapp.com to test new features.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3133,
                        "content": "<p>Connect the production environment into AWS Cloud9 and test the features after committing them.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3134,
                        "content": "<p>Connect a new branch that will deploy a version named https://dev.applicationId.amplifyapp.com to test new features.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3135,
                        "content": "<p>Connect the production environment into AWS Cloud9 and test the features after publishing them.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 764,
            "attributes": {
                "createdAt": "2023-09-07T08:51:26.341Z",
                "updatedAt": "2023-09-07T08:51:26.341Z",
                "content": "<p>An Amazon DynamoDB table has been created using provisioned capacity. A manager needs to understand whether the DynamoDB table is cost-effective. How can the manager query how much provisioned capacity is actually being used?</p>",
                "answerExplanation": "<p>You can monitor Amazon DynamoDB using CloudWatch, which collects and processes raw data from DynamoDB into readable, near real-time metrics. These statistics are retained for a period of time, so that you can access historical information for a better perspective on how your web application or service is performing. By default, DynamoDB metric data is sent to CloudWatch automatically.</p><p>To determine how much of the provisioned capacity is being used you can monitor ConsumedReadCapacityUnits or ConsumedWriteCapacityUnits over the specified time period.</p><p><strong>CORRECT: </strong>\"Monitor the <code>ConsumedReadCapacityUnits</code> and <code>ConsumedWriteCapacityUnits</code> over a specified time period\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Monitor the <code>ReadThrottleEvents</code> and <code>WriteThrottleEvents</code> metrics for the table\" is incorrect as these metrics are used to determine which requests exceed the provisioned throughput limits of a table.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudTrail and monitor the <code>DescribeLimits</code> API action\" is incorrect as CloudTrail records API actions, not performance metrics.</p><p><strong>INCORRECT:</strong> \"Use AWS X-Ray to instrument the DynamoDB table and monitor subsegments\" is incorrect. DynamoDB does not directly integrate with X-Ray but you can record information in subsegments for downstream requests. This is not, however, a method for monitoring provisioned capacity utilization.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/monitoring-cloudwatch.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/monitoring-cloudwatch.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3136,
                        "content": "<p>Monitor the <code>ConsumedReadCapacityUnits</code> and <code>ConsumedWriteCapacityUnits</code> over a specified time period</p>",
                        "isValid": true
                    },
                    {
                        "id": 3137,
                        "content": "<p>Use Amazon CloudTrail and monitor the <code>DescribeLimits</code> API action</p>",
                        "isValid": false
                    },
                    {
                        "id": 3138,
                        "content": "<p>1. Use AWS X-Ray to instrument the DynamoDB table and monitor sub segments</p>",
                        "isValid": false
                    },
                    {
                        "id": 3139,
                        "content": "<p>Monitor the <code>ReadThrottleEvents</code> and <code>WriteThrottleEvents</code> metrics for the table</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 765,
            "attributes": {
                "createdAt": "2023-09-07T08:51:26.413Z",
                "updatedAt": "2023-09-07T08:51:26.413Z",
                "content": "<p>A company maintains a REST API service using Amazon API Gateway with native API key validation. The company recently launched a new registration page, which allows users to sign up for the service. The registration page creates a new API key using CreateApiKey and sends the new key to the user. When the user attempts to call the API using this key, the user receives a 403 Forbidden error. Existing users are unaffected and can still call the API.</p><p>What code updates will grant these new usersâ€™ access to the API?</p>",
                "answerExplanation": "<p>A <em>usage plan</em> specifies who can access one or more deployed API stages and methodsâ€”and also how much and how fast they can access them. The plan uses API keys to identify API clients and meters access to the associated API stages for each key. It also lets you configure throttling limits and quota limits that are enforced on individual client API keys.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_06-18-45-d98c869175c80055460abda8f9de14ff.png\"></p><p><em>API keys</em> are alphanumeric string values that you distribute to application developer customers to grant access to your API. You can use API keys together with <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\">usage plans</a> or <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">Lambda authorizers</a> to control access to your APIs. API Gateway can generate API keys on your behalf, or you can import them from a <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-key-file-format.html\">CSV file</a>. You can generate an API key in API Gateway, or import it into API Gateway from an external source.</p><p>To associate the newly created key with a usage plan the CreatUsagePlanKey API can be called. This creates a usage plan key for adding an existing API key to a usage plan.</p><p><strong>CORRECT: </strong>\"The createUsagePlanKey method must be called to associate the newly created API key with the correct usage plan\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The createDeployment method must be called so the API can be redeployed to include the newly created API key\" is incorrect as you do not need to redeploy an API to a stage in order to associate an API key.</p><p><strong>INCORRECT:</strong> \"The updateAuthorizer method must be called to update the APIâ€™s authorizer to include the newly created API key\" is incorrect as this updates and authorizer resource, not an API key.</p><p><strong>INCORRECT:</strong> \"The importApiKeys method must be called to import all newly created API keys into the current stage of the API\" is incorrect as this imports API keys to API Gateway from an external source such as a CSV file which is not relevant to this scenario.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html</a></p><p><a href=\"http://docs.amazonaws.cn/en_us/sdkfornet/v3/apidocs/items/APIGateway/MAPIGatewayCreateUsagePlanKeyCreateUsagePlanKeyRequest.html\">http://docs.amazonaws.cn/en_us/sdkfornet/v3/apidocs/items/APIGateway/MAPIGatewayCreateUsagePlanKeyCreateUsagePlanKeyRequest.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 3140,
                        "content": "<p>The <code>createUsagePlanKey </code>method must be called to associate the newly created API key with the correct usage plan</p>",
                        "isValid": true
                    },
                    {
                        "id": 3141,
                        "content": "<p>The <code>importApiKeys</code> method must be called to import all newly created API keys into the current stage of the API</p>",
                        "isValid": false
                    },
                    {
                        "id": 3142,
                        "content": "<p>The <code>createDeployment</code> method must be called so the API can be redeployed to include the newly created API key</p>",
                        "isValid": false
                    },
                    {
                        "id": 3143,
                        "content": "<p>The <code>updateAuthorizer</code> method must be called to update the APIâ€™s authorizer to include the newly created API key</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 766,
            "attributes": {
                "createdAt": "2023-09-07T08:51:26.486Z",
                "updatedAt": "2023-09-07T08:51:26.486Z",
                "content": "<p>There are multiple AWS accounts across multiple regions managed by a company. The operations team require a single operational dashboard that displays some key performance metrics from these accounts and regions. What is the SIMPLEST solution?</p>",
                "answerExplanation": "<p>You can create <em>cross-account cross-Region dashboards</em>, which summarize your CloudWatch data from multiple AWS accounts and multiple Regions into one dashboard. From this high-level dashboard you can get a view of your entire application, and also drill down into more specific dashboards without having to log in and out of accounts or switch Regions.</p><p>You can create cross-account cross-Region dashboards in the AWS Management Console and programmatically.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudWatch cross-account cross-region dashboard\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch dashboard in one account and region and import the data from the other accounts and regions\" is incorrect as this is more complex and unnecessary.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that collects metrics from each account and region and pushes the metrics to the account where the dashboard has been created\" is incorrect as this is not a simple solution.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudTrail trail that applies to all regions and deliver the logs to a single Amazon S3 bucket. Create a dashboard using the data in the bucket\" is incorrect as CloudTrail logs API activity, not performance metrics.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_xaxr_dashboard.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_xaxr_dashboard.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3144,
                        "content": "<p>Create an Amazon CloudTrail trail that applies to all regions and deliver the logs to a single Amazon S3 bucket. Create a dashboard using the data in the bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 3145,
                        "content": "<p>Create an Amazon CloudWatch cross-account cross-region dashboard</p>",
                        "isValid": true
                    },
                    {
                        "id": 3146,
                        "content": "<p>Create an Amazon CloudWatch dashboard in one account and region and import the data from the other accounts and regions</p>",
                        "isValid": false
                    },
                    {
                        "id": 3147,
                        "content": "<p>Create an AWS Lambda function that collects metrics from each account and region and pushes the metrics to the account where the dashboard has been created</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 767,
            "attributes": {
                "createdAt": "2023-09-07T08:51:26.569Z",
                "updatedAt": "2023-09-07T08:51:26.569Z",
                "content": "<p>A start-up organization imported their X.509 certificate from another issuer into AWS Certificate Manager (ACM) approximately 11 months ago. What needs to be done to ensure that visitors will continue to have secure access to the website? (Select TWO.)</p>",
                "answerExplanation": "<p>AWS ACM issued certificates are valid for 13 months. They are also renewed automatically. Imported certificates are not automatically renewed and would need to be imported once created from the third party.</p><p><strong>CORRECT: </strong>\"A new certificate will need to be requested from ACM\" is the correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"A new certificate will need to be imported into ACMâ€ is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"A new certificate will automatically be created prior to the certificate expiring at 12 months\" is incorrect. Imported certificates are not automatically renewed by AWS ACM.</p><p><strong>INCORRECT:</strong> \"A new certificate will automatically be created prior to the certificate expiring at 13 months\" is incorrect. Imported certificates are not automatically renewed by AWS ACM.</p><p><strong>INCORRECT:</strong> \"A new certificate will be automatically imported into ACM\" is incorrect. A new certificate issued by a third-party can be imported but it is not automatically done.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/acm/latest/userguide/managed-renewal.html\">https://docs.aws.amazon.com/acm/latest/userguide/managed-renewal.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certificate-manager/\">https://digitalcloud.training/aws-certificate-manager/</a></p>",
                "options": [
                    {
                        "id": 3148,
                        "content": "<p>A new certificate will be automatically imported into ACM.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3149,
                        "content": "<p>A new certificate will need to be requested from ACM.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3150,
                        "content": "<p>A new certificate will automatically be created prior to the certificate expiring at 12 months.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3151,
                        "content": "<p>A new certificate will automatically be created prior to the certificate expiring at 13 months.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3152,
                        "content": "<p>A new certificate will need to be imported into ACM.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 768,
            "attributes": {
                "createdAt": "2023-09-07T08:51:26.638Z",
                "updatedAt": "2023-09-07T08:51:26.638Z",
                "content": "<p>A developer is using AWS AppSync to develop an application that will be used by a healthcare provider that is subject to HIPAA compliance regulations that require data encryption. The team lead has requested to optimize performance and reduce latency as much as possible.</p><p>How can a developer meet the compliance and team lead requirements?</p>",
                "answerExplanation": "<p>AWS AppSyncâ€™s API Cache settings provides three options: None, Full request caching, and Per-resolver caching. To meet the team leadâ€™s requirements for high performance and low latency, the cache settings can be configured for full request caching to cache all requests and responses. To meet HIPAA compliance, the settings would be configured for data encryption at rest and in transit.</p><p><strong>CORRECT</strong>: â€œSet the API Cache for full request caching and adjust the settings for data encryption at rest and in transitâ€ is the correct answer (as explained above.)</p><p><strong>INCORRECT</strong>: \"Set the API Cache for full request caching and leaving the default encryption settings\" is incorrect. The API Cache set to full request caching is correct.</p><p>The default settings for API Cache are for no encryption. To meet HIPAA compliance, data should be protected, and encryption should be enabled for in transit and at rest.</p><p><strong>INCORRECT</strong>: â€œSet the API Cache for â€œNone\" to prevent any changed being made from the default cache settings and adjust the settings for data encryption at rest and in transit\" is incorrect.</p><p>â€œNoneâ€ is the default setting for the API Cache and would not cache any requests and responses. This would not meet the goal of optimizing performance and decreasing latency. The change in the encryption settings is correct.</p><p><strong>INCORRECT</strong>: \"Set the API Cache for per-resolver caching and leaving the default encryption settings \" is incorrect.</p><p>The per-resolver caching will only cache specific calls. The default encryption leaves the data unencrypted which does not meet the HIPAA requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/appsync/latest/devguide/enabling-caching.html\">https://docs.aws.amazon.com/appsync/latest/devguide/enabling-caching.html</a></p>",
                "options": [
                    {
                        "id": 3153,
                        "content": "<p>Set the API Cache for full request caching and adjust the settings for data encryption at rest and in transit.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3154,
                        "content": "<p>Set the API Cache for per-resolver caching and leaving the default encryption settings.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3155,
                        "content": "<p>Set the API Cache for â€œNone\" to prevent any changed being made from the default cache settings and adjust the settings for data encryption at rest and in transit.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3156,
                        "content": "<p>Set the API Cache for full request caching and leaving the default encryption settings.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 769,
            "attributes": {
                "createdAt": "2023-09-07T08:51:26.710Z",
                "updatedAt": "2023-09-07T08:51:26.710Z",
                "content": "<p>An engineer is using a Border Gateway Protocol (BGP) enabled AWS VPN connection to establish communication between on-site servers and Amazon EC2 instances in their account. The engineer is successful in accessing an EC2 instance in Subnet-X but is facing issues when attempting to reach an EC2 instance in Subnet-Y within the same VPC.</p><p>Which logging mechanism can the engineer employ to confirm if the traffic is getting to Subnet-Y?</p>",
                "answerExplanation": "<p>In this scenario, AWS VPC Flow Logs would provide the required visibility. These logs capture information about the IP traffic going to and from network interfaces in the VPC. Therefore, it's the appropriate tool to verify if the traffic is reaching Subnet-Y.</p><p><strong>CORRECT: </strong>\"Use VPC Flow Logs to check the inbound and outbound traffic of Subnet-Y\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Check Amazon CloudWatch Logs for the EC2 instance in Subnet-Y\" is incorrect.</p><p>CloudWatch Logs for EC2 instances primarily provide information about instance status and operations, not about network traffic</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail logs to verify traffic to Subnet-Y\" is incorrect.</p><p>CloudTrail primarily logs API calls in AWS and not the traffic flow at the network interface level</p><p><strong>INCORRECT:</strong> \"Consult AWS Config logs to trace the traffic\" is incorrect.</p><p>AWS Config tracks changes in resource configurations and doesn't provide insight into traffic flow.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 3157,
                        "content": "<p>Use AWS CloudTrail logs to verify traffic to Subnet-Y. CloudTrail primarily logs API calls in AWS and not the traffic flow at the network interface level.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3158,
                        "content": "<p>Use VPC Flow Logs to check the inbound and outbound traffic of Subnet-Y.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3159,
                        "content": "<p>Consult AWS Config logs to trace the traffic. AWS Config tracks changes in resource configurations and doesn't provide insight into traffic flow.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3160,
                        "content": "<p>Check Amazon CloudWatch Logs for the EC2 instance in Subnet-Y. CloudWatch Logs for EC2 instances primarily provide information about instance status and operations, not about network traffic.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 770,
            "attributes": {
                "createdAt": "2023-09-07T08:51:26.783Z",
                "updatedAt": "2023-09-07T08:51:26.783Z",
                "content": "<p>A Development team would like to migrate their existing application code from a GitHub repository to AWS CodeCommit.</p><p>What needs to be created before they can migrate a cloned repository to CodeCommit over HTTPS?</p>",
                "answerExplanation": "<p>The simplest way to set up connections to AWS CodeCommit repositories is to configure Git credentials for CodeCommit in the IAM console, and then use those credentials for HTTPS connections.</p><p>You can also use these same credentials with any third-party tool or individual development environment (IDE) that supports HTTPS authentication using a static user name and password. For examples, see <a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ide.html\">For Connections from Development Tools</a>.</p><p><strong>CORRECT: </strong>\"A set of credentials generated from IAM\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"A GitHub secure authentication token\" is incorrect as this is not how you authenticated to CodeCommit.</p><p><strong>INCORRECT:</strong> \"A public and private SSH key file\" is incorrect as that is required for accessing CodeCommit using SSH.</p><p><strong>INCORRECT:</strong> \"An Amazon EC2 IAM role with CodeCommit permissions\" is incorrect as that would be used to provide access to administer CodeCommit. However, the question is asking how to authenticate a Git client to CodeCommit using HTTPS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3161,
                        "content": "<p>A GitHub secure authentication token</p>",
                        "isValid": false
                    },
                    {
                        "id": 3162,
                        "content": "<p>An Amazon EC2 IAM role with CodeCommit permissions</p>",
                        "isValid": false
                    },
                    {
                        "id": 3163,
                        "content": "<p>A set of credentials generated from IAM</p>",
                        "isValid": true
                    },
                    {
                        "id": 3164,
                        "content": "<p>A public and private SSH key file</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 771,
            "attributes": {
                "createdAt": "2023-09-07T08:51:26.869Z",
                "updatedAt": "2023-09-07T08:51:26.869Z",
                "content": "<p>A company has deployed a REST API using Amazon API Gateway with a Lambda authorizer. The company needs to log who has accessed the API and how the caller accessed the API. They also require logs that include errors and execution traces for the Lambda authorizer.</p><p>Which combination of actions should the Developer take to meet these requirements? (Select TWO.)</p>",
                "answerExplanation": "<p>There are two types of API logging in CloudWatch: execution logging and access logging. In execution logging, API Gateway manages the CloudWatch Logs. The process includes creating log groups and log streams, and reporting to the log streams any caller's requests and responses.</p><p>The logged data includes errors or execution traces (such as request or response parameter values or payloads), data used by Lambda authorizers, whether API keys are required, whether usage plans are enabled, and so on.</p><p>In access logging, you, as an API Developer, want to log who has accessed your API and how the caller accessed the API. You can create your own log group or choose an existing log group that could be managed by API Gateway.</p><p><strong>CORRECT: </strong>\"Enable API Gateway execution logging\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Enable API Gateway access logs\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Enable detailed logging in Amazon CloudWatch\" is incorrect. Detailed logging does not provide the requested information.</p><p><strong>INCORRECT:</strong> \"Create an API Gateway usage plan\" is incorrect. This will not enable logging.</p><p><strong>INCORRECT:</strong> \"Enable server access logging\" is incorrect. This is a type of logging that applies to Amazon S3 buckets.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-logging.html\">https://docs.aws.amazon.com/apigateway/latest/Developerguide/set-up-logging.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3165,
                        "content": "<p>Enable server access logging.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3166,
                        "content": "<p>Create an API Gateway usage plan.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3167,
                        "content": "<p>Enable API Gateway access logs.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3168,
                        "content": "<p>Enable detailed logging in Amazon CloudWatch.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3169,
                        "content": "<p>Enable API Gateway execution logging.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 772,
            "attributes": {
                "createdAt": "2023-09-07T08:51:26.944Z",
                "updatedAt": "2023-09-07T08:51:26.944Z",
                "content": "<p>A team of Developers are working on a shared project and need to be able to collaborate on code. The shared application code must be encrypted at rest, stored on a highly available and durable architecture, and support multiple versions and batch change tracking.</p><p>Which AWS service should the Developer use?</p>",
                "answerExplanation": "<p>AWS CodeCommit is a fully managed source control service that hosts secure Git-based repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem. AWS CodeCommit automatically encrypts your files in transit and at rest.</p><p>AWS CodeCommit helps you collaborate on code with teammates via pull requests, branching, and merging. You can implement workflows that include code reviews and feedback by default, and control who can make changes to specific branches.</p><p><strong>CORRECT: </strong>\"AWS CodeCommit\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodeBuild\" is incorrect. AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages</p><p><strong>INCORRECT:</strong> \"Amazon S3\" is incorrect. Amazon S3 is an object-based storage system and does not support the features required here.</p><p><strong>INCORRECT:</strong> \"AWS Cloud9\" is incorrect. AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/codecommit/\">https://aws.amazon.com/codecommit/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3170,
                        "content": "<p>AWS Cloud9</p>",
                        "isValid": false
                    },
                    {
                        "id": 3171,
                        "content": "<p>Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 3172,
                        "content": "<p>AWS CodeCommit</p>",
                        "isValid": true
                    },
                    {
                        "id": 3173,
                        "content": "<p>AWS CodeBuild</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 773,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.020Z",
                "updatedAt": "2023-09-07T08:51:27.020Z",
                "content": "<p>A Developer is attempting to call the Amazon CloudWatch API and is receiving <code>HTTP 400: ThrottlingException</code> errors intermittently. When a call fails, no data is retrieved.</p><p>What best practice should the Developer first attempt to resolve this issue?</p>",
                "answerExplanation": "<p>Occasionally ,you may receive the 400 ThrottlingException error for PutMetricData API calls in Amazon CloudWatch with a detailed response similar the following:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-20_09-16-39-a9b45a4e63ae15367e7fb66e598d68e4.jpg\"></p><p>CloudWatch requests are throttled for each Amazon Web Services (AWS) account on a per-Region basis to help service performance. For current PutMetricData API request limits, see CloudWatch Limits.</p><p>All calls to the PutMetricData API in an AWS Region count towards the maximum allowed request rate. This number includes calls from any custom or third-party application, such as calls from the CloudWatch Agent, the AWS Command Line Interface (AWS CLI), or the AWS Management Console.</p><p><strong>Resolutions: </strong>It's a best practice to use the following methods to reduce your call rate and avoid API throttling:</p><p>- Distribute your API calls evenly over time rather than making several API calls in a short time span. If you require data to be available with a one-minute resolution, you have an entire minute to emit that metric. Use jitter (randomized delay) to send data points at various times.</p><p>- Combine as many metrics as possible into a single API call. For example, a single PutMetricData call can include 20 metrics and 150 data points. You can also use pre-aggregated data sets, such as StatisticSet, to publish aggregated data points, thus reducing the number of PutMetricData calls per second.</p><p>- Retry your call with exponential backoff and jitter.</p><p>Following attempting the above resolutions AWS suggest the following: â€œIf you still require a higher limit, you can <a href=\"https://console.aws.amazon.com/support/home#/case/create?issueType=service-limit-increase&amp;limitType=service-code-amazon-cloudwatch\">request a limit increase</a>. Increasing the rate limit can have a high financial impact on your AWS bill.â€</p><p>Therefore, the first thing the Developer should do, from the list of options presented, is to retry the call with exponential backoff.</p><p><strong>CORRECT: </strong>\"Retry the call with exponential backoff\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Contact AWS Support for a limit increase\" is incorrect. As mentioned above, there are other resolutions the Developer should attempt before contacting support to raise the limit.</p><p><strong>INCORRECT:</strong> \"Use the AWS CLI to get the metrics\" is incorrect as this will still make the same API calls.</p><p><strong>INCORRECT:</strong> \"Analyze the applications and remove the API call\" is incorrect as this is not a good resolution to the issue as this may mean that important monitoring and logging data is not recorded for the application.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-400-error-throttling/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-400-error-throttling/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3174,
                        "content": "<p>Use the AWS CLI to get the metrics</p>",
                        "isValid": false
                    },
                    {
                        "id": 3175,
                        "content": "<p>Analyze the applications and remove the API call</p>",
                        "isValid": false
                    },
                    {
                        "id": 3176,
                        "content": "<p>Retry the call with exponential backoff</p>",
                        "isValid": true
                    },
                    {
                        "id": 3177,
                        "content": "<p>Contact AWS Support for a limit increase</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 774,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.103Z",
                "updatedAt": "2023-09-07T08:51:27.103Z",
                "content": "<p>A Developer has created an Amazon S3 bucket and uploaded some objects that will be used for a publicly available static website. What steps MUST be performed to configure the bucket as a static website? (Select TWO.)</p>",
                "answerExplanation": "<p>You can use Amazon S3 to host a static website. On a <em>static</em> website, individual webpages include static content. They might also contain client-side scripts.</p><p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.</p><p>When you enable static website hosting for your bucket, you enter the name of the index document (for example, index.html). After you enable static website hosting for your bucket, you upload an HTML file with the index document name to your bucket. Note that an error document is optional.</p><p>To provide permissions, it is necessary to disable â€œblock public accessâ€ settings and then create a bucket policy that grants everyone the <code>s3:GetObject</code> permission. For example:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"Version\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"2012-10-17\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Statement\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"pun\">{</span></li><li class=\"L4\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PublicReadGetObject\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L6\"><span class=\"str\">\"Principal\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"*\"</span><span class=\"pun\">,</span></li><li class=\"L7\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L8\"><span class=\"str\">\"s3:GetObject\"</span></li><li class=\"L9\"><span class=\"pun\">],</span></li><li class=\"L0\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L1\"><span class=\"str\">\"arn:aws:s3:::example.com/*\"</span></li><li class=\"L2\"><span class=\"pun\">]</span></li><li class=\"L3\"><span class=\"pun\">}</span></li><li class=\"L4\"><span class=\"pun\">]</span></li><li class=\"L5\"><span class=\"pun\">}</span></li></ol></pre></div></div><p><strong>CORRECT: </strong>\"Upload an index document and enter the name of the index document when enabling static website hosting\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Enable public access and grant everyone the <code>s3:GetObject</code> permissions\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Upload an index and error document and enter the name of the index and error documents when enabling static website hosting\" is incorrect as the error document is optional and the question specifically asks for the steps that MUST be completed.</p><p><strong>INCORRECT:</strong> \"Create an object access control list (ACL) granting <code>READ</code> permissions to the <code>AllUsers </code>group\" is incorrect. This may be necessary if the bucket objects are not owned by the bucket owner but the question states that the Developer created the bucket and uploaded the objects and so must be the object owner.</p><p><strong>INCORRECT:</strong> \"Upload a certificate from AWS Certificate Manager\" is incorrect as this is not supported or necessary for static websites on Amazon S3.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 3178,
                        "content": "<p>Upload an index and error document and enter the name of the index and error documents when enabling static website hosting</p>",
                        "isValid": false
                    },
                    {
                        "id": 3179,
                        "content": "<p>Create an object access control list (ACL) granting <code>READ</code> permissions to the <code>AllUsers</code> group</p>",
                        "isValid": false
                    },
                    {
                        "id": 3180,
                        "content": "<p>Upload a certificate from AWS Certificate Manager</p>",
                        "isValid": false
                    },
                    {
                        "id": 3181,
                        "content": "<p>Enable public access and grant everyone the <code>s3:GetObject </code>permissions</p>",
                        "isValid": true
                    },
                    {
                        "id": 3182,
                        "content": "<p>Upload an index document and enter the name of the index document when enabling static website hosting</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 775,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.175Z",
                "updatedAt": "2023-09-07T08:51:27.175Z",
                "content": "<p>A programmer is rolling out a new solution to Amazon ECS. This solution necessitates secure handling and access to diverse variables such as credentials, remote API authentication details, and the API URL. The remote API authentication information and API URL should be accessible across all versions of the solution, spanning development, testing, and production environments.</p><p>What is the most efficient method for the programmer to access these variables with minimal modifications to the solution?</p>",
                "answerExplanation": "<p>AWS Secrets Manager is designed to handle sensitive data like this. It enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Using AWS Secrets Manager, you can secure and manage secrets used to access resources in the AWS Cloud, on third-party services, and on-premises.</p><p><strong>CORRECT: </strong>\"Utilize AWS Secrets Manager to store these variables and retrieve them directly within the application using AWS SDK\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Include these variables directly in the application code and use version control system to manage different versions\" is incorrect.</p><p>Including sensitive data directly in the application code is not a secure practice and doesn't comply with the principle of least privilege or best security practices.</p><p><strong>INCORRECT:</strong> \"Create environment variables for these details and access them within the application code\" is incorrect.</p><p>Although environment variables can be used to pass some configuration to the application, they're typically visible in plaintext in multiple places such as ECS Task Definitions or the ECS console. This makes them unsuitable for sensitive data like credentials or API keys.</p><p><strong>INCORRECT:</strong> \"Use AWS S3 to store these variables and access them via AWS SDK within the application\" is incorrect.</p><p>AWS S3 is primarily a storage service and using it to store and manage secrets doesn't align with security best practices.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-secrets-manager/\">https://digitalcloud.training/aws-secrets-manager/</a></p>",
                "options": [
                    {
                        "id": 3183,
                        "content": "<p>Create environment variables for these details and access them within the application code.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3184,
                        "content": "<p>Utilize AWS Secrets Manager to store these variables and retrieve them directly within the application using AWS SDK.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3185,
                        "content": "<p>Include these variables directly in the application code and use version control system to manage different versions.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3186,
                        "content": "<p>Use AWS S3 to store these variables and access them via AWS SDK within the application.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 776,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.245Z",
                "updatedAt": "2023-09-07T08:51:27.245Z",
                "content": "<p>A developer is creating a serverless web application that includes AWS Lambda functions and a REST API deployed using Amazon API Gateway. The developer maintains multiple branches of code. The developer wants to avoid updating the API gateway target endpoint when a new code push is performed.</p><p>What solution would allow the developer to update the Lambda code without needing to update the REST API configuration?</p>",
                "answerExplanation": "<p>You can create one or more aliases for your Lambda function. A Lambda alias is like a pointer to a specific function version. Users can access the function version using the alias Amazon Resource Name (ARN). You can then release new versions of your code without needing to change the alias that applications use to invoke the function.</p><p>In this case the REST API could be configured with aliases for the functions. The developer could also use different stages with different endpoints using the aliases. This would enable calling different versions of the application by changing the stage name in the REST API URL.</p><p><strong>CORRECT: </strong>\"Create aliases and versions in AWS Lambda\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create multiple stages and deployments\" is incorrect.</p><p>Stages and stage variables could be used to reference different functions or aliases. But if only stages and deployments are used (and not a Lambda alias) then the REST API would need to have the endpoint updated every time a new function version is released.</p><p><strong>INCORRECT:</strong> \"Create different tags for each Lambda function\" is incorrect.</p><p>Tags cannot be used to define the Lambda endpoints in the REST API.</p><p><strong>INCORRECT:</strong> \"Create multiple private API endpoints and use CNAMEs\" is incorrect.</p><p>There is no value here in creating multiple private API endpoints as that would be completely different APIs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3187,
                        "content": "<p>Create multiple private API endpoints and use CNAMEs.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3188,
                        "content": "<p>Create different tags for each Lambda function.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3189,
                        "content": "<p>Create aliases and versions in AWS Lambda.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3190,
                        "content": "<p>Create multiple stages and deployments.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 777,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.319Z",
                "updatedAt": "2023-09-07T08:51:27.319Z",
                "content": "<p>A company runs a popular website behind an Amazon CloudFront distribution that uses an Application Load Balancer as the origin. The Developer wants to set up custom HTTP responses to 404 errors for content that has been removed from the origin that redirects the users to another page.</p><p>The Developer wants to use an AWS Lambda@Edge function that is associated with the current CloudFront distribution to accomplish this goal. The solution must use a minimum amount of resources.</p><p>Which CloudFront event type should the Developer use to invoke the Lambda@Edge function that contains the redirect logic?</p>",
                "answerExplanation": "<p>When CloudFront receives an HTTP response from the origin server, if there is an origin-response trigger associated with the cache behavior, you can modify the HTTP response to override what was returned from the origin.</p><p>Some common scenarios for updating HTTP responses include the following:</p><p>&nbsp; &nbsp;â€¢ Changing the status to set an HTTP 200 status code and creating static body content to return to the viewer when an origin returns an error status code (4xx or 5xx)</p><p>&nbsp; &nbsp;â€¢ Changing the status to set an HTTP 301 or HTTP 302 status code, to redirect the user to another website when an origin returns an error status code (4xx or 5xx)</p><p>You can also replace the HTTP responses in viewer and origin request events. However, in this case it is the error response being returned from the origin that must be modified when a 404 error is encountered for a page that has been removed.</p><p><strong>CORRECT: </strong>\"Origin response\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Origin request\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Viewer response\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Viewer request\" is incorrect as explained above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-updating-http-responses.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-updating-http-responses.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3191,
                        "content": "<p>Origin response</p>",
                        "isValid": true
                    },
                    {
                        "id": 3192,
                        "content": "<p>Origin request</p>",
                        "isValid": false
                    },
                    {
                        "id": 3193,
                        "content": "<p>Viewer request</p>",
                        "isValid": false
                    },
                    {
                        "id": 3194,
                        "content": "<p>Viewer response</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 778,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.391Z",
                "updatedAt": "2023-09-07T08:51:27.391Z",
                "content": "<p>A Developer implemented a static website hosted in Amazon S3 that makes web service requests hosted in Amazon API Gateway and AWS Lambda. The site is showing an error that reads:</p><p>â€œNo â€˜Access-Control-Allow-Originâ€™ header is present on the requested resource. Origin â€˜nullâ€™ is therefore not allowed access.â€</p><p>What should the Developer do to resolve this issue?</p>",
                "answerExplanation": "<p>Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. In this scenario the S3 bucket is the requestor and is requesting access to resources served by Amazon API Gateway and AWS Lambda. Therefore, the CORS configuration must be enabled on the requested endpoint which is the method in API Gateway.</p><p><strong>CORRECT: </strong>\"Enable cross-origin resource sharing (CORS) for the method in API Gateway\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable cross-origin resource sharing (CORS) on the S3 bucket\" is incorrect as CORS must be enabled on the requested endpoint which is API Gateway, not S3.</p><p><strong>INCORRECT:</strong> \"Add the Access-Control-Request-Method header to the request\" is incorrect as this is a request header value that asks permission to use a specific HTTP method.</p><p><strong>INCORRECT:</strong> \"Add the Access-Control-Request-Headers header to the request\" is incorrect as this notifies a server what headers will be sent in a request.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 3195,
                        "content": "<p>Enable cross-origin resource sharing (CORS) on the S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 3196,
                        "content": "<p>Add the Access-Control-Request-Method header to the request</p>",
                        "isValid": false
                    },
                    {
                        "id": 3197,
                        "content": "<p>Enable cross-origin resource sharing (CORS) for the method in API Gateway</p>",
                        "isValid": true
                    },
                    {
                        "id": 3198,
                        "content": "<p>Add the Access-Control-Request-Headers header to the request</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 779,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.462Z",
                "updatedAt": "2023-09-07T08:51:27.462Z",
                "content": "<p>An application must be refactored for the cloud. The application data is stored in an Amazon DynamoDB table and is processed by a Lambda function which prepares the data for analytics. The data processing currently takes place once a day, but the data analysts require it to be performed in near-real time.</p><p>Which architecture pattern could be used to enable the data to be processed as it is received?</p>",
                "answerExplanation": "<p>An event driven architecture will ensure that the records are processed as they are received. This can be achieved by creating a DynamoDB Stream for the existing table and then configuring the Lambda function to retrieve messages from the stream. This would be an event-driven architecture as the event (a record being written to the stream) causes the processing layer to do work.</p><p><strong>CORRECT: </strong>\"Use an event-driven architecture\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use a microservices architecture\" is incorrect.</p><p>A microservices architecture is where you have many small individual components of an application that are loosely coupled. Though the architecture described may have components of a microservices architecture, this is not the defining characteristic that meets the requirement of near-real time processing.</p><p><strong>INCORRECT:</strong> \"Use a fan-out architecture\" is incorrect.</p><p>An example of a fan-out architecture is using SNS to send a single notification to many SQS queues.</p><p><strong>INCORRECT:</strong> \"Use a scheduled architecture\" is incorrect.</p><p>A scheduled architecture would not process events as they occur. It would only process them on a fixed schedule.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/event-driven-architecture/\">https://aws.amazon.com/event-driven-architecture/</a></p>",
                "options": [
                    {
                        "id": 3199,
                        "content": "<p>Use a scheduled architecture.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3200,
                        "content": "<p>Use an event-driven architecture.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3201,
                        "content": "<p>Use a microservices architecture.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3202,
                        "content": "<p>Use a fan-out architecture.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 780,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.535Z",
                "updatedAt": "2023-09-07T08:51:27.535Z",
                "content": "<p>A start-up has launched a website using Route 53 for domain name, routing, and health checks. They failed to configure Amazon CloudWatch alarms with the health checks. How can the organization be made aware of any endpoint health issues?</p>",
                "answerExplanation": "<p>Configuring CloudWatch alarms to send a SNS notification if there is an unhealthy endpoint is optional. AWS customers can also review the Route 53 dashboard to identify any health issues.</p><p><strong>CORRECT: </strong>\"They can see the status of Route 53 health checks on the Route 53 dashboard\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"They can see the status of Route 53 health checks on the Shield dashboard\" is incorrect. Route 53 health checks are not available on the Shield dashboard.</p><p><strong>INCORRECT:</strong> \"They can subscribe to SNS topics to receive notifications\" is incorrect. SNS topics can be used to send notifications but must be triggered by CloudWatch alarm first.</p><p><strong>INCORRECT:</strong> \"They can create an alarm using CloudTrail and receive SNS notifications\" is incorrect. CloudTrail is used to track API activity not for monitoring healthy resources.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/welcome-health-checks.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/welcome-health-checks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
                "options": [
                    {
                        "id": 3203,
                        "content": "<p>They can see the status of Route 53 health checks on the Route 53 dashboard.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3204,
                        "content": "<p>They can create an alarm using CloudTrail and receive SNS notifications.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3205,
                        "content": "<p>They can subscribe to SNS topics to receive notifications.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3206,
                        "content": "<p>They can see the status of Route 53 health checks on the Shield dashboard.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 781,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.606Z",
                "updatedAt": "2023-09-07T08:51:27.606Z",
                "content": "<p>A Developer needs to be notified by email for all new object creation events in a specific Amazon S3 bucket. Amazon SNS will be used for sending the messages. How can the Developer enable these notifications?</p>",
                "answerExplanation": "<p>The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the <em>notification</em> subresource that is associated with a bucket.</p><p>Currently, Amazon S3 can publish notifications for the following events:</p><p><strong>New object created events</strong> â€” Amazon S3 supports multiple APIs to create objects. You can request notification when only a specific API is used (for example, s3:ObjectCreated:Put), or you can use a wildcard (for example, s3:ObjectCreated:*) to request notification when an object is created regardless of the API used.</p><p><strong>Object removal events</strong> â€” Amazon S3 supports deletes of versioned and unversioned objects. For information about object versioning, see <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html\">Object Versioning</a> and <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">Using Versioning</a>.</p><p><strong>Restore object events </strong>â€” Amazon S3 supports the restoration of objects archived to the S3 Glacier storage class. You request to be notified of object restoration completion by using s3:ObjectRestore:Completed. You use s3:ObjectRestore:Post to request notification of the initiation of a restore.</p><p><strong>Reduced Redundancy Storage (RRS) object lost events</strong> â€” Amazon S3 sends a notification message when it detects that an object of the RRS storage class has been lost.</p><p><strong>Replication events</strong> â€” Amazon S3 sends event notifications for replication configurations that have S3 Replication Time Control (S3 RTC) enabled. It sends these notifications when an object fails replication, when an object exceeds the 15-minute threshold, when an object is replicated after the 15-minute threshold, and when an object is no longer tracked by replication metrics. It publishes a second event when that object replicates to the destination Region.</p><p>Therefore, the Developer should create an event notification for all s3:ObjectCreated:* API calls as this will capture all new object creation events.</p><p><strong>CORRECT: </strong>\"Create an event notification for all <code>s3:ObjectCreated:*</code> API calls\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an event notification for all <code>s3:ObjectCreated:Put</code> API calls\" is incorrect as this will not capture all new object creation events (e.g. POST or COPY). The wildcard should be used instead.</p><p><strong>INCORRECT:</strong> \"Create an event notification for all <code>s3:ObjectRemoved:Delete</code> API calls\" is incorrect as this is used for object deletions.</p><p><strong>INCORRECT:</strong> \"Create an event notification for all <code>s3:ObjectRestore:Post</code> API calls\" is incorrect as this is used for restore events from Amazon S3 Glacier archives.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p><p><a href=\"https://aws.amazon.com/sns/faqs/\">https://aws.amazon.com/sns/faqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3207,
                        "content": "<p>Create an event notification for all <code>s3:ObjectCreated:*</code> API calls</p>",
                        "isValid": true
                    },
                    {
                        "id": 3208,
                        "content": "<p>Create an event notification for all <code>s3:ObjectRemoved:Delete</code> API calls</p>",
                        "isValid": false
                    },
                    {
                        "id": 3209,
                        "content": "<p>Create an event notification for all <code>s3:ObjectCreated:Put</code> API calls</p>",
                        "isValid": false
                    },
                    {
                        "id": 3210,
                        "content": "<p>Create an event notification for all <code>s3:ObjectRestore:Post</code> API calls</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 782,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.683Z",
                "updatedAt": "2023-09-07T08:51:27.683Z",
                "content": "<p>A developer is configuring health checks using Amazon Route 53 and needs to set values to determine the health of critical endpoints. What is the parameter that Amazon Route 53 reviews before deciding if an endpoint is unhealthy?</p>",
                "answerExplanation": "<p>The failure threshold is specified by the AWS customer. A failure is when the endpoint does not respond to a request.</p><p><strong>CORRECT: </strong>\"failure threshold\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"latency\" is incorrect. Latency is not a parameter that Route 53 uses to determine if an endpoint is healthy.</p><p><strong>INCORRECT:</strong> \"fault tolerance\" is incorrect. Fault tolerance is not a parameter that Route 53 uses to determine if an endpoint is healthy.</p><p><strong>INCORRECT:</strong> \"network response\" is incorrect. Network response is not a parameter that Route 53 uses to determine if an endpoint is healthy.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/welcome-health-checks.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/welcome-health-checks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
                "options": [
                    {
                        "id": 3211,
                        "content": "<p>fault tolerance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3212,
                        "content": "<p>failure threshold.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3213,
                        "content": "<p>latency.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3214,
                        "content": "<p>network response.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 783,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.752Z",
                "updatedAt": "2023-09-07T08:51:27.752Z",
                "content": "<p>A Developer has completed some code updates and needs to deploy the updates to an Amazon Elastic Beanstalk environment. The environment includes twelve Amazon EC2 instances and there can be no reduction in application performance and availability during the update.</p><p>Which deployment policy is the most cost-effective choice to suit these requirements?</p>",
                "answerExplanation": "<p>AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.</p><p>Each deployment policy has advantages and disadvantages and itâ€™s important to select the best policy to use for each situation. The following tables summarizes the different deployment policies:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_14-14-58-6396d9d2f91e7f94c6917aaf9216f5a1.jpg\"></p><p>The â€œrolling with additional batchâ€ policy will add an additional batch of instances, updates those instances, then move onto the next batch.</p><p>Rolling with additional batch:</p><p>Like Rolling but launches new instances in a batch ensuring that there is full availability.</p><p>Application is running at capacity.</p><p>Can set the bucket size.</p><p>Application is running both versions simultaneously.</p><p>Small additional cost.</p><p>Additional batch is removed at the end of the deployment.</p><p>Longer deployment.</p><p>Good for production environments.</p><p>For this scenario there can be no reduction in application performance and availability during the update. The question also asks for the most cost-effective choice.</p><p>Therefore, the â€œrolling with additional batchâ€ is the best choice as it will ensure fully availability of the application but minimize cost as the additional batch size can be kept small.</p><p><strong>CORRECT: </strong>\"Rolling with additional batch\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Rolling\" is incorrect as this will result in a reduction in capacity as there is no additional batch of instances introduced to the environment. This is a better choice if speed is required and a reduction in capacity of a batch size is acceptable.</p><p><strong>INCORRECT:</strong> \"All at once\" is incorrect as this will take the application down and cause a complete outage of the application during the update.</p><p><strong>INCORRECT:</strong> \"Immutable\" is incorrect as this is the most expensive option as it doubles capacity with a whole new set of instances attached to a new ASG.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
                "options": [
                    {
                        "id": 3215,
                        "content": "<p>All at once</p>",
                        "isValid": false
                    },
                    {
                        "id": 3216,
                        "content": "<p>Rolling with additional batch</p>",
                        "isValid": true
                    },
                    {
                        "id": 3217,
                        "content": "<p>Rolling</p>",
                        "isValid": false
                    },
                    {
                        "id": 3218,
                        "content": "<p>Immutable</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 784,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.846Z",
                "updatedAt": "2023-09-07T08:51:27.846Z",
                "content": "<p>A Developer needs to create an instance profile for an Amazon EC2 instance using the AWS CLI. How can this be achieved? (Select THREE.)</p>",
                "answerExplanation": "<p>To add a role to an Amazon EC2 instance using the AWS CLI you must first create an instance profile. Then you need to add the role to the instance profile and finally assign the instance profile to the Amazon EC2 instance.</p><p>The following example commands would achieve this outcome:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pln\">aws iam create</span><span class=\"pun\">-</span><span class=\"pln\">instance</span><span class=\"pun\">-</span><span class=\"pln\">profile </span><span class=\"pun\">--</span><span class=\"pln\">instance</span><span class=\"pun\">-</span><span class=\"pln\">profile</span><span class=\"pun\">-</span><span class=\"pln\">name EXAMPLEPROFILENAME</span></li><li class=\"L1\"><span class=\"pln\">aws iam add</span><span class=\"pun\">-</span><span class=\"pln\">role</span><span class=\"pun\">-</span><span class=\"pln\">to</span><span class=\"pun\">-</span><span class=\"pln\">instance</span><span class=\"pun\">-</span><span class=\"pln\">profile </span><span class=\"pun\">--</span><span class=\"pln\">instance</span><span class=\"pun\">-</span><span class=\"pln\">profile</span><span class=\"pun\">-</span><span class=\"pln\">name EXAMPLEPROFILENAME </span><span class=\"pun\">--</span><span class=\"pln\">role</span><span class=\"pun\">-</span><span class=\"pln\">name EXAMPLEROLENAME</span></li><li class=\"L2\"><span class=\"pln\">aws ec2 associate</span><span class=\"pun\">-</span><span class=\"pln\">iam</span><span class=\"pun\">-</span><span class=\"pln\">instance</span><span class=\"pun\">-</span><span class=\"pln\">profile </span><span class=\"pun\">--</span><span class=\"pln\">iam</span><span class=\"pun\">-</span><span class=\"pln\">instance</span><span class=\"pun\">-</span><span class=\"pln\">profile </span><span class=\"typ\">Name</span><span class=\"pun\">=</span><span class=\"pln\">EXAMPLEPROFILENAME </span><span class=\"pun\">--</span><span class=\"pln\">instance</span><span class=\"pun\">-</span><span class=\"pln\">id i</span><span class=\"pun\">-</span><span class=\"lit\">012345678910abcde</span></li></ol></pre></div></div><p><strong>CORRECT: </strong>\"Run the <code>aws iam create-instance-profile</code> command\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Run the <code>aws iam add-role-to-instance-profile</code> command\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Run the <code>aws ec2 associate-instance-profile</code> command\" is a correct answer.</p><p><strong>INCORRECT:</strong> \"Run the <code>CreateInstanceProfile</code> API\" is incorrect as this is an API action, not an AWS CLI command.</p><p><strong>INCORRECT:</strong> \"Run the <code>AddRoleToInstanceProfile</code> API\" is incorrect as this is an API action, not an AWS CLI command.</p><p><strong>INCORRECT:</strong> \"Run the <code>AssignInstanceProfile</code> API\" is incorrect as this is an API action, not an AWS CLI command.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/attach-replace-ec2-instance-profile/\">https://aws.amazon.com/premiumsupport/knowledge-center/attach-replace-ec2-instance-profile/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 3219,
                        "content": "<p>Run the <code>CreateInstanceProfile</code> API</p>",
                        "isValid": false
                    },
                    {
                        "id": 3220,
                        "content": "<p>Run the <code>aws iam add-role-to-instance-profile</code> command</p>",
                        "isValid": true
                    },
                    {
                        "id": 3221,
                        "content": "<p>Run the <code>AssignInstanceProfile</code> API</p>",
                        "isValid": false
                    },
                    {
                        "id": 3222,
                        "content": "<p>Run the <code>aws iam create-instance-profile</code> command</p>",
                        "isValid": true
                    },
                    {
                        "id": 3223,
                        "content": "<p>Run the <code>AddRoleToInstanceProfile</code> API</p>",
                        "isValid": false
                    },
                    {
                        "id": 3224,
                        "content": "<p>Run the <code>aws ec2 associate-instance-profile</code> command</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 785,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.918Z",
                "updatedAt": "2023-09-07T08:51:27.918Z",
                "content": "<p>A company uses an Amazon S3 bucket to store a large number of sensitive files relating to eCommerce transactions. The company has a policy that states that all data written to the S3 bucket must be encrypted.</p><p>How can a Developer ensure compliance with this policy?</p>",
                "answerExplanation": "<p>To encrypt an object at the time of upload, you need to add a header called x-amz-server-side-encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS. The following code example shows a Put request using SSE-S3.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_12-40-09-64c48b8534cf3f30983c254bcd6927ce.jpg\"></p><p>Enabling encryption on an S3 bucket does not enforce encryption however, so it is still necessary to take extra steps to force compliance with the policy. As the message in the image below states, bucket policies are applied before encryption settings so PUT requests without encryption information can be rejected by a bucket policy:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_12-40-53-e67a2975cd462c0bf30200960a55c8a8.jpg\"></p><p>Therefore, we need to create an S3 bucket policy that denies any S3 Put request that do not include the x-amz-server-side-encryption header. There are two possible values for the x-amz-server-side-encryption header: AES256, which tells S3 to use S3-managed keys, and aws:kms, which tells S3 to use AWS KMSâ€“managed keys.</p><p><strong>CORRECT: </strong>\"Create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a bucket policy that denies the S3 PutObject request with the attribute x-amz-acl having values public-read, public-read-write, or authenticated-read\" is incorrect. This policy means that authenticated users cannot upload objects to the bucket if the objects have public permissions.</p><p><strong>INCORRECT:</strong> \"Enable Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) on the Amazon S3 bucket\" is incorrect as this will enable default encryption but will not enforce encryption on the S3 bucket. You do still need to enable default encryption on the bucket, but this alone will not enforce encryption.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm that notifies an administrator if unencrypted objects are uploaded to the S3 bucket\" is incorrect. This is operationally difficult to manage and only notifies, it does not prevent.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/\">https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 3225,
                        "content": "<p>Create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption</p>",
                        "isValid": true
                    },
                    {
                        "id": 3226,
                        "content": "<p>Enable Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) on the Amazon S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 3227,
                        "content": "<p>Create an Amazon CloudWatch alarm that notifies an administrator if unencrypted objects are uploaded to the S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 3228,
                        "content": "<p>Create a bucket policy that denies the S3 PutObject request with the attribute x-amz-acl having values public-read, public-read-write, or authenticated-read</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 786,
            "attributes": {
                "createdAt": "2023-09-07T08:51:27.998Z",
                "updatedAt": "2023-09-07T08:51:27.998Z",
                "content": "<p>A company needs a version control system for collaborative software development. The solution must include support for batches of changes across multiple files and parallel branching.</p><p>Which AWS service will meet these requirements?</p>",
                "answerExplanation": "<p>AWS CodeCommit is a version control service hosted by Amazon Web Services that you can use to privately store and manage assets (such as documents, source code, and binary files) in the cloud.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_00-44-09-6cbb7709f71b6cac81aa926b106f948d.png\"></p><p>CodeCommit is optimized for team software development. It manages batches of changes across multiple files, which can occur in parallel with changes made by other developers.</p><p><strong>CORRECT: </strong>\"AWS CodeCommit\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodeBuild\" is incorrect as it is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.</p><p><strong>INCORRECT:</strong> \"AWS CodePipeline\" is incorrect as it is a fully managed <a href=\"https://aws.amazon.com/devops/continuous-delivery/\">continuous delivery</a> service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.</p><p><strong>INCORRECT:</strong> \"Amazon S3\" is incorrect. Amazon S3 versioning supports the recovery of past versions of files, but it's not focused on collaborative file tracking features that software development teams need.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3229,
                        "content": "<p>AWS CodeCommit</p>",
                        "isValid": true
                    },
                    {
                        "id": 3230,
                        "content": "<p>AWS CodePipeline</p>",
                        "isValid": false
                    },
                    {
                        "id": 3231,
                        "content": "<p>AWS CodeBuild</p>",
                        "isValid": false
                    },
                    {
                        "id": 3232,
                        "content": "<p>Amazon S3</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 787,
            "attributes": {
                "createdAt": "2023-09-07T08:51:28.065Z",
                "updatedAt": "2023-09-07T08:51:28.065Z",
                "content": "<p>An application uses both Amazon EC2 instances and on-premises servers. The on-premises servers are a critical component of the application, and a developer wants to collect metrics and logs from these servers. The developer would like to use Amazon CloudWatch.</p><p>How can the developer accomplish this?</p>",
                "answerExplanation": "<p>You can download the CloudWatch agent package using either Systems Manager Run Command or an Amazon S3 download link. You then install the agent and specify the IAM credentials to use. The IAM credentials are an access key and secret access key of an IAM user that has permissions to Amazon CloudWatch.</p><p>Once this has been completed the on-premises servers will automatically send metrics and log files to Amazon CloudWatch and can be centrally monitored along with AWS services.</p><p><strong>CORRECT: </strong>Install the CloudWatch agent on the on-premises servers and specify IAM credentials with permissions to CloudWatch\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install the CloudWatch agent on the on-premises servers and specify an IAM role with permissions to CloudWatch\" is incorrect.</p><p>You cannot specify a role with an on-premises server so you must use access keys instead.</p><p><strong>INCORRECT:</strong> \"Write a batch script that uses system utilities to collect performance metrics and application logs. Upload the metrics and logs to CloudWatch\" is incorrect.</p><p>The CloudWatch agent would be a better solution and you must have permissions to send this information to CloudWatch.</p><p><strong>INCORRECT:</strong> \"Install an AWS SDK on the on-premises servers that automatically sends logs to CloudWatch\" is incorrect.</p><p>The CloudWatch agent would be a better solution and you must have permissions to send this information to CloudWatch.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3233,
                        "content": "<p>Install the CloudWatch agent on the on-premises servers and specify an IAM role with permissions to CloudWatch.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3234,
                        "content": "<p>Install the CloudWatch agent on the on-premises servers and specify IAM credentials with permissions to CloudWatch.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3235,
                        "content": "<p>Install an AWS SDK on the on-premises servers that automatically sends logs to CloudWatch.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3236,
                        "content": "<p>Write a batch script that uses system utilities to collect performance metrics and application logs. Upload the metrics and logs to CloudWatch.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 788,
            "attributes": {
                "createdAt": "2023-09-07T08:51:28.140Z",
                "updatedAt": "2023-09-07T08:51:28.140Z",
                "content": "<p>The following permissions policy is applied to an IAM user account:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"Version\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"2012-10-17\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Statement\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[{</span></li><li class=\"L3\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L4\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"sqs:*\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:sqs:*:513246782345:staging-queue*\"</span></li><li class=\"L6\"><span class=\"pun\">}]</span></li><li class=\"L7\"><span class=\"pun\">}</span></li></ol></pre></div></div><p>Due to this policy, what Amazon SQS actions will the user be able to perform?</p>",
                "answerExplanation": "<p>The policy allows the user to use all Amazon SQS actions, but only with queues whose names are prefixed with the literal string â€œstaging-queueâ€. This policy is useful to provide a queue creator the ability to use Amazon SQS actions. Any user who has permissions to create a queue must also have permissions to use other Amazon SQS actions in order to do anything with the created queues.</p><p><strong>CORRECT: </strong>\"The user will be able to use all Amazon SQS actions, but only for queues with names begin with the string â€œstaging-queueâ€œ\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The user will be able to create a queue named â€œstaging-queueâ€œ\" is incorrect as this policy provides the permissions to perform SQS actions on an existing queue.</p><p><strong>INCORRECT:</strong> \"The user will be able to apply a resource-based policy to the Amazon SQS queue named â€œstaging-queueâ€\" is incorrect as this is a single operation and the permissions policy allows all SQS actions.</p><p><strong>INCORRECT:</strong> \"The user will be granted cross-account access from account number â€œ513246782345â€ to queue â€œstaging-queueâ€\" is incorrect as this is not a policy for granting cross-account access. The account number and queue relate to the same account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-overview-of-managing-access.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-overview-of-managing-access.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3237,
                        "content": "<p>The user will be granted cross-account access from account number â€œ513246782345â€ to queue â€œstaging-queueâ€</p>",
                        "isValid": false
                    },
                    {
                        "id": 3238,
                        "content": "<p>The user will be able to use all Amazon SQS actions, but only for queues with names begin with the string â€œstaging-queueâ€œ</p>",
                        "isValid": true
                    },
                    {
                        "id": 3239,
                        "content": "<p>The user will be able to apply a resource-based policy to the Amazon SQS queue named â€œstaging-queueâ€</p>",
                        "isValid": false
                    },
                    {
                        "id": 3240,
                        "content": "<p>The user will be able to create a queue named â€œstaging-queueâ€œ</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 789,
            "attributes": {
                "createdAt": "2023-09-07T08:51:28.210Z",
                "updatedAt": "2023-09-07T08:51:28.210Z",
                "content": "<p>An application writes items to an Amazon DynamoDB table. As the application scales to thousands of instances, calls to the DynamoDB API generate occasional <code>ThrottlingException</code> errors. The application is coded in a language incompatible with the AWS SDK.</p><p>How should the error be handled?</p>",
                "answerExplanation": "<p>Exponential backoff can improve an application's reliability by using progressively longer waits between retries. When using the AWS SDK, this logic is builtâ€‘in. However, in this case the application is incompatible with the AWS SDK so it is necessary to manually implement exponential backoff.</p><p><strong>CORRECT: </strong>\"Add exponential backoff to the application logic\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon SQS as an API message bus\" is incorrect as SQS requires instances or functions to pick up and process the messages and put them in the DynamoDB table. This is unnecessary cost and complexity and will not improve performance.</p><p><strong>INCORRECT:</strong> \"Pass API calls through Amazon API Gateway\" is incorrect as this is not a suitable method of throttling the application. Exponential backoff logic in the application is a better solution.</p><p><strong>INCORRECT:</strong> \"Send the items to DynamoDB through Amazon Kinesis Data Firehose\" is incorrect as DynamoDB is not a destination for Kinesis Data Firehose.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-table-throttled/\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-table-throttled/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3241,
                        "content": "<p>Pass API calls through Amazon API Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 3242,
                        "content": "<p>Send the items to DynamoDB through Amazon Kinesis Data Firehose</p>",
                        "isValid": false
                    },
                    {
                        "id": 3243,
                        "content": "<p>Use Amazon SQS as an API message bus</p>",
                        "isValid": false
                    },
                    {
                        "id": 3244,
                        "content": "<p>Add exponential backoff to the application logic</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 790,
            "attributes": {
                "createdAt": "2023-09-07T08:51:28.282Z",
                "updatedAt": "2023-09-07T08:51:28.282Z",
                "content": "<p>AWS CodeBuild builds code for an application, creates a Docker image, pushes the image to Amazon Elastic Container Registry (ECR), and tags the image with a unique identifier.</p><p>If the Developers already have AWS CLI configured on their workstations, how can the Docker images be pulled to the workstations?&nbsp; </p>",
                "answerExplanation": "<p>If you would like to run a Docker image that is available in Amazon ECR, you can pull it to your local environment with the docker pull command. You can do this from either your default registry or from a registry associated with another AWS account.</p><p>Docker CLI does not support standard AWS authentication methods, so client authentication must be handled so that ECR knows who is requesting to push or pull an image. To do this you can issue the <code>aws ecr get-login-password</code> AWS&nbsp;CLI&nbsp;command and then use the output to login using <code>docker login</code> and then issue a docker pull command specifying the image name using <code>registry/repository[:tag]</code>.</p><p><strong>CORRECT: </strong>\"Run the output of the following: <code>aws ecr get-login-password</code>, and then run <code>docker pull REPOSITORY URI : TAG</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Run the following: docker pull REPOSITORY URI : TAG\" is incorrect as the Developers first need to authenticate before they can pull the image.</p><p><strong>INCORRECT:</strong> \"Run the following: <code>aws ecr get-login-password</code>, and then run: <code>docker pull REPOSITORY URI : TAG</code>\" is incorrect. The Developers need to not just run the login command but run the output of the login command which contains the authentication token required to log in.</p><p><strong>INCORRECT:</strong> \"Run the output of the following: <code>aws ecr get-download-url-for-layer</code>, and then run <code>docker pull REPOSITORY URI : TAG</code>\" is incorrect as this command retrieves a pre-signed Amazon S3 download URL corresponding to an image layer.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html#registry_auth\">https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html#registry_auth</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html\">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 3245,
                        "content": "<p>Run the output of the following: <code>aws ecr get-login-password</code>, and then run: <code>docker pull REPOSITORY URI : TAG </code> </p>",
                        "isValid": true
                    },
                    {
                        "id": 3246,
                        "content": "<p>Run the output of the following: <code>aws ecr get-download-url-for-layer</code>, and then run: <code>docker pull REPOSITORY URI : TAG</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3247,
                        "content": "<p>Run the following: <code>aws ecr get-login-password</code>, and then run: <code>docker pull REPOSITORY URI : TAG</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3248,
                        "content": "<p>Run the following: <code>docker pull REPOSITORY URI : TAG</code> </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 791,
            "attributes": {
                "createdAt": "2023-09-07T08:51:28.376Z",
                "updatedAt": "2023-09-07T08:51:28.376Z",
                "content": "<p>A company runs a decoupled application that uses an Amazon SQS queue. The messages are processed by an AWS Lambda function. The function is not keeping up with the number of messages in the queue. A developer noticed that though the application can process multiple messages per invocation, it is only processing one at a time.</p><p>How can the developer configure the application to process messages more efficiently?</p>",
                "answerExplanation": "<p>The ReceiveMessage API retrieves one or more messages (up to 10), from the specified queue. The MaxNumberOfMessages specifies the maximum number of messages to return. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10. Default: 1.</p><p>Changing the MaxNumberOfMessages using the ReceiveMessage API to a value greater than 1 will therefore enable the application to process more messages in a single invocation, leading to greater efficiency.</p><p><strong>CORRECT: </strong>\"Call the ReceiveMessage API to set MaxNumberOfMessages to a value greater than the default of 1\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Call the ReceiveMessage API to set MaximumMessageSize to a value greater than the default of 1\" is incorrect.</p><p>MaximumMessageSize specifies the maximum bytes a message can contain before SQS rejects it.</p><p><strong>INCORRECT:</strong> \"Call the ChangeMessageVisibility API for the queue and set MessageRetentionPeriod to a value greater than the default of 1\" is incorrect.</p><p>ChangeMessageVisibility changes the visibility timeout of a specified message in a queue to a new value.</p><p><strong>INCORRECT:</strong> \"Call the SetQueueAttributes API for the queue and set MaxNumberOfMessages to a value greater than the default of 1\" is incorrect.</p><p>MaxNumberOfMessages is configured using the ReceiveMessage API, not the SetQueueAttributes API.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3249,
                        "content": "<p>Call the ReceiveMessage API to set MaximumMessageSize to a value greater than the default of 1.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3250,
                        "content": "<p>Call the ChangeMessageVisibility API for the queue and set MessageRetentionPeriod to a value greater than the default of 1.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3251,
                        "content": "<p>Call the SetQueueAttributes API for the queue and set MaxNumberOfMessages to a value greater than the default of 1.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3252,
                        "content": "<p>Call the ReceiveMessage API to set MaxNumberOfMessages to a value greater than the default of 1.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 792,
            "attributes": {
                "createdAt": "2023-09-07T08:51:28.458Z",
                "updatedAt": "2023-09-07T08:51:28.458Z",
                "content": "<p>A legacy service has an XML-based SOAP interface. The Developer wants to expose the functionality of the service to external clients with the Amazon API Gateway. Which technique will accomplish this?</p>",
                "answerExplanation": "<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services as well as data stored in the AWS Cloud.</p><p>In API Gateway, an API's method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, the backend may return an integration response payload different from the method response payload, as expected by the frontend.</p><p>API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.</p><p>If an existing legacy service returns XML-style data, you can use the API Gateway to transform the output to JSON as part of your modernization effort. The API Gateway can be configured to transform the output of legacy services from XML to JSON, allowing them to make a move that is seamless and non-disruptive. The transformation is specified using JSON-Schema.</p><p>Therefore, the technique the Developer should use is to create a RESTful API with the API Gateway and transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates.</p><p><strong>CORRECT: </strong>\"Create a RESTful API with the API Gateway; transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a RESTful API with the API Gateway; pass the incoming JSON to the SOAP interface through an Application Load Balancer\" is incorrect as we donâ€™t need an ALB to do this, we can use a mapping template within the API Gateway which will be more cost-efficient.</p><p><strong>INCORRECT:</strong> \"Create a RESTful API with the API Gateway; pass the incoming XML to the SOAP interface through an Application Load Balancer\" is incorrect as the incoming data will be JSON, not XML as the Developer needs to publish a modern application interface. A mapping template should also be used in place of the ALB.</p><p><strong>INCORRECT:</strong> \"Create a RESTful API with the API Gateway; transform the incoming XML into a valid message for the SOAP interface using mapping templates\" is incorrect as the incoming data will be JSON, not XML as the Developer needs to publish a modern application interface.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 3253,
                        "content": "<p>Create a RESTful API with the API Gateway; transform the incoming XML into a valid message for the SOAP interface using mapping templates</p>",
                        "isValid": false
                    },
                    {
                        "id": 3254,
                        "content": "<p>Create a RESTful API with the API Gateway; pass the incoming JSON to the SOAP interface through an Application Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 3255,
                        "content": "<p>Create a RESTful API with the API Gateway; pass the incoming XML to the SOAP interface through an Application Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 3256,
                        "content": "<p>Create a RESTful API with the API Gateway; transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 793,
            "attributes": {
                "createdAt": "2023-09-07T08:51:28.533Z",
                "updatedAt": "2023-09-07T08:51:28.533Z",
                "content": "<p>An application uses Amazon API Gateway, an AWS Lambda function and a DynamoDB table. The developer requires that another Lambda function is triggered when an item lifecycle activity occurs in the DynamoDB table.</p><p>How can this be achieved?</p>",
                "answerExplanation": "<p>Amazon DynamoDB is integrated with AWS Lambda so that you can create <em>triggers</em>â€”pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.</p><p>If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an AWS Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.</p><p><strong>CORRECT: </strong>\"Enable a DynamoDB stream and trigger the Lambda function synchronously from the stream\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable a DynamoDB stream and trigger the Lambda function asynchronously from the stream\" is incorrect as the invocation should be synchronous.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon CloudWatch alarm that sends an Amazon SNS notification. Trigger the Lambda function asynchronously from the SNS notification\" is incorrect as you cannot configure a CloudWatch alarm that notifies based on item lifecycle events. It is better to use DynamoDB streams and integrate Lambda.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon CloudTrail API alarm that sends a message to an Amazon SQS queue. Configure the Lambda function to poll the queue and invoke the function synchronously\" is incorrect. There is no such alarm that notifies from Amazon CloudTrail relating to item lifecycle events.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3257,
                        "content": "<p>Enable a DynamoDB stream and trigger the Lambda function asynchronously from the stream</p>",
                        "isValid": false
                    },
                    {
                        "id": 3258,
                        "content": "<p>Configure an Amazon CloudTrail API alarm that sends a message to an Amazon SQS queue. Configure the Lambda function to poll the queue and invoke the function synchronously</p>",
                        "isValid": false
                    },
                    {
                        "id": 3259,
                        "content": "<p>Configure an Amazon CloudWatch alarm that sends an Amazon SNS notification. Trigger the Lambda function asynchronously from the SNS notification</p>",
                        "isValid": false
                    },
                    {
                        "id": 3260,
                        "content": "<p>Enable a DynamoDB stream and trigger the Lambda function synchronously from the stream</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 794,
            "attributes": {
                "createdAt": "2023-09-07T08:51:28.610Z",
                "updatedAt": "2023-09-07T08:51:28.610Z",
                "content": "<p>A developer is planning the deployment of a new version of an application to AWS Elastic Beanstalk. The new version of the application should be deployed only to new EC2 instances.</p><p>Which deployment methods will meet these requirements? (Select TWO.)</p>",
                "answerExplanation": "<p>AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.</p><p>All at once:</p><p> â€¢ Deploys the new version to all instances simultaneously.</p><p>Rolling:</p><p> â€¢ Update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy (downtime for 1 bucket at a time).</p><p>Rolling with additional batch:</p><p>â€¢ Like Rolling but launches new instances in a batch ensuring that there is full availability.</p><p>Immutable:</p><p>â€¢ Launches new instances in a new ASG and deploys the version update to these instances before swapping traffic to these instances once healthy.</p><p>â€¢ Zero downtime.</p><p>Blue / Green deployment:</p><p> â€¢ Zero downtime and release facility.</p><p> â€¢ Create a new â€œstageâ€ environment and deploy updates there.</p><p>The immutable and blue/green options both provide zero downtime as they will deploy the new version to a new version of the application. These are also the only two options that will ONLY deploy the updates to new EC2 instances.</p><p><strong>CORRECT: </strong>\"Immutable\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Blue/green\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"All-at-once\" is incorrect as this will deploy the updates to existing instances.</p><p><strong>INCORRECT:</strong> \"Rolling\" is incorrect as this will deploy the updates to existing instances.</p><p><strong>INCORRECT:</strong> \"Rolling with additional batch\" is incorrect as this will launch new instances but will also update the existing instances as well (which is not allowed according to the requirements).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
                "options": [
                    {
                        "id": 3261,
                        "content": "<p>Rolling with additional batch</p>",
                        "isValid": false
                    },
                    {
                        "id": 3262,
                        "content": "<p>Blue/green</p>",
                        "isValid": true
                    },
                    {
                        "id": 3263,
                        "content": "<p>Immutable</p>",
                        "isValid": true
                    },
                    {
                        "id": 3264,
                        "content": "<p>Rolling</p>",
                        "isValid": false
                    },
                    {
                        "id": 3265,
                        "content": "<p>All at once</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 795,
            "attributes": {
                "createdAt": "2023-09-07T08:51:28.680Z",
                "updatedAt": "2023-09-07T08:51:28.680Z",
                "content": "<p>A company is creating a serverless application that uses AWS Lambda functions. The developer has written the code to initialize the AWS SDK outside of the Lambda handler function.</p><p>What is PRIMARY benefit of this action?</p>",
                "answerExplanation": "<p>You should initialize SDK clients and database connections outside of the function handler, and cache static assets locally in the /tmp directory. Subsequent invocations processed by the same instance of your function can reuse these resources. This saves cost by reducing function run time.</p><p>The primary benefit of this technique is to take advantage of execution environment reuse to improve the performance of your function.</p><p><strong>CORRECT: </strong>\"Takes advantage of execution environment reuse\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Creates a new SDK instance for each invocation\" is incorrect.</p><p>This is the opposite of what we are trying to achieve here.</p><p><strong>INCORRECT:</strong> \"It minimizes the deployment package size\" is incorrect.</p><p>This technique does not affect the deployment package size.</p><p><strong>INCORRECT:</strong> \"Improves readability and reduces complexity\" is incorrect.</p><p>It may improve readability but that is debatable. This is not the primary reason you would use this technique.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\">https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3266,
                        "content": "<p>Takes advantage of execution environment reuse.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3267,
                        "content": "<p>Improves readability and reduces complexity.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3268,
                        "content": "<p>It minimizes the deployment package size.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3269,
                        "content": "<p>Creates a new SDK instance for each invocation.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 796,
            "attributes": {
                "createdAt": "2023-09-07T08:51:28.763Z",
                "updatedAt": "2023-09-07T08:51:28.763Z",
                "content": "<p>An application is using Amazon DynamoDB as its data store and needs to be able to read 100 items per second as strongly consistent reads. Each item is 5 KB in size.<br>What value should be set for the table's provisioned throughput for reads?</p>",
                "answerExplanation": "<p>With provisioned capacity mode, you specify the number of data reads and writes per second that you require for your application.</p><p><strong>Read capacity unit (RCU):</strong></p><p>â€¢ Each API call to read data from your table is a read request.</p><p>â€¢ Read requests can be strongly consistent, eventually consistent, or transactional.</p><p>â€¢ For items up to 4 KB in size, one RCU can perform one <em>strongly consistent</em> read request per second.</p><p>â€¢ Items larger than 4 KB require additional RCUs.</p><p>â€¢ For items up to 4 KB in size, one RCU can perform two <em>eventually consistent</em> read requests per second.</p><p>â€¢ <em>Transactional</em> read requests require two RCUs to perform one read per second for items up to 4 KB.</p><p>â€¢ For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs.</p><p><strong>Write capacity unit (WCU):</strong></p><p>â€¢ Each API call to write data to your table is a write request.</p><p>â€¢ For items up to 1 KB in size, one WCU can perform one<em> standard</em> write request per second.</p><p>â€¢ Items larger than 1 KB require additional WCUs.</p><p>â€¢ <em>Transactional</em> write requests require two WCUs to perform one write per second for items up to 1 KB.</p><p>â€¢ For example, a standard write request of a 1 KB item would require one WCU, a standard write request of a 3 KB item would require three WCUs, and a transactional write request of a 3 KB item would require six WCUs.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_12-44-23-d450e4d38292096046a8f25b6106f284.jpg\"></p><p><strong>To determine the number of RCUs required to handle 100 strongly consistent reads per/second with an average item size of 5KB, perform the following steps:</strong></p><p><strong>1. Determine the average item size by rounding up the next multiple of 4KB (5KB rounds up to 8KB).</strong></p><p><strong>2. Determine the RCU per item by dividing the item size by 4KB (8KB/4KB = 2).</strong></p><p>3. <strong>Multiply the value from step 2 with the number of reads required per second (2x100 = 200).</strong></p><p><strong>CORRECT: </strong>\"200 Read Capacity Units\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"50 Read Capacity Units\" is incorrect.</p><p><strong>INCORRECT:</strong> \"250 Read Capacity Units\" is incorrect.</p><p><strong>INCORRECT:</strong> \"500 Read Capacity Units\" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/pricing/provisioned/\">https://aws.amazon.com/dynamodb/pricing/provisioned/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3270,
                        "content": "<p>200 Read Capacity Units</p>",
                        "isValid": true
                    },
                    {
                        "id": 3271,
                        "content": "<p>500 Read Capacity Units</p>",
                        "isValid": false
                    },
                    {
                        "id": 3272,
                        "content": "<p>50 Read Capacity Units</p>",
                        "isValid": false
                    },
                    {
                        "id": 3273,
                        "content": "<p>250 Read Capacity Units</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 797,
            "attributes": {
                "createdAt": "2023-09-07T08:51:28.835Z",
                "updatedAt": "2023-09-07T08:51:28.835Z",
                "content": "<p>A Developer is creating an AWS Lambda function that will process data from an Amazon Kinesis data stream. The function is expected to be invoked 50 times per second and take 100 seconds to complete each request.</p><p>What MUST the Developer do to ensure the functions runs without errors?</p>",
                "answerExplanation": "<p>Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.</p><p>Concurrency is subject to a Regional limit that is shared by all functions in a Region. For an initial burst of traffic, your functions' cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region:</p><p>â€¢ <strong>3000</strong> â€“ US West (Oregon), US East (N. Virginia), Europe (Ireland)</p><p>â€¢ <strong>1000</strong> â€“ Asia Pacific (Tokyo), Europe (Frankfurt)</p><p>â€¢ <strong>500</strong> â€“ Other Regions</p><p>After the initial burst, your functions' concurrency can scale by an additional 500 instances each minute. This continues until there are enough instances to serve all requests, or until a concurrency limit is reached. When requests come in faster than your function can scale, or when your function is at maximum concurrency, additional requests fail with a throttling error (429 status code).</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_14-11-32-36763214cd8269e631d16f5e21a39386.jpg\"></p><p>The function continues to scale until the account's concurrency limit for the function's Region is reached. The function catches up to demand, requests subside, and unused instances of the function are stopped after being idle for some time. Unused instances are frozen while they're waiting for requests and don't incur any charges.</p><p>The regional concurrency limit starts at 1,000. You can increase the limit by submitting a request in the Support Center console.</p><p><strong>Calculating concurrency requirements for this scenario</strong></p><p>To calculate the concurrency requirements for this scenario, simply multiply the invocation requests per second (50) with the average execution time in seconds (100). This calculation is 50 x 100 = 5,000.</p><p>Therefore, 5,000 concurrent executions is over the default limit and the Developer will need to request in the AWS Support Center console.</p><p><strong>CORRECT: </strong>\"Contact AWS and request to increase the limit for concurrent executions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"No action is required as AWS Lambda can easily accommodate this requirement\" is incorrect as by default the AWS account will be limited. Lambda can easily scale to this level of demand however the account limits must first be increased.</p><p><strong>INCORRECT:</strong> \"Increase the concurrency limit for the function\" is incorrect as the default account limit of 1,000 concurrent executions will mean you can only assign up to 900 executions to the function (100 must be left unreserved). This is insufficient for this requirement to the account limit must be increased.</p><p><strong>INCORRECT:</strong> \"Implement exponential backoff in the function code\" is incorrect. Exponential backoff means configuring the application to wait longer between API calls, slowing the demand. However, this is not a good resolution to this issue as it will have negative effects on the application. The correct choice is to raise the account limits so the function can concurrently execute according to its requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3274,
                        "content": "<p>Increase the concurrency limit for the function</p>",
                        "isValid": false
                    },
                    {
                        "id": 3275,
                        "content": "<p>Implement exponential backoff in the function code</p>",
                        "isValid": false
                    },
                    {
                        "id": 3276,
                        "content": "<p>No action is required as AWS Lambda can easily accommodate this requirement</p>",
                        "isValid": false
                    },
                    {
                        "id": 3277,
                        "content": "<p>Contact AWS and request to increase the limit for concurrent executions</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 798,
            "attributes": {
                "createdAt": "2023-09-07T08:51:28.913Z",
                "updatedAt": "2023-09-07T08:51:28.913Z",
                "content": "<p>A financial application is hosted on an Auto Scaling group of EC2 instance with an Elastic Load Balancer. A Developer needs to capture information about the IP traffic going to and from network interfaces in the VPC.</p><p>How can the Developer capture this information?</p>",
                "answerExplanation": "<p>VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. After you've created a flow log, you can retrieve and view its data in the chosen destination.</p><p>Flow logs can help you with a number of tasks, such as:</p><p>â€¢ Diagnosing overly restrictive security group rules</p><p>â€¢ Monitoring the traffic that is reaching your instance</p><p>â€¢ Determining the direction of the traffic to and from the network interfaces</p><p>As you can see in the image below, you can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_00-42-52-226385bbe097171e36f1e176fc58bea1.png\"></p><p>Therefore, the Developer should create a flow log in the VPC and publish data to Amazon S3. The Developer could also choose CloudWatch Logs as a destination for publishing the data, but this is not presented as an option.</p><p><strong>CORRECT: </strong>\"Create a flow log in the VPC and publish data to Amazon S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Capture the information directly into Amazon CloudWatch Logs\" is incorrect as you cannot capture this information directly into CloudWatch Logs. You would need to capture with a flow log and then publish to CloudWatch Logs.</p><p><strong>INCORRECT:</strong> \"Capture the information using a Network ACL\" is incorrect as you cannot capture data using a Network ACL as it is a subnet-level firewall.</p><p><strong>INCORRECT:</strong> \"Create a flow log in the VPC and publish data to Amazon CloudTrail\" is incorrect as you cannot publish data from a flow log to CloudTrail. Amazon CloudTrail captures information about API calls.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 3278,
                        "content": "<p>Create a flow log in the VPC and publish data to Amazon S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 3279,
                        "content": "<p>Capture the information using a Network ACL</p>",
                        "isValid": false
                    },
                    {
                        "id": 3280,
                        "content": "<p>Capture the information directly into Amazon CloudWatch Logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 3281,
                        "content": "<p>Create a flow log in the VPC and publish data to Amazon CloudTrail</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 799,
            "attributes": {
                "createdAt": "2023-09-07T08:51:28.993Z",
                "updatedAt": "2023-09-07T08:51:28.993Z",
                "content": "<p>An engineer is constructing an AWS Lambda function and intends to log specific key events that transpire during the function's execution. To correlate the events with a particular function invocation, the engineer is looking to incorporate a unique identifier.</p><p>The following code segment has been added to the Lambda function:</p><p>function handler (event, context) {</p><p>}</p>",
                "answerExplanation": "<p>The <strong>context</strong> object in a Lambda function provides metadata about the function and the current invocation, including a unique identifier for the request, <strong>awsRequestId</strong>, which can be used to correlate logs from a specific invocation.</p><p><strong>CORRECT: </strong>\"Use <strong>context.awsRequestId</strong> within the function to fetch the unique identifier associated with each invocation\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use <strong>event.requestId</strong> to obtain the unique identifier for each function execution\" is incorrect.</p><p><strong>event.requestId</strong> is incorrect as the event object does not contain a property called <strong>requestId</strong>.</p><p><strong>INCORRECT:</strong> \"Use <strong>context.invocationId</strong> to extract the unique identifier tied to each function run\" is incorrect.</p><p><strong>context.invocationId</strong> is not valid as there is no such property in the context object of a Lambda function.</p><p><strong>INCORRECT:</strong> \"Use <strong>context.lambdaId</strong> to get the unique identifier corresponding to each function invocation\" is incorrect.</p><p><strong>context.lambdaId</strong> is not a valid property within the context object for a Lambda function.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/nodejs-context.html\">https://docs.aws.amazon.com/lambda/latest/dg/nodejs-context.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3282,
                        "content": "<p>Use <strong>context.awsRequestId</strong> within the function to fetch the unique identifier associated with each invocation.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3283,
                        "content": "<p>Use <strong>context.invocationId</strong> to extract the unique identifier tied to each function run.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3284,
                        "content": "<p>Use <strong>context.lambdaId</strong> to get the unique identifier corresponding to each function invocation.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3285,
                        "content": "<p>Use <strong>event.requestId</strong> to obtain the unique identifier for each function execution.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 800,
            "attributes": {
                "createdAt": "2023-09-07T08:51:29.078Z",
                "updatedAt": "2023-09-07T08:51:29.078Z",
                "content": "<p>A Developer is designing a cloud native application. The application will use several AWS Lambda functions that will process items that the functions read from an event source. Which AWS services are supported for Lambda event source mappings? (Select THREE.)</p>",
                "answerExplanation": "<p>An event source mapping is an AWS Lambda resource that reads from an event source and invokes a Lambda function. You can use event source mappings to process items from a stream or queue in services that don't invoke Lambda functions directly. Lambda provides event source mappings for the following services.</p><p><strong>Services That Lambda Reads Events From</strong></p><p>â€¢ <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\">Amazon Kinesis</a></p><p>â€¢ <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html\">Amazon DynamoDB</a></p><p>â€¢ <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\">Amazon Simple Queue Service</a></p><p>An event source mapping uses permissions in the function's <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\">execution role</a> to read and manage items in the event source. Permissions, event structure, settings, and polling behavior vary by event source.</p><p><strong>CORRECT: </strong>\"Amazon Kinesis, Amazon DynamoDB, and Amazon Simple Queue Service (SQS)\" are the correct answers.</p><p><strong>INCORRECT:</strong> \"Amazon Simple Notification Service (SNS)\" is incorrect as SNS should be used as destination for asynchronous invocation.</p><p><strong>INCORRECT:</strong> \"Amazon Simple Storage Service (S3)\" is incorrect as Lambda does not read from Amazon S3, you must configure the event notification on the S3 side.</p><p><strong>INCORRECT:</strong> \"Another Lambda function\" is incorrect as another function should be invoked asynchronously.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventsourcemapping.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventsourcemapping.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3286,
                        "content": "<p>Another Lambda function</p>",
                        "isValid": false
                    },
                    {
                        "id": 3287,
                        "content": "<p>Amazon Simple Queue Service (SQS)</p>",
                        "isValid": true
                    },
                    {
                        "id": 3288,
                        "content": "<p>Amazon Simple Storage Service (S3)</p>",
                        "isValid": false
                    },
                    {
                        "id": 3289,
                        "content": "<p>Amazon Kinesis</p>",
                        "isValid": true
                    },
                    {
                        "id": 3290,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 3291,
                        "content": "<p>Amazon Simple Notification Service (SNS)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 801,
            "attributes": {
                "createdAt": "2023-09-07T08:51:29.151Z",
                "updatedAt": "2023-09-07T08:51:29.151Z",
                "content": "<p>An application runs on a fleet of Amazon EC2 instances and stores data in a Microsoft SQL Server database hosted on Amazon RDS. The developer wants to avoid storing database connection credentials the application code. The developer would also like a solution that automatically rotates the credentials.</p><p>What is the MOST secure way to store and access the database credentials?</p>",
                "answerExplanation": "<p>AWS Secrets Manager can be used for secure storage of secrets such as database connection credentials. Automatic rotation is supported for several RDS database types including Microsoft SQL Server. This is the most secure solution for storing and retrieving the credentials.</p><p><strong>CORRECT: </strong>\"Use AWS Secrets Manager to store the credentials. Retrieve the credentials from Secrets Manager as needed\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Parameter store to store the credentials. Enable automatic rotation of the credentials\" is incorrect.</p><p>With SSM Parameter Store you cannot enable automatic rotation. You can rotate the credentials but you would need to configure your own Lambda function.</p><p><strong>INCORRECT:</strong> \"Create an IAM role that has permissions to access the database. Attach the role to the EC2 instance\" is incorrect.</p><p>RDS for SQL Server does support windows authentication using a managed Microsoft AD with IAM roles for permissions to the AD service, but this is not described in the solution.</p><p><strong>INCORRECT:</strong> \"Store the credentials in an encrypted source code repository. Retrieve the credentials from AWS CodeCommit as needed\" is incorrect.</p><p>This is not a solution that is suitable for retrieving database connection credentials and it does not support automatic rotation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-secrets-manager/\">https://digitalcloud.training/aws-secrets-manager/</a></p>",
                "options": [
                    {
                        "id": 3292,
                        "content": "<p>Store the credentials in an encrypted source code repository. Retrieve the credentials from AWS CodeCommit as needed.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3293,
                        "content": "<p>Create an IAM role that has permissions to access the database. Attach the role to the EC2 instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3294,
                        "content": "<p>Use AWS Systems Manager Parameter store to store the credentials. Enable automatic rotation of the credentials.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3295,
                        "content": "<p>Use AWS Secrets Manager to store the credentials. Retrieve the credentials from Secrets Manager as needed.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 802,
            "attributes": {
                "createdAt": "2023-09-07T08:51:29.227Z",
                "updatedAt": "2023-09-07T08:51:29.227Z",
                "content": "<p>A Development team has deployed several applications running on an Auto Scaling fleet of Amazon EC2 instances. The Operations team have asked for a display that shows a key performance metric for each application on a single screen for monitoring purposes.</p><p>What steps should a Developer take to deliver this capability using Amazon CloudWatch?</p>",
                "answerExplanation": "<p>A <em>namespace</em> is a container for CloudWatch metrics. Metrics in different namespaces are isolated from each other, so that metrics from different applications are not mistakenly aggregated into the same statistics.</p><p>Therefore, the Developer should create a custom namespace with a unique metric name for each application. This namespace will then allow the metrics for each individual application to be shown in a single view through CloudWatch.</p><p><strong>CORRECT: </strong>\"Create a custom namespace with a unique metric name for each application\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a custom dimension with a unique metric name for each application\" is incorrect as a dimension further clarifies what a metric is and what data it stores.</p><p><strong>INCORRECT:</strong> \"Create a custom event with a unique metric name for each application\" is incorrect as an event is not used to organize metrics for display.</p><p><strong>INCORRECT:</strong> \"Create a custom alarm with a unique metric name for each application\" is incorrect as alarms are used to trigger actions when a threshold is reached, this is not relevant to organizing metrics for display.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/certification-training/aws-developer-associate/aws-management-and-governance/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3296,
                        "content": "<p>Create a custom dimension with a unique metric name for each application</p>",
                        "isValid": false
                    },
                    {
                        "id": 3297,
                        "content": "<p>Create a custom event with a unique metric name for each application</p>",
                        "isValid": false
                    },
                    {
                        "id": 3298,
                        "content": "<p>Create a custom alarm with a unique metric name for each application</p>",
                        "isValid": false
                    },
                    {
                        "id": 3299,
                        "content": "<p>Create a custom namespace with a unique metric name for each application</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 803,
            "attributes": {
                "createdAt": "2023-09-07T08:51:29.298Z",
                "updatedAt": "2023-09-07T08:51:29.298Z",
                "content": "<p>A company wants a serverless solution for phased release of static websites hosted on various version control systems. Deployments should be triggered by Git branch merges and all data exchange should be over HTTPS.</p><p>Which option offers the LOWEST operational overhead?</p>",
                "answerExplanation": "<p>AWS Amplify is designed to host static websites with continuous deployment linked to Git repositories. It allows you to connect branches in your repository with environments in Amplify, and to automatically deploy changes when you merge code to those branches. This solution requires the least operational overhead among the options.</p><p><strong>CORRECT: </strong>\"Use AWS Amplify for hosting, connect corresponding repository branches, and initiate deployments by merging changes to the needed branch\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy websites on separate Amazon EC2 instances for each environment, use AWS CodeDeploy for automation, and link it with the version control systems\" is incorrect.</p><p>While Amazon EC2 could host the websites and AWS CodeDeploy could manage deployments, this is not a serverless solution as requested. The EC2 instances would need to run continuously, creating more operational overhead.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 to host the websites, create a manual script to deploy changes when there are code merges in the version control systems\" is incorrect.</p><p>While Amazon S3 can host static websites, creating a manual script to deploy changes adds unnecessary complexity and operational overhead compared to a service like Amplify that automates this process.</p><p><strong>INCORRECT:</strong> \"Use AWS Elastic Beanstalk for hosting and use AWS CodeStar to manage deployments and workflows\" is incorrect.</p><p>AWS Elastic Beanstalk is typically used for dynamic, multi-tier web applications, and AWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place. However, this combination could be more complex and require more operational overhead than AWS Amplify for a static website.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amplify/latest/userguide/welcome.html\">https://docs.aws.amazon.com/amplify/latest/userguide/welcome.html</a></p>",
                "options": [
                    {
                        "id": 3300,
                        "content": "<p>Use AWS Amplify for hosting, connect corresponding repository branches, and initiate deployments by merging changes to the needed branch.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3301,
                        "content": "<p>Deploy websites on separate Amazon EC2 instances for each environment, use AWS CodeDeploy for automation, and link it with the version control systems.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3302,
                        "content": "<p>Use AWS Elastic Beanstalk for hosting and use AWS CodeStar to manage deployments and workflows.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3303,
                        "content": "<p>Use Amazon S3 to host the websites, create a manual script to deploy changes when there are code merges in the version control systems.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 804,
            "attributes": {
                "createdAt": "2023-09-07T08:51:29.371Z",
                "updatedAt": "2023-09-07T08:51:29.371Z",
                "content": "<p>A developer is creating an AWS Serverless Application Model (AWS SAM) template. It includes several AWS Lambda functions, an Amazon S3 bucket, and an Amazon CloudFront distribution. One Lambda function, running on Lambda@Edge, is integrated with the CloudFront distribution, while the S3 bucket serves as an origin for the distribution.</p><p>However, upon deploying the AWS SAM blueprint in the us-west-1 Region, the stack's creation fails.</p><p>What could be the possible reason for this failure?</p>",
                "answerExplanation": "<p>Lambda@Edge functions can only be created in the us-east-1 Region. If you want to deploy such functions, they must be done in this specific region.</p><p><strong>CORRECT: </strong>\"Lambda@Edge functions can only be deployed in the us-east-1 Region\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS SAM templates are not supported in the us-west-1 Region\" is incorrect.</p><p>This is not a restriction, and hence not a reason for failure.</p><p><strong>INCORRECT:</strong> \"Amazon S3 buckets serving as origins for CloudFront must be created in a separate Region from the CloudFront distribution\" is incorrect.</p><p>Amazon S3 buckets serving as origins for CloudFront distributions can be in the same region as the distribution. Therefore, this would not cause a failure.</p><p><strong>INCORRECT:</strong> \"AWS Lambda functions integrated with CloudFront cannot be deployed using AWS SAM templates\" is incorrect.</p><p>AWS Lambda functions can be integrated with Amazon CloudFront and deployed using AWS SAM templates. This statement is incorrect and would not lead to a stack creation failure.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-edge-how-it-works.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-edge-how-it-works.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-sam/\">https://digitalcloud.training/aws-sam/</a></p>",
                "options": [
                    {
                        "id": 3304,
                        "content": "<p>Lambda@Edge functions can only be deployed in the us-east-1 Region.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3305,
                        "content": "<p>AWS SAM templates are not supported in the us-west-1 Region.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3306,
                        "content": "<p>AWS Lambda functions integrated with CloudFront cannot be deployed using AWS SAM templates.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3307,
                        "content": "<p>Amazon S3 buckets serving as origins for CloudFront must be created in a separate Region from the CloudFront distribution.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 805,
            "attributes": {
                "createdAt": "2023-09-07T08:51:29.443Z",
                "updatedAt": "2023-09-07T08:51:29.443Z",
                "content": "<p>A customer requires a schema-less, key/value database that can be used for storing customer orders. Which type of AWS database is BEST suited to this requirement?</p>",
                "answerExplanation": "<p>Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is a non-relational (schema-less), key-value type of database. This is the most suitable solution for this requirement.</p><p><strong>CORRECT: </strong>\"Amazon DynamoDB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon RDS\" is incorrect as this a relational database that has a schema.</p><p><strong>INCORRECT:</strong> \"Amazon ElastiCache\" is incorrect as this is a key/value database but it is used to cache the contents of other databases (including DynamoDB and RDS) for better performance for reads.</p><p><strong>INCORRECT:</strong> \"Amazon S3\" is incorrect as this is an object-based storage system not a database. It is a key/value store but DynamoDB is a better fit for a customer order database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3308,
                        "content": "<p>Amazon RDS</p>",
                        "isValid": false
                    },
                    {
                        "id": 3309,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": true
                    },
                    {
                        "id": 3310,
                        "content": "<p>Amazon ElastiCache</p>",
                        "isValid": false
                    },
                    {
                        "id": 3311,
                        "content": "<p>Amazon S3</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 806,
            "attributes": {
                "createdAt": "2023-09-07T08:51:29.515Z",
                "updatedAt": "2023-09-07T08:51:29.515Z",
                "content": "<p>A Developer has created an Amazon Cognito user pool and configured a domain for it. The Developer wants to add sign-up and sign-in pages to an app with a company logo.</p><p>What should the Developer do to meet these requirements?</p>",
                "answerExplanation": "<p>When you create a user pool in Amazon Cognito and then configure a domain for it, Amazon Cognito automatically provisions a hosted web UI to let you add sign-up and sign-in pages to your app. You can add a custom logo or customize the CSS for the hosted web UI.</p><p><strong>CORRECT: </strong>\"Customize the Amazon Cognito hosted web UI and add the company logo\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a REST API using Amazon API Gateway and add a Cognito authorizer. Upload the company logo to a stage in the API\" is incorrect. There is no need to add a REST API to this solution.</p><p><strong>INCORRECT:</strong> \"Upload the company logo to an Amazon S3 bucket. Specify the S3 object path in the app client settings in Amazon Cognito\" is incorrect. This is not required as the hosted web UI can be used.</p><p><strong>INCORRECT:</strong> \"Create a custom login page that includes the company logo and upload it to Amazon Cognito. Specify the login page in the app client settings\" is incorrect. This is not required as the hosted web UI can be used.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cognito-hosted-web-ui/\">https://aws.amazon.com/premiumsupport/knowledge-center/cognito-hosted-web-ui/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 3312,
                        "content": "<p>Create a REST API using Amazon API Gateway and add a Cognito authorizer. Upload the company logo to a stage in the API.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3313,
                        "content": "<p>Customize the Amazon Cognito hosted web UI and add the company logo.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3314,
                        "content": "<p>Create a custom login page that includes the company logo and upload it to Amazon Cognito. Specify the login page in the app client settings.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3315,
                        "content": "<p>Upload the company logo to an Amazon S3 bucket. Specify the S3 object path in the app client settings in Amazon Cognito.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 807,
            "attributes": {
                "createdAt": "2023-09-07T08:51:29.586Z",
                "updatedAt": "2023-09-07T08:51:29.586Z",
                "content": "<p>A company is planning to use AWS CodeDeploy to deploy a new AWS Lambda function</p><p>What are the MINIMUM properties required in the 'resources' section of the AppSpec file for CodeDeploy to deploy the function successfully?</p>",
                "answerExplanation": "<p>The content in the 'resources' section of the AppSpec file varies, depending on the compute platform of your deployment. The 'resources' section for an AWS Lambda deployment contains the name, alias, current version, and target version of a Lambda function.</p><p>Here is an example of a 'resources' section with the minimum required properties:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-30_07-11-12-a81cd00fae3f3338c68bdeaa23ac61f5.jpg\"><p><strong>CORRECT: </strong>\"name, alias, currentversion, and targetversion\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"name, alias, PlatformVersion, and type\" is incorrect (as explained above.)</p><p><strong>INCORRECT:</strong> \"TaskDefinition, LoadBalancerInfo, and ContainerPort\" is incorrect.</p><p>These properties are related to ECS deployments.</p><p><strong>INCORRECT:</strong> \"TaskDefinition, PlatformVersion, and ContainerName\" is incorrect.</p><p>These properties are related to ECS deployments.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-resources.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-resources.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3316,
                        "content": "<p>TaskDefinition, LoadBalancerInfo, and ContainerPort</p>",
                        "isValid": false
                    },
                    {
                        "id": 3317,
                        "content": "<p>name, alias, currentversion, and targetversion</p>",
                        "isValid": true
                    },
                    {
                        "id": 3318,
                        "content": "<p>name, alias, PlatformVersion, and type</p>",
                        "isValid": false
                    },
                    {
                        "id": 3319,
                        "content": "<p>TaskDefinition, PlatformVersion, and ContainerName</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 808,
            "attributes": {
                "createdAt": "2023-09-07T08:51:29.676Z",
                "updatedAt": "2023-09-07T08:51:29.676Z",
                "content": "<p>A Developer is creating a database solution using an Amazon ElastiCache caching layer. The solution must provide strong consistency to ensure that updates to product data are consistent between the backend database and the ElastiCache cache. Low latency performance is required for all items in the database.</p><p>Which cache writing policy will satisfy these requirements?</p>",
                "answerExplanation": "<p>The write-through strategy adds data or updates data in the cache whenever data is written to the database. The advantages of write-through are as follows:</p><p>- Data in the cache is never stale. Because the data in the cache is updated every time it's written to the database, the data in the cache is always current.</p><p>- Write penalty vs. read penalty.</p><p>Every write involves two trips:</p><p>&nbsp; 1. A write to the cache</p><p>&nbsp; 2. A write to the database</p><p>Which adds latency to the process. That said, end users are generally more tolerant of latency when updating data than when retrieving data. There is an inherent sense that updates are more work and thus take longer.</p><p><strong>CORRECT: </strong>\"Use a write-through caching strategy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a lazy-loading caching strategy\" is incorrect. <em>Lazy loading</em> is a caching strategy that loads data into the cache only when necessary. This will not ensure strong consistency between the database and the cache.</p><p><strong>INCORRECT:</strong> \"Add a short duration TTL value to each write\" is incorrect. A TTL specifies the number of seconds until the key expires. This will not ensure strong consistency between the database and the cache.</p><p><strong>INCORRECT:</strong> \"Invalidate the cache for each database write\" is incorrect. This will allow the cache to be updated when an item is next read but will not ensure the best performance for all items in the database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 3320,
                        "content": "<p>Add a short duration TTL value to each write.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3321,
                        "content": "<p>Invalidate the cache for each database write.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3322,
                        "content": "<p>Use a write-through caching strategy.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3323,
                        "content": "<p>Use a lazy-loading caching strategy.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 809,
            "attributes": {
                "createdAt": "2023-09-07T08:51:29.744Z",
                "updatedAt": "2023-09-07T08:51:29.744Z",
                "content": "<p>A company is releasing an updated version of its APIs for its new mobile application, which uses Amazon API Gateway. The developers aim to gradually and seamlessly roll out the new version of APIs.</p><p>What is the MOST straightforward method for them to introduce the new API version to a subset of users through API Gateway?</p>",
                "answerExplanation": "<p>The canary release deployment in API Gateway enables developers to roll out API changes gradually. By configuring the canarySettings, a percentage of the API traffic can be redirected to the new version, allowing for cautious and controlled rollout.</p><p><strong>CORRECT: </strong>\"Utilize the canary release deployment feature in API Gateway. Configure the canarySettings to redirect a portion of the API traffic\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an Amazon Route 53 failover routing policy to divert a certain percentage of traffic to the updated API version\" is incorrect.</p><p>Amazon Route 53 failover routing policy is used for routing internet traffic to a resource when the primary resource becomes unavailable. It is not a suitable solution for this scenario.</p><p><strong>INCORRECT:</strong> \"Deploy the new API in a separate VPC and use Amazon CloudFront to distribute the API traffic between the old and new versions\" is incorrect.</p><p>Deploying the new API in a separate VPC and using Amazon CloudFront for traffic distribution could work, but this approach is more complex and does not provide an out-of-the-box traffic control mechanism like canary releases.</p><p><strong>INCORRECT:</strong> \"Develop a custom Lambda function to control the API traffic distribution between the two API versions\" is incorrect.</p><p>Developing a custom Lambda function to control API traffic distribution can be overly complicated and might not provide the desired level of control, especially when compared to built-in solutions like canary releases.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 3324,
                        "content": "<p>Utilize the canary release deployment feature in API Gateway. Configure the canarySettings to redirect a portion of the API traffic.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3325,
                        "content": "<p>Deploy the new API in a separate VPC and use Amazon CloudFront to distribute the API traffic between the old and new versions.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3326,
                        "content": "<p>Develop a custom Lambda function to control the API traffic distribution between the two API versions.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3327,
                        "content": "<p>Use an Amazon Route 53 failover routing policy to divert a certain percentage of traffic to the updated API version.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 810,
            "attributes": {
                "createdAt": "2023-09-07T08:51:29.817Z",
                "updatedAt": "2023-09-07T08:51:29.817Z",
                "content": "<p>A company is deploying a new serverless application with an AWS Lambda function. A developer ran some test invocations using the AWS CLI. The function is invoking correctly and returning a success message, but not log data is being generated in Amazon CloudWatch Logs. The developer waited for 15 minutes but the log data is not showing up.</p><p>What is the most likely explanation for this issue?</p>",
                "answerExplanation": "<p>AWS Lambda automatically monitors Lambda functions on your behalf, reporting metrics through Amazon CloudWatch. To help you troubleshoot failures in a function, after you set up permissions, Lambda logs all requests handled by your function and automatically stores logs generated by your code through Amazon CloudWatch Logs.</p><p>You can insert logging statements into your code to help you validate that your code is working as expected. Lambda automatically integrates with CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function, which is named /aws/lambda/<em>&lt;function name&gt;</em>.</p><p>It can take 5-10 minutes for logs to show up after a function invocation. If your Lambda function code is executing, but you don't see any log data being generated after several minutes, this could mean that your execution role for the Lambda function didn't grant permissions to write log data to CloudWatch Logs.</p><p><br></p><p><strong>CORRECT: </strong>\"The function execution role does not have permission to write log data to CloudWatch Logs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The Lambda function does not have any explicit log statements for the log data to send it to CloudWatch Logs\" is incorrect.</p><p>You do need to have logging statements in your code to send meaningful data to CloudWatch Logs. However, the most likely cause of having nothing show up is that the permissions were not assigned.</p><p><strong>INCORRECT:</strong> \"The function configuration does not have CloudWatch Logs configured as a success destination\" is incorrect.</p><p>CloudWatch Logs is not configured as a destination in a Lambda function.</p><p><strong>INCORRECT:</strong> \"A log group and log stream has not been configured for the function in CloudWatch Logs\" is incorrect.</p><p>The log group and log stream are automatically created as long as permissions are assigned.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-monitoring.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-monitoring.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3328,
                        "content": "<p>The function execution role does not have permission to write log data to CloudWatch Logs.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3329,
                        "content": "<p>The Lambda function does not have any explicit log statements for the log data to send it to CloudWatch Logs.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3330,
                        "content": "<p>The function configuration does not have CloudWatch Logs configured as a success destination.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3331,
                        "content": "<p>A log group and log stream has not been configured for the function in CloudWatch Logs.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 811,
            "attributes": {
                "createdAt": "2023-09-07T08:51:29.887Z",
                "updatedAt": "2023-09-07T08:51:29.887Z",
                "content": "<p>A website is running on a single Amazon EC2 instance. A Developer wants to publish the website on the Internet and is creating an A record on Amazon Route 53 for the websiteâ€™s public DNS name.</p><p>What type of IP address MUST be assigned to the EC2 instance and used in the A record to ensure ongoing connectivity?</p>",
                "answerExplanation": "<p>In Amazon Route 53 when you create an A record you must supply an IP address for the resource to connect to. For a public hosted zone this must be a public IP address.</p><p>There are three types of IP address that can be assigned to an Amazon EC2 instance:</p><p>â€¢ Public â€“ public address that is assigned automatically to instances in public subnets and reassigned if instance is stopped/started.</p><p>â€¢ Private â€“ private address assigned automatically to all instances.</p><p>â€¢ Elastic IP â€“ public address that is static.</p><p>To ensure ongoing connectivity the Developer needs to use an Elastic IP address for the EC2 instance and DNS A record as this is the only type of static, public IP address you can assign to an Amazon EC2 instance.</p><p><strong>CORRECT: </strong>\"Elastic IP address\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Public IP address\" is incorrect as though this is a public IP address, it is not static and will change every time the EC2 instance restarts. Therefore, connectivity would be lost until you update the Route 53 A record.</p><p><strong>INCORRECT:</strong> \"Dynamic IP address\" is incorrect as a dynamic IP address is an IP address that will change over time. For this scenario a static, public address is required.</p><p><strong>INCORRECT:</strong> \"Private IP address\" is incorrect as a public IP address is required for the public DNS A record.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
                "options": [
                    {
                        "id": 3332,
                        "content": "<p>Dynamic IP address</p>",
                        "isValid": false
                    },
                    {
                        "id": 3333,
                        "content": "<p>Public IP address</p>",
                        "isValid": false
                    },
                    {
                        "id": 3334,
                        "content": "<p>Elastic IP address</p>",
                        "isValid": true
                    },
                    {
                        "id": 3335,
                        "content": "<p>Private IP address</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 812,
            "attributes": {
                "createdAt": "2023-09-07T08:51:29.972Z",
                "updatedAt": "2023-09-07T08:51:29.972Z",
                "content": "<p>A Developer is using AWS SAM to create a template for deploying a serverless application. The Developer plans deploy an AWS Lambda function and an Amazon DynamoDB table using the template.</p><p>Which resource types should the Developer specify? (Select TWO.)</p>",
                "answerExplanation": "<p>A <strong>serverless application</strong> is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda functionâ€”it can include additional resources such as APIs, databases, and event source mappings.</p><p>AWS SAM templates are an extension of AWS CloudFormation templates, with some additional components that make them easier to work with.</p><p>To create a Lambda function using an AWS SAM template the Developer can use the AWS::Serverless::Function resource type. The AWS::Serverless::Function resource type can be used to Create a Lambda function, IAM execution role, and event source mappings that trigger the function.</p><p>To create a DynamoDB table using an AWS SAM template the Developer can use the AWS::Serverless::SimpleTable resource type which creates a DynamoDB table with a single attribute primary key. It is useful when data only needs to be accessed via a primary key.</p><p><strong>CORRECT: </strong>\"<code>AWS::Serverless:Function</code>\" is a correct answer.</p><p><strong>CORRECT: </strong>\"<code>AWS::Serverless:SimpleTable</code>\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"<code>AWS::Serverless::Application</code>\" is incorrect as this embeds a serverless application from the <a href=\"https://serverlessrepo.aws.amazon.com/applications\">AWS Serverless Application Repository</a> or from an Amazon S3 bucket as a nested application.</p><p><strong>INCORRECT:</strong> \"<code>AWS::Serverless:LayerVersion</code>\" is incorrect as this creates a Lambda LayerVersion that contains library or runtime code needed by a Lambda Function.</p><p><strong>INCORRECT:</strong> \"<code>AWS::Serverless:API</code>\" is incorrect as this creates a collection of Amazon API Gateway resources and methods that can be invoked through HTTPS endpoints.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-sam/\">https://digitalcloud.training/aws-sam/</a></p>",
                "options": [
                    {
                        "id": 3336,
                        "content": "<p><code>AWS::Serverless:API</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3337,
                        "content": "<p><code>AWS::Serverless::Application</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3338,
                        "content": "<p><code>AWS::Serverless::SimpleTable</code> </p>",
                        "isValid": true
                    },
                    {
                        "id": 3339,
                        "content": "<p><code>AWS::Serverless:Function</code> </p>",
                        "isValid": true
                    },
                    {
                        "id": 3340,
                        "content": "<p><code>AWS::Serverless:LayerVersion </code> </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 813,
            "attributes": {
                "createdAt": "2023-09-07T08:51:30.049Z",
                "updatedAt": "2023-09-07T08:51:30.049Z",
                "content": "<p>An engineer is constructing a web-based application that uses Amazon DynamoDB for storing data. The data is distributed across two tables: 'authors' and 'books'. The 'authors' table uses 'authorName' as its partition key, while the 'books' table has 'bookTitle' as the partition key and 'authorName' as the sort key.</p><p>The application requires the ability to fetch multiple books and authors simultaneously in a single database operation for effective performance. The engineer is seeking a solution that maximizes application efficiency and reduces network traffic.</p><p>What strategy should the engineer employ to achieve these requirements?</p>",
                "answerExplanation": "<p>The BatchGetItem operation in DynamoDB permits the retrieval of multiple items from one or more tables in a single operation, thereby minimizing network traffic and improving overall application performance.</p><p><strong>CORRECT: </strong>\"Utilize the DynamoDB BatchGetItem operation to fetch multiple items from both tables in a single network round trip\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Execute individual GetItem operations for each book and author to be retrieved\" is incorrect.</p><p>Executing individual GetItem operations for each book and author would increase network traffic and slow down application performance.</p><p><strong>INCORRECT:</strong> \"Use a DynamoDB Scan operation to fetch the required items from both tables\" is incorrect.</p><p>A DynamoDB Scan operation retrieves every item in the table and then filters to provide the required result, making it inefficient in this context.</p><p><strong>INCORRECT:</strong> \"First query the 'books' table using 'bookTitle' as a key condition, then separately query the 'authors' table using 'authorName'\" is incorrect.</p><p>Querying the 'books' and 'authors' tables separately would increase the number of network operations and negatively affect the application's performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html\">https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3341,
                        "content": "<p>Use a DynamoDB Scan operation to fetch the required items from both tables.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3342,
                        "content": "<p>Utilize the DynamoDB BatchGetItem operation to fetch multiple items from both tables in a single network round trip.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3343,
                        "content": "<p>Execute individual GetItem operations for each book and author to be retrieved.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3344,
                        "content": "<p>First query the 'books' table using 'bookTitle' as a key condition, then separately query the 'authors' table using 'authorName'.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 814,
            "attributes": {
                "createdAt": "2023-09-07T08:51:30.132Z",
                "updatedAt": "2023-09-07T08:51:30.132Z",
                "content": "<p>An application uses an Auto Scaling group of Amazon EC2 instances, an Application Load Balancer (ALB), and an Amazon Simple Queue Service (SQS) queue. An Amazon CloudFront distribution caches content for global users. A Developer needs to add in-transit encryption to the data by configuring end-to-end SSL between the CloudFront Origin and the end users.</p><p>How can the Developer meet this requirement? (Select TWO.)</p>",
                "answerExplanation": "<p>For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so that connections are encrypted when CloudFront communicates with viewers. You also can configure CloudFront to use HTTPS to get objects from your origin, so that connections are encrypted when CloudFront communicates with your origin.</p><p>If you configure CloudFront to require HTTPS both to communicate with viewers and to communicate with your origin, here's what happens when CloudFront receives a request for an object:</p><p>1. A viewer submits an HTTPS request to CloudFront. There's some SSL/TLS negotiation here between the viewer and CloudFront. In the end, the viewer submits the request in an encrypted format.</p><p>2. If the object is in the CloudFront edge cache, CloudFront encrypts the response and returns it to the viewer, and the viewer decrypts it.</p><p>3. If the object is not in the CloudFront cache, CloudFront performs SSL/TLS negotiation with your origin and, when the negotiation is complete, forwards the request to your origin in an encrypted format.</p><p>4. Your origin decrypts the request, encrypts the requested object, and returns the object to CloudFront.</p><p>5. CloudFront decrypts the response, re-encrypts it, and forwards the object to the viewer. CloudFront also saves the object in the edge cache so that the object is available the next time it's requested.</p><p>6. The viewer decrypts the response.</p><p>To enable SSL between the origin and the distribution the Developer can configure the Origin Protocol Policy. Depending on the domain name used (CloudFront default or custom), the steps are different. To enable SSL between the end-user and CloudFront the Viewer Protocol Policy should be configured.</p><p><strong>CORRECT: </strong>\"Configure the Origin Protocol Policy\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Configure the Viewer Protocol Policy\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Origin Access Identity (OAI)\" is incorrect as this is a special user used for securing objects in Amazon S3 origins.</p><p><strong>INCORRECT:</strong> \"Add a certificate to the Auto Scaling Group\" is incorrect as you do not add certificates to an ASG. The certificate should be located on the ALB listener in this scenario.</p><p><strong>INCORRECT:</strong> \"Create an encrypted distribution\" is incorrect as thereâ€™s no such thing as an encrypted distribution</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 3345,
                        "content": "<p>Configure the Origin Protocol Policy</p>",
                        "isValid": true
                    },
                    {
                        "id": 3346,
                        "content": "<p>Create an encrypted distribution</p>",
                        "isValid": false
                    },
                    {
                        "id": 3347,
                        "content": "<p>Configure the Viewer Protocol Policy</p>",
                        "isValid": true
                    },
                    {
                        "id": 3348,
                        "content": "<p>Create an Origin Access Identity (OAI)</p>",
                        "isValid": false
                    },
                    {
                        "id": 3349,
                        "content": "<p>Add a certificate to the Auto Scaling Group</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 815,
            "attributes": {
                "createdAt": "2023-09-07T08:51:30.201Z",
                "updatedAt": "2023-09-07T08:51:30.201Z",
                "content": "<p>A developer plan to deploy an application on Amazon ECS that uses the AWS SDK to make API calls to Amazon DynamoDB. In the development environment the application was configured with access keys. The application is now ready for deployment to a production cluster.</p><p>How should the developer configure the application to securely authenticate to AWS services?</p>",
                "answerExplanation": "<p>Your Amazon ECS tasks can have an IAM role associated with them. The permissions granted in the IAM role are assumed by the containers running in the task. The following explain the benefits of using IAM roles with your tasks.</p><p> â€¢ <strong>Credential Isolation:</strong> A container can only retrieve credentials for the IAM role that is defined in the task definition to which it belongs; a container never has access to credentials that are intended for another container that belongs to another task.</p><p><strong> </strong>â€¢ <strong>Authorization:</strong> Unauthorized containers cannot access IAM role credentials defined for other tasks.</p><p><strong> </strong>â€¢ <strong>Auditability:</strong> Access and event logging is available through CloudTrail to ensure retrospective auditing. Task credentials have a context of taskArn that is attached to the session, so CloudTrail logs show which task is using which role.</p><p><strong>CORRECT: </strong>\"Configure an ECS task IAM role for the application to use\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add the necessary AWS service permissions to an ECS instance profile\" is incorrect.</p><p>The privileges assigned to instance profiles on the Amazon ECS instances are available to all tasks running on the instance. This is not secure and AWS recommend that you limit the permissions you assign to the instance profile.</p><p><strong>INCORRECT:</strong> \"Configure the credentials file with a new access key/secret access key\" is incorrect.</p><p>Access keys are not a secure way of providing authentication. It is better to use roles that obtain temporary security permissions using the AWS STS service.</p><p><strong>INCORRECT:</strong> \"Add environment variables pointing to new access key credentials\" is incorrect.</p><p>As above, access keys should not be used, IAM roles should be used instead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 3350,
                        "content": "<p>Add the necessary AWS service permissions to an ECS instance profile.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3351,
                        "content": "<p>Configure the credentials file with a new access key/secret access key.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3352,
                        "content": "<p>Configure an ECS task IAM role for the application to use.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3353,
                        "content": "<p>Add environment variables pointing to new access key credentials.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 816,
            "attributes": {
                "createdAt": "2023-09-07T08:51:30.279Z",
                "updatedAt": "2023-09-07T08:51:30.279Z",
                "content": "<p>Messages produced by an application must be pushed to multiple Amazon SQS queues. What is the BEST solution for this requirement?</p>",
                "answerExplanation": "<p><a href=\"https://aws.amazon.com/sns/\">Amazon SNS</a> works closely with Amazon Simple Queue Service (Amazon SQS). Both services provide different benefits for developers. Amazon SNS allows applications to send time-critical messages to multiple subscribers through a â€œpushâ€ mechanism, eliminating the need to periodically check or â€œpollâ€ for updates.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_00-34-37-1c1fb8b0a53bea8c2c644e7e8b79f1ab.png\"></p><p>When you subscribe an Amazon SQS queue to an Amazon SNS topic, you can publish a message to the topic and Amazon SNS sends an Amazon SQS message to the subscribed queue. The Amazon SQS message contains the subject and message that were published to the topic along with metadata about the message in a JSON document.</p><p><strong>CORRECT: </strong>\"Publish the messages to an Amazon SNS topic and subscribe each SQS queue to the topic\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Publish the messages to an Amazon SQS queue and configure an AWS Lambda function to duplicate the message into multiple queues\" is incorrect as this seems like an inefficient solution. By using SNS we can eliminate the initial queue and Lambda function.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SWF workflow that receives the messages and pushes them to multiple SQS queues\" is incorrect as this is not a workable solution. Amazon SWF is not suitable for pushing messages to SQS queues.</p><p><strong>INCORRECT:</strong> Create and AWS Step Functions state machine that uses multiple Lambda functions to process and push the messages into multiple SQS queues\"\" is incorrect as this is an inefficient solution and there is not mention on how the functions will be invoked with the message data</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3354,
                        "content": "<p>Publish the messages to an Amazon SQS queue and configure an AWS Lambda function to duplicate the message into multiple queues</p>",
                        "isValid": false
                    },
                    {
                        "id": 3355,
                        "content": "<p>Publish the messages to an Amazon SNS topic and subscribe each SQS queue to the topic</p>",
                        "isValid": true
                    },
                    {
                        "id": 3356,
                        "content": "<p>Create an Amazon SWF workflow that receives the messages and pushes them to multiple SQS queues</p>",
                        "isValid": false
                    },
                    {
                        "id": 3357,
                        "content": "<p>Create and AWS Step Functions state machine that uses multiple Lambda functions to process and push the messages into multiple SQS queues</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 817,
            "attributes": {
                "createdAt": "2023-09-07T08:51:30.349Z",
                "updatedAt": "2023-09-07T08:51:30.349Z",
                "content": "<p>A gaming company is building an application to track the scores for their games using an Amazon DynamoDB table. Each item in the table is identified by a partition key<br>(user_id) and a sort key (game_name). The table also includes the attribute â€œTopScoreâ€. The table design is shown below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_12-46-57-e4f332660cb93be84d6e7c81efbf4f49.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--2kvh_\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_12-46-57-e4f332660cb93be84d6e7c81efbf4f49.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-large ud-btn-link ud-heading-md open-full-size-image--backdrop--20cbM\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span></p><p>A Developer has been asked to write a leaderboard application to display the highest achieved scores for each game (game_name), based on the score identified in the â€œTopScoreâ€ attribute.</p><p>What process will allow the Developer to extract results MOST efficiently from the DynamoDB table?</p>",
                "answerExplanation": "<p>In an Amazon DynamoDB table, the primary key that uniquely identifies each item in the table can be composed not only of a partition key, but also of a sort key.</p><p>Well-designed sort keys have two key benefits:</p><p>- They gather related information together in one place where it can be queried efficiently. Careful design of the sort key lets you retrieve commonly needed groups of related items using range queries with operators such as begins_with, between, &gt;, &lt;, and so on.</p><p>- Composite sort keys let you define hierarchical (one-to-many) relationships in your data that you can query at any level of the hierarchy.</p><p>To speed up queries on non-key attributes, you can create a global secondary index. A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The index key does not need to have any of the key attributes from the table. It doesn't even need to have the same key schema as a table.</p><p>For this scenario we need to identify the top achieved score for each game. The most efficient way to do this is to create a global secondary index using â€œgame_nameâ€ as the partition key and â€œTopScoreâ€ as the sort key. We can then efficiently query the global secondary index to find the top achieved score for each game.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_12-48-28-00f8197d84eb7e7e245b3858da7489b0.jpg\"></p><p><strong>CORRECT: </strong>\"Create a global secondary index with a partition key of â€œgame_nameâ€ and a sort key of â€œTopScoreâ€ and get the results based on the score attribute\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a local secondary index with a partition key of â€œgame_nameâ€ and a sort key of â€œTopScoreâ€ and get the results based on the score attribute\" is incorrect. With a local secondary index you can have a different sort key but the partition key is the same.</p><p><strong>INCORRECT:</strong> \"Use a DynamoDB scan operation to retrieve the scores for â€œgame_nameâ€ using the â€œTopScoreâ€ attribute, and order the results based on the score attribute\" is incorrect. This would be inefficient as it scans the whole table. First, we should create a global secondary index, and then use a query to efficiently retrieve the data.</p><p><strong>INCORRECT:</strong> \"Create a global secondary index with a partition key of â€œuser_idâ€ and a sort key of â€œgame_nameâ€ and get the results based on the score attribute\" is incorrect as with a global secondary index you have a different partition key and sort key. Also, we donâ€™t need â€œuser_idâ€, we need â€œgame_nameâ€ and â€œTopScoreâ€.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-sort-keys.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-sort-keys.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3358,
                        "content": "<p>Create a global secondary index with a partition key of â€œuser_idâ€ and a sort key of â€œgame_nameâ€ and get the results based on the â€œTopScoreâ€ attribute</p>",
                        "isValid": false
                    },
                    {
                        "id": 3359,
                        "content": "<p>Create a local secondary index with a partition key of â€œgame_nameâ€ and a sort key of â€œTopScoreâ€ and get the results based on the score attribute</p>",
                        "isValid": false
                    },
                    {
                        "id": 3360,
                        "content": "<p>Use a DynamoDB scan operation to retrieve the scores for â€œgame_nameâ€ using the â€œTopScoreâ€ attribute, and order the results based on the score attribute</p>",
                        "isValid": false
                    },
                    {
                        "id": 3361,
                        "content": "<p>Create a global secondary index with a partition key of â€œgame_nameâ€ and a sort key of â€œTopScoreâ€ and get the results based on the score attribute</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 818,
            "attributes": {
                "createdAt": "2023-09-07T08:51:30.427Z",
                "updatedAt": "2023-09-07T08:51:30.427Z",
                "content": "<p>A Development team are creating a new REST API that uses Amazon API Gateway and AWS Lambda. To support testing there need to be different versions of the service. What is the BEST way to provide multiple versions of the REST API?</p>",
                "answerExplanation": "<p>A stage is a named reference to a deployment, which is a snapshot of the API. You use a Stage to manage and optimize a particular deployment. For example, you can set up stage settings to enable caching, customize request throttling, configure logging, define stage variables or attach a canary release for testing. APIs are deployed to stages:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_14-09-05-3b0cb95d380260c409fecbb328aa738e.jpg\"></p><p>Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates.</p><p>With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints. For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, <a href=\"http://example.com\">http://example.com</a>).</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_14-09-47-7ba07e9cb83954ecc11e5a91ba4d23b9.jpg\"></p><p>In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example, beta.example.com). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API.</p><p>Therefore, for this scenario the Developers can deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context such as connections to different backend services.</p><p><strong>CORRECT: </strong>\"Deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an API Gateway resource policy to isolate versions and provide context to the Lambda functions\" is incorrect. API Gateway <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html\">resource policies</a> are JSON policy documents that you attach to an API to control whether a specified principal (typically, an IAM user or role) can invoke the API.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda authorizer to route API clients to the correct API version\" is incorrect. A <em>Lambda authorizer</em> (formerly known as a <em>custom authorizer</em>) is an API Gateway feature that uses a Lambda function to control access to your API. This is not used for routing API clients to different versions.</p><p><strong>INCORRECT:</strong> \"Deploy an HTTP Proxy integration and configure the proxy with API versions\" is incorrect. The HTTP proxy integration allows a client to access the backend HTTP endpoints with a streamlined integration setup on a single API method. This is not used for providing multiple versions of the API, use stages and stage variables instead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-stages.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-stages.html</a></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 3362,
                        "content": "<p>Create an API Gateway resource policy to isolate versions and provide context to the Lambda functions</p>",
                        "isValid": false
                    },
                    {
                        "id": 3363,
                        "content": "<p>Deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context</p>",
                        "isValid": true
                    },
                    {
                        "id": 3364,
                        "content": "<p>Create an AWS Lambda authorizer to route API clients to the correct API version</p>",
                        "isValid": false
                    },
                    {
                        "id": 3365,
                        "content": "<p>Deploy an HTTP Proxy integration and configure the proxy with API versions</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 819,
            "attributes": {
                "createdAt": "2023-09-07T08:51:30.504Z",
                "updatedAt": "2023-09-07T08:51:30.504Z",
                "content": "<p>The source code for an application is stored in a file named index.js that is in a folder along with a template file that includes the following code:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"typ\">AWSTemplateFormatVersion</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">'2010-09-09'</span></li><li class=\"L1\"><span class=\"typ\">Transform</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">'AWS::Serverless-2016-10-31'</span></li><li class=\"L2\"><span class=\"typ\">Resources</span><span class=\"pun\">:</span></li><li class=\"L3\"><span class=\"typ\">LambdaFunctionWithAPI</span><span class=\"pun\">:</span></li><li class=\"L4\"><span class=\"typ\">Type</span><span class=\"pun\">:</span><span class=\"pln\"> AWS</span><span class=\"pun\">::</span><span class=\"typ\">Serverless</span><span class=\"pun\">::</span><span class=\"typ\">Function</span></li><li class=\"L5\"><span class=\"typ\">Properties</span><span class=\"pun\">:</span></li><li class=\"L6\"><span class=\"typ\">Handler</span><span class=\"pun\">:</span><span class=\"pln\"> index</span><span class=\"pun\">.</span><span class=\"pln\">handler</span></li><li class=\"L7\"><span class=\"typ\">Runtime</span><span class=\"pun\">:</span><span class=\"pln\"> nodejs12</span><span class=\"pun\">.</span><span class=\"pln\">x</span></li></ol></pre></div></div><p>What does a Developer need to do to prepare the template so it can be deployed using an AWS CLI command?</p>",
                "answerExplanation": "<p>The template shown is an AWS SAM template for deploying a serverless application. This can be identified by the template header: <em>Transform: 'AWS::Serverless-2016-10-31'</em></p><p>The Developer will need to package and then deploy the template. To do this the source code must be available in the same directory or referenced using the â€œcodeuriâ€ parameter. Then, the Developer can use the â€œaws cloudformation packageâ€ or â€œsam packageâ€ commands to prepare the local artifacts (local paths) that your AWS CloudFormation template references.</p><p>The command uploads local artifacts, such as source code for an AWS Lambda function or a Swagger file for an AWS API Gateway REST API, to an S3 bucket. The command returns a copy of your template, replacing references to local artifacts with the S3 location where the command uploaded the artifacts.</p><p>Once that is complete the template can be deployed using the â€œaws cloudformation deployâ€ or â€œsam deployâ€ commands. Therefore, the next step in this scenario is for the Developer to run the â€œaws cloudformationâ€ package command to upload the source code to an Amazon S3 bucket and produce a modified CloudFormation template. An example of this command is provided below:</p><p><em>aws cloudformation package --template-file /</em><code><em>path_to_template/template.json</em></code><em> --s3-bucket </em><code><em>bucket-name</em></code><em> --output-template-file </em><code><em>packaged-template.json</em></code></p><p><strong>CORRECT: </strong>\"Run the <code>aws cloudformation package</code> command to upload the source code to an Amazon S3 bucket and produce a modified CloudFormation template\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Run the aws cloudformation compile command to base64 encode and embed the source file into a modified CloudFormation template\" is incorrect as the Developer should run the â€œaws cloudformation packageâ€ command.</p><p><strong>INCORRECT:</strong> \"Run the <code>aws lambda zip</code> command to package the source file together with the CloudFormation template and deploy the resulting zip archive\" is incorrect as the Developer should run the â€œaws cloudformation packageâ€ command which will automatically copy the relevant files to Amazon S3.</p><p><strong>INCORRECT:</strong> \"Run the <code>aws serverless create-package</code> command to embed the source file directly into the existing CloudFormation template\" is incorrect as the Developer has the choice to run either â€œaws cloudformation packageâ€ or â€œsam packageâ€, but not â€œaws serverless create-packageâ€.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html\">https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-sam/\">https://digitalcloud.training/aws-sam/</a></p>",
                "options": [
                    {
                        "id": 3366,
                        "content": "<p>Run the <code>aws serverless create-package</code> command to embed the source file directly into the existing CloudFormation template</p>",
                        "isValid": false
                    },
                    {
                        "id": 3367,
                        "content": "<p>Run the <code>aws lambda zip</code> command to package the source file together with the CloudFormation template and deploy the resulting zip archive</p>",
                        "isValid": false
                    },
                    {
                        "id": 3368,
                        "content": "<p>Run the <code>aws cloudformation compile</code> command to base64 encode and embed the source file into a modified CloudFormation template</p>",
                        "isValid": false
                    },
                    {
                        "id": 3369,
                        "content": "<p>Run the <code>aws cloudformation package</code> command to upload the source code to an Amazon S3 bucket and produce a modified CloudFormation template</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 820,
            "attributes": {
                "createdAt": "2023-09-07T08:51:30.579Z",
                "updatedAt": "2023-09-07T08:51:30.579Z",
                "content": "<p>A developer must deploy an update to Amazon ECS using AWS CodeDeploy. The deployment should expose 10% of live traffic to the new version. Then after a period of time, route all remaining traffic to the new version.</p><p>Which ECS deployment should the company use to meet these requirements?</p>",
                "answerExplanation": "<p>The <em>blue/green</em> deployment type uses the blue/green deployment model controlled by CodeDeploy. This deployment type enables you to verify a new deployment of a service before sending production traffic to it.</p><p>There are three ways traï¬ƒc can shift during a blue/green deployment:</p><p><strong> </strong>â€¢ <strong>Canary</strong> â€” Traï¬ƒc is shifted in two increments. You can choose from predeï¬ned canary options that specify the percentage of traï¬ƒc shifted to your updated task set in the ï¬rst increment and the interval, in minutes, before the remaining traï¬ƒc is shifted in the second increment.</p><p><strong> </strong>â€¢ <strong>Linear</strong> â€” Traï¬ƒc is shifted in equal increments with an equal number of minutes between each increment. You can choose from predeï¬ned linear options that specify the percentage of traï¬ƒc shifted in each increment and the number of minutes between each increment.</p><p><strong> </strong>â€¢ <strong>All-at-once</strong> â€” All traï¬ƒc is shifted from the original task set to the updated task set all at once.</p><p>The best choice for this use case would be to use the canary traffic shifting strategy. You can see the predefined canary options in the table below:</p><p><br></p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-30_07-15-25-8b13637a401122d36d55415ef8ebe563.JPG\"></p><p><strong>CORRECT: </strong>\"Blue/green with canary\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Blue/green with linear\" is incorrect.</p><p>With this option traffic is shifted in equal increments with an equal amount of time between increments.</p><p><strong>INCORRECT:</strong> \"Blue/green with all at once\" is incorrect.</p><p>With this option all traffic is shifted at once.</p><p><strong>INCORRECT:</strong> \"Rolling update\" is incorrect.</p><p>This is a native ECS deployment model. It does not deploy in two increments with 10% first.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3370,
                        "content": "<p>Blue/green with canary</p>",
                        "isValid": true
                    },
                    {
                        "id": 3371,
                        "content": "<p>Blue/green with linear</p>",
                        "isValid": false
                    },
                    {
                        "id": 3372,
                        "content": "<p>Rolling update</p>",
                        "isValid": false
                    },
                    {
                        "id": 3373,
                        "content": "<p>Blue/green with all at once</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 821,
            "attributes": {
                "createdAt": "2023-09-07T08:51:30.648Z",
                "updatedAt": "2023-09-07T08:51:30.648Z",
                "content": "<p>A Developer wants to debug an application by searching and filtering log data. The application logs are stored in Amazon CloudWatch Logs. The Developer creates a new metric filter to count exceptions in the application logs. However, no results are returned from the logs.</p><p>What is the reason that no filtered results are being returned?</p>",
                "answerExplanation": "<p>After the CloudWatch Logs agent begins publishing log data to Amazon CloudWatch, you can begin searching and filtering the log data by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs.</p><p>CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. You can use any type of CloudWatch statistic, including percentile statistics, when viewing these metrics or setting alarms.</p><p>Filters do not retroactively filter data. Filters only publish the metric data points for events that happen after the filter was created. Filtered results return the first 50 lines, which will not be displayed if the timestamp on the filtered results is earlier than the metric creation time.</p><p>Therefore, the filtered results are not being returned as CloudWatch Logs only publishes metric data for events that happen after the filter is created.</p><p><strong>CORRECT: </strong>\"CloudWatch Logs only publishes metric data for events that happen after the filter is created\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"A setup of the Amazon CloudWatch interface VPC endpoint is required for filtering the CloudWatch Logs in the VPC\" is incorrect as a VPC endpoint is not required.</p><p><strong>INCORRECT:</strong> \"The log group for CloudWatch Logs should be first streamed to Amazon Elasticsearch Service before filtering returns the results\" is incorrect as you do not need to stream the results to Elasticsearch.</p><p><strong>INCORRECT:</strong> \"Metric data points to logs groups can be filtered only after they are exported to an Amazon S3 bucket\" is incorrect as it is not necessary to export the logs to an S3 bucket.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3374,
                        "content": "<p>A setup of the Amazon CloudWatch interface VPC endpoint is required for filtering the CloudWatch Logs in the VPC</p>",
                        "isValid": false
                    },
                    {
                        "id": 3375,
                        "content": "<p>CloudWatch Logs only publishes metric data for events that happen after the filter is created</p>",
                        "isValid": true
                    },
                    {
                        "id": 3376,
                        "content": "<p>The log group for CloudWatch Logs should be first streamed to Amazon Elasticsearch Service before filtering returns the results</p>",
                        "isValid": false
                    },
                    {
                        "id": 3377,
                        "content": "<p>Metric data points to logs groups can be filtered only after they are exported to an Amazon S3 bucket</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 822,
            "attributes": {
                "createdAt": "2023-09-07T08:51:30.722Z",
                "updatedAt": "2023-09-07T08:51:30.722Z",
                "content": "<p>A company wants to implement authentication for its new REST service using Amazon API Gateway. To authenticate the calls, each request must include HTTP headers with a client ID and user ID. These credentials must be compared to authentication data in an Amazon DynamoDB table.</p><p>What MUST the company do to implement this authentication in API Gateway?</p>",
                "answerExplanation": "<p>A <em>Lambda authorizer</em> (formerly known as a <em>custom authorizer</em>) is an API Gateway feature that uses a Lambda function to control access to your API.</p><p>A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity. </p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_00-46-31-e49f86de0e62d216c4bb5bdaffaae5ed.png\"></p><p>When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output.</p><p>There are two types of Lambda authorizers:</p><p>â€¢ A <em>token-based</em> Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.</p><p>â€¢ A <em>request parameter-based</em> Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-mapping-template-reference.html#stagevariables-template-reference\">stageVariables</a>, and <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-mapping-template-reference.html#context-variable-reference\">$context</a> variables.</p><p>â€¢ For WebSocket APIs, only request parameter-based authorizers are supported.</p><p>In this scenario, the authentication is using headers in the request and therefore the request parameter-based Lambda authorizer should be used.</p><p><strong>CORRECT: </strong>\"Implement an AWS Lambda authorizer that references the DynamoDB authentication table\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a model that requires the credentials, then grant API Gateway access to the authentication table\" is incorrect as a model defines the structure of the incoming payload using the JSON Schema.</p><p><strong>INCORRECT:</strong> \"Modify the integration requests to require the credentials, then grant API Gateway access to the authentication table\" is incorrect as API Gateway will not authorize directly using the table information, an authorizer should be used.</p><p><strong>INCORRECT:</strong> \"Implement an Amazon Cognito authorizer that references the DynamoDB authentication table\" is incorrect as a Lambda authorizer should be used in this example as the authentication data is being passed in request headers.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3378,
                        "content": "<p>Implement an AWS Lambda authorizer that references the DynamoDB authentication table</p>",
                        "isValid": true
                    },
                    {
                        "id": 3379,
                        "content": "<p>Modify the integration requests to require the credentials, then grant API Gateway access to the authentication table</p>",
                        "isValid": false
                    },
                    {
                        "id": 3380,
                        "content": "<p>Create a model that requires the credentials, then grant API Gateway access to the authentication table</p>",
                        "isValid": false
                    },
                    {
                        "id": 3381,
                        "content": "<p>Implement an Amazon Cognito authorizer that references the DynamoDB authentication table</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 823,
            "attributes": {
                "createdAt": "2023-09-07T08:51:30.799Z",
                "updatedAt": "2023-09-07T08:51:30.799Z",
                "content": "<p>A development team manage a high-traffic e-Commerce site with dynamic pricing that is updated in real-time. There have been incidents where multiple updates occur simultaneously and cause an original editorâ€™s updates to be overwritten. How can the developers ensure that overwriting does not occur?</p>",
                "answerExplanation": "<p>By default, the DynamoDB write operations (PutItem, UpdateItem, DeleteItem) are <em>unconditional</em>: Each operation overwrites an existing item that has the specified primary key.</p><p>DynamoDB optionally supports conditional writes for these operations. A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in many situations. For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value.</p><p>Conditional writes can be <strong><em>idempotent</em></strong> if the conditional check is on the same attribute that is being updated. This means that DynamoDB performs a given write request only if certain attribute values in the item match what you expect them to be at the time of the request.</p><p>For example, suppose that you issue an UpdateItem request to increase the Price of an item by 3, but only if the Price is currently 20. After you send the request, but before you get the results back, a network error occurs, and you don't know whether the request was successful. Because this conditional write is idempotent, you can retry the same UpdateItem request, and DynamoDB updates the item only if the Price is currently 20.</p><p>The following example shows how to use the condition-expression parameter to achieve a conditional write with idempotence:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pln\">aws dynamodb update</span><span class=\"pun\">-</span><span class=\"pln\">item \\</span></li><li class=\"L1\"><span class=\"pun\">--</span><span class=\"pln\">table</span><span class=\"pun\">-</span><span class=\"pln\">name </span><span class=\"typ\">ProductCatalog</span><span class=\"pln\"> \\</span></li><li class=\"L2\"><span class=\"pun\">--</span><span class=\"pln\">key </span><span class=\"str\">'{\"Id\":{\"N\":\"1\"}}'</span><span class=\"pln\"> \\</span></li><li class=\"L3\"><span class=\"pun\">--</span><span class=\"pln\">update</span><span class=\"pun\">-</span><span class=\"pln\">expression </span><span class=\"str\">\"SET Price = :newval\"</span><span class=\"pln\"> \\</span></li><li class=\"L4\"><span class=\"pun\">--</span><span class=\"pln\">condition</span><span class=\"pun\">-</span><span class=\"pln\">expression </span><span class=\"str\">\"Price = :currval\"</span><span class=\"pln\"> \\</span></li><li class=\"L5\"><span class=\"pun\">--</span><span class=\"pln\">expression</span><span class=\"pun\">-</span><span class=\"pln\">attribute</span><span class=\"pun\">-</span><span class=\"pln\">values file</span><span class=\"pun\">:</span><span class=\"com\">//expression-attribute-values.json</span></li></ol></pre></div></div><p>For this scenario, conditional writes with idempotence will mean that each writer can check the current price and update the price only if the price matches that price. If the price is updated by another writer before the write is made, it will fail as the item price has changed and will not reflect the expected price.</p><p><strong>CORRECT: </strong>\"Use conditional writes\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use concurrent writes\" is incorrect as writing concurrently to the same items is exactly what we want to avoid.</p><p><strong>INCORRECT:</strong> \"Use atomic counters\" is incorrect. An atomic counter is a numeric attribute that is incremented, unconditionally, without interfering with other write requests. This is used for cases such as tracking visitors to a website. This does not prevent recent updated from being overwritten.</p><p><strong>INCORRECT:</strong> \"Use batch operations\" is incorrect. Batch operations can reduce the number of network round trips from your application to DynamoDB. However, this does not solve the problem of preventing recent updates from being overwritten.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ConditionExpressions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ConditionExpressions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3382,
                        "content": "<p>Use batch operations</p>",
                        "isValid": false
                    },
                    {
                        "id": 3383,
                        "content": "<p>Use concurrent writes</p>",
                        "isValid": false
                    },
                    {
                        "id": 3384,
                        "content": "<p>Use conditional writes</p>",
                        "isValid": true
                    },
                    {
                        "id": 3385,
                        "content": "<p>Use atomic counters</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 824,
            "attributes": {
                "createdAt": "2023-09-07T08:51:30.868Z",
                "updatedAt": "2023-09-07T08:51:30.868Z",
                "content": "<p>An application developer is crafting a new software product. To streamline the registration process, they want new users to be able to set up their accounts using their existing social media profiles.</p><p>Which AWS service or feature would be the most appropriate for achieving this goal?</p>",
                "answerExplanation": "<p>Amazon Cognito User Pools provide a secure and scalable directory to manage users. User Pools also support the ability for users to sign in through third-party identity providers, like social media platforms, which makes it the right choice for this requirement.</p><p><strong>CORRECT: </strong>\"Amazon Cognito User Pools\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS Security Token Service\" is incorrect.</p><p>AWS Security Token Service (STS) is primarily used to grant temporary, limited-privilege credentials for IAM users or for users that you authenticate (federated users), which is not related to the registration of application users via social media.</p><p><strong>INCORRECT:</strong> \"AWS Identity and Access Management (IAM)\" is incorrect.</p><p>AWS IAM manages access to AWS services and resources securely. While IAM can federate with external identity providers, it is not designed to manage application user registration and social sign-in functionality directly.</p><p><strong>INCORRECT:</strong> \"AWS Managed Microsoft AD\" is incorrect.</p><p>AWS Managed Microsoft AD is a service that is used to enable directory-aware workloads and AWS resources to use managed Active Directory in AWS. It's used for traditional enterprise applications and does not directly support social media-based user registration.</p><p><strong>References:</strong></p><p><a href=\"https://repost.aws/knowledge-center/cognito-user-pools-identity-pools\">https://repost.aws/knowledge-center/cognito-user-pools-identity-pools</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 3386,
                        "content": "<p>AWS Managed Microsoft AD</p>",
                        "isValid": false
                    },
                    {
                        "id": 3387,
                        "content": "<p>AWS Identity and Access Management (IAM)</p>",
                        "isValid": false
                    },
                    {
                        "id": 3388,
                        "content": "<p>Amazon Cognito User Pools</p>",
                        "isValid": true
                    },
                    {
                        "id": 3389,
                        "content": "<p>AWS Security Token Service</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 825,
            "attributes": {
                "createdAt": "2023-09-07T08:51:30.947Z",
                "updatedAt": "2023-09-07T08:51:30.947Z",
                "content": "<p>A company is running a web application on Amazon EC2 behind an Elastic Load Balancer (ELB). The company is concerned about the security of the web application and would like to secure the application with SSL certificates. The solution should not have any performance impact on the EC2 instances.</p><p>What steps should be taken to secure the web application? (Select TWO.)</p>",
                "answerExplanation": "<p>The requirements clearly state that we cannot impact the performance of the EC2 instances at all. Therefore, we will not be able to add certificates to the EC2 instances as that would place a burden on the CPU when encrypting and decrypting data.</p><p>We are therefore left with configuring SSL on the Elastic Load Balancer itself. For this we need to add an SSL certificate to the ELB and then configure the ELB for SSL termination.</p><p>You can create an HTTPS listener, which uses encrypted connections (also known as <em>SSL offload</em>). This feature enables traffic encryption between your load balancer and the clients that initiate SSL or TLS sessions.</p><p>To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. The load balancer uses a server certificate to terminate the front-end connection and then decrypt requests from clients before sending them to the targets.</p><p>This is the most secure solution we can created without adding any performance impact to the EC2 instances.</p><p><strong>CORRECT: </strong>\"Add an SSL certificate to the Elastic Load Balancer\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Configure the Elastic Load Balancer for SSL termination\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Configure the Elastic Load Balancer with SSL passthrough\" is incorrect as this would be used to forward encrypted packets directly to the EC2 instance for termination but we do not want to add SSL certificates to the EC2 instances due to the extra processing required.</p><p><strong>INCORRECT:</strong> \"Install SSL certificates on the EC2 instances\" is incorrect as we do not want to add SSL certificates to the EC2 instances due to the extra processing required.</p><p><strong>INCORRECT:</strong> \"Configure Server-Side Encryption with KMS managed keys\" is incorrect as this applies to Amazon S3, not ELB.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 3390,
                        "content": "<p>Add an SSL certificate to the Elastic Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 3391,
                        "content": "<p>Install SSL certificates on the EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 3392,
                        "content": "<p>Configure the Elastic Load Balancer for SSL termination</p>",
                        "isValid": true
                    },
                    {
                        "id": 3393,
                        "content": "<p>Configure the Elastic Load Balancer with SSL passthrough</p>",
                        "isValid": false
                    },
                    {
                        "id": 3394,
                        "content": "<p>Configure Server-Side Encryption with KMS managed keys</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 826,
            "attributes": {
                "createdAt": "2023-09-07T08:51:31.046Z",
                "updatedAt": "2023-09-07T08:51:31.046Z",
                "content": "<p>An application is running on an Amazon EC2 Linux instance. The instance needs to make AWS API calls to several AWS services. What is the MOST secure way to provide access to the AWS services with MINIMAL management overhead?</p>",
                "answerExplanation": "<p>An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. Using an instance profile you can attach an IAM Role to an EC2 instance that the instance can then assume in order to gain access to AWS services.</p><p><strong>CORRECT: </strong>\"Use EC2 instance profiles\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS KMS to store and retrieve credentials\" is incorrect as KMS is used to manage encryption keys.</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS CloudHSM\" is incorrect as CloudHSM is also used to manage encryption keys. It is similar to KMS but uses a dedicated hardware device that is not multi-tenant.</p><p><strong>INCORRECT:</strong> \"Store the credentials in the ~/.aws/credentials file\" is incorrect as this is not the most secure option. The credentials file is associated with the AWS CLI and used for passing credentials in the form of an access key ID and secret access key when making programmatic requests from the command line.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 3395,
                        "content": "<p>Store the credentials in the ~/.aws/credentials file</p>",
                        "isValid": false
                    },
                    {
                        "id": 3396,
                        "content": "<p>Use AWS KMS to store and retrieve credentials</p>",
                        "isValid": false
                    },
                    {
                        "id": 3397,
                        "content": "<p>Store the credentials in AWS CloudHSM</p>",
                        "isValid": false
                    },
                    {
                        "id": 3398,
                        "content": "<p>Use EC2 instance profiles</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 827,
            "attributes": {
                "createdAt": "2023-09-07T08:51:31.138Z",
                "updatedAt": "2023-09-07T08:51:31.138Z",
                "content": "<p>An application will use AWS Lambda and an Amazon RDS database. The Developer needs to secure the database connection string and enable automatic rotation every 30 days. What is the SIMPLEST way to achieve this requirement?</p>",
                "answerExplanation": "<p>AWS Secrets Manager encrypts secrets at rest using encryption keys that you own and store in AWS Key Management Service (KMS). When you retrieve a secret, Secrets Manager decrypts the secret and transmits it securely over TLS to your local environment.</p><p>With AWS Secrets Manager, you can rotate secrets on a schedule or on demand by using the Secrets Manager console, AWS SDK, or AWS CLI.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_00-40-22-078df4f6ca2e21741c5dfb063b474201.png\"><p>For example, to rotate a database password, you provide the database type, rotation frequency, and master database credentials when storing the password in Secrets Manager. Secrets Manager natively supports rotating credentials for databases hosted on Amazon RDS and Amazon DocumentDB and clusters hosted on Amazon Redshift.</p><p><strong>CORRECT: </strong>\"Store a secret in AWS Secrets Manager and enable automatic rotation every 30 days\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Store a SecureString in Systems Manager Parameter Store and enable automatic rotation every 30 days\" is incorrect as SSM Parameter Store does not support automatic key rotation.</p><p><strong>INCORRECT:</strong> \"Store the connection string as an encrypted environment variable in Lambda and create a separate function that rotates the connection string every 30 days\" is incorrect as this is not the simplest solution. In this scenario using AWS Secrets Manager would be easier to implement as it provides native features for rotating the secret.</p><p><strong>INCORRECT:</strong> \"Store the connection string in an encrypted Amazon S3 bucket and use a scheduled CloudWatch Event to update the connection string every 30 days\" is incorrect. There is no native capability of CloudWatch to update connection strings so you would need some other service such as a Lambda function to execute and rotate the connection string which is missing from this answer.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/secrets-manager/features/\">https://aws.amazon.com/secrets-manager/features/</a></p>",
                "options": [
                    {
                        "id": 3399,
                        "content": "<p>Store a secret in AWS Secrets Manager and enable automatic rotation every 30 days</p>",
                        "isValid": true
                    },
                    {
                        "id": 3400,
                        "content": "<p>Store a SecureString in Systems Manager Parameter Store and enable automatic rotation every 30 days</p>",
                        "isValid": false
                    },
                    {
                        "id": 3401,
                        "content": "<p>Store the connection string as an encrypted environment variable in Lambda and create a separate function that rotates the connection string every 30 days</p>",
                        "isValid": false
                    },
                    {
                        "id": 3402,
                        "content": "<p>Store the connection string in an encrypted Amazon S3 bucket and use a scheduled CloudWatch Event to update the connection string every 30 days</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 828,
            "attributes": {
                "createdAt": "2023-09-07T08:51:31.211Z",
                "updatedAt": "2023-09-07T08:51:31.211Z",
                "content": "<p>An application is being migrated into the cloud. The application is stateless and will run on a fleet of Amazon EC2 instances. The application should scale elastically. How can a Developer ensure that the number of instances available is sufficient for current demand?</p>",
                "answerExplanation": "<p>Amazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove EC2 instances according to conditions you define. You can use the fleet management features of EC2 Auto Scaling to maintain the health and availability of your fleet.</p><p>You can also use the dynamic and predictive scaling features of EC2 Auto Scaling to add or remove EC2 instances. Dynamic scaling responds to changing demand and predictive scaling automatically schedules the right number of EC2 instances based on predicted demand. Dynamic scaling and predictive scaling can be used together to scale faster.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_00-36-03-e5bec27ced3332eb72a159f11d8848ec.png\"></p><p>A <em>launch configuration</em> is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances. Include the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping. If you've launched an EC2 instance before, you specified the same information in order to launch the instance.</p><p>You can specify your launch configuration with multiple Auto Scaling groups. However, you can only specify one launch configuration for an Auto Scaling group at a time, and you can't modify a launch configuration after you've created it. To change the launch configuration for an Auto Scaling group, you must create a launch configuration and then update your Auto Scaling group with it.</p><p>Therefore, the Developer should create a launch configuration and use Amazon EC2 Auto Scaling.</p><p><strong>CORRECT: </strong>\"Create a launch configuration and use Amazon EC2 Auto Scaling\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a launch configuration and use Amazon CodeDeploy\" is incorrect as CodeDeploy is not used for auto scaling of Amazon EC2 instances.</p><p><strong>INCORRECT:</strong> \"Create a task definition and use an Amazon ECS cluster\" is incorrect as the migrated application will be running on Amazon EC2 instances, not containers.</p><p><strong>INCORRECT:</strong> \"Create a task definition and use an AWS Fargate cluster\" is incorrect as the migrated application will be running on Amazon EC2 instances, not containers.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 3403,
                        "content": "<p>Create a task definition and use an Amazon ECS cluster</p>",
                        "isValid": false
                    },
                    {
                        "id": 3404,
                        "content": "<p>Create a launch configuration and use Amazon CodeDeploy</p>",
                        "isValid": false
                    },
                    {
                        "id": 3405,
                        "content": "<p>Create a launch configuration and use Amazon EC2 Auto Scaling</p>",
                        "isValid": true
                    },
                    {
                        "id": 3406,
                        "content": "<p>Create a task definition and use an AWS Fargate cluster</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 829,
            "attributes": {
                "createdAt": "2023-09-07T08:51:31.284Z",
                "updatedAt": "2023-09-07T08:51:31.284Z",
                "content": "<p>A Developer is storing sensitive documents in Amazon S3. The documents must be encrypted at rest and company policy mandates that the encryption keys must be rotated annually. What is the EASIEST way to achieve this?</p>",
                "answerExplanation": "<p>Cryptographic best practices discourage extensive reuse of encryption keys. To create new cryptographic material for your AWS Key Management Service (AWS KMS) customer master keys (CMKs), you can create new CMKs, and then change your applications or aliases to use the new CMKs. Or, you can enable automatic key rotation for an existing <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk\">customer managed CMK</a>.</p><p>When you enable <em>automatic key rotation</em> for a customer managed CMK, AWS KMS generates new cryptographic material for the CMK every year. AWS KMS also saves the CMK's older cryptographic material in perpetuity so it can be used to decrypt data that it encrypted. AWS KMS does not delete any rotated key material until you <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html\">delete the CMK</a>.</p><p>Key rotation changes only the CMK's <em>backing key</em>, which is the cryptographic material that is used in encryption operations. The CMK is the same logical resource, regardless of whether or how many times its backing key changes. The properties of the CMK do not change, as shown in the following image.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_14-53-16-7305421dbc8087397f612758b56a8720.jpg\"></p><p>Therefore, the easiest way to meet this requirement is to use AWS KMS with automatic key rotation.</p><p><strong>CORRECT: </strong>\"Use AWS KMS with automatic key rotation\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Encrypt the data before sending it to Amazon S3\" is incorrect as that requires managing your own encryption infrastructure which is not the easiest way to achieve the requirements.</p><p><strong>INCORRECT:</strong> \"Import a custom key into AWS KMS with annual rotation enabled\" is incorrect as when you import key material into AWS KMS you are still responsible for the key material while allowing KMS to use a copy of it. Therefore, this is not the easiest solution as you must manage the key materials.</p><p><strong>INCORRECT:</strong> \"Export a key from AWS KMS to encrypt the data\" is incorrect as when you export a data encryption key you are then responsible for using it and managing it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
                "options": [
                    {
                        "id": 3407,
                        "content": "<p>Use AWS KMS with automatic key rotation</p>",
                        "isValid": true
                    },
                    {
                        "id": 3408,
                        "content": "<p>Encrypt the data before sending it to Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 3409,
                        "content": "<p>Import a custom key into AWS KMS with annual rotation enabled</p>",
                        "isValid": false
                    },
                    {
                        "id": 3410,
                        "content": "<p>Export a key from AWS KMS to encrypt the data</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 830,
            "attributes": {
                "createdAt": "2023-09-07T08:51:31.354Z",
                "updatedAt": "2023-09-07T08:51:31.354Z",
                "content": "<p>A Developer is designing a fault-tolerant environment where client sessions will be saved. How can the Developer ensure that no sessions are lost if an Amazon EC2 instance fails?</p>",
                "answerExplanation": "<p>The <strong>DynamoDB Session Handler</strong> is a custom session handler for PHP that allows developers to use Amazon DynamoDB as a session store. Using DynamoDB for session storage alleviates issues that occur with session handling in a distributed web application by moving sessions off of the local file system and into a shared location. DynamoDB is fast, scalable, easy to setup, and handles replication of your data automatically.</p><p><strong>CORRECT: </strong>\"Use Amazon DynamoDB to perform scalable session handling\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use sticky sessions with an Elastic Load Balancer target group\" is incorrect as this involves maintaining session state data on the EC2 instances which means that data is lost if an instance fails.</p><p><strong>INCORRECT:</strong> \"Use Amazon SQS to save session data\" is incorrect as SQS is not used for session data, it is used for application component decoupling.</p><p><strong>INCORRECT:</strong> \"Use Elastic Load Balancer connection draining to stop sending requests to failing instances\" is incorrect as this does not solve the problem of ensuring the session data is available, the data will be on the failing instance and will be lost.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html\">https://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3411,
                        "content": "<p>Use Elastic Load Balancer connection draining to stop sending requests to failing instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 3412,
                        "content": "<p>Use Amazon DynamoDB to perform scalable session handling</p>",
                        "isValid": true
                    },
                    {
                        "id": 3413,
                        "content": "<p>Use sticky sessions with an Elastic Load Balancer target group</p>",
                        "isValid": false
                    },
                    {
                        "id": 3414,
                        "content": "<p>Use Amazon SQS to save session data</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 831,
            "attributes": {
                "createdAt": "2023-09-07T08:51:31.428Z",
                "updatedAt": "2023-09-07T08:51:31.428Z",
                "content": "<p>A Developer is creating a DynamoDB table for storing application logs. The table has 5 write capacity units (WCUs). The Developer needs to configure the read capacity units (RCUs) for the table. Which of the following configurations represents the most efficient use of throughput?</p>",
                "answerExplanation": "<p>In this scenario the Developer needs to maximize efficiency of RCUs. Therefore, the Developer will need to consider the item size and consistency model to determine the most efficient usage of RCUs.</p><p><strong>Item size/consistency model:</strong> we know that both 1 KB items and 4 KB items consume the same number of RCUs as a <em>read capacity unit</em> represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size.</p><p>The following bullets provide the read throughput for each configuration:</p><p>â€¢ Eventually consistent, 15 RCUs, 1 KB item = 30 items/s = 2 items per RCU</p><p>â€¢ Strongly consistent, 15 RCUs, 1 KB item = 15 items/s = 1 item per RCU</p><p>â€¢ Eventually consistent, 5 RCUs, 4 KB item = 10 items/s = 2 items per RCU</p><p>â€¢ Strongly consistent, 5 RCUs, 4 KB item = 5 items/s = 1 item per RCU</p><p>From the above we can see that 4 KB items with eventually consistent reads is the most efficient option. Therefore, the Developer should choose the option â€œEventually consistent reads of 5 RCUs reading items that are 4 KB in sizeâ€. This will achieve 2x 4 KB items per RCU.</p><p><strong>CORRECT: </strong>\"Eventually consistent reads of 5 RCUs reading items that are 4 KB in size\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Eventually consistent reads of 15 RCUs reading items that are 1 KB in size\" is incorrect as described above.</p><p><strong>INCORRECT:</strong> \"Strongly consistent reads of 5 RCUs reading items that are 4 KB in size\" is incorrect as described above.</p><p><strong>INCORRECT:</strong> \"Strongly consistent reads of 15 RCUs reading items that are 1KB in size\" is incorrect as described above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3415,
                        "content": "<p>Strongly consistent reads of 15 RCUs reading items that are 1KB in size</p>",
                        "isValid": false
                    },
                    {
                        "id": 3416,
                        "content": "<p>Eventually consistent reads of 5 RCUs reading items that are 4 KB in size</p>",
                        "isValid": true
                    },
                    {
                        "id": 3417,
                        "content": "<p>Strongly consistent reads of 5 RCUs reading items that are 4 KB in size</p>",
                        "isValid": false
                    },
                    {
                        "id": 3418,
                        "content": "<p>Eventually consistent reads of 15 RCUs reading items that are 1 KB in size</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 832,
            "attributes": {
                "createdAt": "2023-09-07T08:51:31.515Z",
                "updatedAt": "2023-09-07T08:51:31.515Z",
                "content": "<p>An application that processes financial transactions receives thousands of transactions each second. The transactions require end-to-end encryption, and the application implements this by using the AWS KMS GenerateDataKey operation. During operation the application receives the following error message:</p><p><em>â€œYou have exceeded the rate at which you may call KMS. Reduce the frequency of your calls.</em></p><p><em>(Service: AWSKMS; Status Code: 400; Error Code: ThrottlingException; Request ID: &lt;ID&gt;â€</em></p><p>Which actions are best practices to resolve this error? (Select TWO.)</p>",
                "answerExplanation": "<p>To ensure that AWS KMS can provide fast and reliable responses to API requests from all customers, it throttles API requests that exceed certain boundaries. <em>Throttling</em> occurs when AWS KMS rejects an otherwise valid request and returns a ThrottlingException error.</p><p><em>Data key caching</em> stores data keys and related cryptographic material in a cache. When you encrypt or decrypt data, the AWS Encryption SDK looks for a matching data key in the cache. If it finds a match, it uses the cached data key rather than generating a new one. Data key caching can improve performance, reduce cost, and help you stay within service limits as your application scales.</p><p>Your application can benefit from data key caching if:</p><p> â€¢ It can reuse data keys.</p><p> â€¢ It generates numerous data keys.</p><p> â€¢ Your cryptographic operations are unacceptably slow, expensive, limited, or resource-intensive.</p><p>To create an instance of the local cache, use the LocalCryptoMaterialsCache constructor in Java and Python, the getLocalCryptographicMaterialsCache function in JavaScript, or the aws_cryptosdk_materials_cache_local_new constructor in C.</p><p>Additionally, the developer can request an increase in the quota for AWS KMS which will provide the ability to submit more API calls the AWS KMS.</p><p><strong>CORRECT: </strong>\"Create a local cache using the AWS Encryption SDK and the LocalCryptoMaterialsCache feature\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create a case in the AWS Support Center to increase the quota for the account\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Call the AWS KMS Encrypt operation directly to allow AWS KMS to encrypt the data\" is incorrect.</p><p>This will not reduce API calls to AWS KMS. Additionally, there are limits to the maximum size of the data that can be encrypted using this method. The max is 4096 bytes.</p><p><strong>INCORRECT:</strong> \"Use Amazon SQS to queue the requests and configure AWS KMS to poll the queue\" is incorrect.</p><p>KMS cannot be configured to poll and SQS queue.</p><p><strong>INCORRECT:</strong> \"Create an AWS KMS custom key store and generate data keys through AWS CloudHSM\" is incorrect.</p><p>This is an unnecessary step and would incur additional cost. CloudHSM is not beneficial for this specific situation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/data-key-caching.html\">https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/data-key-caching.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
                "options": [
                    {
                        "id": 3419,
                        "content": "<p>Create a case in the AWS Support Center to increase the quota for the account.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3420,
                        "content": "<p>Create an AWS KMS custom key store and generate data keys through AWS CloudHSM.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3421,
                        "content": "<p>Use Amazon SQS to queue the requests and configure AWS KMS to poll the queue.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3422,
                        "content": "<p>Create a local cache using the AWS Encryption SDK and the LocalCryptoMaterialsCache feature.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3423,
                        "content": "<p>Call the AWS KMS Encrypt operation directly to allow AWS KMS to encrypt the data.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 833,
            "attributes": {
                "createdAt": "2023-09-07T08:51:31.592Z",
                "updatedAt": "2023-09-07T08:51:31.592Z",
                "content": "<p>In the process of developing an application, a software engineer deploys an Amazon API Gateway REST API within the us-west-2 Region. The plan is to use Amazon CloudFront and a custom domain name for the API, using an SSL/TLS certificate acquired from a third-party provider.</p><p>What is the appropriate strategy for configuring the custom domain name?</p>",
                "answerExplanation": "<p>The developer needs to import the SSL/TLS certificate into AWS Certificate Manager (ACM) to use a custom domain name with Amazon API Gateway. After that, the developer can link this certificate to the custom domain name in the API Gateway. Finally, set an alias (A) record in Route 53 to point to the API Gateway's domain name.</p><p><strong>CORRECT: </strong>\"Import the third-party SSL/TLS certificate to AWS Certificate Manager (ACM), link it with the custom domain name in API Gateway, and then create an alias (A) record in Route 53 for the custom domain name\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Route 53 to create a simple routing policy with the custom domain name directly pointed to the API Gateway\" is incorrect.</p><p>You cannot point a custom domain name directly to the API Gateway using a simple routing policy in Route 53 without setting up the custom domain name in API Gateway first.</p><p><strong>INCORRECT:</strong> \"Directly install the third-party SSL/TLS certificate on the API Gateway and establish a CNAME record in Route 53 for the custom domain name\" is incorrect.</p><p>API Gateway doesn't support direct installation of third-party SSL/TLS certificates. Also, for API Gateway, an alias record is preferred over a CNAME record in Route 53.</p><p><strong>INCORRECT:</strong> \"Use the third-party SSL/TLS certificate directly with Amazon CloudFront, and bypass API Gateway, creating an alias (A) record in Route 53\" is incorrect.</p><p>Amazon CloudFront is inherently used and managed by API Gateway, and you cannot bypass API Gateway using Amazon CloudFront directly for a custom domain. Also, an SSL/TLS certificate needs to be managed via ACM.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-api-gateway.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-api-gateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certificate-manager/\">https://digitalcloud.training/aws-certificate-manager/</a></p>",
                "options": [
                    {
                        "id": 3424,
                        "content": "<p>Import the third-party SSL/TLS certificate to AWS Certificate Manager (ACM), link it with the custom domain name in API Gateway, and then create an alias (A) record in Route 53 for the custom domain name.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3425,
                        "content": "<p>Use the third-party SSL/TLS certificate directly with Amazon CloudFront, and bypass API Gateway, creating an alias (A) record in Route 53.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3426,
                        "content": "<p>Directly install the third-party SSL/TLS certificate on the API Gateway and establish a CNAME record in Route 53 for the custom domain name.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3427,
                        "content": "<p>Use Route 53 to create a simple routing policy with the custom domain name directly pointed to the API Gateway.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 834,
            "attributes": {
                "createdAt": "2023-09-07T08:51:31.667Z",
                "updatedAt": "2023-09-07T08:51:31.667Z",
                "content": "<p>To include objects defined by the AWS Serverless Application Model (SAM) in an AWS CloudFormation template, in addition to Resources, what section MUST be included in the document root?</p>",
                "answerExplanation": "<p>The primary differences between AWS SAM templates and AWS CloudFormation templates are the following:</p><p>â€¢ <strong>Transform declaration.</strong> The declaration Transform: AWS::Serverless-2016-10-31 is required for AWS SAM templates. This declaration identifies an AWS CloudFormation template as an AWS SAM template.</p><p>â€¢ <strong>Globals section. </strong>The Globals section is unique to AWS SAM. It defines properties that are common to all your serverless functions and APIs. All the AWS::Serverless::Function, AWS::Serverless::Api, and AWS::Serverless::SimpleTable resources inherit the properties that are defined in the Globals section.</p><p>â€¢ <strong>Resources section. </strong>In AWS SAM templates the Resources section can contain a combination of AWS CloudFormation resources and AWS SAM resources.</p><p>Of these three sections, only the Transform section and Resources sections are required; the Globals section is optional.</p><p><strong>CORRECT: </strong>\"<code>Transform</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"<code>Globals</code>\" is incorrect as this is not a required section.</p><p><strong>INCORRECT:</strong> \"<code>Conditions</code>\" is incorrect as this is an optional section.</p><p><strong>INCORRECT:</strong> \"<code>Properties</code>\" is incorrect as this is not a section in a template, it is used within a resource.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-sam/\">https://digitalcloud.training/aws-sam/</a></p>",
                "options": [
                    {
                        "id": 3428,
                        "content": "<p>Transform</p>",
                        "isValid": true
                    },
                    {
                        "id": 3429,
                        "content": "<p>Properties</p>",
                        "isValid": false
                    },
                    {
                        "id": 3430,
                        "content": "<p>Conditions</p>",
                        "isValid": false
                    },
                    {
                        "id": 3431,
                        "content": "<p>Globals</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 835,
            "attributes": {
                "createdAt": "2023-09-07T08:51:31.741Z",
                "updatedAt": "2023-09-07T08:51:31.741Z",
                "content": "<p>A developer is creating a microservices application that includes and AWS Lambda function. The function generates a unique file for each execution and must commit the file to an AWS CodeCommit repository.</p><p>How should the developer accomplish this?</p>",
                "answerExplanation": "<p>The developer can instantiate a CodeCommit client using the AWS SDK. This provides the ability to programmatically work with the AWS CodeCommit repository. The PutFile method is used to add or modify a single file in a specified repository and branch. The CreateCommit method creates a commit for changes to a repository.</p><p><strong>CORRECT: </strong>\"Use an AWS SDK to instantiate a CodeCommit client. Invoke the PutFile method to add the file to the repository and execute a commit with CreateCommit\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Send a message to an Amazon SQS queue with the file attached. Configure an AWS Step Function as a destination for messages in the queue. Configure the Step Function to add the new file to the repository and commit the change\" is incorrect.</p><p>A Step Function cannot be a destination for messages in an SQS queue. There would need to be another Lambda function or other method to trigger the state machine and pass the information across. This would be a highly inefficient solution.</p><p><strong>INCORRECT:</strong> \"After the new file is created in Lambda, use CURL to invoke the CodeCommit API. Send the file to the repository and automatically commit the change\" is incorrect.</p><p>CURL cannot be used to work with the CodeCommit API. The developer must use the AWS SDK.</p><p><strong>INCORRECT:</strong> \"Upload the new file to an Amazon S3 bucket. Create an AWS Step Function to accept S3 events. Use AWS Lambda functions in the Step Function, to add the file to the repository and commit the change\" is incorrect.</p><p>Step Functions is not a supported destination for Amazon S3 event notifications. Supported destinations are SNS topics, SQS queues, Lambda functions, and EventBridge event buses.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/codecommit/AWSCodeCommitClient.html\">https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/codecommit/AWSCodeCommitClient.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3432,
                        "content": "<p>Send a message to an Amazon SQS queue with the file attached. Configure an AWS Step Function as a destination for messages in the queue. Configure the Step Function to add the new file to the repository and commit the change.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3433,
                        "content": "<p>Use an AWS SDK to instantiate a CodeCommit client. Invoke the PutFile method to add the file to the repository and execute a commit with CreateCommit.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3434,
                        "content": "<p>Upload the new file to an Amazon S3 bucket. Create an AWS Step Function to accept S3 events. Use AWS Lambda functions in the Step Function, to add the file to the repository and commit the change.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3435,
                        "content": "<p>After the new file is created in Lambda, use CURL to invoke the CodeCommit API. Send the file to the repository and automatically commit the change.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 836,
            "attributes": {
                "createdAt": "2023-09-07T08:51:31.817Z",
                "updatedAt": "2023-09-07T08:51:31.817Z",
                "content": "<p>An application has been instrumented to use the AWS X-Ray SDK to collect data about the requests the application serves. The Developer has set the user field on segments to a string that identifies the <code>user</code> who sent the request.</p><p>How can the Developer search for segments associated with specific users?</p>",
                "answerExplanation": "<p>A <strong>segment document</strong> conveys information about a segment to X-Ray. A segment document can be up to 64 kB and contain a whole segment with subsegments, a fragment of a segment that indicates that a request is in progress, or a single subsegment that is sent separately. You can send segment documents directly to X-Ray by using the <a href=\"https://docs.aws.amazon.com/xray/latest/api/API_PutTraceSegments.html\">PutTraceSegments</a> API.</p><p>Example minimally complete segment:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"name\"</span><span class=\"pln\"> </span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"example.com\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"id\"</span><span class=\"pln\"> </span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"70de5b6f19ff9a0a\"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"start_time\"</span><span class=\"pln\"> </span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"lit\">1.478293361271E9</span><span class=\"pun\">,</span></li><li class=\"L4\"><span class=\"str\">\"trace_id\"</span><span class=\"pln\"> </span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"1-581cf771-a006649127e371903a2de979\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"str\">\"end_time\"</span><span class=\"pln\"> </span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"lit\">1.478293361449E9</span></li><li class=\"L6\"><span class=\"pun\">}</span></li></ol></pre></div></div><p><br></p><p>A subset of segment fields are indexed by X-Ray for use with filter expressions. For example, if you set the user field on a segment to a unique identifier, you can search for segments associated with specific users in the X-Ray console or by using the <code>GetTraceSummaries</code> API.</p><p><strong>CORRECT: </strong>\"By using the <code>GetTraceSummaries </code>API with a filter expression\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"By using the <code>GetTraceGraph</code> API with a filter expression\" is incorrect as this API action retrieves a service graph for one or more specific trace IDs.</p><p><strong>INCORRECT:</strong> \"Use a filter expression to search for the <code>user</code> field in the segment metadata\" is incorrect as the <code>user</code> field is not part of the segment metadata and metadata is not is not indexed for search.</p><p><strong>INCORRECT:</strong> \"Use a filter expression to search for the <code>user</code> field in the segment annotations\" is incorrect as the <code>user</code> field is not part of the segment annotations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p><p><br></p>",
                "options": [
                    {
                        "id": 3436,
                        "content": "<p>By using the <code>GetTraceSummaries</code> API with a filter expression</p>",
                        "isValid": true
                    },
                    {
                        "id": 3437,
                        "content": "<p>Use a filter expression to search for the<code> user</code> field in the segment annotations</p>",
                        "isValid": false
                    },
                    {
                        "id": 3438,
                        "content": "<p>By using the <code>GetTraceGraph</code> API with a filter expression</p>",
                        "isValid": false
                    },
                    {
                        "id": 3439,
                        "content": "<p>Use a filter expression to search for the <code>user</code> field in the segment metadata</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 837,
            "attributes": {
                "createdAt": "2023-09-07T08:51:31.889Z",
                "updatedAt": "2023-09-07T08:51:31.889Z",
                "content": "<p>A programmer is creating an application that requires signed requests (Signature Version 4) for invoking other AWS services. Having constructed a canonical request, created the string to sign, and calculated the signing information, which strategies can the programmer apply to finalize a signed request? (Select TWO.)</p>",
                "answerExplanation": "<p>When sending a request to AWS services using Signature Version 4, the signature should be included in the \"Authorization\" HTTP header or as a parameter \"X-Amz-Signature\" in the query string.</p><p><strong>CORRECT: </strong>\"Incorporate the signature into an HTTP header called \"Authorization\"\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Insert the signature into a query string parameter referred to as \"X-Amz-Signature\"\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Append the signature to a query string parameter known as \"X-Amz-Credentials\"\" is incorrect.</p><p>\"X-Amz-Credentials\" is a query string parameter, but it's not meant for adding signatures. It's used to include access key ID and scoped credential details.</p><p><strong>INCORRECT:</strong> \"Embed the signature in an HTTP header labelled \"Authorization-Key\"\" is incorrect.</p><p>\"Authorization-Key\" is not a recognized HTTP header for adding AWS Signature Version 4.</p><p><strong>INCORRECT:</strong> \"Add the signature to a query string parameter named \"Signature-Token\"\" is incorrect.</p><p>\"Signature-Token\" is not a recognized query string parameter for adding AWS Signature Version 4.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-signing.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-signing.html</a></p>",
                "options": [
                    {
                        "id": 3440,
                        "content": "<p>Append the signature to a query string parameter known as \"X-Amz-Credentials\".</p>",
                        "isValid": false
                    },
                    {
                        "id": 3441,
                        "content": "<p>Insert the signature into a query string parameter referred to as \"X-Amz-Signature\".</p>",
                        "isValid": true
                    },
                    {
                        "id": 3442,
                        "content": "<p>Incorporate the signature into an HTTP header called \"Authorization\".</p>",
                        "isValid": true
                    },
                    {
                        "id": 3443,
                        "content": "<p>Embed the signature in an HTTP header labelled \"Authorization-Key\".</p>",
                        "isValid": false
                    },
                    {
                        "id": 3444,
                        "content": "<p>Add the signature to a query string parameter named \"Signature-Token\".</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 838,
            "attributes": {
                "createdAt": "2023-09-07T08:51:31.963Z",
                "updatedAt": "2023-09-07T08:51:31.963Z",
                "content": "<p>An engineer wishes to share a software project they've developed with their team members for review. The shared application code needs to be preserved over time, with multiple versions and batch modifications being tracked. What is the most appropriate AWS service for this purpose?</p>",
                "answerExplanation": "<p>AWS CodeCommit is a fully managed source control service that hosts secure Git-based repositories. It's designed to collaborate on code, maintain version control, and keep track of changes. This makes it an optimal choice for this scenario.</p><p><strong>CORRECT: </strong>\"AWS CodeCommit\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS CodeStar\" is incorrect.</p><p>AWS CodeStar enables you to quickly develop, build, and deploy applications on AWS. It isn't focused on source control and versioning, which is the primary need in this scenario.</p><p><strong>INCORRECT:</strong> \"AWS Cloud9\" is incorrect.</p><p>AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. While it allows for code sharing, it does not offer the robust version control and change tracking of CodeCommit.</p><p><strong>INCORRECT:</strong> \"AWS CodeDeploy\" is incorrect.</p><p>AWS CodeDeploy is a deployment service that enables developers to automate software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/codecommit/\">https://aws.amazon.com/codecommit/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3445,
                        "content": "<p>AWS CodeStar</p>",
                        "isValid": false
                    },
                    {
                        "id": 3446,
                        "content": "<p>AWS Cloud9</p>",
                        "isValid": false
                    },
                    {
                        "id": 3447,
                        "content": "<p>AWS CodeDeploy</p>",
                        "isValid": false
                    },
                    {
                        "id": 3448,
                        "content": "<p>AWS CodeCommit</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 839,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.033Z",
                "updatedAt": "2023-09-07T08:51:32.033Z",
                "content": "<p>A company runs an application on a fleet of web servers running on Amazon EC2 instances. The web servers are behind an Elastic Load Balancer (ELB) and use an Amazon DynamoDB table for storing session state. A Developer has been asked to implement a mechanism for automatically deleting session state data that is older than 24 hours.</p><p>What is the SIMPLEST solution to this requirement?</p>",
                "answerExplanation": "<p>Time to Live (TTL) for Amazon DynamoDB lets you define when items in a table expire so that they can be automatically deleted from the database. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant.</p><p>TTL is useful if you have continuously accumulating data that loses relevance after a specific time period (for example, session data, event logs, usage patterns, and other temporary data). If you have sensitive data that must be retained only for a certain amount of time according to contractual or regulatory obligations, TTL helps you ensure that it is removed promptly and as scheduled.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_12-45-44-de98b590380f9976a6dfd87ff1f39e99.jpg\"></p><p>When Time to Live (TTL) is enabled on a table in Amazon DynamoDB, a background job checks the TTL attribute of items to determine whether they are expired.</p><p>DynamoDB compares the current time, in <a href=\"https://en.wikipedia.org/wiki/Unix_time\">epoch time format</a>, to the value stored in the user-defined Number attribute of an item. If the attributeâ€™s value is in the epoch time format, is less than the current time, and is not older than 5 years, the item is deleted.</p><p>Processing takes place automatically, in the background, and doesn't affect read or write traffic to the table. In addition, deletes performed via TTL are not counted towards capacity units or request units. TTL deletes are available at no additional cost.</p><p>For this requirement, the Developer must add an attribute to each item with the expiration time in epoch format and then enable the Time To Live (TTL) feature based on that attribute.</p><p><strong>CORRECT: </strong>\"Add an attribute with the expiration time; enable the Time To Live feature based on that attribute\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Each day, create a new table to hold session data; delete the previous day's table\" is incorrect. This solution would delete some data that is not 24 hours old as it would have to run at a specific time.</p><p><strong>INCORRECT:</strong> \"Write a script that deletes old records; schedule the scripts as a cron job on an Amazon EC2 instance\" is incorrect. This is not an elegant solution and would also cost more as it requires RCUs/WCUs to delete the items.</p><p><strong>INCORRECT:</strong> \"Add an attribute with the expiration time; name the attribute ItemExpiration\" is incorrect as this is not a complete solution. You also need to enable the TTL feature on the table.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3449,
                        "content": "<p>Write a script that deletes old records; schedule the scripts as a cron job on an Amazon EC2 instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 3450,
                        "content": "<p>Each day, create a new table to hold session data; delete the previous day's table</p>",
                        "isValid": false
                    },
                    {
                        "id": 3451,
                        "content": "<p>Add an attribute with the expiration time; name the attribute ItemExpiration</p>",
                        "isValid": false
                    },
                    {
                        "id": 3452,
                        "content": "<p>Add an attribute with the expiration time; enable the Time To Live feature based on that attribute</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 840,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.111Z",
                "updatedAt": "2023-09-07T08:51:32.111Z",
                "content": "<p>A company is creating an application that will require users to access AWS services and allow them to reset their own passwords. Which of the following would allow the company to manage users and authorization while allowing users to reset their own passwords?</p>",
                "answerExplanation": "<p>There are two key requirements in this scenario. Firstly the company wants to manage user accounts using a system that allows users to reset their own passwords. The company also wants to authorize users to access AWS services.</p><p>The first requirement is provided by an Amazon Cognito User Pool. With a Cognito user pool you can add sign-up and sign-in to mobile and web apps and it also offers a user directory so user accounts can be created directly within the user pool. Users also have the ability to reset their passwords.</p><p>To access AWS services you need a Cognito Identity Pool. An identity pool can be used with a user pool and enables a user to obtain temporary limited-privilege credentials to access AWS services.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_00-45-06-c12af29c40eeac5da22793dd2408937f.png\"><p>Therefore, the best answer is to use Amazon Cognito user pools and identity pools.</p><p><strong>CORRECT: </strong>\"Amazon Cognito user pools and identity pools\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon Cognito identity pools and AWS STS\" is incorrect as there is no user directory in this solution. A Cognito user pool is required.</p><p><strong>INCORRECT:</strong> \"Amazon Cognito identity pools and AWS IAM\" is incorrect as a Cognito user pool should be used as the directory source for creating and managing users. IAM is used for accounts that are used to administer AWS services, not for application user access.</p><p><strong>INCORRECT:</strong> \"Amazon Cognito user pools and AWS KMS\" is incorrect as KMS is used for encryption, not for authentication to AWS services.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html</a></p>",
                "options": [
                    {
                        "id": 3453,
                        "content": "<p>Amazon Cognito identity pools and AWS STS</p>",
                        "isValid": false
                    },
                    {
                        "id": 3454,
                        "content": "<p>Amazon Cognito user pools and identity pools</p>",
                        "isValid": true
                    },
                    {
                        "id": 3455,
                        "content": "<p>Amazon Cognito identity pools and AWS IAM</p>",
                        "isValid": false
                    },
                    {
                        "id": 3456,
                        "content": "<p>Amazon Cognito user pools and AWS KMS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 841,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.190Z",
                "updatedAt": "2023-09-07T08:51:32.190Z",
                "content": "<p>An application is instrumented to generate traces using AWS X-Ray and generates a large amount of trace data. A Developer would like to use filter expressions to filter the results to specific key-value pairs added to custom subsegments.</p><p>How should the Developer add the key-value pairs to the custom subsegments?</p>",
                "answerExplanation": "<p>You can record additional information about requests, the environment, or your application with annotations and metadata. You can add annotations and metadata to the segments that the X-Ray SDK creates, or to custom subsegments that you create.</p><p><strong>Annotations</strong> are key-value pairs with string, number, or Boolean values. Annotations are indexed for use with <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html\">filter expressions</a>. Use annotations to record data that you want to use to group traces in the console, or when calling the <a href=\"https://docs.aws.amazon.com/xray/latest/api/API_GetTraceSummaries.html\">GetTraceSummaries</a> API.</p><p><strong>Metadata</strong> are key-value pairs that can have values of any type, including objects and lists, but are not indexed for use with filter expressions. Use metadata to record additional data that you want stored in the trace but don't need to use with search.</p><p>Annotations can be used with filter expressions, so this is the best solution for this requirement. The Developer can add annotations to the custom subsegments and will then be able to use filter expressions to filter the results in AWS X-Ray.</p><p><strong>CORRECT: </strong>\"Add annotations to the custom subsegments\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Add metadata to the custom subsegments\" is incorrect as though you can add metadata to custom subsegments it is not indexed and cannot be used with filters.</p><p><strong>INCORRECT:</strong> \"Add the key-value pairs to the Trace ID\" is incorrect as this is not something you can do.</p><p><strong>INCORRECT:</strong> \"Setup sampling for the custom subsegments \" is incorrect as this is a mechanism used by X-Ray to send only statistically significant data samples to the API.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-segment.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-segment.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3457,
                        "content": "<p>Add annotations to the custom subsegments</p>",
                        "isValid": true
                    },
                    {
                        "id": 3458,
                        "content": "<p>Add the key-value pairs to the Trace ID</p>",
                        "isValid": false
                    },
                    {
                        "id": 3459,
                        "content": "<p>Setup sampling for the custom subsegments</p>",
                        "isValid": false
                    },
                    {
                        "id": 3460,
                        "content": "<p>Add metadata to the custom subsegments</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 842,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.269Z",
                "updatedAt": "2023-09-07T08:51:32.269Z",
                "content": "<p>A team of Developers are building a continuous integration and delivery pipeline using AWS Developer Tools. Which services should they use for running tests against source code and installing compiled code on their AWS resources? (Select TWO.)</p>",
                "answerExplanation": "<p>AWS CodeBuild is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides pre-packaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more.</p><p>CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_00-32-50-133a07af320f839fc765901afcd858ff.png\"></p><p><strong>CORRECT: </strong>\"AWS CodeBuild for running tests against source code\" is a correct answer.</p><p><strong>CORRECT: </strong>\"AWS CodeDeploy for installing compiled code on their AWS resources\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodePipeline for running tests against source code\" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. This service works with the other Developer Tools to create a pipeline.</p><p><strong>INCORRECT:</strong> \"AWS CodeCommit for installing compiled code on their AWS resources\" is incorrect as AWS CodeCommit is a fully-managed <a href=\"https://aws.amazon.com/devops/source-control/\">source control</a> service that hosts secure Git-based repositories.</p><p><strong>INCORRECT:</strong> \"AWS Cloud9 for running tests against source code\" is incorrect as AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3461,
                        "content": "<p>AWS Cloud9 for running tests against source code</p>",
                        "isValid": false
                    },
                    {
                        "id": 3462,
                        "content": "<p>AWS CodeBuild for running tests against source code</p>",
                        "isValid": true
                    },
                    {
                        "id": 3463,
                        "content": "<p>AWS CodeCommit for installing compiled code on their AWS resources</p>",
                        "isValid": false
                    },
                    {
                        "id": 3464,
                        "content": "<p>AWS CodeDeploy for installing compiled code on their AWS resources</p>",
                        "isValid": true
                    },
                    {
                        "id": 3465,
                        "content": "<p>AWS CodePipeline for running tests against source code</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 843,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.338Z",
                "updatedAt": "2023-09-07T08:51:32.338Z",
                "content": "<p>An application reads data from Amazon S3 and makes 55,000 read requests per second. A Developer must design the storage solution to ensure the performance requirements are met cost-effectively.</p><p>How can the storage be optimized to meet these requirements?</p>",
                "answerExplanation": "<p>To avoid throttling in Amazon S3 you must ensure you do not exceed certain limits on a per-prefix basis. You can send 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in an Amazon S3 bucket. There are no limits to the number of prefixes that you can have in your bucket.</p><p>In this case the Developer would need to split the files across at least 10 prefixes in a single Amazon S3 bucket. The application should then read the files across the prefixes in parallel.</p><p><strong>CORRECT: </strong>\"Create at least 10 prefixes and split the files across the prefixes\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create at least 10 S3 buckets and split the files across the buckets\" is incorrect. Performance is improved based on splitting reads across prefixes, not buckets.</p><p><strong>INCORRECT:</strong> \"Move the files to Amazon EFS. Index the files with S3 metadata\" is incorrect. This is not cost-effective.</p><p><strong>INCORRECT:</strong> \"Move the files to Amazon DynamoDB. Index the files with S3 metadata\" is incorrect. This is not cost-effective.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-request-limit-avoid-throttling/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-request-limit-avoid-throttling/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 3466,
                        "content": "<p>Create at least 10 S3 buckets and split the files across the buckets.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3467,
                        "content": "<p>Move the files to Amazon DynamoDB. Index the files with S3 metadata.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3468,
                        "content": "<p>Move the files to Amazon EFS. Index the files with S3 metadata.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3469,
                        "content": "<p>Create at least 10 prefixes and split the files across the prefixes.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 844,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.410Z",
                "updatedAt": "2023-09-07T08:51:32.410Z",
                "content": "<p>An eCommerce application uses an Amazon RDS database with Amazon ElastiCache in front. Stock volume data is updated dynamically in listings as sales are made. Customers have complained that occasionally the stock volume data is incorrect, and they end up purchasing items that are out of stock. A Developer has checked the front end and indeed some items display the incorrect stock count.</p><p>What could be causing this issue?</p>",
                "answerExplanation": "<p>Amazon ElastiCache is being used to cache data from the Amazon RDS database to improve performance when performing queries. In this case the cache has stale stock volume data stored and is returning this information when customers are purchasing items.</p><p>The resolution is to ensure that the cache is invalidated whenever the stock volume data is changed. This can be done in the application layer.</p><p><strong>CORRECT: </strong>\"The cache is not being invalidated when the stock volume data is changed\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The stock volume data is being retrieved using a write-through ElastiCache cluster\" is incorrect. If this was the case the data would not be stale.</p><p><strong>INCORRECT:</strong> \"The Amazon RDS database is deployed as Multi-AZ and the standby is inconsistent\" is incorrect. Multi-AZ standbys are not used for reading data and the replication is synchronous so it would not be inconsistent.</p><p><strong>INCORRECT:</strong> \"The Amazon RDS database has insufficient IOPS provisioned for its EBS volumes\" is incorrect. This is not the issue here; the stale data is being retrieved from the ElastiCache database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Strategies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 3470,
                        "content": "<p>The cache is not being invalidated when the stock volume data is changed.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3471,
                        "content": "<p>The stock volume data is being retrieved using a write-through ElastiCache cluster.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3472,
                        "content": "<p>The Amazon RDS database has insufficient IOPS provisioned for its EBS volumes.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3473,
                        "content": "<p>The Amazon RDS database is deployed as Multi-AZ and the standby is inconsistent.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 845,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.480Z",
                "updatedAt": "2023-09-07T08:51:32.480Z",
                "content": "<p>A Developer has added a Global Secondary Index (GSI) to an existing Amazon DynamoDB table. The GSI is used mainly for read operations whereas the primary table is extremely write-intensive. Recently, the Developer has noticed throttling occurring under heavy write activity on the primary table. However, the write capacity units on the primary table are not fully utilized.</p><p>What is the best explanation for why the writes are being throttled on the primary table?</p>",
                "answerExplanation": "<p>Some applications might need to perform many kinds of queries, using a variety of different attributes as query criteria. To support these requirements, you can create one or more global secondary indexes and issue Query requests against these indexes in Amazon DynamoDB.</p><p>When items from a primary table are written to the GSI they consume write capacity units. It is essential to ensure the GSI has sufficient WCUs (typically, at least as many as the primary table). If writes are throttled on the GSI, the main table will be throttled (even if thereâ€™s enough WCUs on the main table). LSIs do not cause any special throttling considerations.</p><p>In this scenario, it is likely that the Developer assumed that the GSI would need fewer WCUs as it is more read-intensive and neglected to factor in the WCUs required for writing data into the GSI. Therefore, the most likely explanation is that the write capacity units on the GSI are under provisioned</p><p><strong>CORRECT: </strong>\"The write capacity units on the GSI are under provisioned\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"There are insufficient read capacity units on the primary table\" is incorrect as the table is being throttled due to writes, not reads.</p><p><strong>INCORRECT:</strong> \"The Developer should have added an LSI instead of a GSI\" is incorrect as a GSI has specific advantages and there was likely good reason for adding a GSI. Also, you cannot add an LSI to an existing table.</p><p><strong>INCORRECT:</strong> \"There are insufficient write capacity units on the primary table\" is incorrect as the question states that the WCUs are underutilized.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html</a></p>",
                "options": [
                    {
                        "id": 3474,
                        "content": "<p>There are insufficient write capacity units on the primary table</p>",
                        "isValid": false
                    },
                    {
                        "id": 3475,
                        "content": "<p>The write capacity units on the GSI are under provisioned</p>",
                        "isValid": true
                    },
                    {
                        "id": 3476,
                        "content": "<p>The Developer should have added an LSI instead of a GSI</p>",
                        "isValid": false
                    },
                    {
                        "id": 3477,
                        "content": "<p>There are insufficient read capacity units on the primary table</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 846,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.549Z",
                "updatedAt": "2023-09-07T08:51:32.549Z",
                "content": "<p>A company needs to ingest several terabytes of data every hour from a large number of distributed sources. The messages are delivered continually 24 hrs a day. Messages must be delivered in real time for security analysis and live operational dashboards.</p><p>Which approach will meet these requirements?</p>",
                "answerExplanation": "<p>You can use Amazon Kinesis Data Streams to collect and process large <a href=\"https://aws.amazon.com/streaming-data/\">streams</a> of data records in real time. You can create data-processing applications, known as <em>Kinesis Data Streams applications</em>. A typical Kinesis Data Streams application reads data from a <em>data stream</em> as data records.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_00-47-32-5c696a2961e35feb923ac606485a5d12.png\"></p><p>These applications can use the Kinesis Client Library, and they can run on Amazon EC2 instances. You can send the processed records to dashboards, use them to generate alerts, dynamically change pricing and advertising strategies, or send data to a variety of other AWS services.</p><p>This scenario is an ideal use case for Kinesis Data Streams as large volumes of real time streaming data are being ingested. Therefore, the best approach is to use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messages</p><p><strong>CORRECT: </strong>\"Use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messages\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Send the messages to an Amazon SQS queue, then process the messages by using a fleet of Amazon EC2 instances\" is incorrect as this is not an ideal use case for SQS because SQS is used for decoupling application components, not for ingesting streaming data. It would require more cost (lots of instances to process data) and introduce latency. Also, the message size limitations could be an issue.</p><p><strong>INCORRECT:</strong> \"Use the Amazon S3 API to write messages to an S3 bucket, then process the messages by using Amazon RedShift\" is incorrect as RedShift does not process messages from S3. RedShift is a data warehouse which is used for analytics.</p><p><strong>INCORRECT:</strong> \"Use AWS Data Pipeline to automate the movement and transformation of data\" is incorrect as the question is not asking for transformation of data. The scenario calls for a solution for ingesting and processing the real time streaming data for analytics and feeding some data into a system that generates an operational dashboard.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/introduction.html\">https://docs.aws.amazon.com/streams/latest/dev/introduction.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 3478,
                        "content": "<p>Send the messages to an Amazon SQS queue, then process the messages by using a fleet of Amazon EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 3479,
                        "content": "<p>Use the Amazon S3 API to write messages to an S3 bucket, then process the messages by using Amazon RedShift</p>",
                        "isValid": false
                    },
                    {
                        "id": 3480,
                        "content": "<p>Use AWS Data Pipeline to automate the movement and transformation of data</p>",
                        "isValid": false
                    },
                    {
                        "id": 3481,
                        "content": "<p>Use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messages</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 847,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.625Z",
                "updatedAt": "2023-09-07T08:51:32.625Z",
                "content": "<p>An Amazon API Gateway API developer aims to integrate request validation in a production setting but wants to test it before deployment. Which of the following methods offers the least operational overhead for testing via a tool by sending test requests?</p>",
                "answerExplanation": "<p>The primary concern here is reducing operational overhead. The correct answer is the most straightforward and efficient because it leverages the functionality of Amazon API Gateway stages.</p><p>By modifying the existing API to add request validation and deploying the updated API to a new API Gateway stage, it allows for testing in a controlled environment before deploying to production. It provides the least operational overhead because it does not involve creating a new API or exporting/importing an OpenAPI file.</p><p><strong>CORRECT: </strong>\"Modify the existing API to include request validation, deploy this to a new API Gateway stage, test it, then deploy it to the production stage\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Clone the existing API, add the request validation, run the tests, then modify the original API to include request validation before deploying to production\" is incorrect.</p><p>Cloning the API or exporting it to an OpenAPI file and importing into a new API both involve unnecessary steps that increase operational overhead.</p><p><strong>INCORRECT:</strong> \"First export the current API to an OpenAPI file, create and modify a new API by importing the OpenAPI file and adding request validation, test it, then modify and deploy the original API\" is incorrect.</p><p>Cloning the API or exporting it to an OpenAPI file and importing into a new API both involve unnecessary steps that increase operational overhead.</p><p><strong>INCORRECT:</strong> \"Create a new API from scratch with the necessary resources, methods and request validation, run the tests, then modify and deploy the original API\" is incorrect.</p><p>Creating a new API from scratch can be time-consuming and requires extra effort which again adds to operational overhead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 3482,
                        "content": "<p>Modify the existing API to include request validation, deploy this to a new API Gateway stage, test it, then deploy it to the production stage.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3483,
                        "content": "<p>Clone the existing API, add the request validation, run the tests, then modify the original API to include request validation before deploying to production.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3484,
                        "content": "<p>Create a new API from scratch with the necessary resources, methods, and request validation, run the tests, then modify and deploy the original API.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3485,
                        "content": "<p>First export the current API to an OpenAPI file, create and modify a new API by importing the OpenAPI file and adding request validation, test it, then modify and deploy the original API.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 848,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.700Z",
                "updatedAt": "2023-09-07T08:51:32.700Z",
                "content": "<p>A development team are creating a mobile application that customers will use to receive notifications and special offers. Users will not be required to log in.</p><p>What is the MOST efficient method to grant users access to AWS resources?</p>",
                "answerExplanation": "<p>Amazon Cognito Identity Pools can support unauthenticated identities by providing a unique identifier and AWS credentials for users who do not authenticate with an identity provider. If your application allows users who do not log in, you can enable access for unauthenticated identities.</p><p>This is the most efficient and secure way to allow unauthenticated access as the process to set it up is simple and the IAM role can be configured with permissions allowing only the access permitted for unauthenticated users.</p><p><strong>CORRECT: </strong>\"Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an IAM SAML 2.0 identity provider to establish trust\" is incorrect as we need to allow unauthenticated users access to the AWS resources, not those who have been authenticated elsewhere (i.e. Active Directory).</p><p><strong>INCORRECT:</strong> \"Use Amazon Cognito Federated Identities and setup authentication using a Cognito User Pool\" is incorrect as we need to setup unauthenticated access, not authenticated access through a user pool.</p><p><strong>INCORRECT:</strong> \"Embed access keys in the application that have limited access to resources\" is incorrect. We should try and avoid embedding access keys in application code, it is better to use the built-in features of Amazon Cognito.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 3486,
                        "content": "<p>Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources</p>",
                        "isValid": true
                    },
                    {
                        "id": 3487,
                        "content": "<p>Use an IAM SAML 2.0 identity provider to establish trust</p>",
                        "isValid": false
                    },
                    {
                        "id": 3488,
                        "content": "<p>Use Amazon Cognito Federated Identities and setup authentication using a Cognito User Pool</p>",
                        "isValid": false
                    },
                    {
                        "id": 3489,
                        "content": "<p>Embed access keys in the application that have limited access to resources</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 849,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.770Z",
                "updatedAt": "2023-09-07T08:51:32.770Z",
                "content": "<p>A Developer has updated an AWS Lambda function and published a new version. To ensure the code is working as expected the Developer needs to initially direct a percentage of traffic to the new version and gradually increase this over time. It is important to be able to rollback if there are any issues reported.</p><p>What is the BEST way the Developer can implement the migration to the new version SAFELY?</p>",
                "answerExplanation": "<p>You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.</p><p>Each alias has a unique ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function. You can also use traffic shifting to direct a percentage of traffic to a specific version as showing in the image below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-15_06-09-45-2cf65f92dcd1f5200fff3aa6549539af.png\"></p><p>This is the recommended way to direct traffic to multiple function versions and shift traffic when testing code updated. Therefore, the best answer is to create an Alias, assign the current and new versions and use traffic shifting to assign a percentage of traffic to the new version.</p><p><strong>CORRECT: </strong>\"Create an Alias, assign the current and new versions and use traffic shifting to assign a percentage of traffic to the new version\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Route 53 weighted routing policy pointing to the current and new versions, assign a lower weight to the new version\" is incorrect. AWS Lambda endpoints are not DNS names that you can route to with Route 53. The best way to route traffic to multiple versions is using an alias.</p><p><strong>INCORRECT:</strong> \"Use an immutable update with a new ASG to deploy the new version in parallel, following testing cutover to the new version\" is incorrect as immutable updates are associated with Amazon Elastic Beanstalk and this service does not deploy updates to AWS Lambda.</p><p><strong>INCORRECT:</strong> \"Use an Amazon Elastic Load Balancer to direct a percentage of traffic to each target group containing the Lambda function versions\" is incorrect as this introduces an unnecessary layer (complexity and cost) to the architecture. The best choice is to use an alias instead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.amazonaws.cn/en_us/lambda/latest/dg/configuration-aliases.html\">https://docs.amazonaws.cn/en_us/lambda/latest/dg/configuration-aliases.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/aliases-intro.html\">https://docs.aws.amazon.com/lambda/latest/dg/aliases-intro.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-traffic-shifting-using-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-traffic-shifting-using-aliases.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3490,
                        "content": "<p>Use an Amazon Elastic Load Balancer to direct a percentage of traffic to each target group containing the Lambda function versions</p>",
                        "isValid": false
                    },
                    {
                        "id": 3491,
                        "content": "<p>Create an Amazon Route 53 weighted routing policy pointing to the current and new versions, assign a lower weight to the new version</p>",
                        "isValid": false
                    },
                    {
                        "id": 3492,
                        "content": "<p>Use an immutable update with a new ASG to deploy the new version in parallel, following testing cutover to the new version</p>",
                        "isValid": false
                    },
                    {
                        "id": 3493,
                        "content": "<p>Create an Alias, assign the current and new versions and use traffic shifting to assign a percentage of traffic to the new version</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 850,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.843Z",
                "updatedAt": "2023-09-07T08:51:32.843Z",
                "content": "<p>A Developer is trying to make API calls using AWS SDK. The IAM user credentials used by the application require multi-factor authentication for all API calls.</p><p>Which method should the Developer use to access the multi-factor authentication protected API?</p>",
                "answerExplanation": "<p>The GetSessionToken API call returns a set of temporary credentials for an AWS account or IAM user. The credentials consist of an access key ID, a secret access key, and a security token. Typically, you use GetSessionToken if you want to use MFA to protect programmatic calls to specific AWS API operations</p><p>Therefore, the Developer can use GetSessionToken with an MFA device to make secure API calls using the AWS SDK.</p><p><strong>CORRECT: </strong>\"<code>GetSessionToken</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"<code>GetFederationToken</code>\" is incorrect as this is used with federated users to return a set of temporary security credentials (consisting of an access key ID, a secret access key, and a security token).</p><p><strong>INCORRECT:</strong> \"<code>GetCallerIdentity</code>\" is incorrect as this API action returns details about the IAM user or role whose credentials are used to call the operation.</p><p><strong>INCORRECT:</strong> \"<code>DecodeAuthorizationMessage</code>\" is incorrect as this API action decodes additional information about the authorization status of a request from an encoded message returned in response to an AWS request.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 3494,
                        "content": "<p><code>GetFederationToken</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3495,
                        "content": "<p><code>GetSessionToken</code> </p>",
                        "isValid": true
                    },
                    {
                        "id": 3496,
                        "content": "<p><code>GetCallerIdentity</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3497,
                        "content": "<p><code>DecodeAuthorizationMessage</code> </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 851,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.910Z",
                "updatedAt": "2023-09-07T08:51:32.910Z",
                "content": "<p>A firm intends to utilize AWS CodeDeploy to deploy an application to Amazon Elastic Container Service (Amazon ECS). While deploying an updated version of the application, the company's initial requirement is to direct 10% of active traffic to the updated application version. Following a 15-minute interval, all remaining active traffic must be rerouted to the updated application.</p><p>Which predefined CodeDeploy configuration aligns with these needs?</p>",
                "answerExplanation": "<p>CodeDeployDefault.ECSCanary10Percent15Minutes is a predefined CodeDeploy deployment configuration that will first direct 10% of traffic to the newly deployed application and then, after 15 minutes, route the rest of the traffic to the new version. This fits perfectly with the company's requirements.</p><p><strong>CORRECT: </strong>\"CodeDeployDefault.ECSCanary10Percent15Minutes\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"CodeDeployDefault.ECSAllAtOnce\" is incorrect.</p><p>CodeDeployDefault.ECSAllAtOnce deploys the application revision to all instances at once. This does not meet the requirement to initially expose only 10% of live traffic to the new version.</p><p><strong>INCORRECT:</strong> \"CodeDeployDefault.LambdaLinear10PercentEvery1Minutes\" is incorrect.</p><p>CodeDeployDefault.LambdaLinear10PercentEvery1Minutes is for AWS Lambda deployments, not Amazon ECS deployments.</p><p><strong>INCORRECT:</strong> \"CodeDeployDefault.ECSLinear10PercentEvery10Minutes\" is incorrect.</p><p>CodeDeployDefault.ECSLinear10PercentEvery10Minutes increases the traffic routed to the new version by 10% every 10 minutes, not all at once after 15 minutes as per the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3498,
                        "content": "<p>CodeDeployDefault.ECSLinear10PercentEvery10Minutes</p>",
                        "isValid": false
                    },
                    {
                        "id": 3499,
                        "content": "<p>CodeDeployDefault.ECSAllAtOnce</p>",
                        "isValid": false
                    },
                    {
                        "id": 3500,
                        "content": "<p>CodeDeployDefault.LambdaLinear10PercentEvery1Minutes</p>",
                        "isValid": false
                    },
                    {
                        "id": 3501,
                        "content": "<p>CodeDeployDefault.ECSCanary10Percent15Minutes</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 852,
            "attributes": {
                "createdAt": "2023-09-07T08:51:32.987Z",
                "updatedAt": "2023-09-07T08:51:32.987Z",
                "content": "<p>A Developer manages a monitoring service for a fleet of IoT sensors in a major city. The monitoring application uses an Amazon Kinesis Data Stream with a group of EC2 instances processing the data. Amazon CloudWatch custom metrics show that the instances a reaching maximum processing capacity and there are insufficient shards in the Data Stream to handle the rate of data flow.</p><p>What course of action should the Developer take to resolve the performance issues?</p>",
                "answerExplanation": "<p>By increasing the instance size and number of shards in the Kinesis stream, the developer can allow the instances to handle more record processors, which are running in parallel within the instance. It also allows the stream to properly accommodate the rate of data being sent in. The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_01-15-12-8a5ce5fcbda31da7dd2c49bd66e22dcc.png\"></p><p>Therefore, the best answer is to increase both the EC2 instance size and add shards to the stream.</p><p><strong>CORRECT: </strong>\"Increase the EC2 instance size and add shards to the stream\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Increase the number of EC2 instances to match the number of shards\" is incorrect as you can have an individual instance running multiple KCL workers.</p><p><strong>INCORRECT:</strong> \"Increase the EC2 instance size\" is incorrect as the Developer would also need to add shards to the stream to increase the capacity of the stream.</p><p><strong>INCORRECT:</strong> \"Increase the number of open shards\" is incorrect as this does not include increasing the instance size or quantity which is required as they are running at capacity.</p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.partial.html\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.partial.html</a></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-kcl.html\">https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-kcl.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 3502,
                        "content": "<p>Increase the EC2 instance size</p>",
                        "isValid": false
                    },
                    {
                        "id": 3503,
                        "content": "<p>Increase the EC2 instance size and add shards to the stream</p>",
                        "isValid": true
                    },
                    {
                        "id": 3504,
                        "content": "<p>Increase the number of EC2 instances to match the number of shards</p>",
                        "isValid": false
                    },
                    {
                        "id": 3505,
                        "content": "<p>Increase the number of open shards</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 853,
            "attributes": {
                "createdAt": "2023-09-07T08:51:33.060Z",
                "updatedAt": "2023-09-07T08:51:33.060Z",
                "content": "<p>An application uses AWS Lambda to process many files. The Lambda function takes approximately 3 minutes to process each file and does not return any important data. A Developer has written a script that will invoke the function using the AWS CLI.</p><p>What is the FASTEST way to process all the files?</p>",
                "answerExplanation": "<p>You can invoke Lambda functions directly with the Lambda console, the Lambda API, the AWS SDK, the AWS CLI, and AWS toolkits.</p><p>You can also configure other AWS services to invoke your function, or you can configure Lambda to read from a stream or queue and invoke your function.</p><p>When you invoke a function, you can choose to invoke it synchronously or asynchronously.</p><p> â€¢ Synchronous invocation:</p><p>o You wait for the function to process the event and return a response.</p><p>o To invoke a function synchronously with the AWS CLI, use the invoke command.</p><p>o The Invocation-type can be used to specify a value of â€œRequestResponseâ€. This instructs AWS to execute your Lambda function and wait for the function to complete.</p><p> â€¢ Asynchronous invocation:</p><p>o When you invoke a function asynchronously, you donâ€™t wait for a response from the function code.</p><p>o For asynchronous invocation, Lambda handles retries and can send invocation records to a destination.</p><p>o To invoke a function asynchronously, set the invocation type parameter to Event.</p><p>The fastest way to process all the files is to use asynchronous invocation and process the files in parallel. To do this you should specify the invocation type of Event</p><p><strong>CORRECT: </strong>\"Invoke the Lambda function asynchronously with the invocation type Event and process the files in parallel\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Invoke the Lambda function synchronously with the invocation type Event and process the files in parallel\" is incorrect as the invocation type for a synchronous invocation should be <code>RequestResponse</code>.</p><p><strong>INCORRECT:</strong> \"Invoke the Lambda function synchronously with the invocation type <code>RequestResponse</code> and process the files sequentially\" is incorrect as this is not the fastest way of processing the files as Lambda will wait for completion of once file before moving on to the next one.</p><p><strong>INCORRECT:</strong> \"Invoke the Lambda function asynchronously with the invocation type <code>RequestResponse</code> and process the files sequentially\" is incorrect as the invocation type <code>RequestResponse</code> is used for synchronous invocations.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/architecture/understanding-the-different-ways-to-invoke-lambda-functions/\">https://aws.amazon.com/blogs/architecture/understanding-the-different-ways-to-invoke-lambda-functions/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3506,
                        "content": "<p>Invoke the Lambda function asynchronously with the invocation type <code>RequestResponse</code> and process the files sequentially</p>",
                        "isValid": false
                    },
                    {
                        "id": 3507,
                        "content": "<p>Invoke the Lambda function asynchronously with the invocation type Event and process the files in parallel</p>",
                        "isValid": true
                    },
                    {
                        "id": 3508,
                        "content": "<p>Invoke the Lambda function synchronously with the invocation type Event and process the files in parallel</p>",
                        "isValid": false
                    },
                    {
                        "id": 3509,
                        "content": "<p>Invoke the Lambda function synchronously with the invocation type <code>RequestResponse</code> and process the files sequentially</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 854,
            "attributes": {
                "createdAt": "2023-09-07T08:51:33.137Z",
                "updatedAt": "2023-09-07T08:51:33.137Z",
                "content": "<p>A company uses continuous integration and continuous delivery (CI/CD) systems. A Developer needs to automate the deployment of a software package to Amazon EC2 instances as well as to on-premises virtual servers.</p><p>Which AWS service can be used for the software deployment?</p>",
                "answerExplanation": "<p>CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.</p><p>CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy.</p><p>The image below shows the flow of a typical CodeDeploy in-place deployment.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_05-31-52-a3039c98d6bf643e653e943781107a7e.jpg\"></p><p>The above deployment could also be directed at on-premises servers. Therefore, the best answer is to use AWS CodeDeploy to deploy the software package to both EC2 instances and on-premises virtual servers.</p><p><strong>CORRECT: </strong>\"AWS CodeDeploy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodePipeline\" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. You can use CodeDeploy in a CodePipeline pipeline however it is actually CodeDeploy that deploys the software packages.</p><p><strong>INCORRECT:</strong> \"AWS CloudBuild\" is incorrect as this is a build tool, not a deployment tool.</p><p><strong>INCORRECT:</strong> \"AWS Elastic Beanstalk\" is incorrect as you cannot deploy software packages to on-premise virtual servers using Elastic Beanstalk</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3510,
                        "content": "<p>AWS CodePipeline</p>",
                        "isValid": false
                    },
                    {
                        "id": 3511,
                        "content": "<p>AWS Elastic Beanstalk</p>",
                        "isValid": false
                    },
                    {
                        "id": 3512,
                        "content": "<p>AWS CloudBuild</p>",
                        "isValid": false
                    },
                    {
                        "id": 3513,
                        "content": "<p>AWS CodeDeploy</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 855,
            "attributes": {
                "createdAt": "2023-09-07T08:51:33.211Z",
                "updatedAt": "2023-09-07T08:51:33.211Z",
                "content": "<p>A team of Developers have been assigned to a new project. The team will be collaborating on the development and delivery of a new application and need a centralized private repository for managing source code. The repository should support updates from multiple sources. Which AWS service should the development team use?</p>",
                "answerExplanation": "<p>CodeCommit is a secure, highly scalable, managed source control service that hosts private Git repositories. CodeCommit eliminates the need for you to manage your own source control system or worry about scaling its infrastructure.</p><p>You can use CodeCommit to store anything from code to binaries. It supports the standard functionality of Git, so it works seamlessly with your existing Git-based tools.</p><p>With CodeCommit, you can:</p><p><strong> </strong>â€¢ <strong>Benefit from a fully managed service hosted by AWS</strong>. CodeCommit provides high service availability and durability and eliminates the administrative overhead of managing your own hardware and software. There is no hardware to provision and scale and no server software to install, configure, and update.</p><p><strong> </strong>â€¢ <strong>Store your code securely</strong>. CodeCommit repositories are encrypted at rest as well as in transit.</p><p><strong> </strong>â€¢ <strong>Work collaboratively on code.</strong> CodeCommit repositories support pull requests, where users can review and comment on each other's code changes before merging them to branches; notifications that automatically send emails to users about pull requests and comments; and more.</p><p><strong> </strong>â€¢ <strong>Easily scale your version control projects</strong>. CodeCommit repositories can scale up to meet your development needs. The service can handle repositories with large numbers of files or branches, large file sizes, and lengthy revision histories.</p><p><strong> </strong>â€¢ <strong>Store anything, anytime</strong>. CodeCommit has no limit on the size of your repositories or on the file types you can store.</p><p><strong> </strong>â€¢ <strong>Integrate with other AWS and third-party services</strong>. CodeCommit keeps your repositories close to your other production resources in the AWS Cloud, which helps increase the speed and frequency of your development lifecycle. It is integrated with IAM and can be used with other AWS services and in parallel with other repositories. <strong>Easily migrate files from other remote repositories</strong>. You can migrate to CodeCommit from any Git-based repository.</p><p><strong> </strong>â€¢ <strong>Use the Git tools you already know</strong>. CodeCommit supports Git commands as well as its own AWS CLI commands and APIs.</p><p>Therefore, the development team should select AWS CodeCommit as the repository they use for storing code related to the new project.</p><p><strong>CORRECT: </strong>\"AWS CodeCommit\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodeBuild\" is incorrect. AWS CodeBuild is a fully managed continuous integration (CI) service that compiles source code, runs tests, and produces software packages that are ready to deploy.</p><p><strong>INCORRECT:</strong> \"AWS CodeDeploy\" is incorrect. CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.</p><p><strong>INCORRECT:</strong> \"AWS CodePipeline\" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3514,
                        "content": "<p>AWS CodeBuild</p>",
                        "isValid": false
                    },
                    {
                        "id": 3515,
                        "content": "<p>AWS CodeCommit</p>",
                        "isValid": true
                    },
                    {
                        "id": 3516,
                        "content": "<p>AWS CodePipeline</p>",
                        "isValid": false
                    },
                    {
                        "id": 3517,
                        "content": "<p>AWS CodeDeploy</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 856,
            "attributes": {
                "createdAt": "2023-09-07T08:51:33.283Z",
                "updatedAt": "2023-09-07T08:51:33.283Z",
                "content": "<p>A set of APIs are exposed to customers using Amazon API Gateway. These APIs have caching enabled on the API Gateway. Customers have asked for an option to invalidate this cache for each of the APIs.</p><p>What action can be taken to allow API customers to invalidate the API Cache?</p>",
                "answerExplanation": "<p>A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the Cache-Control: max-age=0 header.</p><p>The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.</p><p>Therefore, the company should ask customers to pass an HTTP header called<code> Cache-Control:max-age=0</code>.</p><p><strong>CORRECT: </strong>\"Ask customers to pass an HTTP header called <code>Cache-Control:max-age=0</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Ask customers to use AWS credentials to call the <code>InvalidateCache API</code>\" is incorrect as this API action is used to invalidate the cache but is not the method the clients use to invalidate the cache.</p><p><strong>INCORRECT:</strong> \"Ask customers to invoke an AWS API endpoint which invalidates the cache\" is incorrect as you donâ€™t invalidate the cache by invoking an endpoint, the HTTP header mentioned in the explanation is required.</p><p><strong>INCORRECT:</strong> \"Ask customers to add a query string parameter called <code>INVALIDATE_CACHE</code>â€ when making an API call\" is incorrect as this is not a valid method of invalidating an API Gateway cache.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 3518,
                        "content": "<p>Ask customers to pass an HTTP header called <code>Cache-Control:max-age=0</code> </p>",
                        "isValid": true
                    },
                    {
                        "id": 3519,
                        "content": "<p>Ask customers to invoke an AWS API endpoint which invalidates the cache</p>",
                        "isValid": false
                    },
                    {
                        "id": 3520,
                        "content": "<p>Ask customers to use AWS credentials to call the <code>InvalidateCache</code> API</p>",
                        "isValid": false
                    },
                    {
                        "id": 3521,
                        "content": "<p>Ask customers to add a query string parameter called <code>INVALIDATE_CACHE</code>â€ when making an API call</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 857,
            "attributes": {
                "createdAt": "2023-09-07T08:51:33.352Z",
                "updatedAt": "2023-09-07T08:51:33.352Z",
                "content": "<p>A developer is debugging an application by sifting through log data stored in Amazon CloudWatch Logs. A fresh metric filter has been established to identify exceptions in these logs. Yet, the logs are not returning any results filtered through the new metric.</p><p>What could be the reason behind the absence of filtered results?</p>",
                "answerExplanation": "<p>Amazon CloudWatch Logs starts to publish metric data points only for log events that are ingested after the metric filter is created. This means the filter does not apply retrospectively to log data ingested prior to its creation.</p><p><strong>CORRECT: </strong>\"CloudWatch Logs only publish metric data for events that occur after the filter has been established\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The CloudWatch Logs agent hasn't been installed on the EC2 instances\" is incorrect.</p><p>The CloudWatch Logs agent is responsible for sending logs to CloudWatch and does not need to be installed for filtering to work.</p><p><strong>INCORRECT:</strong> \"The application logs have been archived to Amazon S3, making them non-filterable\" is incorrect.</p><p>CloudWatch Logs can still filter logs even if they have been archived to Amazon S3.</p><p><strong>INCORRECT:</strong> \"The time range for the filter hasn't been properly configured\" is incorrect.</p><p>The time range for a metric filter is not relevant as it operates on new logs, regardless of their timestamp.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3522,
                        "content": "<p>The CloudWatch Logs agent hasn't been installed on the EC2 instances.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3523,
                        "content": "<p>CloudWatch Logs only publish metric data for events that occur after the filter has been established.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3524,
                        "content": "<p>The application logs have been archived to Amazon S3, making them non-filterable.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3525,
                        "content": "<p>The time range for the filter hasn't been properly configured.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 858,
            "attributes": {
                "createdAt": "2023-09-07T08:51:33.425Z",
                "updatedAt": "2023-09-07T08:51:33.425Z",
                "content": "<p>A serverless application is used to process customer information and outputs a JSON file to an Amazon S3 bucket. AWS Lambda is used for processing the data. The data is sensitive and should be encrypted.</p><p>How can a Developer modify the Lambda function to ensure the data is encrypted before it is uploaded to the S3 bucket?</p>",
                "answerExplanation": "<p>The <code>GenerateDataKey</code> API is used with the AWS KMS services and generates a unique symmetric data key. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.</p><p>For this scenario we can use <code>GenerateDataKey</code> to obtain an encryption key from KMS that we can then use within the function code to encrypt the file. This ensures that the file is encrypted BEFORE it is uploaded to Amazon S3.</p><p><strong>CORRECT: </strong>\"Use the <code>GenerateDataKey</code> API, then use the data key to encrypt the file using the Lambda code\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable server-side encryption on the S3 bucket and create a policy to enforce encryption\" is incorrect. This would not encrypt data before it is uploaded as S3 would only encrypt the data as it is written to storage.</p><p><strong>INCORRECT:</strong> \"Use the S3 managed key and call the <code>GenerateDataKey</code> API to encrypt the file\" is incorrect as you do not use an encryption key to call KMS. You call KMS with the <code>GenerateDataKey</code> API to obtain an encryption key. Also, the S3 managed key can only be used within the S3 service.</p><p><strong>INCORRECT:</strong> \"Use the default KMS key for S3 and encrypt the file using the Lambda code\" is incorrect. You cannot use the default KMS key for S3 within the Lambda code as it can only be used within the S3 service.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
                "options": [
                    {
                        "id": 3526,
                        "content": "<p>Use the <code>GenerateDataKey</code> API, then use the data key to encrypt the file using the Lambda code</p>",
                        "isValid": true
                    },
                    {
                        "id": 3527,
                        "content": "<p>Use the default KMS key for S3 and encrypt the file using the Lambda code</p>",
                        "isValid": false
                    },
                    {
                        "id": 3528,
                        "content": "<p>Use the S3 managed key and call the <code>GenerateDataKey</code> API to encrypt the file</p>",
                        "isValid": false
                    },
                    {
                        "id": 3529,
                        "content": "<p>Enable server-side encryption on the S3 bucket and create a policy to enforce encryption</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 859,
            "attributes": {
                "createdAt": "2023-09-07T08:51:33.495Z",
                "updatedAt": "2023-09-07T08:51:33.495Z",
                "content": "<p>A Developer has written some code that will connect and pull information from several hundred websites. The code needs to run on a daily schedule and execution time will be less than 60 seconds.</p><p>Which AWS service will be most suitable and cost-effective?</p>",
                "answerExplanation": "<p>AWS Lambda is a serverless service with a maximum execution time of 900 seconds. This will be the most suitable and cost-effective option for this use case. You can also schedule Lambda functions to run using Amazon CloudWatch Events.</p><p><strong>CORRECT: </strong>\"AWS Lambda\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon ECS Fargate\" is incorrect as this is used for running Docker containers and is a better fit for microservices applications rather than running code for a short period of time.</p><p><strong>INCORRECT:</strong> \"Amazon EC2\" is incorrect as this would require running EC2 instances which would not be cost-effective.</p><p><strong>INCORRECT:</strong> \"Amazon API Gateway\" is incorrect as this service is used for creating APIs, not running code.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3530,
                        "content": "<p>AWS Lambda</p>",
                        "isValid": true
                    },
                    {
                        "id": 3531,
                        "content": "<p>Amazon EC2</p>",
                        "isValid": false
                    },
                    {
                        "id": 3532,
                        "content": "<p>Amazon API Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 3533,
                        "content": "<p>Amazon ECS Fargate</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 860,
            "attributes": {
                "createdAt": "2023-09-07T08:51:33.570Z",
                "updatedAt": "2023-09-07T08:51:33.570Z",
                "content": "<p>An application running on a fleet of EC2 instances use the AWS SDK for Java to copy files into several AWS buckets using access keys stored in environment variables. A Developer has modified the instances to use an assumed IAM role with a more restrictive policy that allows access to only one bucket.</p><p>However, after applying the change the Developer logs into one of the instances and is still able to write to all buckets. What is the MOST likely explanation for this situation?</p>",
                "answerExplanation": "<p>When you initialize a new service client without supplying any arguments, the AWS SDK for Java attempts to find AWS credentials by using the <em>default credential provider chain</em> implemented by the <a href=\"https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html?com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html\">DefaultAWSCredentialsProviderChain</a> class. The default credential provider chain looks for credentials in this order:</p><p><strong>&nbsp; 1. Environment variables</strong>â€“AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. The AWS SDK for Java uses the <a href=\"https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html?com/amazonaws/auth/EnvironmentVariableCredentialsProvider.html\">EnvironmentVariableCredentialsProvider</a> class to load these credentials.</p><p><strong> 2. Java system properties</strong>â€“aws.accessKeyId and aws.secretKey. The AWS SDK for Java uses the <a href=\"https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html?com/amazonaws/auth/SystemPropertiesCredentialsProvider.html\">SystemPropertiesCredentialsProvider</a> to load these credentials.</p><p><strong> 3. The default credential profiles file</strong>â€“ typically located at ~/.aws/credentials (location can vary per platform) and shared by many of the AWS SDKs and by the AWS CLI. The AWS SDK for Java uses the <a href=\"https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html?com/amazonaws/auth/profile/ProfileCredentialsProvider.html\">ProfileCredentialsProvider</a> to load these credentials.</p><p><strong> 4. Amazon ECS container credentials</strong>â€“ loaded from the Amazon ECS if the environment variable AWS_CONTAINER_CREDENTIALS_RELATIVE_URI is set. The AWS SDK for Java uses the <a href=\"https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html?com/amazonaws/auth/ContainerCredentialsProvider.html\">ContainerCredentialsProvider</a> to load these credentials. You can specify the IP address for this value.</p><p><strong> 5. Instance profile credentials</strong>â€“ used on EC2 instances and delivered through the Amazon EC2 metadata service. The AWS SDK for Java uses the <a href=\"https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html?com/amazonaws/auth/InstanceProfileCredentialsProvider.html\">InstanceProfileCredentialsProvider</a> to load these credentials. You can specify the IP address for this value.</p><p>Therefore, the AWS SDK for Java will find the credentials stored in environment variables before it checks for instance provide credentials and will allow access to the extra S3 buckets.</p><p><strong>NOTE:</strong> The Default Credential Provider Chain is very similar for other SDKs and the CLI as well. Check the references below for an article showing the steps for the AWS CLI.</p><p><strong>CORRECT: </strong>\"The AWS credential provider looks for instance profile credentials last\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"An IAM inline policy is being used on the IAM role\" is incorrect. If an inline policy was also applied to the role with a less restrictive policy it wouldnâ€™t matter, as the most restrictive policy would be applied.</p><p><strong>INCORRECT:</strong> \"An IAM managed policy is being used on the IAM role\" is incorrect. Though the managed policies are less restrictive by default (read-only or full access), this is not the most likely cause of the situation as we were told the policy is more restrictive and we know the environments variables have access keys in them which will be used before the policy is checked.</p><p><strong>INCORRECT:</strong> \"The AWS CLI is corrupt and needs to be reinstalled\" is incorrect. There is a plausible explanation for this situation so no reason to suspect a software bug is to blame.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html\">https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html</a></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\">https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html</a></p>",
                "options": [
                    {
                        "id": 3534,
                        "content": "<p>The AWS credential provider looks for instance profile credentials last</p>",
                        "isValid": true
                    },
                    {
                        "id": 3535,
                        "content": "<p>An IAM inline policy is being used on the IAM role</p>",
                        "isValid": false
                    },
                    {
                        "id": 3536,
                        "content": "<p>An IAM managed policy is being used on the IAM role</p>",
                        "isValid": false
                    },
                    {
                        "id": 3537,
                        "content": "<p>The AWS CLI is corrupt and needs to be reinstalled</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 861,
            "attributes": {
                "createdAt": "2023-09-07T08:51:33.647Z",
                "updatedAt": "2023-09-07T08:51:33.647Z",
                "content": "<p>A Developer needs to write some code to invoke an AWS Lambda function using the AWS Command Line Interface (CLI). Which option must be specified to cause the function to be invoked asynchronously?&nbsp; &nbsp;</p>",
                "answerExplanation": "<p>Several AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Simple Notification Service (Amazon SNS), invoke functions asynchronously to process events.</p><p>When you invoke a function asynchronously, you don't wait for a response from the function code. You hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors and can send invocation records to a downstream resource to chain together components of your application.</p><p>The following code snippet is an example of invoking the â€œmy-functionâ€ function asynchronously:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_06-39-01-4546fe267322b840d7ce8d9fb05c69cb.jpg\"></p><p>The Developer will therefore need to set the <code>â€“invocation-type</code> option to <code>Event</code>.</p><p><strong>CORRECT: </strong>\"Set the <code>â€“invocation-type</code> option to <code>Event</code> \" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Set the <code>â€“invocation-type</code> option to <code>Invoke</code>\" is incorrect as this is not valid value for this option.</p><p><strong>INCORRECT:</strong> \"Set the <code>â€“payload</code> option to <code>Asynchronou</code>s\" is incorrect as this option is used to provide the JSON blob that you want to provide to your Lambda function as input. You cannot supply â€œasynchronousâ€ as a value.</p><p><strong>INCORRECT:</strong> \"Set the <code>â€“qualifier </code>option to <code>Asynchronous</code>\" is incorrect as this is used to specify a version or alias to invoke a published version of the function. You cannot supply â€œasynchronousâ€ as a value.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html</a></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/lambda/invoke.html\">https://docs.aws.amazon.com/cli/latest/reference/lambda/invoke.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3538,
                        "content": "<p>Set the <code>â€“payload</code> option to <code>Asynchronous</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3539,
                        "content": "<p>Set the<code> â€“qualifier</code> option to <code>Asynchronous</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3540,
                        "content": "<p>Set the â€“invocation-type option to <code>Invoke</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3541,
                        "content": "<p>Set the <code>â€“invocation-type</code> option to <code>Event</code> </p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 862,
            "attributes": {
                "createdAt": "2023-09-07T08:51:33.719Z",
                "updatedAt": "2023-09-07T08:51:33.719Z",
                "content": "<p>An AWS Lambda function requires several environment variables with secret values. The secret values should be obscured in the Lambda console and API output even for users who have permission to use the key.</p><p>What is the best way to achieve this outcome and MINIMIZE complexity and latency?</p>",
                "answerExplanation": "<p>You can use environment variables to store secrets securely for use with Lambda functions. Lambda always encrypts environment variables at rest.</p><p>Additionally, you can use the following features to customize how environment variables are encrypted.</p><p><strong> </strong>â€¢ <strong>Key configuration</strong> â€“ On a per-function basis, you can configure Lambda to use an encryption key that you create and manage in AWS Key Management Service. These are referred to as <em>customer managed</em> customer master keys (CMKs) or customer managed keys. If you don't configure a customer managed key, Lambda uses an AWS managed CMK named aws/lambda, which Lambda creates in your account.</p><p><strong> </strong>â€¢ <strong>Encryption helpers</strong> â€“ The Lambda console lets you encrypt environment variable values client side, before sending them to Lambda. This enhances security further by preventing secrets from being displayed unencrypted in the Lambda console, or in function configuration that's returned by the Lambda API. The console also provides sample code that you can adapt to decrypt the values in your function handler.</p><p>The configuration for using encryption helps to encrypt data client-side looks like this:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_00-50-15-1e05fef0cb751da67a515a336b9ce91c.jpg\"></p><p>This is the best way to achieve this outcome and minimizes complexity as the encryption infrastructure will still use AWS KMS and be able to decrypt the values during function execution.</p><p><strong>CORRECT: </strong>\"Encrypt the secret values client-side using encryption helpers\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Encrypt the secret values with a customer-managed CMK\" is incorrect as this alone will not achieve the desired outcome as the environment variables should be encrypted client-side with the encryption helper to ensure users cannot see the secret values.</p><p><strong>INCORRECT:</strong> \"Store the encrypted values in an encrypted Amazon S3 bucket and reference them from within the code\" is incorrect as this would introduce complexity and latency.</p><p><strong>INCORRECT:</strong> \"Use an external encryption infrastructure to encrypt the values and add them as environment variables\" is incorrect as this would introduce complexity and latency.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/security-dataprotection.html\">https://docs.aws.amazon.com/lambda/latest/dg/security-dataprotection.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3542,
                        "content": "<p>Encrypt the secret values client-side using encryption helpers</p>",
                        "isValid": true
                    },
                    {
                        "id": 3543,
                        "content": "<p>Use an external encryption infrastructure to encrypt the values and add them as environment variables</p>",
                        "isValid": false
                    },
                    {
                        "id": 3544,
                        "content": "<p>Store the encrypted values in an encrypted Amazon S3 bucket and reference them from within the code</p>",
                        "isValid": false
                    },
                    {
                        "id": 3545,
                        "content": "<p>Encrypt the secret values with a customer-managed CMK</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 863,
            "attributes": {
                "createdAt": "2023-09-07T08:51:33.792Z",
                "updatedAt": "2023-09-07T08:51:33.792Z",
                "content": "<p>A static website is hosted on Amazon S3 using the bucket name of dctlabs.com. Some HTML pages on the site use JavaScript to download images that are located in the bucket https://dctlabsimages.s3.amazonaws.com/. Users have reported that the images are not being displayed.</p><p>What is the MOST likely cause?</p>",
                "answerExplanation": "<p>Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.</p><p>To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operation-specific information.</p><p>In this case, you would apply the CORS configuration to the dctlabsimages bucket so that it will allow GET requests from the dctlabs.com origin.</p><p><strong>CORRECT: </strong>\"Cross Origin Resource Sharing is not enabled on the dctlabsimages bucket\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Cross Origin Resource Sharing is not enabled on the dctlabs.com bucket\" is incorrect as in this case the images that are being blocked are located in the dctlabsimages bucket. You need to apply the CORS configuration to the dctlabsimages bucket so it allows requests from the dctlabs.com origin.</p><p><strong>INCORRECT:</strong> \"The dctlabsimages bucket is not in the same region as the dctlabs.com bucket\" is incorrect as it doesnâ€™t matter what regions the buckets are in.</p><p><strong>INCORRECT:</strong> \"Amazon S3 Transfer Acceleration should be enabled on the dctlabs.com bucket\" is incorrect as this feature of Amazon S3 is used to speed uploads to S3.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 3546,
                        "content": "<p>The dctlabsimages bucket is not in the same region as the dctlabs.com bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 3547,
                        "content": "<p>Cross Origin Resource Sharing is not enabled on the dctlabs.com bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 3548,
                        "content": "<p>Amazon S3 Transfer Acceleration should be enabled on the dctlabs.com bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 3549,
                        "content": "<p>Cross Origin Resource Sharing is not enabled on the dctlabsimages bucket</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 864,
            "attributes": {
                "createdAt": "2023-09-07T08:51:33.864Z",
                "updatedAt": "2023-09-07T08:51:33.864Z",
                "content": "<p>A three tier web application has been deployed on Amazon EC2 instances using Amazon EC2 Auto Scaling. The EC2 instances in the web tier sometimes receive bursts of traffic and the application tier cannot scale fast enough to keep up with messages sometimes resulting in message loss.</p><p>How can a Developer decouple the application to prevent loss of messages?</p>",
                "answerExplanation": "<p>Amazon SQS queues messages received from one application component ready for consumption by another component. A queue is a temporary repository for messages that are awaiting processing. The queue acts as a buffer between the component producing and saving data, and the component receiving the data for processing.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_01-27-18-4a4a844583848e547ddd2db3bd8ca529.png\"></p><p>With this scenario the best choice for the Developer is to implement an Amazon SQS queue between the web tier and the application tier. This will mean when the web tier receives bursts of traffic the messages will not overburden the application tier. Instead, they will be placed in the queue and can be processed by the app tier.</p><p><strong>CORRECT: </strong>\"Add an Amazon SQS queue between the web tier and the application tier\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Add an Amazon SQS queue between the application tier and the database tier\" is incorrect as the burst of messages are being received by the web tier and it is the application tier that is having difficulty keeping up with demand.</p><p><strong>INCORRECT:</strong> \"Configure the web tier to publish messages to an SNS topic and subscribe the application tier to the SNS topic\" is incorrect as SNS is used for notifications and those notifications are not queued, they are sent to all subscribers. The messages being passed in this scenario are better suited to being placed in a queue.</p><p><strong>INCORRECT:</strong> \"Migrate the database tier to Amazon DynamoDB and enable scalable session handling\" is incorrect as this is of no relevance to the situation. We donâ€™t know what type of database is being used and there is not stated issue with the database layer.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3550,
                        "content": "<p>Migrate the database tier to Amazon DynamoDB and enable scalable session handling</p>",
                        "isValid": false
                    },
                    {
                        "id": 3551,
                        "content": "<p>Configure the web tier to publish messages to an SNS topic and subscribe the application tier to the SNS topic</p>",
                        "isValid": false
                    },
                    {
                        "id": 3552,
                        "content": "<p>Add an Amazon SQS queue between the application tier and the database tier</p>",
                        "isValid": false
                    },
                    {
                        "id": 3553,
                        "content": "<p>Add an Amazon SQS queue between the web tier and the application tier</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 865,
            "attributes": {
                "createdAt": "2023-09-07T08:51:33.937Z",
                "updatedAt": "2023-09-07T08:51:33.937Z",
                "content": "<p>A Development team is creating a microservices application running on Amazon ECS. The release process workflow of the application requires a manual approval step before the code is deployed into the production environment.<br>What is the BEST way to achieve this using AWS CodePipeline?</p>",
                "answerExplanation": "<p>In AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_05-45-49-bdddf1e8fd7431925591106af878002f.jpg\"></p><p>If the action is approved, the pipeline execution resumes. If the action is rejectedâ€”or if no one approves or rejects the action within seven days of the pipeline reaching the action and stoppingâ€”the result is the same as an action failing, and the pipeline execution does not continue.</p><p>In this scenario, the manual approval stage would be placed in the pipeline before the deployment stage that deploys the application update into production:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_05-46-36-62ec436b0885d337fd3a5f88e8127f0e.jpg\"></p><p>Therefore, the best answer is to use an approval action in a stage before deployment to production</p><p><strong>CORRECT: </strong>\"Use an approval action in a stage before deployment\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an Amazon SNS notification from the deployment stage\" is incorrect as this would send a notification when the actual deployment is already occurring.</p><p><strong>INCORRECT:</strong> \"Disable the stage transition to allow manual approval\" is incorrect as this requires manual intervention as could be easily missed and allow the deployment to continue.</p><p><strong>INCORRECT:</strong> \"Disable a stage just prior the deployment stage\" is incorrect as disabling the stage prior would prevent that stage from running, which may be necessary (could be the build / test stage). It is better to use an approval action in a stage in the pipeline before the deployment occurs</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3554,
                        "content": "<p>Use an approval action in a stage before deployment</p>",
                        "isValid": true
                    },
                    {
                        "id": 3555,
                        "content": "<p>Disable the stage transition to allow manual approval</p>",
                        "isValid": false
                    },
                    {
                        "id": 3556,
                        "content": "<p>Use an Amazon SNS notification from the deployment stage</p>",
                        "isValid": false
                    },
                    {
                        "id": 3557,
                        "content": "<p>Disable a stage just prior the deployment stage</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 866,
            "attributes": {
                "createdAt": "2023-09-07T08:51:34.019Z",
                "updatedAt": "2023-09-07T08:51:34.019Z",
                "content": "<p>A company is in the process of migrating an application from a monolithic architecture to a microservices-based architecture. The developers need to refactor the application so that the many microservices can asynchronously communicate with each other in a decoupled manner.</p><p>Which AWS services can be used for asynchronous message passing? (Select TWO.)</p>",
                "answerExplanation": "<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.</p><p>Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.</p><p>These services both enable asynchronous message passing in the form of a message bus (SQS) and notifications (SNS).</p><p><strong>CORRECT: </strong>\"Amazon SQS\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Amazon SNS\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon Kinesis\" is incorrect. Kinesis is used for streaming data, it is used for real-time analytics, mobile data capture and IoT and similar use cases.</p><p><strong>INCORRECT:</strong> \"Amazon ECS\" is incorrect. ECS is a service providing Docker containers on Amazon EC2.</p><p><strong>INCORRECT:</strong> \"AWS Lambda\" is incorrect. AWS Lambda is a compute service that runs functions in response to triggers.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p><p><a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3558,
                        "content": "<p>Amazon ECS</p>",
                        "isValid": false
                    },
                    {
                        "id": 3559,
                        "content": "<p>Amazon Kinesis</p>",
                        "isValid": false
                    },
                    {
                        "id": 3560,
                        "content": "<p>Amazon SNS</p>",
                        "isValid": true
                    },
                    {
                        "id": 3561,
                        "content": "<p>AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 3562,
                        "content": "<p>Amazon SQS</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 867,
            "attributes": {
                "createdAt": "2023-09-07T08:51:34.091Z",
                "updatedAt": "2023-09-07T08:51:34.091Z",
                "content": "<p>A company is providing APIs as a web-based service to allow anonymous access to daily updated statistical data, using Amazon API Gateway and AWS Lambda for API development. The service's popularity has grown, and the company aims to improve the API responsiveness.</p><p>What measure should the company undertake to fulfill this objective?</p>",
                "answerExplanation": "<p>Enabling caching in API Gateway allows the service to cache the endpoint's responses, which improves the performance of the APIs by reducing the number of calls made to the endpoint and by improving the latency of the requests.</p><p><strong>CORRECT: </strong>\"Activate caching in API Gateway\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Set up API Gateway with an interface VPC endpoint\" is incorrect.</p><p>Configuring API Gateway to use an interface VPC endpoint helps in enhancing the security and privacy of the data being exchanged with the service. It does not directly affect the API responsiveness.</p><p><strong>INCORRECT:</strong> \"Cache API results in an Amazon ElastiCache cluster\" is incorrect.</p><p>Amazon ElastiCache can be used for caching data from databases but is not suitable for caching API results for API gateway.</p><p><strong>INCORRECT:</strong> \"Set up API keys and usage plans in API Gateway\" is incorrect.</p><p>Configuring usage plans and API keys in API Gateway can help control and manage usage of the APIs. However, they don't directly improve API responsiveness.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 3563,
                        "content": "<p>Activate caching in API Gateway.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3564,
                        "content": "<p>Set up API Gateway with an interface VPC endpoint.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3565,
                        "content": "<p>Cache API results in an Amazon ElastiCache cluster.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3566,
                        "content": "<p>Set up API keys and usage plans in API Gateway.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 868,
            "attributes": {
                "createdAt": "2023-09-07T08:51:34.163Z",
                "updatedAt": "2023-09-07T08:51:34.163Z",
                "content": "<p>A developer needs to implement a caching layer in front of an Amazon RDS database. If the caching layer fails, it is time consuming to repopulate cached data so the solution should be designed for maximum uptime. Which solution is best for this scenario?</p>",
                "answerExplanation": "<p>Amazon ElastiCache provides fully managed implementations of two popular in-memory data stores â€“ Redis and Memcached. ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis protocol-compliant server nodes in the cloud.</p><p>The in-memory caching provided by ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads or compute-intensive workloads. It is common to use ElastiCache as a cache in front of databases such as Amazon RDS.</p><p>The two implementations, Memcached, and Redis, each offer different capabilities and limitations. As you can see from the table below, only Redis supports read replicas and auto-failover:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_01-07-09-cc41c877a0a4bc3bd1f3da242b9ac912.png\">The Redis implementation must be used if high availability is required, as is necessary for this scenario. Therefore the correct answer is to use Amazon ElastiCache Redis.</p><p><strong>CORRECT: </strong>\"Implement Amazon ElastiCache Redis\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement Amazon ElastiCache Memcached\" is incorrect as Memcached does not offer read replicas or auto-failover and therefore cannot provide high availability.</p><p><strong>INCORRECT:</strong> \"Migrate the database to Amazon RedShift\" is incorrect as RedShift is a data warehouse for use in online analytics processing (OLAP) use cases. It is not suitable to be used as a caching layer.</p><p><strong>INCORRECT:</strong> \"Implement Amazon DynamoDB DAX\" is incorrect as DAX is used in front of DynamoDB, not Amazon RDS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 3567,
                        "content": "<p>Migrate the database to Amazon RedShift</p>",
                        "isValid": false
                    },
                    {
                        "id": 3568,
                        "content": "<p>Implement Amazon DynamoDB DAX</p>",
                        "isValid": false
                    },
                    {
                        "id": 3569,
                        "content": "<p>Implement Amazon ElastiCache Memcached</p>",
                        "isValid": false
                    },
                    {
                        "id": 3570,
                        "content": "<p>Implement Amazon ElastiCache Redis</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 869,
            "attributes": {
                "createdAt": "2023-09-07T08:51:34.240Z",
                "updatedAt": "2023-09-07T08:51:34.240Z",
                "content": "<p>A Developer needs to configure an Elastic Load Balancer that is deployed through AWS Elastic Beanstalk. Where should the Developer place the load-balancer.config file in the application source bundle?</p>",
                "answerExplanation": "<p>You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains.</p><p>Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle.</p><p>For example, you could include a configuration file for setting the load balancer type into:</p><p><em>.ebextensions/load-balancer.config</em></p><p>This example makes a simple configuration change. It modifies a configuration option to set the type of your environment's load balancer to Network Load Balancer:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_05-53-05-6f0748e67551ee9e84c02f1e6f700902.jpg\"></p><p><strong>Requirements</strong></p><p><strong> </strong>â€¢&nbsp; <strong>Location</strong> â€“ Place all of your configuration files in a single folder, named .ebextensions, in the root of your source bundle. Folders starting with a dot can be hidden by file browsers, so make sure that the folder is added when you create your source bundle.</p><p><strong> </strong>â€¢&nbsp; <strong>Naming</strong> â€“ Configuration files must have the .config file extension.</p><p><strong> </strong>â€¢&nbsp; <strong>Formatting</strong> â€“ Configuration files must conform to YAML or JSON specifications.</p><p><strong> </strong>â€¢&nbsp; <strong>Uniqueness</strong> â€“ Use each key only once in each configuration file.</p><p>Therefore, the Developer should place the file in the .ebextensions folder in the application source bundle.</p><p><strong>CORRECT: </strong>\"In the .ebextensions folder\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"In the root of the source code\" is incorrect. You need to place .config files in the .ebextensions folder.</p><p><strong>INCORRECT:</strong> \"In the bin folder\" is incorrect. You need to place .config files in the .ebextensions folder.</p><p><strong>INCORRECT:</strong> \"In the load-balancer.config.root\" is incorrect. You need to place .config files in the .ebextensions folder.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
                "options": [
                    {
                        "id": 3571,
                        "content": "<p>In the bin folder</p>",
                        "isValid": false
                    },
                    {
                        "id": 3572,
                        "content": "<p>In the root of the source code</p>",
                        "isValid": false
                    },
                    {
                        "id": 3573,
                        "content": "<p>In the .ebextensions folder</p>",
                        "isValid": true
                    },
                    {
                        "id": 3574,
                        "content": "<p>In the load-balancer.config.root</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 870,
            "attributes": {
                "createdAt": "2023-09-07T08:51:34.319Z",
                "updatedAt": "2023-09-07T08:51:34.319Z",
                "content": "<p>A web application is using Amazon Kinesis Data Streams for ingesting IoT data that is then stored before processing for up to 24 hours.<br>How can the Developer implement encryption at rest for data stored in Amazon Kinesis Data Streams?</p>",
                "answerExplanation": "<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p><p>Server-side encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it's written to the Kinesis stream storage layer and decrypted after itâ€™s retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. This allows you to meet strict regulatory requirements and enhance the security of your data.</p><p>With server-side encryption, your Kinesis stream producers and consumers don't need to manage master keys or cryptographic operations. Your data is automatically encrypted as it enters and leaves the Kinesis Data Streams service, so your data at rest is encrypted. AWS KMS provides all the master keys that are used by the server-side encryption feature. AWS KMS makes it easy to use a CMK for Kinesis that is managed by AWS, a user-specified AWS KMS CMK, or a master key imported into the AWS KMS service.</p><p>Therefore, in this scenario the Developer can enable server-side encryption on Kinesis Data Streams with an AWS KMS CMK</p><p><strong>CORRECT: </strong>\"Enable server-side encryption on Kinesis Data Streams with an AWS KMS CMK\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Add a certificate and enable SSL/TLS connections to Kinesis Data Streams\" is incorrect as SSL/TLS is already used with Kinesis (you donâ€™t need to add a certificate) and this only provides encryption in-transit, not encryption at rest.</p><p><strong>INCORRECT:</strong> \"Use the Amazon Kinesis Consumer Library (KCL) to encrypt the data\" is incorrect. The KCL provides design patterns and code for Amazon Kinesis Data Streams consumer applications. The KCL is not used for adding encryption to the data in a stream.</p><p><strong>INCORRECT:</strong> \"Encrypt the data once it is at rest with an AWS Lambda function\" is incorrect as this is unnecessary when Kinesis natively supports server-side encryption.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html\">https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 3575,
                        "content": "<p>Encrypt the data once it is at rest with an AWS Lambda function</p>",
                        "isValid": false
                    },
                    {
                        "id": 3576,
                        "content": "<p>Enable server-side encryption on Kinesis Data Streams with an AWS KMS CMK</p>",
                        "isValid": true
                    },
                    {
                        "id": 3577,
                        "content": "<p>Use the Amazon Kinesis Consumer Library (KCL) to encrypt the data</p>",
                        "isValid": false
                    },
                    {
                        "id": 3578,
                        "content": "<p>Add a certificate and enable SSL/TLS connections to Kinesis Data Streams</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 871,
            "attributes": {
                "createdAt": "2023-09-07T08:51:34.398Z",
                "updatedAt": "2023-09-07T08:51:34.398Z",
                "content": "<p>A company is using Amazon CloudFront to provide low-latency access to a web application to its global users. The organization must encrypt all traffic between users and CloudFront, and all traffic between CloudFront and the web application.</p><p>How can these requirements be met? (Select TWO.)</p>",
                "answerExplanation": "<p>This scenario requires encryption of in-flight data which can be done by implementing HTTPS. To do this the organization must configure the Origin Protocol Policy and the Viewer Protocol Policy on the CloudFront Distribution.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_06-45-16-9759ea1c1d03ff97a61072a087e68431.jpg\"></p><p>The Origin Protocol Policy can be used to select whether you want CloudFront to connect to your origin using only HTTP, only HTTPS, or to connect by matching the protocol used by the viewer. For example, if you select Match Viewer for the Origin Protocol Policy, and if the viewer connects to CloudFront using HTTPS, CloudFront will connect to your origin using HTTPS.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_06-45-36-4f33e89728df840567353aa59b2e4b74.jpg\"></p><p>If you want CloudFront to allow viewers to access your web content using either HTTP or HTTPS, specify HTTP and HTTPS. If you want CloudFront to redirect all HTTP requests to HTTPS, specify Redirect HTTP to HTTPS. If you want CloudFront to require HTTPS, specify HTTPS Only.</p><p><strong>CORRECT: </strong>â€œSet the Origin Protocol Policy to â€œHTTPS Onlyâ€â€ is a correct answer.</p><p><strong>CORRECT: </strong>â€œSet the Viewer Protocol Policy to â€œHTTPS Onlyâ€ or â€œRedirect HTTP to HTTPSâ€â€ is also a correct answer.</p><p><strong>INCORRECT:</strong> â€œUse AWS KMS to encrypt traffic between CloudFront and the web applicationâ€ is incorrect as KMS is used for encrypting data at rest.</p><p><strong>INCORRECT:</strong> â€œSet the Originâ€™s HTTP Port to 443â€ is incorrect as you must configure the origin protocol policy to HTTPS. The HTTPS port should be set to 443.</p><p><strong>INCORRECT:</strong> â€œEnable the CloudFront option Restrict Viewer Accessâ€ is incorrect as this is used to configure whether you want CloudFront to require users to access your content using a signed URL or a signed cookie.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 3579,
                        "content": "<p>Use AWS KMS to encrypt traffic between CloudFront and the web application</p>",
                        "isValid": false
                    },
                    {
                        "id": 3580,
                        "content": "<p>Set the Viewer Protocol Policy to â€œHTTPS Onlyâ€ or â€œRedirect HTTP to HTTPSâ€</p>",
                        "isValid": true
                    },
                    {
                        "id": 3581,
                        "content": "<p>Set the Origin Protocol Policy to â€œHTTPS Onlyâ€</p>",
                        "isValid": true
                    },
                    {
                        "id": 3582,
                        "content": "<p>Enable the CloudFront option Restrict Viewer Access</p>",
                        "isValid": false
                    },
                    {
                        "id": 3583,
                        "content": "<p>Set the Originâ€™s HTTP Port to 443</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 872,
            "attributes": {
                "createdAt": "2023-09-07T08:51:34.477Z",
                "updatedAt": "2023-09-07T08:51:34.477Z",
                "content": "<p>A Developer recently created an Amazon DynamoDB table. The table has the following configuration:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_06-24-26-3049820bc84172af5842585123d278c9.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--2kvh_\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_06-24-26-3049820bc84172af5842585123d278c9.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-large ud-btn-link ud-heading-md open-full-size-image--backdrop--20cbM\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span></p><p>The Developer attempted to add two items for userid â€œuser0001â€ with unique timestamps and received an error for the second item stating: â€œThe conditional request failedâ€.</p><p>What MUST the Developer do to resolve the issue?</p><p><br></p>",
                "answerExplanation": "<p>DynamoDB stores and retrieves data based on a Primary key. There are two types of Primary key:</p><p> â€¢Partition key â€“ unique attribute (e.g. user ID).</p><p> â€¢ Value of the Partition key is input to an internal hash function which determines the partition or physical location on which the data is stored.</p><p> â€¢ If you are using the Partition key as your Primary key, then no two items can have the same partition key.</p><p>Composite key â€“ Partition key + Sort key in combination.</p><p>â€¢ Example is user posting to a forum. Partition key would be the user ID, Sort key would be the timestamp of the post.</p><p> â€¢ 2 items may have the same Partition key, but they must have a different Sort key.</p><p> â€¢ All items with the same Partition key are stored together, then sorted according to the Sort key value.</p><p> â€¢ Allows you to store multiple items with the same partition key.</p><p>As stated above, if using a partition key alone as per the configuration provided with the question, then you cannot have two items with the same partition key. The only resolution is to recreate the table with a composite key consisting of the userid and timestamp attributes. In that case the Developer will be able to add multiple items with the same userid as long as the timestamp is unique.</p><p><strong>CORRECT: </strong>\"Recreate the table with a composite key consisting of userid and timestamp\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Update the table with a primary sort key for the timestamp attribute\" is incorrect as you cannot update the table in this case, it must be recreated.</p><p><strong>INCORRECT:</strong> \"Add a local secondary index (LSI) for the timestamp attribute\" is incorrect as the Developer will still not be able to add multiple entries to the main table for the same userid.</p><p><strong>INCORRECT:</strong> \"Use the SDK to add the items\" is incorrect as it doesnâ€™t matter whether you use the console, CLI or SDK, the conditional update will still fail with this configuration.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3584,
                        "content": "<p>Add a local secondary index (LSI) for the timestamp attribute</p>",
                        "isValid": false
                    },
                    {
                        "id": 3585,
                        "content": "<p>Recreate the table with a composite key consisting of userid and timestamp</p>",
                        "isValid": true
                    },
                    {
                        "id": 3586,
                        "content": "<p>Use the SDK to add the items</p>",
                        "isValid": false
                    },
                    {
                        "id": 3587,
                        "content": "<p>Update the table with a primary sort key for the timestamp attribute</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 873,
            "attributes": {
                "createdAt": "2023-09-07T08:51:34.553Z",
                "updatedAt": "2023-09-07T08:51:34.553Z",
                "content": "<p>A Developer is deploying an application using Docker containers on Amazon ECS. One of the containers runs a database and should be placed on instances in the â€œdatabasesâ€ task group.</p><p>What should the Developer use to control the placement of the database task?</p>",
                "answerExplanation": "<p>A <em>task placement constraint</em> is a rule that is considered during task placement. Task placement constraints can be specified when either running a task or creating a new service. The task placement constraints can be updated for existing services as well.</p><p>Amazon ECS supports the following types of task placement constraints:</p><p>distinctInstance</p><p>Place each task on a different container instance. This task placement constraint can be specified when either running a task or creating a new service.</p><p>memberOf</p><p>Place tasks on container instances that satisfy an expression. For more information about the expression syntax for constraints, see <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-query-language.html\">Cluster Query Language</a>.</p><p>The memberOf task placement constraint can be specified with the following actions:</p><p> â€¢ Running a task</p><p> â€¢ Creating a new service</p><p> â€¢ Creating a new task definition</p><p> â€¢ Creating a new revision of an existing task definition</p><p>The example task placement constraint below uses the memberOf constraint to place tasks on instances in the databases task group. It can be specified with the following actions: <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CreateService.html\">CreateService</a>, <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_UpdateService.html\">UpdateService</a>, <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RegisterTaskDefinition.html\">RegisterTaskDefinition</a>, and <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RunTask.html\">RunTask</a>.</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"placementConstraints\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L1\"><span class=\"pun\">{</span></li><li class=\"L2\"><span class=\"str\">\"expression\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"task:group == databases\"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"type\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"memberOf\"</span></li><li class=\"L4\"><span class=\"pun\">}</span></li><li class=\"L5\"><span class=\"pun\">]</span></li></ol></pre></div></div><p>The Developer should therefore use task placement constraints as in the above example to control the placement of the database task.</p><p><strong>CORRECT: </strong>\"Task Placement Constraint\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Cluster Query Language\" is incorrect. Cluster queries are expressions that enable you to group objects. For example, you can group container instances by attributes such as Availability Zone, instance type, or custom metadata.</p><p><strong>INCORRECT:</strong> \"IAM Group\" is incorrect as you cannot control task placement on ECS with IAM Groups. IAM groups are used for organizing IAM users and applying policies to them.</p><p><strong>INCORRECT:</strong> \"ECS Container Agent\" is incorrect. The Amazon ECS container agent allows container instances to connect to your cluster.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p><p><br></p>",
                "options": [
                    {
                        "id": 3588,
                        "content": "<p>ECS Container Agent</p>",
                        "isValid": false
                    },
                    {
                        "id": 3589,
                        "content": "<p>Task Placement Constraint</p>",
                        "isValid": true
                    },
                    {
                        "id": 3590,
                        "content": "<p>IAM Group</p>",
                        "isValid": false
                    },
                    {
                        "id": 3591,
                        "content": "<p>Cluster Query Language</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 874,
            "attributes": {
                "createdAt": "2023-09-07T08:51:34.628Z",
                "updatedAt": "2023-09-07T08:51:34.628Z",
                "content": "<p>A Developer received the following error when attempting to launch an Amazon EC2 instance using the AWS CLI.</p><p><code>An error occurred (UnauthorizedOperation) when calling the RunInstances operation: You are not authorized to perform this operation. Encoded authorization failure message: VNVaHFdCohROkbyT_rIXoRyNTp7vXFJCqnGiwPuyKnsSVf-WSSGK_06H3vKnrkUa3qx5D40hqj9HEG8kznr04Acmi6lvc8m51tfqtsomFSDylK15x96ZrxMW7MjDJLrMkM0BasPvy8ixo1wi6X2b0C-J1ThyWU9IcrGd7WbaRDOiGbBhJtKs1z01WSn2rVa5_7sr5PwEK-ARrC9y5Pl54pmeF6wh7QhSv2pFO0y39WVBajL2GmByFmQ4p8s-6Lcgxy23b4NJdJwWOF4QGxK9HcKof1VTVZ2oIpsI-dH6_0t2DI0BTwaIgmaT7ldontI1p7OGz-3wPgXm67x2NVNgaK63zPxjYNbpl32QuXLKUKNlB9DdkSdoLvsuFIvf-lQOXLPHnZKCWMqrkI87eqKHYpYKyV5c11TIZTAJ3MntTGO_TJ4U9ySYvTzU2LgswYOtKF_O76-13fryGG5dhgOW5NxwCWBj6WT2NSJvqOeLykAFjR_ET4lM6Dl1XYfQITWCqIzlvlQdLmHJ1jqjp4gW56VcQCdqozLv2UAg8IdrZIXd0OJ047RQcvvN1IyZN0ElL7dR6RzAAQrftoKMRhZQng6THZs8PZM6wep6-yInzwfg8J5_FW6G_PwYqO-4VunVtJSTzM_F_8kojGlRmzqy7eCk5or__bIisUoslw</code></p><p>What action should the Developer perform to make this error more human-readable?</p>",
                "answerExplanation": "<p>The AWS STS decode-authorization-message API decodes additional information about the authorization status of a request from an encoded message returned in response to an AWS request. The output is then decoded into a more human-readable output that can be viewed in a JSON editor.</p><p>The following example is the decoded output from the error shown in the question:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"DecodedMessage\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"{\\\"allowed\\\":false,\\\"explicitDeny\\\":false,\\\"matchedStatements\\\":{\\\"items\\\":[]},\\\"failures\\\":{\\\"items\\\":[]},\\\"context\\\":{\\\"principal\\\":{\\\"id\\\":\\\"AIDAXP4J2EKU7YXXG3EJ4\\\",\\\"name\\\":\\\"Paul\\\",\\\"arn\\\":\\\"arn:aws:iam::515148227241:user/Paul\\\"},\\\"action\\\":\\\"ec2:RunInstances\\\",\\\"resource\\\":\\\"arn:aws:ec2:ap-southeast-2:515148227241:instance/*\\\",\\\"conditions\\\":{\\\"items\\\":[{\\\"key\\\":\\\"ec2:InstanceMarketType\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"on-demand\\\"}]}},{\\\"key\\\":\\\"aws:Resource\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"instance/*\\\"}]}},{\\\"key\\\":\\\"aws:Account\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"515148227241\\\"}]}},{\\\"key\\\":\\\"ec2:AvailabilityZone\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"ap-southeast-2a\\\"}]}},{\\\"key\\\":\\\"ec2:ebsOptimized\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"false\\\"}]}},{\\\"key\\\":\\\"ec2:IsLaunchTemplateResource\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"false\\\"}]}},{\\\"key\\\":\\\"ec2:InstanceType\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"t2.micro\\\"}]}},{\\\"key\\\":\\\"ec2:RootDeviceType\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"ebs\\\"}]}},{\\\"key\\\":\\\"aws:Region\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"ap-southeast-2\\\"}]}},{\\\"key\\\":\\\"aws:Service\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"ec2\\\"}]}},{\\\"key\\\":\\\"ec2:InstanceID\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"*\\\"}]}},{\\\"key\\\":\\\"aws:Type\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"instance\\\"}]}},{\\\"key\\\":\\\"ec2:Tenancy\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"default\\\"}]}},{\\\"key\\\":\\\"ec2:Region\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"ap-southeast-2\\\"}]}},{\\\"key\\\":\\\"aws:ARN\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"arn:aws:ec2:ap-southeast-2:515148227241:instance/*\\\"}]}}]}}}\"</span></li><li class=\"L2\"><span class=\"pun\">}</span></li></ol></pre></div></div><p>Therefore, the best answer is to use the AWS STS decode-authorization-message API to decode the message.</p><p><strong>CORRECT: </strong>\"Use the AWS STS decode-authorization-message API to decode the message\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Make a call to AWS KMS to decode the message\" is incorrect as the message is not encrypted, it is base64 encoded.</p><p><strong>INCORRECT:</strong> \"Use an open source decoding library to decode the message\" is incorrect as you can use the AWS STS decode-authorization-message API.</p><p><strong>INCORRECT:</strong> \"Use the AWS IAM decode-authorization-message API to decode this message\" is incorrect as the decode-authorization-message API is associated with STS, not IAM.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html\">https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html</a></p>",
                "options": [
                    {
                        "id": 3592,
                        "content": "<p>Use the AWS STS decode-authorization-message API to decode the message</p>",
                        "isValid": true
                    },
                    {
                        "id": 3593,
                        "content": "<p>Use an open source decoding library to decode the message</p>",
                        "isValid": false
                    },
                    {
                        "id": 3594,
                        "content": "<p>Use the AWS IAM decode-authorization-message API to decode this message</p>",
                        "isValid": false
                    },
                    {
                        "id": 3595,
                        "content": "<p>Make a call to AWS KMS to decode the message</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 875,
            "attributes": {
                "createdAt": "2023-09-07T08:51:34.707Z",
                "updatedAt": "2023-09-07T08:51:34.707Z",
                "content": "<p>A serverless application uses Amazon API Gateway an AWS Lambda function and a Lambda authorizer function. There is a failure with the application and a developer needs to trace and analyze user requests that pass through API Gateway through to the back end services.</p><p>Which AWS service is MOST suitable for this purpose?</p>",
                "answerExplanation": "<p>You can use <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-services-apigateway.html\">AWS X-Ray</a> to trace and analyze user requests as they travel through your Amazon API Gateway APIs to the underlying services. API Gateway supports X-Ray tracing for all API Gateway endpoint types: Regional, edge-optimized, and private. You can use X-Ray with Amazon API Gateway in all AWS Regions where X-Ray is available.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_06-41-09-9b52f0ecb696bf045a78747ae008ee59.jpg\"></p><p>Because X-Ray gives you an end-to-end view of an entire request, you can analyze latencies in your APIs and their backend services. You can use an X-Ray service map to view the latency of an entire request and that of the downstream services that are integrated with X-Ray. You can also configure sampling rules to tell X-Ray which requests to record and at what sampling rates, according to criteria that you specify.</p><p>The following diagram shows a trace view generated for the example API described above, with a Lambda backend function and a Lambda authorizer function. A successful API method request is shown with a response code of 200.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_06-41-43-5bbe7dbeaad527ff14b7ce120b3b2717.jpg\"></p><p><strong>CORRECT: </strong>\"AWS X-Ray\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon CloudWatch\" is incorrect as it is used to collect metrics and logs. You can use these for troubleshooting however it will be more effective to use AWS X-Ray for analyzing and tracing a distributed application such as this one.</p><p><strong>INCORRECT:</strong> \"Amazon Inspector\" is incorrect as this is an automated security assessment service. It is not used for analyzing and tracing serverless applications.</p><p><strong>INCORRECT:</strong> \"VPC Flow Logs\" is incorrect as this is a feature that captures information about TCP/IP traffic related to network interfaces in a VPC.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html\">https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3596,
                        "content": "<p>Amazon Inspector</p>",
                        "isValid": false
                    },
                    {
                        "id": 3597,
                        "content": "<p>Amazon CloudWatch</p>",
                        "isValid": false
                    },
                    {
                        "id": 3598,
                        "content": "<p>AWS X-Ray</p>",
                        "isValid": true
                    },
                    {
                        "id": 3599,
                        "content": "<p>VPC Flow Logs</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 876,
            "attributes": {
                "createdAt": "2023-09-07T08:51:34.779Z",
                "updatedAt": "2023-09-07T08:51:34.779Z",
                "content": "<p>A developer is in the process of revising multiple AWS Lambda functions and notes that these functions utilize the same bespoke libraries. The developer intends to centralize these libraries, implement updates with minimal effort, and keep the libraries version controlled.</p><p>Which solution aligns with these needs while requiring the least development effort?</p>",
                "answerExplanation": "<p>Lambda layers allow the sharing of code, libraries, or other resources across multiple Lambda functions, making it easier to manage common code across multiple functions. This enables a developer to maintain versioned libraries that can be easily updated and shared, thus requiring the least development effort.</p><p><strong>CORRECT: </strong>\"Create a Lambda layer including all the custom libraries\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS CodeCommit repository for storing the custom libraries\" is incorrect.</p><p>AWS CodeCommit is a version control service hosted by AWS; it is not designed to serve as a central library for Lambda functions.</p><p><strong>INCORRECT:</strong> \"Create a custom Amazon Machine Image (AMI) that includes the custom libraries\" is incorrect.</p><p>Creating a custom Amazon Machine Image (AMI) is not related to Lambda functions as Lambda runs without requiring a server setup.</p><p><strong>INCORRECT:</strong> \"Create an Amazon S3 bucket to store all the custom libraries\" is incorrect.</p><p>Amazon S3 buckets are primarily used for storage and not optimal for serving as a centralized library for Lambda functions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3600,
                        "content": "<p>Create a Lambda layer including all the custom libraries.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3601,
                        "content": "<p>Create an AWS CodeCommit repository for storing the custom libraries.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3602,
                        "content": "<p>Create an Amazon S3 bucket to store all the custom libraries.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3603,
                        "content": "<p>Create a custom Amazon Machine Image (AMI) that includes the custom libraries.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 877,
            "attributes": {
                "createdAt": "2023-09-07T08:51:34.855Z",
                "updatedAt": "2023-09-07T08:51:34.855Z",
                "content": "<p>A software firm is introducing a multimedia application that allows guest users to sample content before deciding to fully register. The company requires a mechanism that can identify users who have already created an account and to monitor the quantity of guest users who ultimately register.</p><p>What two actions would best satisfy these needs? (Select TWO.)</p>",
                "answerExplanation": "<p>Amazon Cognito User Pools serve as a user directory that offers the backend capabilities required for user registration, authentication, and account recovery. This makes it the right solution for identifying users who have created accounts.</p><p>AWS IAM roles can be used to provide distinct permissions for guest users and registered users. While they do not directly track account conversions, they can help delineate between different user states.</p><p><strong>CORRECT: </strong>\"Implement AWS IAM roles to provide distinct permissions for guest users and registered users\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use Amazon Cognito User Pools for managing user registration and authentication\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to oversee user registration and account conversion\" is incorrect.</p><p>AWS Glue is an ETL service designed for easy data preparation and loading for analytics, not for managing user registration or authentication.</p><p><strong>INCORRECT:</strong> \"Deploy Amazon S3 for managing user data and monitoring account conversions\" is incorrect.</p><p>Amazon S3 is a storage service and is not designed for managing user registration or tracking account conversions.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda to track the transitions from guest to full accounts\" is incorrect.</p><p>AWS Lambda is a serverless compute service and is not typically used for managing user registration or tracking conversions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 3604,
                        "content": "<p>Use AWS Lambda to track the transitions from guest to full accounts.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3605,
                        "content": "<p>Use AWS Glue to oversee user registration and account conversion.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3606,
                        "content": "<p>Use Amazon Cognito User Pools for managing user registration and authentication.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3607,
                        "content": "<p>Implement AWS IAM roles to provide distinct permissions for guest users and registered users.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3608,
                        "content": "<p>Deploy Amazon S3 for managing user data and monitoring account conversions.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 878,
            "attributes": {
                "createdAt": "2023-09-07T08:51:34.938Z",
                "updatedAt": "2023-09-07T08:51:34.938Z",
                "content": "<p>An organization has a new AWS account and is setting up IAM users and policies. According to AWS best practices, which of the following strategies should be followed? (Select TWO.)</p>",
                "answerExplanation": "<p>AWS provide a number of best practices for AWS IAM that help you to secure your resources. The key best practices referenced in this scenario are as follows:</p><p> â€¢ Use groups to assign permissions to users â€“ this is correct as you should create permissions policies and assign them to groups. Users can be added to the groups to get the permissions they need to perform their jobs.</p><p> â€¢ Create standalone policies instead of using inline policies (<a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#best-practice-managed-vs-inline\">Use Customer Managed Policies Instead of Inline Policies</a> in the AWS best practices) â€“ this refers to creating your own policies that are standalone policies which can be reused multiple times (assigned to multiple entities such as groups, and users). This is better than using inline policies which are directly attached to a single entity.</p><p><strong>CORRECT: </strong>\"Use groups to assign permissions to users\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Create standalone policies instead of using inline policies\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use user accounts to delegate permissions\" is incorrect as you should use roles to delegate permissions.</p><p><strong>INCORRECT:</strong> \"Create user accounts that can be shared for efficiency\" is incorrect as you should not share user accounts. Always create individual user accounts.</p><p><strong>INCORRECT:</strong> \"Always use customer managed policies instead of AWS managed policies\" is incorrect as this is not a best practice. AWS recommend getting started by using AWS managed policies (<a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#bp-use-aws-defined-policies\">Get Started Using Permissions with AWS Managed Policies</a>).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 3609,
                        "content": "<p>Use user accounts to delegate permissions</p>",
                        "isValid": false
                    },
                    {
                        "id": 3610,
                        "content": "<p>Create user accounts that can be shared for efficiency</p>",
                        "isValid": false
                    },
                    {
                        "id": 3611,
                        "content": "<p>Create standalone policies instead of using inline policies</p>",
                        "isValid": true
                    },
                    {
                        "id": 3612,
                        "content": "<p>Use groups to assign permissions to users</p>",
                        "isValid": true
                    },
                    {
                        "id": 3613,
                        "content": "<p>Always use customer managed policies instead of AWS managed policies</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 879,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.008Z",
                "updatedAt": "2023-09-07T08:51:35.008Z",
                "content": "<p>A Developer is creating a service on Amazon ECS and needs to ensure that each task is placed on a different container instance.</p><p>How can this be achieved?</p>",
                "answerExplanation": "<p>A <em>task placement constraint</em> is a rule that is considered during task placement. Task placement constraints can be specified when either running a task or creating a new service.</p><p>Amazon ECS supports the following types of task placement constraints:</p><p><code>distinctInstance</code></p><p>Place each task on a different container instance. This task placement constraint can be specified when either running a task or creating a new service.</p><p><code>memberOf</code></p><p>Place tasks on container instances that satisfy an expression. For more information about the expression syntax for constraints, see <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-query-language.html\">Cluster Query Language</a>.</p><p>The <code>memberOf</code> task placement constraint can be specified with the following actions:</p><p>Running a task</p><p>Creating a new service</p><p>Creating a new task definition</p><p>Creating a new revision of an existing task definition</p><p>The following code can be used in a task definition to specify a task placement constraint that ensures that each task will run on a distinct instance:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"placementConstraints\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L1\"><span class=\"pun\">{</span></li><li class=\"L2\"><span class=\"str\">\"type\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"distinctInstance\"</span></li><li class=\"L3\"><span class=\"pun\">}</span></li><li class=\"L4\"><span class=\"pun\">]</span></li></ol></pre></div></div><p><strong>CORRECT: </strong>\"Use a task placement constraint\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a task placement strategy\" is incorrect as this is used to select instances for task placement using the binpack, random and spread algorithms.</p><p><strong>INCORRECT:</strong> \"Create a service on Fargate\" is incorrect as Fargate spreads tasks across AZs but not instances.</p><p><strong>INCORRECT:</strong> \"Create a cluster with multiple container instances\" is incorrect as this will not guarantee that each task runs on a different container instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 3614,
                        "content": "<p>Use a task placement constraint</p>",
                        "isValid": true
                    },
                    {
                        "id": 3615,
                        "content": "<p>Create a service on Fargate</p>",
                        "isValid": false
                    },
                    {
                        "id": 3616,
                        "content": "<p>Create a cluster with multiple container instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 3617,
                        "content": "<p>Use a task placement strategy</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 880,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.079Z",
                "updatedAt": "2023-09-07T08:51:35.079Z",
                "content": "<p>A team of Developers require access to an AWS account that is a member account in AWS Organizations. The administrator of the master account needs to restrict the AWS services, resources, and API actions that can be accessed by the users in the account.</p><p>What should the administrator create?</p>",
                "answerExplanation": "<p>As an administrator of the master account of an organization, you can use service control policies (SCPs) to specify the maximum permissions for member accounts in the organization.</p><p>In SCPs, you can restrict which AWS services, resources, and individual API actions the users and roles in each member account can access. You can also define conditions for when to restrict access to AWS services, resources, and API actions.</p><p>The following example shows how an SCP can be created to restrict the EC2 instance types that any user can run in the account:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_01-26-26-8b057fa9dfc190f1f9378c2f876b44b8.png\"><p>These restrictions even override the administrators of member accounts in the organization. When AWS Organizations blocks access to a service, resource, or API action for a member account, a user or role in that account can't access it. This block remains in effect even if an administrator of a member account explicitly grants such permissions in an IAM policy.</p><p><strong>CORRECT: </strong>\"A Service Control Policy (SCP)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"A Tag Policy\" is incorrect as these are used to maintain consistent tags, including the preferred case treatment of tag keys and tag values.</p><p><strong>INCORRECT:</strong> \"An Organizational Unit\" is incorrect as this is used to group accounts for administration.</p><p><strong>INCORRECT:</strong> \"A Consolidated Billing account\" is incorrect as consolidated billing is not related to controlling access to resources within an account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html</a></p>",
                "options": [
                    {
                        "id": 3618,
                        "content": "<p>An Organizational Unit</p>",
                        "isValid": false
                    },
                    {
                        "id": 3619,
                        "content": "<p>A Tag Policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 3620,
                        "content": "<p>A Service Control Policy (SCP)</p>",
                        "isValid": true
                    },
                    {
                        "id": 3621,
                        "content": "<p>A Consolidated Billing account</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 881,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.148Z",
                "updatedAt": "2023-09-07T08:51:35.148Z",
                "content": "<p>A company is developing a game for the Android and iOS platforms. The mobile game will securely store user game history and other data locally on the device. The company would like users to be able to use multiple mobile devices and synchronize data between devices.</p><p>Which service can be used to synchronize the data across mobile devices without the need to create a backend application?</p>",
                "answerExplanation": "<p>Amazon Cognito lets you save end user data in datasets containing key-value pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. To sync this data between the Amazon Cognito service and an end userâ€™s devices, invoke the synchronize method. Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity.</p><p>The Amazon Cognito Sync client creates a local cache for the identity data. Your app talks to this local cache when it reads and writes keys. This guarantees that all of your changes made on the device are immediately available on the device, even when you are offline. When the synchronize method is called, changes from the service are pulled to the device, and any local changes are pushed to the service. At this point the changes are available to other devices to synchronize.</p><p><strong>CORRECT: </strong>\"Amazon Cognito\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Lambda\" is incorrect. AWS Lambda provides serverless functions that run your code, it is not used for mobile client data synchronization.</p><p><strong>INCORRECT:</strong> \"Amazon API Gateway\" is incorrect as API Gateway provides APIs for traffic coming into AWS. It is not used for mobile client data synchronization.</p><p><strong>INCORRECT:</strong> \"Amazon DynamoDB\" is incorrect as DynamoDB is a NoSQL database. It is not used for mobile client data synchronization.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/synchronizing-data.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/synchronizing-data.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 3622,
                        "content": "<p>AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 3623,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 3624,
                        "content": "<p>Amazon Cognito</p>",
                        "isValid": true
                    },
                    {
                        "id": 3625,
                        "content": "<p>Amazon API Gateway</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 882,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.222Z",
                "updatedAt": "2023-09-07T08:51:35.222Z",
                "content": "<p>A mobile application has hundreds of users. Each user may use multiple devices to access the application. The Developer wants to assign unique identifiers to these users regardless of the device they use.</p><p>Which of the following methods should be used to obtain unique identifiers?</p>",
                "answerExplanation": "<p>Amazon Cognito supports developer authenticated identities, in addition to web identity federation through <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/facebook.html\">Facebook (Identity Pools)</a>, <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/google.html\">Google (Identity Pools)</a>, <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/amazon.html\">Login with Amazon (Identity Pools)</a>, and Sign in with Apple (identity Pools).</p><p>With developer authenticated identities, you can register and authenticate users via your own existing authentication process, while still using Amazon Cognito to synchronize user data and access AWS resources.</p><p>Using developer authenticated identities involves interaction between the end user device, your backend for authentication, and Amazon Cognito.</p><p>Therefore, the Developer can implement developer-authenticated identities by using Amazon Cognito, and get credentials for these identities.</p><p><strong>CORRECT: </strong>\"Implement developer-authenticated identities by using Amazon Cognito, and get credentials for these identities\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a user table in Amazon DynamoDB as key-value pairs of users and their devices. Use these keys as unique identifiers\" is incorrect as this solution would require additional application logic and would be more complex.</p><p><strong>INCORRECT:</strong> \"Use IAM-generated access key IDs for the users as the unique identifier, but do not store secret keys\" is incorrect as it is not a good practice to provide end users of mobile applications with IAM user accounts and access keys. Cognito is a better solution for this use case.</p><p><strong>INCORRECT:</strong> \"Assign IAM users and roles to the users. Use the unique IAM resource ID as the unique identifier\" is incorrect. AWS Cognito is better suited to mobile users and with developer authenticated identities the users can be assigned unique identities.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 3626,
                        "content": "<p>Use IAM-generated access key IDs for the users as the unique identifier, but do not store secret keys</p>",
                        "isValid": false
                    },
                    {
                        "id": 3627,
                        "content": "<p>Create a user table in Amazon DynamoDB as key-value pairs of users and their devices. Use these keys as unique identifiers</p>",
                        "isValid": false
                    },
                    {
                        "id": 3628,
                        "content": "<p>Implement developer-authenticated identities by using Amazon Cognito, and get credentials for these identities</p>",
                        "isValid": true
                    },
                    {
                        "id": 3629,
                        "content": "<p>Assign IAM users and roles to the users. Use the unique IAM resource ID as the unique identifier</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 883,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.298Z",
                "updatedAt": "2023-09-07T08:51:35.298Z",
                "content": "<p>A corporation plans to deploy an application on AWS utilizing an Elastic Load Balancer that operates with HTTP/HTTPS listeners. The application must have the ability to retrieve client IP addresses.</p><p>Which load-balancing solution would satisfy these needs?</p>",
                "answerExplanation": "<p>Application Load Balancer operates at the application layer (Layer 7), and it supports path-based routing, and it can route requests to one or more ports on each container instance in your cluster. The X-Forwarded-For request header helps to preserve the client-side source IP address which is needed in this scenario.</p><p><strong>CORRECT: </strong>\"Application Load Balancer with X-Forwarded-For headers enabled\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Network Load Balancer with Proxy Protocol enabled\" is incorrect.</p><p>Network Load Balancer operates at the transport layer (Layer 4) and handles millions of requests per second. The NLB cannot use HTTP/HTTPS listeners so is not suitable for this solution.</p><p><strong>INCORRECT:</strong> \"Gateway Load Balancer with X-Forwarded-For headers enabled\" is incorrect.</p><p>A gateway load balancer is used for distributing traffic to appliances such as IDS and IPS devices.</p><p><strong>INCORRECT:</strong> \"Application Load Balancer with Proxy Protocol enabled\" is incorrect.</p><p>You cannot use the proxy protocol with a layer 7 load balancer.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/load-balancer-getting-started.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/load-balancer-getting-started.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 3630,
                        "content": "<p>Application Load Balancer with Proxy Protocol enabled.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3631,
                        "content": "<p>Application Load Balancer with X-Forwarded-For headers enabled.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3632,
                        "content": "<p>Network Load Balancer with Proxy Protocol enabled.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3633,
                        "content": "<p>Gateway Load Balancer with X-Forwarded-For headers enabled.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 884,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.371Z",
                "updatedAt": "2023-09-07T08:51:35.371Z",
                "content": "<p>A Developer is troubleshooting an issue with a DynamoDB table. The table is used to store order information for a busy online store and uses the order date as the partition key. During busy periods writes to the table are being throttled despite the consumed throughput being well below the provisioned throughput.</p><p>According to AWS best practices, how can the Developer resolve the issue at the LOWEST cost?</p>",
                "answerExplanation": "<p>DynamoDB stores data as groups of attributes, known as <em>items. </em>Items are similar to rows or records in other database systems. DynamoDB stores and retrieves each item based on the primary key value, which must be unique.</p><p>Items are distributed across 10-GB storage units, called partitions (physical storage internal to DynamoDB). Each table has one or more partitions, as shown in the following illustration.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_05-28-40-6b16884eef9101228e54e74b4de2613f.jpg\"></p><p>DynamoDB uses the partition keyâ€™s value as an input to an internal hash function. The output from the hash function determines the partition in which the item is stored. Each itemâ€™s location is determined by the hash value of its partition key.</p><p>All items with the same partition key are stored together, and for composite partition keys, are ordered by the sort key value. DynamoDB splits partitions by sort key if the collection size grows bigger than 10 GB.</p><p>DynamoDB evenly distributes provisioned throughputâ€”read capacity units (RCUs) and write capacity units (WCUs)â€”among partitions and automatically supports your access patterns using the throughput you have provisioned. However, if your access pattern exceeds 3000 RCU or 1000 WCU for a single partition key value, your requests might be throttled with a ProvisionedThroughputExceededException error.</p><p>To avoid request throttling, design your DynamoDB table with the right partition key to meet your access requirements and provide even distribution of data. Recommendations for doing this include the following:</p><p> â€¢ Use high cardinality attributes (e.g. email_id, employee_no, customer_id etc.)</p><p> â€¢ Use composite attributes</p><p> â€¢ Cache popular items</p><p> â€¢ Add random numbers or digits from a pre-determined range for write-heavy use cases</p><p>In this case there is a hot partition due to the order date being used as the partition key and this is causing writes to be throttled. Therefore, the best solution to ensure the writes are more evenly distributed in this scenario is to add a random number suffix to the partition key values.</p><p><strong>CORRECT: </strong>\"Add a random number suffix to the partition key values\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Increase the read and write capacity units for the table\" is incorrect as this will not solve the hot partition issue and we know that the consumed throughput is lower than provisioned throughput.</p><p><strong>INCORRECT:</strong> \"Add a global secondary index to the table\" is incorrect as a GSI is used for querying data more efficiently, it will not solve the problem of write performance due to a hot partition.</p><p><strong>INCORRECT:</strong> \"Use an Amazon SQS queue to buffer the incoming writes\" is incorrect as this is not the lowest cost option. You would need to have producers and consumers of the queue as well as paying for the queue itself.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3634,
                        "content": "<p>Add a random number suffix to the partition key values</p>",
                        "isValid": true
                    },
                    {
                        "id": 3635,
                        "content": "<p>Increase the read and write capacity units for the table</p>",
                        "isValid": false
                    },
                    {
                        "id": 3636,
                        "content": "<p>Add a global secondary index to the table</p>",
                        "isValid": false
                    },
                    {
                        "id": 3637,
                        "content": "<p>Use an Amazon SQS queue to buffer the incoming writes</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 885,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.444Z",
                "updatedAt": "2023-09-07T08:51:35.444Z",
                "content": "<p>A company stores session information for a serverless application in an Amazon DynamoDB table. The company requires an automated process to eliminate outdated items from the table.</p><p>What is the most straightforward and lowest cost method to accomplish this?</p>",
                "answerExplanation": "<p>Amazon DynamoDB Time to Live (TTL) enables you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table, making this the simplest automated method to remove old items.</p><p><strong>CORRECT: </strong>\"Use Amazon DynamoDB Time to Live (TTL) to automatically delete old items\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement a periodic AWS Lambda function to scan and delete old items\" is incorrect.</p><p>Although a periodic AWS Lambda function could potentially scan and delete old items, this would require additional development and maintenance efforts compared to using DynamoDB TTL.</p><p><strong>INCORRECT:</strong> \"Use AWS DMS (Database Migration Service) to purge old items\" is incorrect.</p><p>AWS DMS (Database Migration Service) is used for migrating databases to AWS and is not designed to automatically purge old items from a database.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch to monitor and delete old items\" is incorrect.</p><p>Amazon CloudWatch is a monitoring service, not a tool designed to delete items from a database based on their age.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3638,
                        "content": "<p>Use Amazon CloudWatch to monitor and delete old items.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3639,
                        "content": "<p>Use Amazon DynamoDB Time to Live (TTL) to automatically delete old items.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3640,
                        "content": "<p>Implement a periodic AWS Lambda function to scan and delete old items.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3641,
                        "content": "<p>Use AWS DMS (Database Migration Service) to purge old items.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 886,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.515Z",
                "updatedAt": "2023-09-07T08:51:35.515Z",
                "content": "<p>A Developer is deploying an update to a serverless application that includes AWS Lambda using the AWS Serverless Application Model (SAM). The traffic needs to move from the old Lambda version to the new Lambda version gradually, within the shortest period of time.</p><p>Which deployment configuration is MOST suitable for these requirements?</p>",
                "answerExplanation": "<p>If you use AWS SAM to create your serverless application, it comes built-in with <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html\">CodeDeploy</a> to provide gradual Lambda deployments. With just a few lines of configuration, AWS SAM does the following for you:</p><p> â€¢ Deploys new versions of your Lambda function, and automatically creates aliases that point to the new version.</p><p> â€¢ Gradually shifts customer traffic to the new version until you're satisfied that it's working as expected, or you roll back the update.</p><p> â€¢ Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected.</p><p> â€¢ Rolls back the deployment if CloudWatch alarms are triggered.</p><p>There are several options for how CodeDeploy shifts traffic to the new Lambda version. You can choose from the following:</p><p>â€¢ <strong>Canary</strong>: Traffic is shifted in two increments. You can choose from predefined canary options. The options specify the percentage of traffic that's shifted to your updated Lambda function version in the first increment, and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p><p><strong> </strong>â€¢ <strong>Linear</strong>: Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic that's shifted in each increment and the number of minutes between each increment.</p><p><strong>All-at-once</strong>: All traffic is shifted from the original Lambda function to the updated Lambda function version at once.</p><p>Therefore <code>CodeDeployDefault.LambdaCanary10Percent5Minutes</code> is the best answer as this will shift 10 percent of the traffic and then after 5 minutes shift the remainder of the traffic. The entire deployment will take 5 minutes to cut over.</p><p><strong>CORRECT: </strong>\"<code>CodeDeployDefault.LambdaCanary10Percent5Minutes</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"<code>CodeDeployDefault.HalfAtATime</code>\" is incorrect as this is a CodeDeploy traffic shifting strategy that is not applicable to AWS Lambda. You can use Half at a Time with EC2 and on-premises instances.</p><p><strong>INCORRECT:</strong> \"<code>CodeDeployDefault.LambdaLinear10PercentEvery1Minute</code>\" is incorrect as this option will take longer. CodeDeploy will shift 10 percent every 1 minute and therefore the deployment time will be 10 minutes.</p><p><strong>INCORRECT:</strong> \"C<code>odeDeployDefault.LambdaLinear10PercentEvery2Minutes</code>\" is incorrect as this option will take longer. CodeDeploy will shift 10 percent every 2 minutes and therefore the deployment time will be 20 minutes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-sam/\">https://digitalcloud.training/aws-sam/</a></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3642,
                        "content": "<p><code>CodeDeployDefault.LambdaCanary10Percent5Minutes</code> </p>",
                        "isValid": true
                    },
                    {
                        "id": 3643,
                        "content": "<p><code>CodeDeployDefault.LambdaLinear10PercentEvery2Minutes</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3644,
                        "content": "<p><code>CodeDeployDefault.HalfAtATime</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3645,
                        "content": "<p><code>CodeDeployDefault.LambdaLinear10PercentEvery1Minute</code> </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 887,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.587Z",
                "updatedAt": "2023-09-07T08:51:35.587Z",
                "content": "<p>A website is being delivered using Amazon CloudFront and a Developer recently modified some images that are displayed on website pages. Upon testing the changes, the Developer noticed that the new versions of the images are not displaying.</p><p>What should the Developer do to force the new images to be displayed?</p>",
                "answerExplanation": "<p>If you need to remove a file from CloudFront edge caches before it expires, you can do one of the following:</p><p> â€¢ Invalidate the file from edge caches. The next time a viewer requests the file, CloudFront returns to the origin to fetch the latest version of the file.</p><p> â€¢ Use file versioning to serve a different version of the file that has a different name. For more information, see <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/UpdatingExistingObjects.html#ReplacingObjects\">Updating Existing Files Using Versioned File Names</a>.</p><p>To invalidate files, you can specify either the path for individual files or a path that ends with the * wildcard, which might apply to one file or to many, as shown in the following examples:</p><p> â€¢ /images/image1.jpg</p><p> â€¢ /images/image*</p><p> â€¢ /images/*</p><p>Therefore, the Developer should invalidate the old versions of the images on the edge cache as this will remove the cached images and the new versions of the images will then be cached when the next request is received.</p><p><strong>CORRECT: </strong>\"Invalidate the old versions of the images on the edge caches\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Delete the images from the origin and then save the new version on the origin\" is incorrect as this will not cause the cache entries to expire. The Developer needs to remove the cached entries to cause a cache miss to occur which will then result in the updated images being cached.</p><p><strong>INCORRECT:</strong> \"Invalidate the old versions of the images on the origin\" is incorrect as the Developer needs to invalidate the cache entries on the edge caches, not the images on the origin.</p><p><strong>INCORRECT:</strong> \"Force an update of the cache\" is incorrect as there is no way to directly update the cache. The Developer should invalidate the relevant cache entries and then the cache will be updated next time a request is received for the images.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 3646,
                        "content": "<p>Invalidate the old versions of the images on the origin</p>",
                        "isValid": false
                    },
                    {
                        "id": 3647,
                        "content": "<p>Delete the images from the origin and then save the new version on the origin</p>",
                        "isValid": false
                    },
                    {
                        "id": 3648,
                        "content": "<p>Force an update of the cache</p>",
                        "isValid": false
                    },
                    {
                        "id": 3649,
                        "content": "<p>Invalidate the old versions of the images on the edge caches</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 888,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.664Z",
                "updatedAt": "2023-09-07T08:51:35.664Z",
                "content": "<p>A security officer has requested that a Developer enable logging for API actions for all AWS regions to a single Amazon S3 bucket.</p><p>What is the EASIEST way for the Developer to achieve this requirement?</p>",
                "answerExplanation": "<p>The easiest way to achieve the desired outcome is to create an AWS CloudTrail trail and apply it to all regions and configure logging to a single S3 bucket. This is a supported configuration and will achieve the requirement.</p><p><strong>CORRECT: </strong>\"Create an AWS CloudTrail trail and apply it to all regions, configure logging to a single S3 bucket\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudTrail trail in each region, configure logging to a single S3 bucket\" is incorrect. The Developer should apply a trail to all regions. This will be easier.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudTrail trail in each region, configure logging to a local bucket, and then use cross-region replication to replicate all logs to a single S3 bucket\" is incorrect. This is unnecessary, the Developer can simply create a trail that is applied to all regions and log to a single bucket.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudTrail trail and apply it to all regions, configure logging to a local bucket, and then use cross-region replication to replicate all logs to a single S3 bucket\" is incorrect. This is unnecessary, the Developer can simply create a trail that is applied to all regions and log to a single bucket.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudtrail/\">https://digitalcloud.training/aws-cloudtrail/</a></p>",
                "options": [
                    {
                        "id": 3650,
                        "content": "<p>Create an AWS CloudTrail trail in each region, configure logging to a single S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 3651,
                        "content": "<p>Create an AWS CloudTrail trail in each region, configure logging to a local bucket, and then use cross-region replication to replicate all logs to a single S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 3652,
                        "content": "<p>Create an AWS CloudTrail trail and apply it to all regions, configure logging to a local bucket, and then use cross-region replication to replicate all logs to a single S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 3653,
                        "content": "<p>Create an AWS CloudTrail trail and apply it to all regions, configure logging to a single S3 bucket</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 889,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.738Z",
                "updatedAt": "2023-09-07T08:51:35.738Z",
                "content": "<p>An AWS developer is building an application that processes sensitive personally identifiable information (PII). The application operates on AWS Lambda and writes diagnostic data to Amazon CloudWatch. However, the developer wants to ensure that PII is not accidentally logged in CloudWatch.</p><p>What strategy should the developer adopt to ensure this?</p>",
                "answerExplanation": "<p>Amazon Macie is a fully managed data privacy and security service that uses machine learning and pattern matching to discover and protect sensitive data in AWS, such as PII. This includes being able to scan CloudWatch logs for PII, making it a good fit for this requirement.</p><p><strong>CORRECT: </strong>\"Use Amazon Macie to regularly scan and identify any PII within the logs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Manually insert logging commands into the application code while ensuring PII is not included\" is incorrect.</p><p>Manually incorporating logging commands could prevent PII from being logged but lacks an automated mechanism for detecting or preventing accidental inclusion of PII.</p><p><strong>INCORRECT:</strong> \"Implement AWS Secrets Manager for secure logging of sensitive information\" is incorrect.</p><p>AWS Secrets Manager is used for managing secrets and does not provide direct capabilities to prevent PII from being logged.</p><p><strong>INCORRECT:</strong> \"Incorporate AWS X-Ray and configure it to filter out sensitive PII before logging\" is incorrect.</p><p>AWS X-Ray provides insights into the behavior of your applications, but it does not inherently include features to detect or filter out PII before logging.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/macie/\">https://aws.amazon.com/macie/</a></p>",
                "options": [
                    {
                        "id": 3654,
                        "content": "<p>Implement AWS Secrets Manager for secure logging of sensitive information.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3655,
                        "content": "<p>Manually insert logging commands into the application code while ensuring PII is not included.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3656,
                        "content": "<p>Use Amazon Macie to regularly scan and identify any PII within the logs.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3657,
                        "content": "<p>Incorporate AWS X-Ray and configure it to filter out sensitive PII before logging.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 890,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.811Z",
                "updatedAt": "2023-09-07T08:51:35.811Z",
                "content": "<p>Based on the following AWS CLI command the resulting output, what has happened here?</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pln\">$ aws </span><span class=\"kwd\">lambda</span><span class=\"pln\"> invoke </span><span class=\"pun\">--</span><span class=\"kwd\">function</span><span class=\"pun\">-</span><span class=\"pln\">name </span><span class=\"typ\">MyFunction</span><span class=\"pln\"> </span><span class=\"pun\">--</span><span class=\"pln\">invocation</span><span class=\"pun\">-</span><span class=\"pln\">type </span><span class=\"typ\">Event</span><span class=\"pln\"> </span><span class=\"pun\">--</span><span class=\"pln\">payload ewogICJrZXkxIjogInZhbHVlMSIsCiAgImtleTIiOiAidmFsdWUyIiwKICAia2V5MyI6ICJ2YWx1ZTMiCn0</span><span class=\"pun\">=</span><span class=\"pln\"> response</span><span class=\"pun\">.</span><span class=\"pln\">json</span></li><li class=\"L1\"><span class=\"pun\">{</span></li><li class=\"L2\"><span class=\"str\">\"StatusCode\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"lit\">202</span></li><li class=\"L3\"><span class=\"pun\">}</span></li></ol></pre></div></div>",
                "answerExplanation": "<p>Several AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Simple Notification Service (Amazon SNS), invoke functions asynchronously to process events.</p><p>When you invoke a function asynchronously, you don't wait for a response from the function code. You hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors, and can send invocation records to a downstream resource to chain together components of your application.</p><p>The following diagram shows clients invoking a Lambda function asynchronously. Lambda queues the events before sending them to the function.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_00-43-56-3aabc746a4671b74907940323d137b09.jpg\"></p><p>For asynchronous invocation, Lambda places the event in a queue and returns a success response without additional information. A separate process reads events from the queue and sends them to your function. To invoke a function asynchronously, set the invocation type parameter to Event.</p><p>In this scenario the Event parameter has been used so we know the function has been invoked asynchronously. For asynchronous invocation the status code 202 indicates a successful execution.</p><p><strong>CORRECT: </strong>\"An AWS Lambda function has been invoked asynchronously and has completed successfully\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"An AWS Lambda function has been invoked synchronously and has completed successfully\" is incorrect as the Event parameter indicates an asynchronous invocation.</p><p><strong>INCORRECT:</strong> \"An AWS Lambda function has been invoked synchronously and has not completed successfully\" is incorrect as the Event parameter indicates an asynchronous invocation (a status code 200 would be a successful execution for a synchronous invocation).</p><p><strong>INCORRECT:</strong> \"An AWS Lambda function has been invoked asynchronously and has not completed successfully\" is incorrect as the status code 202 indicates a successful execution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3658,
                        "content": "<p>An AWS Lambda function has been invoked synchronously and has completed successfully</p>",
                        "isValid": false
                    },
                    {
                        "id": 3659,
                        "content": "<p>An AWS Lambda function has been invoked asynchronously and has not completed successfully</p>",
                        "isValid": false
                    },
                    {
                        "id": 3660,
                        "content": "<p>An AWS Lambda function has been invoked asynchronously and has completed successfully</p>",
                        "isValid": true
                    },
                    {
                        "id": 3661,
                        "content": "<p>An AWS Lambda function has been invoked synchronously and has not completed successfully</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 891,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.890Z",
                "updatedAt": "2023-09-07T08:51:35.890Z",
                "content": "<p>A Developer must deploy a new AWS Lambda function using an AWS CloudFormation template.</p><p>Which procedures will deploy a Lambda function? (Select TWO.)</p>",
                "answerExplanation": "<p>Of the options presented there are two workable procedures for deploying the Lambda function.</p><p>Firstly, you can create an <code>AWS::Lambda::Function</code> resource in the template, then write the code directly inside the CloudFormation template. This is possible for simple functions using Node.js or Python which allow you to declare the code inline in the CloudFormation template. For example:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_07-05-29-e4b3016d3f9d89c64fd56765e6373d24.jpg\"></p><p>The other option is to upload a ZIP file containing the function code to Amazon S3, then add a reference to it in an AWS::Lambda::Function resource in the template. To declare this in your AWS CloudFormation template, you can use the following syntax (within AWS::Lambda::Function Code):</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_07-05-55-850648311f9e6c66dad786c0e2df9748.jpg\"></p><p><strong>CORRECT: </strong>\"Create an <code>AWS::Lambda::Function</code> resource in the template, then write the code directly inside the CloudFormation template\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Upload a ZIP file containing the function code to Amazon S3, then add a reference to it in an <code>AWS::Lambda::Function</code> resource in the template\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Upload the code to an AWS CodeCommit repository, then add a reference to it in an <code>AWS::Lambda::Function</code> resource in the template\" is incorrect as you cannot add a reference to code in a CodeCommit repository.</p><p><strong>INCORRECT:</strong> \"Upload a ZIP file to AWS CloudFormation containing the function code, then add a reference to it in an <code>AWS::Lambda::Function</code> resource in the template\" is incorrect as you cannot reference a zip file in CloudFormation.</p><p><strong>INCORRECT:</strong> \"Upload the function code to a private Git repository, then add a reference to it in an <code>AWS::Lambda::Function</code> resource in the template\" is incorrect as you cannot reference the function code in a private Git repository.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-function.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-function.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3662,
                        "content": "<p>Create an <code>AWS::Lambda::Function</code> resource in the template, then write the code directly inside the CloudFormation template</p>",
                        "isValid": true
                    },
                    {
                        "id": 3663,
                        "content": "<p>Upload the code to an AWS CodeCommit repository, then add a reference to it in an <code>AWS::Lambda::Function</code> resource in the template</p>",
                        "isValid": false
                    },
                    {
                        "id": 3664,
                        "content": "<p>Upload a ZIP file containing the function code to Amazon S3, then add a reference to it in an <code>AWS::Lambda::Function</code> resource in the template</p>",
                        "isValid": true
                    },
                    {
                        "id": 3665,
                        "content": "<p>Upload a ZIP file to AWS CloudFormation containing the function code, then add a reference to it in an <code>AWS::Lambda::Function</code> resource in the template</p>",
                        "isValid": false
                    },
                    {
                        "id": 3666,
                        "content": "<p>1. Upload the function code to a private Git repository, then add a reference to it in an <code>AWS::Lambda::Function</code> resource in the template</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 892,
            "attributes": {
                "createdAt": "2023-09-07T08:51:35.962Z",
                "updatedAt": "2023-09-07T08:51:35.962Z",
                "content": "<p>A Developer is using AWS SAM to create a template for deploying a serverless application. The Developer plans deploy a Lambda function using the template.</p><p>Which resource type should the Developer specify?</p>",
                "answerExplanation": "<p>A <strong>serverless application</strong> is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda functionâ€”it can include additional resources such as APIs, databases, and event source mappings.</p><p>AWS SAM templates are an extension of AWS CloudFormation templates, with some additional components that make them easier to work with. To create a Lambda function using an AWS SAM template the Developer can use the AWS::Serverless::Function resource type.</p><p>The AWS::Serverless::Function resource type can be used to Create a Lambda function, IAM execution role, and event source mappings that trigger the function.</p><p><strong>CORRECT: </strong>\"<code>AWS::Serverless:Function</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"<code>AWS::Serverless::Application</code>\" is incorrect as this embeds a serverless application from the <a href=\"https://serverlessrepo.aws.amazon.com/applications\">AWS Serverless Application Repository</a> or from an Amazon S3 bucket as a nested application.</p><p><strong>INCORRECT:</strong> \"AWS::Serverless:LayerVersion\" is incorrect as this creates a Lambda LayerVersion that contains library or runtime code needed by a Lambda Function.</p><p><strong>INCORRECT:</strong> \"<code>AWS::Serverless:API</code>\" is incorrect as this creates a collection of Amazon API Gateway resources and methods that can be invoked through HTTPS endpoints.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-sam/\">https://digitalcloud.training/aws-sam/</a></p>",
                "options": [
                    {
                        "id": 3667,
                        "content": "<p><code>AWS::Serverless:LayerVersion</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3668,
                        "content": "<p><code>AWS::Serverless:Function</code> </p>",
                        "isValid": true
                    },
                    {
                        "id": 3669,
                        "content": "<p><code>AWS::Serverless:API</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3670,
                        "content": "<p><code>AWS::Serverless::Application</code> </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 893,
            "attributes": {
                "createdAt": "2023-09-07T08:51:36.039Z",
                "updatedAt": "2023-09-07T08:51:36.039Z",
                "content": "<p>An application is using Amazon DynamoDB as its data store and needs to be able to read 200 items per second as eventually consistent reads. Each item is 12 KB in size.<br>What value should be set for the table's provisioned throughput for reads?</p>",
                "answerExplanation": "<p>With provisioned capacity mode, you specify the number of data reads and writes per second that you require for your application.</p><p><strong>Read capacity unit (RCU):</strong></p><p> â€¢ Each API call to read data from your table is a read request.</p><p> â€¢ Read requests can be strongly consistent, eventually consistent, or transactional.</p><p> â€¢ For items up to 4 KB in size, one RCU can perform one <em>strongly consistent</em> read request per second.</p><p> â€¢ Items larger than 4 KB require additional RCUs.</p><p> â€¢ For items up to 4 KB in size, one RCU can perform two <em>eventually consistent</em> read requests per second.</p><p><em> </em>â€¢ <em>Transactional</em> read requests require two RCUs to perform one read per second for items up to 4 KB.</p><p> â€¢ For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs.</p><p><strong>Write capacity unit (WCU):</strong></p><p> â€¢ Each API call to write data to your table is a write request.</p><p> â€¢ For items up to 1 KB in size, one WCU can perform one<em> standard</em> write request per second.</p><p> â€¢ Items larger than 1 KB require additional WCUs.</p><p><em> </em>â€¢ <em>Transactional</em> write requests require two WCUs to perform one write per second for items up to 1 KB.</p><p> â€¢ For example, a standard write request of a 1 KB item would require one WCU, a standard write request of a 3 KB item would require three WCUs, and a transactional write request of a 3 KB item would require six WCUs.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_05-19-50-627aac2c69721154875d3456f06f00ae.jpg\"></p><p>To determine the number of RCUs required to handle 200 eventually consistent reads per/second with an average item size of 12KB, perform the following steps:</p><p>&nbsp; &nbsp; 1. Determine the average item size by rounding up the next multiple of 4KB (12KB rounds up to 12KB).</p><p>&nbsp; &nbsp; 2. Determine the RCU per item by dividing the item size by 8KB (12KB/8KB = 1.5).</p><p>&nbsp; &nbsp; 3. Multiply the value from step 2 with the number of reads required per second (1.5x200 = 300).</p><p><strong>CORRECT: </strong>\"300 Read Capacity Units\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"600 Read Capacity Units\" is incorrect. This would be the value for strongly consistent reads.</p><p><strong>INCORRECT:</strong> \"1200 Read Capacity Units\" is incorrect. This would be the value for transactional reads.</p><p><strong>INCORRECT:</strong> \"150 Read Capacity Units\" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/pricing/provisioned/\">https://aws.amazon.com/dynamodb/pricing/provisioned/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3671,
                        "content": "<p>600 Read Capacity Units</p>",
                        "isValid": false
                    },
                    {
                        "id": 3672,
                        "content": "<p>300 Read Capacity Units</p>",
                        "isValid": true
                    },
                    {
                        "id": 3673,
                        "content": "<p>1200 Read Capacity Units</p>",
                        "isValid": false
                    },
                    {
                        "id": 3674,
                        "content": "<p>150 Read Capacity Units</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 894,
            "attributes": {
                "createdAt": "2023-09-07T08:51:36.109Z",
                "updatedAt": "2023-09-07T08:51:36.109Z",
                "content": "<p>An online retail application developer is planning to migrate to AWS to accommodate a future surge in traffic. Currently, a web server, which hosts the web application and manages session state in memory, and a separate server hosting a MySQL database for order details, are used.</p><p>During peak traffic, memory usage on the web server reaches its limit, leading to considerable slowdowns. As part of the migration plan, the developer intends to use Amazon EC2 instances with an Auto Scaling group and an Application Load Balancer for the web server.</p><p>What other changes can the developer implement to enhance application performance?</p>",
                "answerExplanation": "<p>The main performance issue with the current application setup is due to managing additional user sessions, which significantly increases memory usage. To address this, Amazon ElastiCache for Memcached is an ideal service as it is designed to offload the burden of managing session state from web servers, providing fast, in-memory data storage.</p><p>For application data, Amazon RDS for MySQL DB instance is a fully managed relational database service, which takes care of database management tasks and provides cost-efficient and resizable capacity.</p><p><strong>CORRECT: </strong>\"Use Amazon ElastiCache for Memcached to store and manage session data, while utilizing Amazon RDS for MySQL DB instance for application data storage\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store both the session data and the application data in a MySQL database hosted on an EC2 instance\" is incorrect.</p><p>Storing both session and application data in a MySQL database hosted on an EC2 instance might overload the database server leading to performance issues.</p><p><strong>INCORRECT:</strong> \"Use Amazon ElastiCache for Memcached to store and manage both the session data and the application data\" is incorrect.</p><p>Amazon ElastiCache for Memcached is not typically used for application data storage but for caching frequently accessed data and offloading the database.</p><p><strong>INCORRECT:</strong> \"Leverage the EC2 instance store for managing the session data and Amazon RDS for MySQL DB instance for application data storage\" is incorrect.</p><p>EC2 instance store provides temporary block-level storage for instances. This storage is ideal for temporary data, but not for session data, as data is lost if the instance is stopped or fails.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/memcached/\">https://aws.amazon.com/elasticache/memcached/</a></p><p><a href=\"https://aws.amazon.com/rds/mysql/\">https://aws.amazon.com/rds/mysql/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 3675,
                        "content": "<p>Store both the session data and the application data in a MySQL database hosted on an EC2 instance.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3676,
                        "content": "<p>Use Amazon ElastiCache for Memcached to store and manage session data, while utilizing Amazon RDS for MySQL DB instance for application data storage.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3677,
                        "content": "<p>Use Amazon ElastiCache for Memcached to store and manage both the session data and the application data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3678,
                        "content": "<p>Leverage the EC2 instance store for managing the session data and Amazon RDS for MySQL DB instance for application data storage.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 895,
            "attributes": {
                "createdAt": "2023-09-07T08:51:36.186Z",
                "updatedAt": "2023-09-07T08:51:36.186Z",
                "content": "<p>A company uses Amazon SQS to decouple an online application that generates memes. The SQS consumers poll the queue regularly to keep throughput high and this is proving to be costly and resource intensive. A Developer has been asked to review the system and propose changes that can reduce costs and the number of empty responses.</p><p>What would be the BEST approach to MINIMIZING cost? </p>",
                "answerExplanation": "<p>The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses <em>short polling</em>, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response. You can use <em>long polling</em> to reduce your costs while allowing your consumers to receive messages as soon as they arrive in the queue.</p><p>When the wait time for the ReceiveMessage API action is greater than 0, <em>long polling</em> is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\">ReceiveMessage</a> request) and false empty responses (when messages are available but aren't included in a response).</p><p>Therefore, the best way to optimize resource usage and reduce the number of empty responses (and cost) is to configure long polling by setting the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds.</p><p><strong>CORRECT: </strong>\"Set the Imaging queue <code>ReceiveMessageWaitTimeSeconds</code> attribute to 20 seconds\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Set the imaging queue visibility Timeout attribute to 20 seconds\" is incorrect. This attribute configures message visibility which will not reduce empty responses.</p><p><strong>INCORRECT:</strong> \"Set the imaging queue <code>MessageRetentionPeriod</code> attribute to 20 seconds\" is incorrect. This attribute sets the length of time, in seconds, for which Amazon SQS retains a message.</p><p><strong>INCORRECT:</strong> \"Set the <code>DelaySeconds</code> parameter of a message to 20 seconds\" is incorrect. This attribute sets the length of time, in seconds, for which the delivery of all messages in the queue is delayed.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3679,
                        "content": "<p>Set the <code>DelaySeconds</code> parameter of a message to 20 seconds</p>",
                        "isValid": false
                    },
                    {
                        "id": 3680,
                        "content": "<p>Set the Imaging queue <code>ReceiveMessageWaitTimeSeconds</code> attribute to 20 seconds</p>",
                        "isValid": true
                    },
                    {
                        "id": 3681,
                        "content": "<p>Set the imaging queue visibility Timeout attribute to 20 seconds</p>",
                        "isValid": false
                    },
                    {
                        "id": 3682,
                        "content": "<p>Set the imaging queue <code>MessageRetentionPeriod</code> attribute to 20 seconds</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 896,
            "attributes": {
                "createdAt": "2023-09-07T08:51:36.263Z",
                "updatedAt": "2023-09-07T08:51:36.263Z",
                "content": "<p>A company is creating a REST service using an Amazon API Gateway with AWS Lambda integration. The service must run different versions for testing purposes.</p><p>What would be the BEST way to accomplish this?</p>",
                "answerExplanation": "<p>A stage is a named reference to a deployment, which is a snapshot of the API. You use a <a href=\"https://docs.aws.amazon.com/apigateway/api-reference/resource/stage/\">Stage</a> to manage and optimize a particular deployment. For example, you can configure stage settings to enable caching, customize request throttling, configure logging, define stage variables, or attach a canary release for testing.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_01-21-19-652a88b3187118ee7b600703ea09c4f1.png\"></p><p>Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates.</p><p>With stages and stage variables, you can configure different settings for different versions of the application and point to different versions of your Lambda function.</p><p><strong>CORRECT: </strong>\"Deploy the API version as unique stages with unique endpoints and use stage variables to provide further context\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an X-Version header to denote which version is being called and pass that header to the Lambda function(s)\" is incorrect as you cannot pass a value in a header to a Lambda function and have that determine which version is executed. Versions have unique ARNs and must be connected to separately.</p><p><strong>INCORRECT:</strong> \"Create an API Gateway Lambda authorizer to route API clients to the correct API version\" is incorrect as a Lambda authorizer is used for authentication, and different versions of an API are created using stages.</p><p><strong>INCORRECT:</strong> \"Create an API Gateway resource policy to isolate versions and provide context to the Lambda function(s)\" is incorrect as resource policies are not used to isolate versions or provide context. In this scenario, stages and stage variables should be used.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-stages.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-stages.html</a></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3683,
                        "content": "<p>Create an API Gateway Lambda authorizer to route API clients to the correct API version</p>",
                        "isValid": false
                    },
                    {
                        "id": 3684,
                        "content": "<p>Create an API Gateway resource policy to isolate versions and provide context to the Lambda function(s)</p>",
                        "isValid": false
                    },
                    {
                        "id": 3685,
                        "content": "<p>Use an X-Version header to denote which version is being called and pass that header to the Lambda function(s)</p>",
                        "isValid": false
                    },
                    {
                        "id": 3686,
                        "content": "<p>Deploy the API version as unique stages with unique endpoints and use stage variables to provide further context</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 897,
            "attributes": {
                "createdAt": "2023-09-07T08:51:36.342Z",
                "updatedAt": "2023-09-07T08:51:36.342Z",
                "content": "<p>A company is migrating several applications to the AWS cloud. The security team has strict security requirements and mandate that a log of all API calls to AWS resources must be maintained.</p><p>Which AWS service should be used to record this information for the security team?</p>",
                "answerExplanation": "<p>AWS CloudTrail is a web service that records activity made on your account. A CloudTrail trail can be created which delivers log files to an Amazon S3 bucket. CloudTrail is about logging and saves a history of API calls for your AWS account. It enables governance, compliance, and operational and risk auditing of your AWS account.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_01-25-19-afb6ab00d650b63323cd33a322d3207b.png\"></p><p>Therefore, AWS CloudTrail is the best solution for maintaining a log of API calls for the security team.</p><p><strong>CORRECT: </strong>\"AWS CloudTrail\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon CloudWatch\" is incorrect as this service records metrics related to performance.</p><p><strong>INCORRECT:</strong> \"Amazon CloudWatch Logs\" is incorrect as this records log files from services and applications, it does not record a history of API activity.</p><p><strong>INCORRECT:</strong> \"AWS X-Ray\" is incorrect as this is used for tracing applications to view performance-related statistics.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudtrail/\">https://digitalcloud.training/aws-cloudtrail/</a></p>",
                "options": [
                    {
                        "id": 3687,
                        "content": "<p>AWS CloudTrail</p>",
                        "isValid": true
                    },
                    {
                        "id": 3688,
                        "content": "<p>AWS X-Ray</p>",
                        "isValid": false
                    },
                    {
                        "id": 3689,
                        "content": "<p>Amazon CloudWatch Logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 3690,
                        "content": "<p>Amazon CloudWatch</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 898,
            "attributes": {
                "createdAt": "2023-09-07T08:51:36.418Z",
                "updatedAt": "2023-09-07T08:51:36.418Z",
                "content": "<p>A development team have deployed a new application and users have reported some performance issues. The developers need to enable monitoring for specific metrics with a data granularity of one second. How can this be achieved?</p>",
                "answerExplanation": "<p>You can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console.</p><p>CloudWatch stores data about a metric as a series of data points. Each data point has an associated time stamp. You can even publish an aggregated set of data points called a <em>statistic set</em>.</p><p>Each metric is one of the following:</p><p> â€¢ Standard resolution, with data having a one-minute granularity</p><p> â€¢ High resolution, with data at a granularity of one second</p><p>Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p><p>High-resolution metrics can give you more immediate insight into your application's sub-minute activity. Keep in mind that every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a high-resolution metric can lead to higher charges.</p><p>Therefore, the best action to take is to Create custom metrics and configure them as high resolution. This will ensure that granularity can be down to 1 second.</p><p><strong>CORRECT: </strong>\"Create custom metrics and configure them as high resolution\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Do nothing, CloudWatch uses standard resolution metrics by default\" is incorrect as standard resolution has a granularity of one-minute.</p><p><strong>INCORRECT:</strong> \"Create custom metrics and configure them as standard resolution\" is incorrect as standard resolution has a granularity of one-minute.</p><p><strong>INCORRECT:</strong> \"Create custom metrics and enable detailed monitoring\" is incorrect as detailed monitoring has a granularity of one-minute.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3691,
                        "content": "<p>Create custom metrics and enable detailed monitoring</p>",
                        "isValid": false
                    },
                    {
                        "id": 3692,
                        "content": "<p>Create custom metrics and configure them as high resolution</p>",
                        "isValid": true
                    },
                    {
                        "id": 3693,
                        "content": "<p>Create custom metrics and configure them as standard resolution</p>",
                        "isValid": false
                    },
                    {
                        "id": 3694,
                        "content": "<p>Do nothing, CloudWatch uses standard resolution metrics by default</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 899,
            "attributes": {
                "createdAt": "2023-09-07T08:51:36.493Z",
                "updatedAt": "2023-09-07T08:51:36.493Z",
                "content": "<p>A Developer has deployed an AWS Lambda function and an Amazon DynamoDB table. The function code returns data from the DynamoDB table when it receives a request. The Developer needs to implement a front end that can receive HTTP GET requests and proxy the request information to the Lambda function.</p><p>What is the SIMPLEST and most COST-EFFECTIVE solution?</p>",
                "answerExplanation": "<p>Amazon API Gateway Lambda proxy integration is a simple, powerful, and nimble mechanism to build an API with a setup of a single API method. The Lambda proxy integration allows the client to call a single Lambda function in the backend. The function accesses many resources or features of other AWS services, including calling other Lambda functions.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_01-19-39-17152d82765011b62d3818c5b19e8f83.png\"></p><p>In Lambda proxy integration, when a client submits an API request, API Gateway passes to the integrated Lambda function the raw request as-is, except that the order of the request parameters is not preserved. This <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-input-format\">request data</a> includes the request headers, query string parameters, URL path variables, payload, and API configuration data.</p><p>This solution provides a front end that can listen for HTTP GET requests and then proxy them to the Lambda function and is the simplest option to implement and also the most cost-effective.</p><p><strong>CORRECT: </strong>\"Implement an API Gateway API with Lambda proxy integration\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement an API Gateway API with a POST method\" is incorrect as a GET method should be implemented. A GET method is a request for data whereas a POST method is a request to upload data.</p><p><strong>INCORRECT:</strong> \"Implement an Elastic Load Balancer with a Lambda function target\" is incorrect as though you can do this it is not the simplest or most cost-effective solution.</p><p><strong>INCORRECT:</strong> \"Implement an Amazon Cognito User Pool with a Lambda proxy integration\" is incorrect as you cannot create Lambda proxy integrations with Cognito.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 3695,
                        "content": "<p>Implement an API Gateway API with a POST method</p>",
                        "isValid": false
                    },
                    {
                        "id": 3696,
                        "content": "<p>Implement an Elastic Load Balancer with a Lambda function target</p>",
                        "isValid": false
                    },
                    {
                        "id": 3697,
                        "content": "<p>Implement an Amazon Cognito User Pool with a Lambda proxy integration</p>",
                        "isValid": false
                    },
                    {
                        "id": 3698,
                        "content": "<p>Implement an API Gateway API with Lambda proxy integration</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 900,
            "attributes": {
                "createdAt": "2023-09-07T08:51:36.563Z",
                "updatedAt": "2023-09-07T08:51:36.563Z",
                "content": "<p>A company is running an application built on AWS Lambda functions. One Lambda function has performance issues when it has to download a 50 MB file from the internet every execution. This function is called multiple times a second.</p><p>What solution would give the BEST performance increase?</p>",
                "answerExplanation": "<p>The /tmp directory provides 512 MB of storage space that can be used by a function. When a file is cached by a function in the /tmp directory it is available to be used by subsequent executions of the function which will reduce latency.</p><p><strong>CORRECT: </strong>\"Cache the file in the /tmp directory\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Increase the Lambda maximum execution time\" is incorrect as the function is not timing out.</p><p><strong>INCORRECT:</strong> \"Put an Elastic Load Balancer in front of the Lambda function\" is incorrect as this would not reduce latency or improve performance.</p><p><strong>INCORRECT:</strong> \"Cache the file in Amazon S3\" is incorrect as this would not provide better performance as it would still need to be retrieved from S3 for each execution if it is not cached in the /tmp directory.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html\">https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3699,
                        "content": "<p>Put an Elastic Load Balancer in front of the Lambda function</p>",
                        "isValid": false
                    },
                    {
                        "id": 3700,
                        "content": "<p>Cache the file in Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 3701,
                        "content": "<p>Increase the Lambda maximum execution time</p>",
                        "isValid": false
                    },
                    {
                        "id": 3702,
                        "content": "<p>Cache the file in the /tmp directory</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 901,
            "attributes": {
                "createdAt": "2023-09-07T08:51:36.640Z",
                "updatedAt": "2023-09-07T08:51:36.640Z",
                "content": "<p>A Developer is working on an AWS Lambda function that accesses Amazon DynamoDB. The Lambda function must retrieve an item and update some of its attributes or create the item if it does not exist. The Lambda function has access to the primary key.</p><p>Which IAM permission should the Developer request for the Lambda function to achieve this functionality? </p>",
                "answerExplanation": "<p>The Developer needs the permissions to retrieve items, update/modify items, and create items. Therefore permissions for the following API actions are required:</p><p> â€¢ GetItem - The GetItem operation returns a set of attributes for the item with the given primary key.</p><p> â€¢ UpdateItem - Edits an existing item's attributes, or adds a new item to the table if it does not already exist. You can put, delete, or add attribute values.</p><p> â€¢ PutItem - Creates a new item, or replaces an old item with a new item. If an item that has the same primary key as the new item already exists in the specified table, the new item completely replaces the existing item.</p><p><strong>CORRECT: </strong>\"â€œ<code>dynamodb:UpdateItem</code>â€, â€œ<code>dynamodb:GetItem</code>â€, and â€œ<code>dynamodb:PutItem</code>â€\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"â€œ<code>dynamodb:DeleteItem</code>â€, â€œ<code>dynamodb:GetItem</code>â€, and â€œ<code>dynamodb:PutItem</code>â€\" is incorrect as the Developer does not need the permission to delete items.</p><p><strong>INCORRECT:</strong> \"â€œ<code>dynamodb:UpdateItem</code>â€, â€œ<code>dynamodb:GetItem</code>â€, and â€œ<code>dynamodb:DescribeTable</code>â€\" is incorrect as the Developer does not need to return information about the table (DescribeTable) such as the current status of the table, when it was created, the primary key schema, and any indexes on the table.</p><p><strong>INCORRECT:</strong> \"â€œ<code>dynamodb:GetRecords</code>â€, â€œ<code>dynamodb:PutItem</code>â€, and â€œ<code>dynamodb:UpdateTable</code>â€\" is incorrect as GetRecords is not a valid API action/permission for DynamoDB.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Operations.html\">https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Operations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3703,
                        "content": "<p>â€œ<code>dynamodb:UpdateItem</code>â€, â€œ<code>dynamodb:GetItem</code>â€, and â€œ<code>dynamodb:DescribeTable</code>â€</p>",
                        "isValid": false
                    },
                    {
                        "id": 3704,
                        "content": "<p>â€œ<code>dynamodb:UpdateItem</code>â€, â€œ<code>dynamodb:GetItem</code>â€, and â€œ<code>dynamodb:PutItem</code>â€</p>",
                        "isValid": true
                    },
                    {
                        "id": 3705,
                        "content": "<p>â€œ<code>dynamodb:GetRecords</code>â€, â€œ<code>dynamodb:PutItem</code>â€, and â€œ<code>dynamodb:UpdateTable</code>â€</p>",
                        "isValid": false
                    },
                    {
                        "id": 3706,
                        "content": "<p>â€œ<code>dynamodb:DeleteItem</code>â€, â€œ<code>dynamodb:GetItem</code>â€, and â€œ<code>dynamodb:PutItem</code>â€</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 902,
            "attributes": {
                "createdAt": "2023-09-07T08:51:36.711Z",
                "updatedAt": "2023-09-07T08:51:36.711Z",
                "content": "<p>A small team of Developers require access to an Amazon S3 bucket. An admin has created a resource-based policy. Which element of the policy should be used to specify the ARNs of the user accounts that will be granted access?</p>",
                "answerExplanation": "<p>Use the Principal element in a policy to specify the principal that is allowed or denied access to a resource. You cannot use the Principal element in an IAM identity-based policy. You can use it in the trust policies for IAM roles and in resource-based policies. Resource-based policies are policies that you embed directly in an IAM resource.</p><p><strong>CORRECT: </strong>\"<code>Principal</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"<code>Conditio</code>n\" is incorrect. The <code>Condition</code> element (or Condition <em>block</em>) lets you specify conditions for when a policy is in effect.</p><p><strong>INCORRECT:</strong> \"<code>Sid</code>\" is incorrect. The <code>Sid</code> (statement ID) is an optional identifier that you provide for the policy statement.</p><p><strong>INCORRECT:</strong> \"<code>Id</code>\" is incorrect. The <code>Id</code> element specifies an optional identifier for the policy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 3707,
                        "content": "<p><code>Principal</code> </p>",
                        "isValid": true
                    },
                    {
                        "id": 3708,
                        "content": "<p><code>Id</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3709,
                        "content": "<p><code>Sid</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3710,
                        "content": "<p><code>Condition</code> </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 903,
            "attributes": {
                "createdAt": "2023-09-07T08:51:36.788Z",
                "updatedAt": "2023-09-07T08:51:36.788Z",
                "content": "<p>A company is running an order processing system on AWS. Amazon SQS is used to queue orders and an AWS Lambda function processes them. The company recently started noticing a lot of orders are failing to process.</p><p>How can a Developer MOST effectively manage these failures to debug the failed orders later and reprocess them, as necessary?</p>",
                "answerExplanation": "<p>Amazon SQS supports <em>dead-letter queues</em>, which other queues (<em>source queues</em>) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_01-23-53-a972148ac0a49873ac19bc1f39a108c3.png\"></p><p>The Developer should therefore implement dead-letter queues for failed orders from the order queue. This will allow full debugging as the entire message is available for analysis.</p><p><strong>CORRECT: </strong>\"Implement dead-letter queues for failed orders from the order queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Publish failed orders from the order queue to an Amazon SNS topic\" is incorrect as there is no way to isolate messages that have failed to process when subscribing an SQS queue to an SNS topic.</p><p><strong>INCORRECT:</strong> \"Log the failed orders from the order queue using Amazon CloudWatch Logs\" is incorrect as SQS does not publish message success/failure to CloudWatch Logs.</p><p><strong>INCORRECT:</strong> \"Send failed orders from the order queue to AWS CloudTrail logs\" is incorrect as CloudTrail records API activity not performance metrics or logs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3711,
                        "content": "<p>Log the failed orders from the order queue using Amazon CloudWatch Logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 3712,
                        "content": "<p>Implement dead-letter queues for failed orders from the order queue</p>",
                        "isValid": true
                    },
                    {
                        "id": 3713,
                        "content": "<p>Send failed orders from the order queue to AWS CloudTrail logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 3714,
                        "content": "<p>Publish failed orders from the order queue to an Amazon SNS topic</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 904,
            "attributes": {
                "createdAt": "2023-09-07T08:51:36.863Z",
                "updatedAt": "2023-09-07T08:51:36.863Z",
                "content": "<p>A company is migrating a stateful web service into the AWS cloud. The objective is to refactor the application to realize the benefits of cloud computing. How can the Developer leading the project refactor the application to enable more elasticity? (Select TWO.)</p>",
                "answerExplanation": "<p>As this is a stateful application the session data needs to be stored somewhere. Amazon DynamoDB is designed to be used for storing session data and it highly scalable. To add elasticity to the architecture an Amazon Elastic Load Balancer (ELB) and Amazon EC2 Auto Scaling group (ASG) can be used.</p><p>With this architecture the web service can scale elastically using the ASG and the ELB will distribute traffic to all new instances that the ASG launches. This is a good example of utilizing some of the key benefits of refactoring applications into the AWS cloud.</p><p><strong>CORRECT: </strong>\"Use an Elastic Load Balancer and Auto Scaling Group\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Store the session state in an Amazon DynamoDB table\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudFormation and the Serverless Application Model\" is incorrect. AWS SAM is used in CloudFormation templates for expressing serverless applications using a simplified syntax. This application is not a serverless application.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudFront with a Web Application Firewall\" is incorrect neither protection from web exploits nor improved performance for content delivery are requirements in this scenario.</p><p><strong>INCORRECT:</strong> \"Store the session state in an Amazon RDS database\" is incorrect as RDS is not suitable for storing session state data. DynamoDB is a better fit for this use case.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html\">https://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3715,
                        "content": "<p>Store the session state in an Amazon RDS database</p>",
                        "isValid": false
                    },
                    {
                        "id": 3716,
                        "content": "<p>Use Amazon CloudFormation and the Serverless Application Model</p>",
                        "isValid": false
                    },
                    {
                        "id": 3717,
                        "content": "<p>Store the session state in an Amazon DynamoDB table</p>",
                        "isValid": true
                    },
                    {
                        "id": 3718,
                        "content": "<p>Use Amazon CloudFront with a Web Application Firewall</p>",
                        "isValid": false
                    },
                    {
                        "id": 3719,
                        "content": "<p>Use an Elastic Load Balancer and Auto Scaling Group</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 905,
            "attributes": {
                "createdAt": "2023-09-07T08:51:36.939Z",
                "updatedAt": "2023-09-07T08:51:36.939Z",
                "content": "<p>A gaming application stores scores for players in an Amazon DynamoDB table that has four attributes: user_id, user_name, user_score, and user_rank. The users are allowed to update their names only. A user is authenticated by web identity federation.</p><p>Which set of conditions should be added in the policy attached to the role for the <code>dynamodb:PutItem</code> API call?</p>",
                "answerExplanation": "<p>The users are authenticated by web identity federation. The user_id value should be used to identify the user in the policy and the policy needs to then allow the user to change the user_name value when using the <code>dynamodb:PutItem</code> API call.</p><p>The key parts of the code to look for are the dynamodb:LeadingKeys which represents the partition key of the table and the dynamodb:Attributes which represents the items that can be changed.</p><p><strong>CORRECT: </strong>The answer that includes dynamodb:LeadingKeys identifying user_id and dynamodb:Attributes identifying user_name is the correct answer.</p><p><strong>INCORRECT:</strong> The other answers provide a few incorrect code samples where either the dynamodb:LeadingKeys identifies user_name (which is incorrect as it is the item to be changed) or dynamodb:Attributes identifying the wrong attributes for modification (should be user_name).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html#FGAC_DDB.ConditionKeys\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html#FGAC_DDB.ConditionKeys</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3720,
                        "content": "<p>\"Condition\": {</p><p>\"ForAllValues:StringEquals\": {</p><p>\"dynamodb:LeadingKeys\": [</p><p>\"${www.amazon.com:user_name}\"</p><p>],</p><p>\"dynamodb:Attributes\": [</p><p>\"user_id\"</p><p>]</p><p>}</p><p>}</p>",
                        "isValid": false
                    },
                    {
                        "id": 3721,
                        "content": "<p>\"Condition\": {</p><p>\"ForAllValues:StringEquals\": {</p><p>\"dynamodb:LeadingKeys\": [</p><p>\"${www.amazon.com:user_id}\"</p><p>],</p><p>\"dynamodb:Attributes\": [</p><p>\"user_name\"</p><p>]</p><p>}</p><p>}</p>",
                        "isValid": true
                    },
                    {
                        "id": 3722,
                        "content": "<p>\"Condition\": {</p><p>\"ForAllValues:StringEquals\": {</p><p>\"dynamodb:LeadingKeys\": [</p><p>\"${www.amazon.com:user_id}\"</p><p>],</p><p>\"dynamodb:Attributes\": [</p><p>\"user_name\", \"user_id\"</p><p>]</p><p>}</p><p>}</p>",
                        "isValid": false
                    },
                    {
                        "id": 3723,
                        "content": "<p>\"Condition\": {</p><p>\"ForAllValues:StringEquals\": {</p><p>\"dynamodb:LeadingKeys\": [</p><p>\"${www.amazon.com:user_name}\"</p><p>],</p><p>\"dynamodb:Attributes\": [</p><p>\"user_name\", \"user_id\"</p><p>]</p><p>}</p><p>}</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 906,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.009Z",
                "updatedAt": "2023-09-07T08:51:37.009Z",
                "content": "<p>A Developer has completed some code updates and needs to deploy the updates to an Amazon Elastic Beanstalk environment. Due to the criticality of the application, the ability to quickly roll back must be prioritized of any other considerations.</p><p>Which deployment policy should the Developer choose?</p>",
                "answerExplanation": "<p>AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.</p><p>Each deployment policy has advantages and disadvantages and itâ€™s important to select the best policy to use for each situation. The following tables summarizes the different deployment policies:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-22_05-49-02-7f7314ca37c3c5f15857fb9849da03b8.jpg\"></p><p>The â€œimmutableâ€ policy will create a new ASG with a whole new set of instances and deploy the updates there.</p><p>Immutable:</p><p> â€¢ Launches new instances in a new ASG and deploys the version update to these instances before swapping traffic to these instances once healthy.</p><p> â€¢ Zero downtime.</p><p> â€¢ New code is deployed to new instances using an ASG.</p><p> â€¢ High cost as double the number of instances running during updates.</p><p> â€¢ Longest deployment.</p><p> â€¢ Quick rollback in case of failures.</p><p> â€¢ Great for production environments.</p><p>For this scenario a quick rollback must be prioritized over all other considerations. Therefore, the best choice is â€œimmutableâ€. This deployment policy is the most expensive and longest (duration) option. However, you can roll back quickly and safely as the original instances are all available and unmodified.</p><p><strong>CORRECT: </strong>\"Immutable\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Rolling\" is incorrect as this policy requires manual redeployment if there are any issues caused by the update.</p><p><strong>INCORRECT:</strong> \"Rolling with additional batch\" is incorrect as this policy requires manual redeployment if there are any issues caused by the update.</p><p><strong>INCORRECT:</strong> \"All at once\" is incorrect as this takes the entire environment down at once and requires manual redeployment if there are any issues caused by the update.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
                "options": [
                    {
                        "id": 3724,
                        "content": "<p>Immutable</p>",
                        "isValid": true
                    },
                    {
                        "id": 3725,
                        "content": "<p>All at once</p>",
                        "isValid": false
                    },
                    {
                        "id": 3726,
                        "content": "<p>Rolling with additional batch</p>",
                        "isValid": false
                    },
                    {
                        "id": 3727,
                        "content": "<p>Rolling</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 907,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.080Z",
                "updatedAt": "2023-09-07T08:51:37.080Z",
                "content": "<p>A company has transferred some of its confidential documents to a private Amazon S3 bucket that is not publicly accessible. Now, the company intends to build a serverless application that allows its staff to securely share these files with others.</p><p>Which AWS service should the company utilize to ensure secure file sharing and access?</p>",
                "answerExplanation": "<p>S3 presigned URLs are the correct answer because they provide secure, temporary access to a specific S3 object without requiring AWS security credentials. This allows users to share the files securely with others.</p><p><strong>CORRECT: </strong>\"S3 presigned URLs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS Identity and Access Management (IAM) roles\" is incorrect.</p><p>AWS Identity and Access Management (IAM) roles are used for granting applications or AWS services permissions to access AWS resources. They don't directly facilitate the secure sharing of S3 objects with external users.</p><p><strong>INCORRECT:</strong> \"Amazon Cognito identity pool\" is incorrect.</p><p>Amazon Cognito identity pools are used to provide temporary AWS credentials to users of your application, not for directly enabling secure file sharing.</p><p><strong>INCORRECT:</strong> \"S3 Access Control Lists (ACLs)\" is incorrect.</p><p>S3 Access Control Lists (ACLs) can be used to manage permissions at the object level, but they do not provide a method to securely share specific S3 objects with others.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 3728,
                        "content": "<p>S3 presigned URLs</p>",
                        "isValid": true
                    },
                    {
                        "id": 3729,
                        "content": "<p>S3 Access Control Lists (ACLs)</p>",
                        "isValid": false
                    },
                    {
                        "id": 3730,
                        "content": "<p>Amazon Cognito identity pool</p>",
                        "isValid": false
                    },
                    {
                        "id": 3731,
                        "content": "<p>AWS Identity and Access Management (IAM) roles</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 908,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.155Z",
                "updatedAt": "2023-09-07T08:51:37.155Z",
                "content": "<p>A company uses an Amazon Simple Queue Service (SQS) Standard queue for an application. An issue has been identified where applications are picking up messages from the queue that are still being processed causing duplication. What can a Developer do to resolve this issue? </p>",
                "answerExplanation": "<p>When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Because Amazon SQS is a distributed system, there's no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_01-17-17-29e7e364b35bd616db3f2076b1979dae.png\"></p><p>Immediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a <em>visibility timeout</em>, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours.</p><p>Therefore, the best thing the Developer can do in this situation is to increase the <code>VisibilityTimeout</code> API action on the queue</p><p><strong>CORRECT: </strong>\"Increase the <code>VisibilityTimeout</code> API action on the queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Increase the <code>DelaySecond</code>s API action on the queue\" is incorrect as this controls the length of time, in seconds, for which the delivery of all messages in the queue is delayed.</p><p><strong>INCORRECT:</strong> \"Increase the <code>ReceiveMessageWaitTimeSeconds </code>API action on the queue\" is incorrect as this is the length of time, in seconds, for which a <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\">ReceiveMessage</a> action waits for a message to arrive. This is used to configure long polling.</p><p><strong>INCORRECT:</strong> \"Create a <code>RedrivePolicy</code> for the queue\" is incorrect as this is a string that includes the parameters for the dead-letter queue functionality of the source queue as a JSON object.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3732,
                        "content": "<p>Increase the <code>ReceiveMessageWaitTimeSeconds</code> API action on the queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 3733,
                        "content": "<p>Create a <code>RedrivePolicy</code> for the queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 3734,
                        "content": "<p>Increase the <code>DelaySeconds</code> API action on the queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 3735,
                        "content": "<p>Increase the <code>VisibilityTimeou</code>t API action on the queue</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 909,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.228Z",
                "updatedAt": "2023-09-07T08:51:37.228Z",
                "content": "<p>An application exports files which must be saved for future use but are not frequently accessed. Compliance requirements necessitate redundant retention of data across AWS regions. Which solution is the MOST cost-effective for these requirements?</p>",
                "answerExplanation": "<p>Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. You can copy objects between different AWS Regions or within the same Region.</p><p>To enable object replication, you add a replication configuration to your source bucket. The minimum configuration must provide the following:</p><p>The destination bucket where you want Amazon S3 to replicate objects</p><p>An AWS Identity and Access Management (IAM) role that Amazon S3 can assume to replicate objects on your behalf</p><p>You can replicate objects between different AWS Regions or within the same AWS Region.</p><p><strong>Cross-Region replication</strong> (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions.</p><p><strong>Same-Region replication</strong> (SRR) is used to copy objects across Amazon S3 buckets in the same AWS Region.</p><p>For this scenario, CRR would be a better fit as the data must be replicated across regions.</p><p><strong>CORRECT: </strong>\"Amazon S3 with Cross-Region Replication (CRR)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon S3 with Same-Region Replication (CRR)\" is incorrect as the requirement is to replicated data across AWS regions.</p><p><strong>INCORRECT:</strong> \"Amazon DynamoDB with Global Tables\" is incorrect as this is unlikely to be the most cost-effective solution when data is infrequently accessed. It also may not be possible to store the files in the database, they may need to be referenced from an external location such as S3.</p><p><strong>INCORRECT:</strong> \"AWS Storage Gateway with a replicated file gateway\" is incorrect. AWS Storage Gateway connects an on-premises software appliance with cloud-based storage to provide seamless integration. This is not used for replicating data within the AWS cloud across regions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 3736,
                        "content": "<p>Amazon S3 with Cross-Region Replication (CRR)</p>",
                        "isValid": true
                    },
                    {
                        "id": 3737,
                        "content": "<p>AWS Storage Gateway with a replicated file gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 3738,
                        "content": "<p>Amazon DynamoDB with Global Tables</p>",
                        "isValid": false
                    },
                    {
                        "id": 3739,
                        "content": "<p>Amazon S3 with Same-Region Replication (CRR)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 910,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.301Z",
                "updatedAt": "2023-09-07T08:51:37.301Z",
                "content": "<p>An Amazon DynamoDB table will store authentication credentials for a mobile app. The table must be secured so only a small group of Developers are able to access it.</p><p>How can table access be secured according to this requirement and following AWS best practice?</p>",
                "answerExplanation": "<p>Amazon DynamoDB supports identity-based policies only. The best practice method to assign permissions to the table is to create a permissions policy that grants access to the table and assigning that policy to an IAM group that contains the Developerâ€™s user accounts.</p><p>This will provide all users with accounts in the IAM group with the access required to access the DynamoDB table.</p><p><strong>CORRECT: </strong>\"Attach a permissions policy to an IAM group containing the Developerâ€™s IAM user accounts that grants access to the table\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Attach a resource-based policy to the table and add an IAM group containing the Developerâ€™s IAM user accounts as a Principal in the policy\" is incorrect as you cannot assign resource-based policies to DynamoDB tables.</p><p><strong>INCORRECT:</strong> \"Create an AWS KMS resource-based policy to a CMK and grant the developerâ€™s user accounts the permissions to decrypt data in the table using the CMK\" is incorrect as the questions requires that the Developers can access the table, not to be able to decrypt data.</p><p><strong>INCORRECT:</strong> \"Create a shared user account and attach a permissions policy granting access to the table. Instruct the Developerâ€™s to login with the user account\" is incorrect as this is against AWS best practice. You should never share user accounts.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3740,
                        "content": "<p>Attach a permissions policy to an IAM group containing the Developerâ€™s IAM user accounts that grants access to the table</p>",
                        "isValid": true
                    },
                    {
                        "id": 3741,
                        "content": "<p>Attach a resource-based policy to the table and add an IAM group containing the Developerâ€™s IAM user accounts as a Principal in the policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 3742,
                        "content": "<p>Create an AWS KMS resource-based policy to a CMK and grant the developerâ€™s user accounts the permissions to decrypt data in the table using the CMK</p>",
                        "isValid": false
                    },
                    {
                        "id": 3743,
                        "content": "<p>Create a shared user account and attach a permissions policy granting access to the table. Instruct the Developerâ€™s to login with the user account</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 911,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.377Z",
                "updatedAt": "2023-09-07T08:51:37.377Z",
                "content": "<p>A Developer has created the code for a Lambda function saved the code in a file named lambda_function.py. He has also created a template that named template.yaml. The following code is included in the template file:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"typ\">AWSTemplateFormatVersion</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">'2010-09-09'</span></li><li class=\"L1\"><span class=\"typ\">Transform</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">'AWS::Serverless-2016-10-31'</span></li><li class=\"L2\"><span class=\"typ\">Resources</span><span class=\"pun\">:</span></li><li class=\"L3\"><span class=\"pln\">microservicehttpendpointpython3</span><span class=\"pun\">:</span></li><li class=\"L4\"><span class=\"typ\">Type</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">'AWS::Serverless::Function'</span></li><li class=\"L5\"><span class=\"typ\">Properties</span><span class=\"pun\">:</span></li><li class=\"L6\"><span class=\"typ\">Handler</span><span class=\"pun\">:</span><span class=\"pln\"> lambda_function</span><span class=\"pun\">.</span><span class=\"pln\">lambda_handler</span></li><li class=\"L7\"><span class=\"typ\">CodeUri</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">.</span></li></ol></pre></div></div><p>What commands can the Developer use to prepare and then deploy this template? (Select TWO.)</p>",
                "answerExplanation": "<p>The template shown is an AWS SAM template for deploying a serverless application. This can be identified by the template header: <em>Transform: 'AWS::Serverless-2016-10-31'</em></p><p>The Developer will need to package and then deploy the template. To do this the source code must be available in the same directory or referenced using the â€œcodeuriâ€ parameter. Then, the Developer can use the â€œaws cloudformation packageâ€ or â€œsam packageâ€ commands to prepare the local artifacts (local paths) that your AWS CloudFormation template references.</p><p>The command uploads local artifacts, such as source code for an AWS Lambda function or a Swagger file for an AWS API Gateway REST API, to an S3 bucket. The command returns a copy of your template, replacing references to local artifacts with the S3 location where the command uploaded the artifacts.</p><p>Once that is complete the template can be deployed using the â€œaws cloudformation deployâ€ or â€œsam deployâ€ commands. Therefore, the developer has two options to prepare and then deploy this package:</p><p>1. Run <code>aws cloudformation package</code> and then aws cloudformation deploy</p><p>2. Run <code>sam package</code> and then <code>sam deploy</code></p><p><strong>CORRECT: </strong>\"Run <code>aws cloudformation</code> package and then <code>aws cloudformation deploy</code>\" is a correct answer.</p><p><strong>INCORRECT:</strong> \"Run <code>sam package</code> and then <code>sam deploy</code>\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Run <code>aws cloudformation compile</code> and then <code>aws cloudformation deploy</code>\" is incorrect as the â€œcompileâ€ command should be replaced with the â€œpackageâ€ command.</p><p><strong>INCORRECT:</strong> \"Run <code>sam build</code> and then <code>sam package</code>\" is incorrect as the Developer needs to run the â€œpackageâ€ command first and then the â€œdeployâ€ command to actually deploy the function.</p><p><strong>INCORRECT:</strong> \"Run <code>aws serverless package</code> and then <code>aws serverless deploy</code>\" is incorrect as there is no AWS CLI command named â€œserverlessâ€.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html\">https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-sam/\">https://digitalcloud.training/aws-sam/</a></p>",
                "options": [
                    {
                        "id": 3744,
                        "content": "<p>Run <code>aws cloudformatio</code>n compile and then <code>aws cloudformation deploy</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3745,
                        "content": "<p>Run <code>aws cloudformation </code>package and then <code>aws cloudformation deploy</code> </p>",
                        "isValid": true
                    },
                    {
                        "id": 3746,
                        "content": "<p>Run <code>sam package</code> and then <code>sam deploy</code> </p>",
                        "isValid": true
                    },
                    {
                        "id": 3747,
                        "content": "<p>Run <code>aws serverless package</code> and then <code>aws serverless deploy</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3748,
                        "content": "<p>Run <code>sam build</code> and then <code>sam package</code> </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 912,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.454Z",
                "updatedAt": "2023-09-07T08:51:37.454Z",
                "content": "<p>A Developer is writing a web application that allows users to view images from an Amazon S3 bucket. The users will log in with their Amazon login, as well as Facebook and/or Google accounts.</p><p>How can the Developer provide this authentication capability? </p>",
                "answerExplanation": "<p><strong>Explanation:</strong></p><p>Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. Amazon Cognito identity pools support the following identity providers:</p><p> â€¢ Public providers: <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/amazon.html\">Login with Amazon (Identity Pools)</a>, <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/facebook.html\">Facebook (Identity Pools)</a>, <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/google.html\">Google (Identity Pools)</a> <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/apple.html\">Sign in with Apple (Identity Pools)</a>.</p><p> â€¢ <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">Amazon Cognito User Pools</a></p><p> â€¢ <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/open-id.html\">Open ID Connect Providers (Identity Pools)</a></p><p> â€¢ <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/saml-identity-provider.html\">SAML Identity Providers (Identity Pools)</a></p><p> â€¢ <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html\">Developer Authenticated Identities (Identity Pools)</a></p><p>With the temporary, limited-privilege AWS credentials users will be able to access the images in the S3 bucket. Therefore, the Developer should use Amazon Cognito with web identity federation</p><p><strong>CORRECT: </strong>\"Use Amazon Cognito with web identity federation\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon Cognito with SAML-based identity federation\" is incorrect as SAML is used with directory sources such as Microsoft Active Directory, not Facebook or Google.</p><p><strong>INCORRECT:</strong> \"Use AWS IAM Access/Secret keys in the application code to allow <code>Get* </code>on the S3 bucket\" is incorrect as this insecure and against best practice. Always try to avoid embedding access keys in application code.</p><p><strong>INCORRECT:</strong> \"Use AWS STS <code>AssumeRole</code> in the application code and assume a role with <code>Get* </code>permissions on the S3 bucket\" is incorrect as you cannot do this directly through a Facebook or Google login. For this scenario, a Cognito Identity Pool is required to authenticate the user from the social IdP and provide access to the AWS services.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 3749,
                        "content": "<p>Use Amazon Cognito with web identity federation</p>",
                        "isValid": true
                    },
                    {
                        "id": 3750,
                        "content": "<p>Use AWS STS <code>AssumeRole</code> in the application code and assume a role with <code>Get*</code> permissions on the S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 3751,
                        "content": "<p>Use Amazon Cognito with SAML-based identity federation</p>",
                        "isValid": false
                    },
                    {
                        "id": 3752,
                        "content": "<p>Use AWS IAM Access/Secret keys in the application code to allow <code>Get*</code> on the S3 bucket</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 913,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.526Z",
                "updatedAt": "2023-09-07T08:51:37.526Z",
                "content": "<p>An international research organization holds a wide range of data across several Amazon S3 buckets. They recently received an alert indicating potential exposure of sensitive financial data via a public-facing web portal. The developer's job is to trace all potential data leakage points across their AWS infrastructure.</p><p>What is the most effective strategy for this task?</p>",
                "answerExplanation": "<p>Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect sensitive data in AWS. Using the <strong>SensitiveData:S3Object/financial</strong> finding type within Macie, it is possible to automatically detect and identify sensitive financial data in your S3 buckets.</p><p><strong>CORRECT: </strong>\"Implement Amazon Macie and apply the SensitiveData:S3Object/financial finding type across all S3 buckets to automatically identify potential exposure of sensitive financial data\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement Amazon Macie and apply the SensitiveData:S3Object/personal finding type across all S3 buckets. This approach would identify personal data but may not effectively identify all potential exposure of sensitive financial data\" is incorrect.</p><p>Using Amazon Macie with the <strong>SensitiveData:S3Object/personal</strong> finding type will only help in identifying potential exposure of sensitive personal data but not specifically financial data. It is not an effective solution for this specific requirement.</p><p><strong>INCORRECT:</strong> \"Manually inspect each S3 bucket and the data tables contained within to identify any potential financial data exposures\" is incorrect.</p><p>Manual inspection of each S3 bucket and data table would be time-consuming and prone to human error, especially when dealing with a large amount of data and numerous S3 buckets.</p><p><strong>INCORRECT:</strong> \"Utilize AWS CloudTrail logs to track activity and find potential data exposures, specifically focusing on financial data transactions\" is incorrect.</p><p>AWS CloudTrail logs would provide data about AWS account activity, but they are not specifically designed to detect sensitive data exposures.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/macie/latest/user/findings-types.html\">https://docs.aws.amazon.com/macie/latest/user/findings-types.html</a></p>",
                "options": [
                    {
                        "id": 3753,
                        "content": "<p>Implement Amazon Macie and apply the SensitiveData:S3Object/financial finding type across all S3 buckets to automatically identify potential exposure of sensitive financial data.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3754,
                        "content": "<p>Manually inspect each S3 bucket and the data tables contained within to identify any potential financial data exposures.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3755,
                        "content": "<p>Implement Amazon Macie and apply the SensitiveData:S3Object/personal finding type across all S3 buckets. This approach would identify personal data but may not effectively identify all potential exposure of sensitive financial data.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3756,
                        "content": "<p>Utilize AWS CloudTrail logs to track activity and find potential data exposures, specifically focusing on financial data transactions.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 914,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.601Z",
                "updatedAt": "2023-09-07T08:51:37.601Z",
                "content": "<p>A Developer manages a website running behind an Elastic Load Balancer in the us-east-1 region. The Developer has recently deployed an identical copy of the website in us-west-1 and needs to send 20% of the traffic to the new site.</p><p>How can the Developer achieve this requirement?</p>",
                "answerExplanation": "<p>Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-10-25-ab6d0fe50495e5d954b16e81bd1154b5.jpg\"></p><p>In this case the Developer can use a weighted routing policy to direct 20% of the incoming traffic to the new site as required.</p><p><strong>CORRECT: </strong>\"Use an Amazon Route 53 Weighted Routing Policy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an Amazon Route 53 Geolocation Routing Policy\" is incorrect as the Developer should use a weighted routing policy for this requirement as a specified percentage of traffic needs to be directed to the new website.</p><p><strong>INCORRECT:</strong> \"Use a blue/green deployment with Amazon Elastic Beanstalk\" is incorrect as the question does not state that Elastic Beanstalk is being used and the new website has already been deployed.</p><p><strong>INCORRECT:</strong> \"Use a blue/green deployment with Amazon CodeDeploy\" is incorrect as the question does not state that Amazon CodeDeploy is being used and the website has already been deployed.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
                "options": [
                    {
                        "id": 3757,
                        "content": "<p>Use a blue/green deployment with Amazon CodeDeploy</p>",
                        "isValid": false
                    },
                    {
                        "id": 3758,
                        "content": "<p>Use an Amazon Route 53 Geolocation Routing Policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 3759,
                        "content": "<p>Use a blue/green deployment with Amazon Elastic Beanstalk</p>",
                        "isValid": false
                    },
                    {
                        "id": 3760,
                        "content": "<p>Use an Amazon Route 53 Weighted Routing Policy</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 915,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.682Z",
                "updatedAt": "2023-09-07T08:51:37.682Z",
                "content": "<p>An organization needs to add encryption in-transit to an existing website running behind an Elastic Load Balancer. The websiteâ€™s Amazon EC2 instances are CPU-constrained and therefore load on their CPUs should not be increased. What should be done to secure the website? (Select TWO.)</p>",
                "answerExplanation": "<p>The company need to add security to their website by encrypting traffic in-transit using HTTPS. This requires adding SSL/TLS certificates to enable the encryption. The process of encrypting and decrypting data is CPU intensive and therefore the company need to avoid adding certificates to the EC2 instances as that will place further load on their CPUs.</p><p>Therefore, the solution is to configure SSL certificates on the Elastic Load Balancer and then configure SSL termination. This can be done by adding a certificate to a HTTPS listener on the load balancer.</p><p><strong>CORRECT: </strong>\"Configure SSL certificates on an Elastic Load Balancer\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Configure an Elastic Load Balancer with SSL termination\" is a correct answer.</p><p><strong>INCORRECT:</strong> \"Configure an Elastic Load Balancer with SSL pass-through\" is incorrect as with pass-through the SSL session must be terminated on the EC2 instances which should be avoided as they are CPU-constrained.</p><p><strong>INCORRECT:</strong> \"Configure an Elastic Load Balancer with a KMS CMK\" is incorrect as a KMS CMK is used to encrypt data at rest, it is not used for in-transit encryption.</p><p><strong>INCORRECT:</strong> \"Install SSL certificates on the EC2 instances\" is incorrect as this would increase the load on the CPUs</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
                "options": [
                    {
                        "id": 3761,
                        "content": "<p>Configure an Elastic Load Balancer with a KMS CMK</p>",
                        "isValid": false
                    },
                    {
                        "id": 3762,
                        "content": "<p>Configure an Elastic Load Balancer with SSL termination</p>",
                        "isValid": true
                    },
                    {
                        "id": 3763,
                        "content": "<p>Configure SSL certificates on an Elastic Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 3764,
                        "content": "<p>Install SSL certificates on the EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 3765,
                        "content": "<p>Configure an Elastic Load Balancer with SSL pass-through</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 916,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.753Z",
                "updatedAt": "2023-09-07T08:51:37.753Z",
                "content": "<p>A company manages an application that stores data in an Amazon DynamoDB table. The company need to keep a record of all new changes made to the DynamoDB table in another table within the same AWS region. What is the MOST suitable way to deliver this requirement?</p>",
                "answerExplanation": "<p>A <em>DynamoDB stream</em> is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.</p><p>Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A <em>stream record</em> contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the \"before\" and \"after\" images of modified items.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-40-07-0e8588f5b4f0c74121d9011e3edaa400.jpg\"></p><p>This is the best way to capture a record of new changes made to the DynamoDB table. Another table can then be populated with this data so the data is stored persistently.</p><p><strong>CORRECT: </strong>\"Use Amazon DynamoDB streams\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use CloudWatch events\" is incorrect. CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. However, it does not capture the information that changes in a DynamoDB table so is unsuitable for this purpose.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudTrail\" is incorrect as CloudTrail records a history of API calls on your account. It is used for creating an audit trail of events.</p><p><strong>INCORRECT:</strong> \"Use Amazon DynamoDB snapshots\" is incorrect as snapshots only capture a point in time, they are not used for recording item-level changes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3766,
                        "content": "<p>Use CloudWatch events</p>",
                        "isValid": false
                    },
                    {
                        "id": 3767,
                        "content": "<p>Use Amazon DynamoDB snapshots</p>",
                        "isValid": false
                    },
                    {
                        "id": 3768,
                        "content": "<p>Use Amazon CloudTrail</p>",
                        "isValid": false
                    },
                    {
                        "id": 3769,
                        "content": "<p>Use Amazon DynamoDB streams</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 917,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.836Z",
                "updatedAt": "2023-09-07T08:51:37.836Z",
                "content": "<p>A Development team are deploying an AWS Lambda function that will be used by a production application. The function code will be updated regularly, and new versions will be published. The development team do not want to modify application code to point to each new version.</p><p>How can the Development team setup a static ARN that will point to the latest published version?</p>",
                "answerExplanation": "<p>You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-29-51-1f00a9304cc2f7e0318f540b1814ba83.png\"></p><p>This is the best way to setup the Lambda function so you donâ€™t need to modify the application code when a new version is published. Instead, the developer will simply need to update the Alias to point to the new version:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-12-31-637507fdd536a3b9809d552c1572c952.jpg\"></p><p>As you can see above you can also point to multiple versions and send a percentage of traffic to each. This is great for testing new code.</p><p><strong>CORRECT: </strong>\"Setup an Alias that will point to the latest version\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Publish a mutable version and point it to the $LATEST version\" is incorrect as all published versions are immutable (cannot be modified) and you cannot modify a published version to point to the $LATEST version.</p><p><strong>INCORRECT:</strong> \"Use an unqualified ARN\" is incorrect as this is an ARN that does not have a version number which means it points to the $LATEST version, not to a published version (as published versions always have version numbers).</p><p><strong>INCORRECT:</strong> \"Setup a Route 53 Alias record that points to the published version\" is incorrect as you cannot point a Route 53 Alias record to an AWS Lambda function.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3770,
                        "content": "<p>Use an unqualified ARN</p>",
                        "isValid": false
                    },
                    {
                        "id": 3771,
                        "content": "<p>Publish a mutable version and point it to the $LATEST version</p>",
                        "isValid": false
                    },
                    {
                        "id": 3772,
                        "content": "<p>Setup a Route 53 Alias record that points to the published version</p>",
                        "isValid": false
                    },
                    {
                        "id": 3773,
                        "content": "<p>Setup an Alias that will point to the latest version</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 918,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.908Z",
                "updatedAt": "2023-09-07T08:51:37.908Z",
                "content": "<p>A Developer has created a serverless function that processes log files. The function should be invoked once every 15 minutes. How can the Developer automatically invoke the function using serverless services?</p>",
                "answerExplanation": "<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams.</p><p>You can use Amazon CloudWatch Events to invoke the Lambda function on a recurring schedule of 15 minutes. This solution is entirely automated and serverless.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudWatch Events rule that is scheduled to run and invoke the function\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Launch an EC2 Linux instance and add a command to periodically invoke the function to its /etc/crontab file \" is incorrect as this is automatic but it is not serverless.</p><p><strong>INCORRECT:</strong> \"Configure the Lambda scheduler to run based on recurring time value\" is incorrect as there is no Lambda scheduler that can be used.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS rule to send a notification to Lambda to instruct it to run\" is incorrect as you cannot invoke a function by sending a notification to it from Amazon SNS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3774,
                        "content": "<p>Launch an EC2 Linux instance and add a command to periodically invoke the function to its /etc/crontab file</p>",
                        "isValid": false
                    },
                    {
                        "id": 3775,
                        "content": "<p>Create an Amazon SNS rule to send a notification to Lambda to instruct it to run</p>",
                        "isValid": false
                    },
                    {
                        "id": 3776,
                        "content": "<p>Create an Amazon CloudWatch Events rule that is scheduled to run and invoke the function</p>",
                        "isValid": true
                    },
                    {
                        "id": 3777,
                        "content": "<p>Configure the Lambda scheduler to run based on recurring time value</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 919,
            "attributes": {
                "createdAt": "2023-09-07T08:51:37.983Z",
                "updatedAt": "2023-09-07T08:51:37.983Z",
                "content": "<p>A Developer has joined a team and needs to connect to the AWS CodeCommit repository using SSH. What should the Developer do to configure access using Git?</p>",
                "answerExplanation": "<p>You need to configure your Git client to communicate with CodeCommit repositories. As part of this configuration, you provide IAM credentials that CodeCommit can use to authenticate you. IAM supports CodeCommit with three types of credentials:</p><p> â€¢ Git credentials, an IAM -generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.</p><p> â€¢ SSH keys, a locally generated public-private key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.</p><p> â€¢ <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html\">AWS access keys</a>, which you can use with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.</p><p>As the Developer is going to use SSH, he first needs to generate an SSH private and public key. These can then be used for authentication. The method of creating these depends on the operating system the Developer is using. Then, the Developer can upload the public key (by copying the contents of the file) into his IAM account under security credentials.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-17-00-02b21d08eb5bbcd140ffcb7e14ff00f3.jpg\"></p><p><strong>CORRECT: </strong>\"Generate an SSH public and private key. Upload the public key to the Developerâ€™s IAM account\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"On the Developerâ€™s IAM account, under security credentials, choose to create HTTPS Git credentials for AWS CodeCommit\" is incorrect as this method is used for creating credentials when you want to connect to CodeCommit using HTTPS.</p><p><strong>INCORRECT:</strong> \"Create an account on Github and user those login credentials to login to AWS CodeCommit\" is incorrect as you cannot login to AWS CodeCommit using credentials from Github.</p><p><strong>INCORRECT:</strong> \"On the Developerâ€™s IAM account, under security credentials, choose to create an access key and secret ID\" is incorrect as though you can use access keys to authenticated to CodeCommit, this requires the credential helper, and enables access over HTTPS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html</a></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-unixes.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-unixes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3778,
                        "content": "<p>Create an account on Github and user those login credentials to login to AWS CodeCommit</p>",
                        "isValid": false
                    },
                    {
                        "id": 3779,
                        "content": "<p>On the Developerâ€™s IAM account, under security credentials, choose to create an access key and secret ID</p>",
                        "isValid": false
                    },
                    {
                        "id": 3780,
                        "content": "<p>On the Developerâ€™s IAM account, under security credentials, choose to create HTTPS Git credentials for AWS CodeCommit</p>",
                        "isValid": false
                    },
                    {
                        "id": 3781,
                        "content": "<p>Generate an SSH public and private key. Upload the public key to the Developerâ€™s IAM account</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 920,
            "attributes": {
                "createdAt": "2023-09-07T08:51:38.056Z",
                "updatedAt": "2023-09-07T08:51:38.056Z",
                "content": "<p>An application is running on a fleet of EC2 instances running behind an Elastic Load Balancer (ELB). The EC2 instances session data in a shared Amazon S3 bucket. Security policy mandates that data must be encrypted in transit.</p><p>How can the Developer ensure that all data that is sent to the S3 bucket is encrypted in transit?</p>",
                "answerExplanation": "<p>At the Amazon S3 bucket level, you can configure permissions through a bucket policy. For example, you can limit access to the objects in a bucket by IP address range or specific IP addresses. Alternatively, you can make the objects accessible only through HTTPS.</p><p>The following bucket policy allows access to Amazon S3 objects only through HTTPS (the policy was generated with the AWS Policy Generator).</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-15-33-c945d6087ebcc622bf8f8ec6d087332b.jpg\"></p><p>Here the bucket policy explicitly denies (\"Effect\": \"Deny\") all read access (\"Action\": \"s3:GetObject\") from anybody who browses (\"Principal\": \"*\") to Amazon S3 objects within an Amazon S3 bucket if they are not accessed through HTTPS (\"aws:SecureTransport\": \"false\").</p><p><strong>CORRECT: </strong>\"Create an S3 bucket policy that denies traffic where SecureTransport is false\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an S3 bucket policy that denies traffic where SecureTransport is true\" is incorrect. This will not work as it is denying traffic that IS encrypted in transit.</p><p><strong>INCORRECT:</strong> \"Create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption\" is incorrect. This will ensure that the data is encrypted at rest, but not in-transit.</p><p><strong>INCORRECT:</strong> \"Configure HTTP to HTTPS redirection on the Elastic Load Balancer\" is incorrect. This will ensure the client traffic reaching the ELB is encrypted however we need to ensure the traffic from the EC2 instances to S3 is encrypted and the ELB is not involved in this communication.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/\">https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 3782,
                        "content": "<p>Create an S3 bucket policy that denies traffic where SecureTransport is true</p>",
                        "isValid": false
                    },
                    {
                        "id": 3783,
                        "content": "<p>Create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption</p>",
                        "isValid": false
                    },
                    {
                        "id": 3784,
                        "content": "<p>Create an S3 bucket policy that denies traffic where SecureTransport is false</p>",
                        "isValid": true
                    },
                    {
                        "id": 3785,
                        "content": "<p>Configure HTTP to HTTPS redirection on the Elastic Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 921,
            "attributes": {
                "createdAt": "2023-09-07T08:51:38.126Z",
                "updatedAt": "2023-09-07T08:51:38.126Z",
                "content": "<p>A Developer is managing an application that includes an Amazon SQS queue. The consumers that process the data from the queue are connecting in short cycles and the queue often does not return messages. The cost for API calls is increasing. How can the Developer optimize the retrieval of messages and reduce cost?</p>",
                "answerExplanation": "<p>The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses <em>short polling</em>, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response. You can use <em>long polling</em> to reduce your costs while allowing your consumers to receive messages as soon as they arrive in the queue.</p><p>When you consume messages from a queue using short polling, Amazon SQS samples a subset of its servers (based on a weighted random distribution) and returns messages from only those servers. Thus, a particular <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\">ReceiveMessage</a> request might not return all of your messages. However, if you have fewer than 1,000 messages in your queue, a subsequent request will return your messages. If you keep consuming from your queues, Amazon SQS samples all of its servers, and you receive all of your messages.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-26-16-63a23f3e82c8aced28fea56a79fbead4.png\"></p><p>When the wait time for the ReceiveMessage API action is greater than 0, <em>long polling</em> is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\">ReceiveMessage</a> request) and false empty responses (when messages are available but aren't included in a response)</p><p>Therefore, the Developer should call the <code>ReceiveMessage</code> API with the <code>WaitTimeSeconds</code> parameter set to 20 to enable long polling.</p><p><strong>CORRECT: </strong>\"Call the <code>ReceiveMessage</code> API with the <code>WaitTimeSecond</code>s parameter set to 20 \" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Call the <code>ReceiveMessage</code> API with the <code>VisibilityTimeout </code>parameter set to 30\" is incorrect</p><p><strong>INCORRECT:</strong> \"Call the <code>SetQueueAttributes</code> API with the <code>DelaySeconds</code> parameter set to 900\" is incorrect</p><p><strong>INCORRECT:</strong> \"Call the <code>SetQueueAttributes </code>API with the <code>maxReceiveCount</code> set to 20\" is incorrect</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3786,
                        "content": "<p>Call the <code>ReceiveMessage </code>API with the <code>WaitTimeSeconds </code>parameter set to 20</p>",
                        "isValid": true
                    },
                    {
                        "id": 3787,
                        "content": "<p>Call the <code>ReceiveMessage</code> API with the <code>VisibilityTimeout</code> parameter set to 30</p>",
                        "isValid": false
                    },
                    {
                        "id": 3788,
                        "content": "<p>Call the <code>SetQueueAttributes</code> API with the <code>maxReceiveCount</code> set to 20</p>",
                        "isValid": false
                    },
                    {
                        "id": 3789,
                        "content": "<p>Call the <code>SetQueueAttributes</code> API with the <code>DelaySeconds</code> parameter set to 900</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 922,
            "attributes": {
                "createdAt": "2023-09-07T08:51:38.202Z",
                "updatedAt": "2023-09-07T08:51:38.202Z",
                "content": "<p>A Developer has setup an Amazon Kinesis Data Stream with 6 shards to ingest a maximum of 2000 records per second. An AWS Lambda function has been configured to process these records. In which order will these records be processed?</p>",
                "answerExplanation": "<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p><p>KDS receives data from producers, and the data is stored in shards. Consumers then take the data and process it. In this case the AWS Lambda function is consuming the records from the shards.</p><p>In this scenario an application will be producing records and placing them in the stream as in step 1 of the image below. The AWS Lambda function will then consume the records (step 2) and will then execute the function by assuming the execution role specified (step 3).</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-53-56-58214fabd5311dafd38861f672fb92d9.jpg\"></p><p>A shard is an append-only log and a unit of streaming capability. A shard contains an ordered sequence of records ordered by arrival time. The order is guaranteed within a shard but not across shards.</p><p>Therefore, the best answer to this question is that AWS Lambda will receive each record in the exact order it was placed into the shard but there is no guarantee of order across shards</p><p><strong>CORRECT: </strong>\"Lambda will receive each record in the exact order it was placed into the shard. There is no guarantee of order across shards\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Lambda will receive each record in the exact order it was placed into the stream \" is incorrect as there are multiple shards in the stream and the order of records is not guaranteed across shards.</p><p><strong>INCORRECT:</strong> \"Lambda will receive each record in the reverse order it was placed into the stream\" is incorrect as the order is guaranteed within a shard.</p><p><strong>INCORRECT:</strong> \"The Developer can select exact order or reverse order using the GetRecords API\" is incorrect as you cannot choose the order you receive records with the GetRecords API.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/getting-started/\">https://aws.amazon.com/kinesis/data-streams/getting-started/</a></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 3790,
                        "content": "<p>Lambda will receive each record in the exact order it was placed into the stream</p>",
                        "isValid": false
                    },
                    {
                        "id": 3791,
                        "content": "<p>The Developer can select exact order or reverse order using the <code>GetRecords</code> API</p>",
                        "isValid": false
                    },
                    {
                        "id": 3792,
                        "content": "<p>Lambda will receive each record in the reverse order it was placed into the stream</p>",
                        "isValid": false
                    },
                    {
                        "id": 3793,
                        "content": "<p>Lambda will receive each record in the exact order it was placed into the shard. There is no guarantee of order across shards</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 923,
            "attributes": {
                "createdAt": "2023-09-07T08:51:38.277Z",
                "updatedAt": "2023-09-07T08:51:38.277Z",
                "content": "<p>A multimedia streaming service wants to migrate its user authentication system to AWS. The system keeps user session data during an active session, which is crucial for seamless user experience. The new system needs to be fault-tolerant, highly scalable natively, and any service disruption must not affect user experience.</p><p>What is the best option to store the user session data?</p>",
                "answerExplanation": "<p>ElastiCache is a fully managed in-memory caching service that provides a high-performance, scalable, and cost-effective caching solution, while removing the complexity associated with deploying and managing a distributed cache environment. It is well-suited for storing user session data due to its high-speed and low-latency capabilities.</p><p><strong>CORRECT: </strong>\"Store the user session data in Amazon ElastiCache\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the user session data in Amazon CloudFront\" is incorrect.</p><p>CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. However, it is not designed for session state storage and thus, would not be the best choice for storing user session data.</p><p><strong>INCORRECT:</strong> \"Store the user session data in Amazon S3\" is incorrect.</p><p>Amazon S3 provides durable, scalable object storage but is not optimized for the type of high-speed, read/write operations typically associated with session data. While you could technically use it to store session data, it would likely not provide the same level of performance as an in-memory store like ElastiCache.</p><p><strong>INCORRECT:</strong> \"Enable session stickiness using elastic load balancers\" is incorrect.</p><p>Enabling session stickiness in load balancers can help ensure that a client is consistently directed to the same instance if that instance is healthy. However, this is more of a traffic routing mechanism rather than a storage solution for session data.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 3794,
                        "content": "<p>Store the user session data in Amazon ElastiCache.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3795,
                        "content": "<p>Store the user session data in Amazon CloudFront.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3796,
                        "content": "<p>Enable session stickiness using elastic load balancers.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3797,
                        "content": "<p>Store the user session data in Amazon S3.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 924,
            "attributes": {
                "createdAt": "2023-09-07T08:51:38.359Z",
                "updatedAt": "2023-09-07T08:51:38.359Z",
                "content": "<p>A gaming application displays the results of games in a leaderboard. The leaderboard is updated by 4 KB messages that are retrieved from an Amazon SQS queue. The updates are received infrequently but the Developer needs to minimize the time between the messages arriving in the queue and the leaderboard being updated.</p><p>Which technique provides the shortest delay in updating the leaderboard?</p>",
                "answerExplanation": "<p>The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses <em>short polling</em>, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response.</p><p>You can use <em>long polling</em> to reduce your costs while allowing your consumers to receive messages as soon as they arrive in the queue. When the wait time for the ReceiveMessage API action is greater than 0, <em>long polling</em> is in effect. The maximum long polling wait time is 20 seconds.</p><p>Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\">ReceiveMessage</a> request) and false empty responses (when messages are available but aren't included in a response). It also returns messages as soon as they become available.</p><p><strong>CORRECT: </strong>\"Retrieve the messages from the queue using long polling every 15 seconds\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Retrieve the messages from the queue using short polling every 10 seconds\" is incorrect as short polling is configured when the WaitTimeSeconds parameter of a <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\">ReceiveMessage</a> request is set to 0. Any number above zero indicates long polling is in effect.</p><p><strong>INCORRECT:</strong> \"Reduce the size of the messages with compression before sending them\" is incorrect as this will not mean messages are picked up earlier and there is no reason to compress messages that are 4 KB in size.</p><p><strong>INCORRECT:</strong> \"Store the message payload in Amazon S3 and use the SQS Extended Client Library for Java\" is incorrect as this is unnecessary for messages of this size and will also not result in the shortest delay when updating the leaderboard.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3798,
                        "content": "<p>Store the message payload in Amazon S3 and use the SQS Extended Client Library for Java</p>",
                        "isValid": false
                    },
                    {
                        "id": 3799,
                        "content": "<p>Reduce the size of the messages with compression before sending them</p>",
                        "isValid": false
                    },
                    {
                        "id": 3800,
                        "content": "<p>Retrieve the messages from the queue using long polling every 15 seconds</p>",
                        "isValid": true
                    },
                    {
                        "id": 3801,
                        "content": "<p>Retrieve the messages from the queue using short polling every 10 seconds</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 925,
            "attributes": {
                "createdAt": "2023-09-07T08:51:38.445Z",
                "updatedAt": "2023-09-07T08:51:38.445Z",
                "content": "<p>A Developer has created a task definition that includes the following JSON code:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"placementConstraints\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L1\"><span class=\"pun\">{</span></li><li class=\"L2\"><span class=\"str\">\"expression\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"task:group == databases\"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"type\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"memberOf\"</span></li><li class=\"L4\"><span class=\"pun\">}</span></li><li class=\"L5\"><span class=\"pun\">]</span></li></ol></pre></div></div><p>What will be the effect for tasks using this task definition?</p>",
                "answerExplanation": "<p>A <em>task placement constraint</em> is a rule that is considered during task placement. Task placement constraints can be specified when either running a task or creating a new service.</p><p> The memberOf task placement constraint places tasks on container instances that satisfy an expression.</p><p> The memberOf task placement constraint can be specified with the following actions:</p><p> â€¢ Running a task</p><p> â€¢ Creating a new service</p><p> â€¢ Creating a new task definition</p><p> â€¢ Creating a new revision of an existing task definition</p><p>The example JSON code uses the memberOf constraint to place tasks on instances in the databases task group. It can be specified with the following actions: <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CreateService.html\">CreateService</a>, <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_UpdateService.html\">UpdateService</a>, <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RegisterTaskDefinition.html\">RegisterTaskDefinition</a>, and <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RunTask.html\">RunTask</a>.</p><p><strong>CORRECT: </strong>\"They will be placed on container instances in the â€œdatabasesâ€ task group\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"They will become members of a task group called â€œdatabasesâ€\" is incorrect. They will be placed on container instances in the â€œdatabasesâ€ task group.</p><p><strong>INCORRECT:</strong> \"They will not be placed on container instances in the â€œdatabasesâ€ task group\" is incorrect. This statement ensures the tasks ARE placed on the container instances in the â€œdatabasesâ€ task group.</p><p><strong>INCORRECT:</strong> \"They will not be allowed to run unless they have the â€œdatabasesâ€ tag assigned\" is incorrect. This JSON code is not related to tagging of the tasks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 3802,
                        "content": "<p>They will become members of a task group called â€œdatabasesâ€</p>",
                        "isValid": false
                    },
                    {
                        "id": 3803,
                        "content": "<p>They will be placed on container instances in the â€œdatabasesâ€ task group</p>",
                        "isValid": true
                    },
                    {
                        "id": 3804,
                        "content": "<p>They will not be allowed to run unless they have the â€œdatabasesâ€ tag assigned</p>",
                        "isValid": false
                    },
                    {
                        "id": 3805,
                        "content": "<p>They will not be placed on container instances in the â€œdatabasesâ€ task group</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 926,
            "attributes": {
                "createdAt": "2023-09-07T08:51:38.519Z",
                "updatedAt": "2023-09-07T08:51:38.519Z",
                "content": "<p>An application runs on Amazon EC2 and generates log files. A Developer needs to centralize the log files so they can be queried and retained. What is the EASIEST way for the Developer to centralize the log files?</p>",
                "answerExplanation": "<p>You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-37-19-bd6f9b9d88a836d9fbc2594bc0953dc5.png\"></p><p>CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. You can then easily view them, search them for specific error codes or patterns, filter them based on specific fields, or archive them securely for future analysis.</p><p>To collect logs from Amazon EC2 and on-premises instances it is necessary to install an agent. There are two options: the unified CloudWatch Agent which collects logs and advanced metrics (such as memory usage), or the older CloudWatch Logs agent which only collects logs from Linux servers.</p><p><strong>CORRECT: </strong>\"Install the Amazon CloudWatch Logs agent and collect the logs from the instances\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a script that copies the log files to Amazon S3 and use a cron job to run the script on a recurring schedule\" is incorrect as the best place to move the log files to for querying and long term retention would be CloudWatch Logs. It is also easier to use the agent than to create and maintain a script.</p><p><strong>INCORRECT:</strong> \"Create a script that uses the AWS SDK to collect and send the log files to Amazon CloudWatch Logs\" is incorrect as this is not the easiest way to achieve this outcome. It will be easier to use the CloudWatch Logs agent.</p><p><strong>INCORRECT:</strong> \"Setup a CloudWatch Events rule to trigger an SNS topic when an application log file is generated\" is incorrect as CloudWatch Events does not collect log files, it monitors state changes in resources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3806,
                        "content": "<p>Setup a CloudWatch Events rule to trigger an SNS topic when an application log file is generated</p>",
                        "isValid": false
                    },
                    {
                        "id": 3807,
                        "content": "<p>Install the Amazon CloudWatch Logs agent and collect the logs from the instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 3808,
                        "content": "<p>Create a script that copies the log files to Amazon S3 and use a cron job to run the script on a recurring schedule</p>",
                        "isValid": false
                    },
                    {
                        "id": 3809,
                        "content": "<p>Create a script that uses the AWS SDK to collect and send the log files to Amazon CloudWatch Logs</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 927,
            "attributes": {
                "createdAt": "2023-09-07T08:51:38.594Z",
                "updatedAt": "2023-09-07T08:51:38.594Z",
                "content": "<p>A Developer is looking for a way to use shorthand syntax to express functions, APIs, databases, and event source mappings. The Developer will test using AWS SAM to create a simple Lambda function using Nodejs.12x.</p><p>What is the SIMPLEST way for the Developer to get started with a Hello World Lambda function? </p>",
                "answerExplanation": "<p>The <code>sam init</code> command initializes a serverless application with an AWS SAM template. The template provides a folder structure for your Lambda functions and is connected to an event source such as APIs, S3 buckets, or DynamoDB tables. This application includes everything you need to get started and to eventually extend it into a production-scale application.</p><p>This is the simplest way for the Developer to quickly get started with testing AWS SAM. Before the Developer can use the â€œsamâ€ commands it is necessary to install the AWS SAM CLI. This is separate to the AWS CLI.</p><p><strong>CORRECT: </strong>\"Install the AWS SAM CLI, run <code>sam init</code> and use one of the AWS Quick Start Templates\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Install the AWS CLI, run aws sam init and use one of the AWS Quick Start Templates\" is incorrect as â€œ<code>sam init</code>â€ is not an AWS CLI command, therefore you cannot put â€œawsâ€ in front of â€œsamâ€.</p><p><strong>INCORRECT:</strong> \"Use the AWS Management Console to access AWS SAM and deploy a Hello World function\" is incorrect as you cannot access AWS SAM through the console. You can, however, access the Serverless Application Repository through the console and deploy SAM templates.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to deploy a Hello World stack using AWS SAM\" is incorrect as though AWS SAM does use CloudFormation you cannot deploy SAM templates through the AWS CloudFormation console. You must use the SAM CLI or deploy using the Serverless Application Repository.</p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html\">https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-sam/\">https://digitalcloud.training/aws-sam/</a></p>",
                "options": [
                    {
                        "id": 3810,
                        "content": "<p>Install the AWS CLI, run <code>aws sam init</code> and use one of the AWS Quick Start Templates</p>",
                        "isValid": false
                    },
                    {
                        "id": 3811,
                        "content": "<p>Use the AWS Management Console to access AWS SAM and deploy a Hello World function</p>",
                        "isValid": false
                    },
                    {
                        "id": 3812,
                        "content": "<p>Install the AWS SAM CLI, run <code>sam init</code> and use one of the AWS Quick Start Templates</p>",
                        "isValid": true
                    },
                    {
                        "id": 3813,
                        "content": "<p>Use AWS CloudFormation to deploy a Hello World stack using AWS SAM</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 928,
            "attributes": {
                "createdAt": "2023-09-07T08:51:38.666Z",
                "updatedAt": "2023-09-07T08:51:38.666Z",
                "content": "<p>An AWS Lambda function must be connected to an Amazon VPC private subnet that does not have Internet access. The function also connects to an Amazon DynamoDB table. What MUST a Developer do to enable access to the DynamoDB table?</p>",
                "answerExplanation": "<p>To connect to AWS services from a private subnet with no internet access, use VPC endpoints. A <em>VPC endpoint</em> for DynamoDB enables resources in a VPC to use their private IP addresses to access DynamoDB with no exposure to the public internet.</p><p>When you create a VPC endpoint for DynamoDB, any requests to a DynamoDB endpoint within the Region (for example, <em>dynamodb.us-west-2.amazonaws.com</em>) are routed to a private DynamoDB endpoint within the Amazon network.</p><p><strong>CORRECT: </strong>\"Configure a VPC endpoint\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Attach an Internet Gateway\" is incorrect as you do not attach these to a private subnet.</p><p><strong>INCORRECT:</strong> \"Create a route table\" is incorrect as a route table will exist for all subnets and it does not help to route out from a private subnet via the Internet unless an entry for a NAT Gateway or Instance is added.</p><p><strong>INCORRECT:</strong> \"Attach an ENI to the DynamoDB table\" is incorrect as you do not attach Elastic Network Interfaces to DynamoDB tables.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/troubleshooting-networking.html\">https://docs.aws.amazon.com/lambda/latest/dg/troubleshooting-networking.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 3814,
                        "content": "<p>Attach an ENI to the DynamoDB table</p>",
                        "isValid": false
                    },
                    {
                        "id": 3815,
                        "content": "<p>Attach an Internet Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 3816,
                        "content": "<p>Configure a VPC endpoint</p>",
                        "isValid": true
                    },
                    {
                        "id": 3817,
                        "content": "<p>Create a route table</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 929,
            "attributes": {
                "createdAt": "2023-09-07T08:51:38.742Z",
                "updatedAt": "2023-09-07T08:51:38.742Z",
                "content": "<p>A development team require a fully-managed source control service that is compatible with Git.</p><p>Which service should they use?</p>",
                "answerExplanation": "<p>AWS CodeCommit is a version control service hosted by Amazon Web Services that you can use to privately store and manage assets (such as documents, source code, and binary files) in the cloud. CodeCommit is a fully-managed service that hosts secure Git-based repositories.</p><p><strong>CORRECT: </strong>\"AWS CodeCommit\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodeDeploy\" is incorrect. CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.</p><p><strong>INCORRECT:</strong> \"AWS CodePipeline\" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.</p><p><strong>INCORRECT:</strong> \"AWS Cloud9\" is incorrect. AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3818,
                        "content": "<p>AWS CodePipeline</p>",
                        "isValid": false
                    },
                    {
                        "id": 3819,
                        "content": "<p>AWS Cloud9</p>",
                        "isValid": false
                    },
                    {
                        "id": 3820,
                        "content": "<p>AWS CodeDeploy</p>",
                        "isValid": false
                    },
                    {
                        "id": 3821,
                        "content": "<p>AWS CodeCommit</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 930,
            "attributes": {
                "createdAt": "2023-09-07T08:51:38.834Z",
                "updatedAt": "2023-09-07T08:51:38.834Z",
                "content": "<p>An IT automation architecture uses many AWS Lambda functions invoking one another as a large state machine. The coordination of this state machine is legacy custom code that breaks easily.<br>Which AWS Service can help refactor and manage the state machine?</p>",
                "answerExplanation": "<p>AWS Step Functions lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker, into feature-rich applications.</p><p>Workflows are made up of a series of steps, with the output of one step acting as input into the next. Application development is simpler and more intuitive using Step Functions, because it translates your workflow into a state machine diagram that is easy to understand, easy to explain to others, and easy to change.</p><p>Step Functions automatically triggers and tracks each step, and retries when there are errors, so your application executes in order and as expected. With Step Functions, you can craft long-running workflows such as machine learning model training, report generation, and IT automation.</p><p>Therefore, AWS Step Functions is the best AWS service to use when refactoring the application away from the legacy code.</p><p><strong>CORRECT: </strong>\"AWS Step Functions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CloudFormation\" is incorrect as CloudFormation is used for deploying resources no AWS but not for ongoing automation.</p><p><strong>INCORRECT:</strong> \"AWS CodePipeline\" is incorrect as this is used as part of a continuous integration and delivery (CI/CD) pipeline to deploy software updates to applications.</p><p><strong>INCORRECT:</strong> \"AWS CodeBuild\" is incorrect as this an AWS build/test service.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3822,
                        "content": "<p>AWS Step Functions</p>",
                        "isValid": true
                    },
                    {
                        "id": 3823,
                        "content": "<p> AWS CloudFormation</p>",
                        "isValid": false
                    },
                    {
                        "id": 3824,
                        "content": "<p>AWS CodePipeline</p>",
                        "isValid": false
                    },
                    {
                        "id": 3825,
                        "content": "<p>AWS CodeBuild</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 931,
            "attributes": {
                "createdAt": "2023-09-07T08:51:38.908Z",
                "updatedAt": "2023-09-07T08:51:38.908Z",
                "content": "<p>A company is running a Docker application on Amazon ECS. The application must scale based on user load in the last 15 seconds.</p><p>How should the Developer instrument the code so that the requirement can be met?</p>",
                "answerExplanation": "<p>Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p><p>User activity is not a standard CloudWatch metric and as stated above for the resolution we need in this scenario a custom CloudWatch metric is required anyway. Therefore, for this scenario the Developer should create a high-resolution custom Amazon CloudWatch metric for user activity data and publish the data every 5 seconds.</p><p><strong>CORRECT: </strong>\"Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds\" is incorrect as the resolution is lower than required which will not provide the granularity required.</p><p><strong>INCORRECT:</strong> \"Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds\" is incorrect as standard resolution metrics have a granularity of one minute.</p><p><strong>INCORRECT:</strong> \"Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds\" is incorrect as standard resolution metrics have a granularity of one minute.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3826,
                        "content": "<p>Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds</p>",
                        "isValid": false
                    },
                    {
                        "id": 3827,
                        "content": "<p>Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds</p>",
                        "isValid": true
                    },
                    {
                        "id": 3828,
                        "content": "<p>Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds</p>",
                        "isValid": false
                    },
                    {
                        "id": 3829,
                        "content": "<p>Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 932,
            "attributes": {
                "createdAt": "2023-09-07T08:51:38.986Z",
                "updatedAt": "2023-09-07T08:51:38.986Z",
                "content": "<p>A Developer is deploying an Amazon ECS update using AWS CodeDeploy. In the appspec.yaml file, which of the following is a valid structure for the order of hooks that should be specified?</p>",
                "answerExplanation": "<p>The content in the 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment. The 'hooks' section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts.</p><p>The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.</p><p>The following code snippet shows a valid example of the structure of hooks for an Amazon ECS deployment:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-44-56-2017625f2c25c61736eb3231c9b6895f.jpg\"></p><p>Therefore, in this scenario a valid structure for the order of hooks that should be specified in the appspec.yml file is: BeforeInstall &gt; AfterInstall &gt; AfterAllowTestTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic</p><p><strong>CORRECT: </strong>\"BeforeInstall &gt; AfterInstall &gt; AfterAllowTestTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"BeforeInstall &gt; AfterInstall &gt; ApplicationStart &gt; ValidateService\" is incorrect as this would be valid for Amazon EC2.</p><p><strong>INCORRECT:</strong> \"BeforeAllowTraffic &gt; AfterAllowTraffic\" is incorrect as this would be valid for AWS Lambda.</p><p><strong>INCORRECT:</strong> \"BeforeBlockTraffic &gt; AfterBlockTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic\" is incorrect as this is a partial listing of hooks for Amazon EC2 but is incomplete.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3830,
                        "content": "<p><br></p><p>BeforeInstall &gt; AfterInstall &gt; ApplicationStart &gt; ValidateService</p>",
                        "isValid": false
                    },
                    {
                        "id": 3831,
                        "content": "<p>BeforeInstall &gt; AfterInstall &gt; AfterAllowTestTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
                        "isValid": true
                    },
                    {
                        "id": 3832,
                        "content": "<p>BeforeBlockTraffic &gt; AfterBlockTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
                        "isValid": false
                    },
                    {
                        "id": 3833,
                        "content": "<p>BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 933,
            "attributes": {
                "createdAt": "2023-09-07T08:51:39.054Z",
                "updatedAt": "2023-09-07T08:51:39.054Z",
                "content": "<p>A media company uses Amazon EC2 instances managed by AWS Elastic Beanstalk to run its high-traffic website. The engineering team needs to introduce a new feature, which requires upgrading the underlying platform to a newer version of Node.js. The deployment of the new code and the platform upgrade need to happen without causing any downtime.</p><p>Which strategy should the team adopt to fulfill these requirements?</p>",
                "answerExplanation": "<p>The Blue/Green deployment strategy involves creating a separate environment (the 'green' environment) with the new application version and platform updates. It allows full testing and validation. Once you're confident in the 'green' environment, you can swap the environment URLs to redirect traffic to the new environment. This results in zero downtime.</p><p><strong>CORRECT: </strong>\"Implement Blue/Green (CNAME Swap) deployment using Elastic Beanstalk. Prepare a separate environment with the new version of Node.js and the new code, and once testing is complete, swap the CNAMEs of the two environments\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Upgrade the platform version of the current Elastic Beanstalk environment, then deploy the new application code. Monitor the application and quickly roll back changes if any issues occur\" is incorrect.</p><p>This approach introduces risks that could result in downtime during the upgrade and deployment process. If any issues occur during the upgrade or after the deployment of the new application code, rolling back these changes could cause further downtime.</p><p><strong>INCORRECT:</strong> \"Create a duplicate EC2 instance manually with the new version of Node.js and the updated code. Test the new instance and replace one of the instances behind the ELB once the testing is successful\" is incorrect.</p><p>Managing the process manually and replacing instances behind the load balancer may result in inconsistencies and is not as streamlined or reliable as using the deployment and environment management capabilities of Elastic Beanstalk.</p><p><strong>INCORRECT:</strong> \"Use a rolling deployment for the new application code. Apply the code to a subset of EC2 instances until the tests pass. Redeploy the previous code if the tests fail\" is incorrect.</p><p>While AWS CodeDeploy is a robust deployment service, performing the platform upgrade manually after the code deployment doesn't guarantee zero downtime, as the platform upgrade might require a restart of the EC2 instances, causing service interruptions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
                "options": [
                    {
                        "id": 3834,
                        "content": "<p>Upgrade the platform version of the current Elastic Beanstalk environment, then deploy the new application code. Monitor the application and quickly roll back changes if any issues occur.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3835,
                        "content": "<p>Use AWS CodeDeploy to deploy the new application code first, then manually update Node.js on the Elastic Beanstalk environment.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3836,
                        "content": "<p>Create a duplicate EC2 instance manually with the new version of Node.js and the updated code. Test the new instance and replace one of the instances behind the ELB once the testing is successful.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3837,
                        "content": "<p>Implement Blue/Green (CNAME Swap) deployment using Elastic Beanstalk. Prepare a separate environment with the new version of Node.js and the new code, and once testing is complete, swap the CNAMEs of the two environments.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 934,
            "attributes": {
                "createdAt": "2023-09-07T08:51:39.126Z",
                "updatedAt": "2023-09-07T08:51:39.126Z",
                "content": "<p>An application writes items to an Amazon DynamoDB table. As the application scales to thousands of instances, calls to the DynamoDB API generate occasional ThrottlingException errors. The application is coded in a language that is incompatible with the AWS SDK.</p><p>What can be done to prevent the errors from occurring?</p>",
                "answerExplanation": "<p>Implementing error retries and exponential backoff is a good way to resolve this issue. Exponential backoff can improve an application's reliability by using progressively longer waits between retries. If you're using an AWS SDK, this logic is builtâ€‘in. If you're not using an AWS SDK, consider manually implementing exponential backoff.</p><p>Additional options for preventing throttling from occurring include:</p><p> â€¢ Distribute read and write operations as evenly as possible across your table. A hot partition can degrade the overall performance of your table. For more information, see Designing Partition Keys to Distribute Your Workload Evenly.</p><p> â€¢ Implement a caching solution. If your workload is mostly read access to static data, then query results can be delivered much faster if the data is in a wellâ€‘designed cache rather than in a database. DynamoDB Accelerator (DAX) is a caching service that offers fast inâ€‘memory performance for your application. You can also use Amazon ElastiCache.</p><p><strong>CORRECT: </strong>\"Add exponential backoff to the application logic\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon SQS as an API message bus\" is incorrect. SQS is used for decoupling (messages, nut not APIs), however for this scenario it would add extra cost and complexity.</p><p><strong>INCORRECT:</strong> \"Pass API calls through Amazon API Gateway\" is incorrect. For this scenario we donâ€™t want to add an additional layer in when we can simply configure the application to back off and retry.</p><p><strong>INCORRECT:</strong> \"Send the items to DynamoDB through Amazon Kinesis Data Firehose\" is incorrect as DynamoDB is not a supported destination for Kinesis Data Firehose.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/general/latest/gr/api-retries.html\">https://docs.aws.amazon.com/general/latest/gr/api-retries.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-table-throttled/\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-table-throttled/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3838,
                        "content": "<p>Use Amazon SQS as an API message bus</p>",
                        "isValid": false
                    },
                    {
                        "id": 3839,
                        "content": "<p>Add exponential backoff to the application logic</p>",
                        "isValid": true
                    },
                    {
                        "id": 3840,
                        "content": "<p>Send the items to DynamoDB through Amazon Kinesis Data Firehose</p>",
                        "isValid": false
                    },
                    {
                        "id": 3841,
                        "content": "<p>Pass API calls through Amazon API Gateway</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 935,
            "attributes": {
                "createdAt": "2023-09-07T08:51:39.200Z",
                "updatedAt": "2023-09-07T08:51:39.200Z",
                "content": "<p>A company provides a large number of services on AWS to customers. The customers connect to one or more services directly and the architecture is becoming complex. How can the architecture be refactored to provide a single interface for the services?</p>",
                "answerExplanation": "<p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services.</p><p>Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-32-09-13e8e2f70cfb9c8992122d15b7483be8.jpg\"></p><p>API Gateway can be used as the single interface for consumers of the services provided by the organization in this scenario. This solution will simplify the architecture.</p><p><strong>CORRECT: </strong>\"Amazon API Gateway\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS X-Ray\" is incorrect. AWS X-Ray is used for analyzing and debugging applications.</p><p><strong>INCORRECT:</strong> \"AWS Cognito\" is incorrect. AWS Cognito is used for adding sign-up, sign-in and access control to web and mobile apps.</p><p><strong>INCORRECT:</strong> \"AWS Single Sign On (SSO)\" is incorrect. AWS SSO is used to provide central management of multiple AWS accounts and business applications and to provide single sign-on to accounts.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/api-gateway/features/\">https://aws.amazon.com/api-gateway/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 3842,
                        "content": "<p>Amazon API Gateway</p>",
                        "isValid": true
                    },
                    {
                        "id": 3843,
                        "content": "<p>AWS Cognito</p>",
                        "isValid": false
                    },
                    {
                        "id": 3844,
                        "content": "<p>AWS X-Ray</p>",
                        "isValid": false
                    },
                    {
                        "id": 3845,
                        "content": "<p>AWS Single Sign On (SSO)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 936,
            "attributes": {
                "createdAt": "2023-09-07T08:51:39.269Z",
                "updatedAt": "2023-09-07T08:51:39.269Z",
                "content": "<p>A Development team have moved their continuous integration and delivery (CI/CD) pipeline into the AWS Cloud. The team is leveraging AWS CodeCommit for management of source code. The team need to compile their source code, run tests, and produce software packages that are ready for deployment.</p><p>Which AWS service can deliver these outcomes?</p>",
                "answerExplanation": "<p>AWS CodeBuild is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more.</p><p>You can also customize build environments in CodeBuild to use your own build tools. CodeBuild scales automatically to meet peak build requests.</p><p>CodeBuild provides these benefits:</p><p><strong> </strong>â€¢ <strong>Fully managed</strong> â€“ CodeBuild eliminates the need to set up, patch, update, and manage your own build servers.</p><p><strong> </strong>â€¢ <strong>On demand</strong> â€“ CodeBuild scales on demand to meet your build needs. You pay only for the number of build minutes you consume.</p><p><strong> </strong>â€¢ <strong>Out of the box</strong> â€“ CodeBuild provides preconfigured build environments for the most popular programming languages. All you need to do is point to your build script to start your first build.</p><p>Therefore, AWS CodeBuild is the best service to use to compile the Development teamâ€™s source code, run tests, and produce software packages that are ready for deployment.</p><p><strong>CORRECT: </strong>\"AWS CodeBuild\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodeCommit\" is incorrect. The team are already using CodeCommit for its correct purpose, which is to manage source code. CodeCommit cannot perform compiling of source code, testing, or package creation.</p><p><strong>INCORRECT:</strong> \"AWS CodePipeline\" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.</p><p><strong>INCORRECT:</strong> \"AWS Cloud9\" is incorrect. AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3846,
                        "content": "<p>AWS CodeCommit</p>",
                        "isValid": false
                    },
                    {
                        "id": 3847,
                        "content": "<p>AWS CodePipeline</p>",
                        "isValid": false
                    },
                    {
                        "id": 3848,
                        "content": "<p>AWS Cloud9</p>",
                        "isValid": false
                    },
                    {
                        "id": 3849,
                        "content": "<p>AWS CodeBuild</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 937,
            "attributes": {
                "createdAt": "2023-09-07T08:51:39.347Z",
                "updatedAt": "2023-09-07T08:51:39.347Z",
                "content": "<p>An application needs to read up to 100 items at a time from an Amazon DynamoDB. Each item is up to 100 KB in size and all attributes must be retrieved.</p><p>What is the BEST way to minimize latency?</p>",
                "answerExplanation": "<p>The BatchGetItem operation returns the attributes of one or more items from one or more tables. You identify requested items by primary key.</p><p>A single operation can retrieve up to 16 MB of data, which can contain as many as 100 items. In order to minimize response latency, BatchGetItem retrieves items in parallel.</p><p>By default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables.</p><p><strong>CORRECT: </strong>\"Use <code>BatchGetItem</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use <code>GetItem</code> and use a projection expression\" is incorrect as this will limit the attributes returned and will retrieve the items sequentially which results in more latency.</p><p><strong>INCORRECT:</strong> \"Use a Scan operation with pagination\" is incorrect as a Scan operation is the least efficient way to retrieve the data as all items in the table are returned and then filtered. Pagination just breaks the results into pages.</p><p><strong>INCORRECT:</strong> \"Use a Query operation with a <code>FilterExpression</code>\" is incorrect as this would limit the results that are returned.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html\">https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3850,
                        "content": "<p>Use a Scan operation with pagination</p>",
                        "isValid": false
                    },
                    {
                        "id": 3851,
                        "content": "<p>Use <code>BatchGetItem</code> </p>",
                        "isValid": true
                    },
                    {
                        "id": 3852,
                        "content": "<p>Use a Query operation with a <code>FilterExpression</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3853,
                        "content": "<p>Use <code>GetItem</code> and use a projection expression</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 938,
            "attributes": {
                "createdAt": "2023-09-07T08:51:39.424Z",
                "updatedAt": "2023-09-07T08:51:39.424Z",
                "content": "<p>The development team is experiencing issues with their application hosted on Amazon EC2 instances, as they are unable to connect to an Amazon S3 bucket during test runs.</p><p>What should be the appropriate measures to resolve this issue? (Select TWO.)</p>",
                "answerExplanation": "<p>IAM roles attached to the EC2 instances allow applications running on these instances to use AWS services without the need to manage and store AWS credentials. If an EC2 instance is unable to access an S3 bucket, one possibility is that the IAM role associated with the EC2 instance does not have the necessary permissions to access the S3 bucket. Checking and updating the IAM roles can resolve this issue.</p><p>Amazon S3 bucket policies define who can access the contents of the bucket and what actions they can perform. If the EC2 instances are unable to access the S3 bucket, another possible cause is that the bucket policy does not permit access to the EC2 instances. Checking and modifying the bucket policies to grant access to the EC2 instances can fix this problem.</p><p><strong>CORRECT: </strong>\"Verify the IAM roles attached to the EC2 instances and ensure they have the necessary permissions to access the S3 bucket\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Check the bucket policies for the Amazon S3 bucket and confirm that they permit access from the EC2 instances\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Check the security groups attached to the EC2 instances to ensure the required inbound rules are in place\" is incorrect.</p><p>Security groups are associated with EC2 instances and act as a firewall that controls the traffic for one or more instances. Inbound rules affect traffic coming towards the EC2 instance and this does not affect access to S3. For S3, the instance needs to have an outbound rule that allows the relevant protocols.</p><p><strong>INCORRECT:</strong> \"Validate the VPC peering connections to ensure the S3 bucket is reachable\" is incorrect.</p><p>Amazon VPC peering connection allows you to route traffic between two VPCs using private IPv4 or IPv6 addresses. However, S3 buckets are not accessed via VPC peering connections; they are accessed using the Amazon S3 service endpoint. Hence, validating VPC peering connections won't solve the issue here.</p><p><strong>INCORRECT:</strong> \"Modify the Amazon S3 bucket to be public to allow EC2 instances to access it\" is incorrect.</p><p>Making an Amazon S3 bucket public can pose serious security risks, as it will allow anyone to access the contents of the bucket. This action is generally not recommended, especially not to solve access issues for EC2 instances. The best practice is to manage access at a more granular level using IAM roles and bucket policies.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 3854,
                        "content": "<p>Check the security groups attached to the EC2 instances to ensure the required inbound rules are in place.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3855,
                        "content": "<p>Modify the Amazon S3 bucket to be public to allow EC2 instances to access it.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3856,
                        "content": "<p>Verify the IAM roles attached to the EC2 instances and ensure they have the necessary permissions to access the S3 bucket.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3857,
                        "content": "<p>Validate the VPC peering connections to ensure the S3 bucket is reachable.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3858,
                        "content": "<p>Check the bucket policies for the Amazon S3 bucket and confirm that they permit access from the EC2 instances.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 939,
            "attributes": {
                "createdAt": "2023-09-07T08:51:39.510Z",
                "updatedAt": "2023-09-07T08:51:39.510Z",
                "content": "<p>A company needs to encrypt a large quantity of data. The data encryption keys must be generated from a dedicated, tamper-resistant hardware device.</p><p>To deliver these requirements, which AWS service should the company use?</p>",
                "answerExplanation": "<p>The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud.</p><p>A Hardware Security Module (HSM) provides secure key storage and cryptographic operations within a tamper-resistant hardware device. CloudHSM allows you to securely generate, store, and manage cryptographic keys used for data encryption in a way that keys are accessible only by you.</p><p><strong>CORRECT: </strong>\"AWS CloudHSM\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS KMS\" is incorrect as it uses shared infrastructure (multi-tenant) and is therefore not a dedicated HSM.</p><p><strong>INCORRECT:</strong> \"AWS Certificate Manager\" is incorrect as this is used to generate and manage SSL/TLS certificates, it does not generate data encryption keys.</p><p><strong>INCORRECT:</strong> \"AWS IAM\" is incorrect as this service is not involved with generating encryption keys.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudhsm/faqs/\">https://aws.amazon.com/cloudhsm/faqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudhsm/\">https://digitalcloud.training/aws-cloudhsm/</a></p>",
                "options": [
                    {
                        "id": 3859,
                        "content": "<p>AWS Certificate Manager</p>",
                        "isValid": false
                    },
                    {
                        "id": 3860,
                        "content": "<p>AWS CloudHSM</p>",
                        "isValid": true
                    },
                    {
                        "id": 3861,
                        "content": "<p>AWS KMS</p>",
                        "isValid": false
                    },
                    {
                        "id": 3862,
                        "content": "<p>AWS IAM</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 940,
            "attributes": {
                "createdAt": "2023-09-07T08:51:39.580Z",
                "updatedAt": "2023-09-07T08:51:39.580Z",
                "content": "<p>Based on the following AWS CLI command the resulting output, what has happened here?</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pln\">$ aws </span><span class=\"kwd\">lambda</span><span class=\"pln\"> invoke </span><span class=\"pun\">--</span><span class=\"kwd\">function</span><span class=\"pun\">-</span><span class=\"pln\">name </span><span class=\"typ\">MyFunction</span><span class=\"pln\"> </span><span class=\"pun\">--</span><span class=\"pln\">payload ewogICJrZXkxIjogInZhbHVlMSIsCiAgImtleTIiOiAidmFsdWUyIiwKICAia2V5MyI6ICJ2YWx1ZTMiCn0</span><span class=\"pun\">=</span><span class=\"pln\"> response</span><span class=\"pun\">.</span><span class=\"pln\">json</span></li><li class=\"L1\"><span class=\"pun\">{</span></li><li class=\"L2\"><span class=\"str\">\"StatusCode\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"lit\">200</span></li><li class=\"L3\"><span class=\"pun\">}</span></li></ol></pre></div></div>",
                "answerExplanation": "<p>When you invoke a function synchronously, Lambda runs the function and waits for a response. When the function execution ends, Lambda returns the response from the function's code with additional data, such as the version of the function that was executed. To invoke a function synchronously with the AWS CLI, use the invoke command.</p><p>The following diagram shows clients invoking a Lambda function synchronously. Lambda sends the events directly to the function and sends the function's response back to the invoker.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_00-54-43-14be40d280284280078fb2c39c93a4b4.jpg\"></p><p>We know the function has been run synchronously as the<code> --invocation-type Event</code> parameter has not been included. Also, the status code 200 indicates a successful execution of a synchronous execution.</p><p><strong>CORRECT: </strong>\"An AWS Lambda function has been invoked synchronously and has completed successfully\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"An AWS Lambda function has been invoked synchronously and has not completed successfully\" is incorrect as the status code 200 indicates a successful execution.</p><p><strong>INCORRECT:</strong> \"An AWS Lambda function has been invoked asynchronously and has completed successfully\" is incorrect as the <code>--invocation-type Event</code> has parameter is not included so this is not an asynchronous invocation.</p><p><strong>INCORRECT:</strong> \"An AWS Lambda function has been invoked asynchronously and has not completed successfully\" is incorrect as the<code> --invocation-type Event</code> has parameter is not included so this is not an asynchronous invocation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-sync.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-sync.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3863,
                        "content": "<p>An AWS Lambda function has been invoked asynchronously and has not completed successfully</p>",
                        "isValid": false
                    },
                    {
                        "id": 3864,
                        "content": "<p>An AWS Lambda function has been invoked synchronously and has completed successfully</p>",
                        "isValid": true
                    },
                    {
                        "id": 3865,
                        "content": "<p>An AWS Lambda function has been invoked synchronously and has not completed successfully</p>",
                        "isValid": false
                    },
                    {
                        "id": 3866,
                        "content": "<p>An AWS Lambda function has been invoked asynchronously and has completed successfully</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 941,
            "attributes": {
                "createdAt": "2023-09-07T08:51:39.655Z",
                "updatedAt": "2023-09-07T08:51:39.655Z",
                "content": "<p>An application is running on a cluster of Amazon EC2 instances. The application has received an error when trying to read objects stored within an Amazon S3 bucket. The bucket is encrypted with server-side encryption and AWS KMS managed keys (SSE-KMS). The error is as follows:</p><p><code>Service: AWSKMS; Status Code: 400, Error Code: ThrottlingException</code></p><p>Which combination of steps should be taken to prevent this failure? (Select TWO.)</p>",
                "answerExplanation": "<p>AWS KMS establishes quotas for the number of API operations requested in each second. When you exceed an API request quota, AWS KMS <em>throttles</em> the request, that is, it rejects an otherwise valid request and returns a ThrottlingException error like the following one.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-32-56-3ab1f386892eaf23c4dc3af057bf8b63.jpg\"></p><p>As the error indicates, one of the recommendations is to reduce the frequency of calls which can be implemented by using exponential backoff logic in the application code. It is also possible to contact AWS and request an increase in the quota.</p><p><strong>CORRECT: </strong>\"Contact AWS support to request an AWS KMS rate limit increase\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Perform error retries with exponential backoff in the application code\" is a correct answer.</p><p><strong>INCORRECT:</strong> \"Contact AWS support to request an S3 rate limit increase\" is incorrect as the error indicates throttling in AWS KMS.</p><p><strong>INCORRECT:</strong> \"Import a customer master key (CMK) with a larger key size\" is incorrect as the key size does not affect the quota for requests to AWS KMS.</p><p><strong>INCORRECT:</strong> \"Use more than once customer master key (CMK) to encrypt S3 data\" is incorrect as the issue is not the CMK it is the request quota on AWS KMS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/requests-per-second.html\">https://docs.aws.amazon.com/kms/latest/developerguide/requests-per-second.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
                "options": [
                    {
                        "id": 3867,
                        "content": "<p>Perform error retries with exponential backoff in the application code</p>",
                        "isValid": true
                    },
                    {
                        "id": 3868,
                        "content": "<p>Use more than once customer master key (CMK) to encrypt S3 data</p>",
                        "isValid": false
                    },
                    {
                        "id": 3869,
                        "content": "<p>Contact AWS support to request an AWS KMS rate limit increase</p>",
                        "isValid": true
                    },
                    {
                        "id": 3870,
                        "content": "<p>Contact AWS support to request an S3 rate limit increase</p>",
                        "isValid": false
                    },
                    {
                        "id": 3871,
                        "content": "<p>Import a customer master key (CMK) with a larger key size</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 942,
            "attributes": {
                "createdAt": "2023-09-07T08:51:39.736Z",
                "updatedAt": "2023-09-07T08:51:39.736Z",
                "content": "<p>An application is being instrumented to send trace data using AWS X-Ray. A Developer needs to upload segment documents using JSON-formatted strings to X-Ray using the API. Which API action should the developer use?</p>",
                "answerExplanation": "<p>You can send trace data to X-Ray in the form of segment documents. A segment document is a JSON formatted string that contains information about the work that your application does in service of a request. Your application can record data about the work that it does itself in segments, or work that uses downstream services and resources in subsegments.</p><p>Segments record information about the work that your application does. A segment, at a minimum, records the time spent on a task, a name, and two IDs. The trace ID tracks the request as it travels between services. The segment ID tracks the work done for the request by a single service.</p><p>Example Minimal complete segment:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-14-03-7e305995aabee50345423568ced7525c.jpg\"></p><p>You can upload segment documents with the <a href=\"https://docs.aws.amazon.com/xray/latest/api/API_PutTraceSegments.html\"><code>PutTraceSegments</code></a><code> </code>API. The API has a single parameter, <code>TraceSegmentDocuments</code>, that takes a list of JSON segment documents.</p><p>Therefore, the Developer should use the <code>PutTraceSegments</code> API action.</p><p><br></p><p><strong>CORRECT: </strong>\"The <code>PutTraceSegments</code> API action\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The <code>PutTelemetryRecords</code> API action\" is incorrect as this is used by the AWS X-Ray daemon to upload telemetry.</p><p><strong>INCORRECT:</strong> \"The <code>UpdateGroup</code> API action\" is incorrect as this updates a group resource.</p><p><strong>INCORRECT:</strong> \"The <code>GetTraceSummaries</code> API action\" is incorrect as this retrieves IDs and annotations for traces available for a specified time frame using an optional filter.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-api-sendingdata.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-api-sendingdata.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3872,
                        "content": "<p>The <code>GetTraceSummaries</code> API action</p>",
                        "isValid": false
                    },
                    {
                        "id": 3873,
                        "content": "<p>The <code>UpdateGroup</code> API action</p>",
                        "isValid": false
                    },
                    {
                        "id": 3874,
                        "content": "<p>The <code>PutTelemetryRecords</code> API action</p>",
                        "isValid": false
                    },
                    {
                        "id": 3875,
                        "content": "<p>The <code>PutTraceSegments</code> API action</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 943,
            "attributes": {
                "createdAt": "2023-09-07T08:51:39.808Z",
                "updatedAt": "2023-09-07T08:51:39.808Z",
                "content": "<p>A Developer needs to run some code using Lambda in response to an event and forward the execution result to another application using a pub/sub notification.</p><p>How can the Developer accomplish this?</p>",
                "answerExplanation": "<p>With Destinations, you can send asynchronous function execution results to a destination resource without writing code. A function execution result includes version, timestamp, request context, request payload, response context, and response payload. For each execution status (i.e. Success and Failure), you can choose one destination from four options: another <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/welcome.html\">Lambda function</a>, an <a href=\"https://docs.aws.amazon.com/sns/latest/dg/welcome.html\">SNS topic</a>, an <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html\">SQS standard queue</a>, or <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">EventBridge</a>.</p><p>For this scenario, the code will be run by Lambda and the execution result will then be sent to the SNS topic. The application that is subscribed to the SNS topics will then receive the notification.</p><p><strong>CORRECT: </strong>\"Configure a Lambda â€œon successâ€ destination and route the execution results to Amazon SNS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SNS\" is incorrect as CloudWatch Events is used to track changes in the state of AWS resources. To forward execution results from Lambda a destination should be used.</p><p><strong>INCORRECT:</strong> \"Configure a Lambda â€œon successâ€ destination and route the execution results to Amazon SQS\" is incorrect as SQS is a message queue not a pub/sub notification service.</p><p><strong>INCORRECT:</strong> \"Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SQS\" is incorrect as CloudWatch Events is used to track changes in the state of AWS resources. To forward execution results from Lambda a destination should be used (with an SNS topic).</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/11/aws-lambda-supports-destinations-for-asynchronous-invocations/\">https://aws.amazon.com/about-aws/whats-new/2019/11/aws-lambda-supports-destinations-for-asynchronous-invocations/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3876,
                        "content": "<p>Configure a Lambda â€œon successâ€ destination and route the execution results to Amazon SNS</p>",
                        "isValid": true
                    },
                    {
                        "id": 3877,
                        "content": "<p>Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SQS</p>",
                        "isValid": false
                    },
                    {
                        "id": 3878,
                        "content": "<p>Configure a Lambda â€œon successâ€ destination and route the execution results to Amazon SQS</p>",
                        "isValid": false
                    },
                    {
                        "id": 3879,
                        "content": "<p>Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SNS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 944,
            "attributes": {
                "createdAt": "2023-09-07T08:51:39.885Z",
                "updatedAt": "2023-09-07T08:51:39.885Z",
                "content": "<p>A software organization has developed a new feature in its serverless application hosted on AWS. This feature involves an AWS Lambda function that gets invoked by an Amazon API Gateway API. Currently, the API uses a specific Lambda alias to invoke the Lambda function. The organization wants to roll out this new feature to a select group of users for beta testing without affecting the application's existing users.</p><p>What would be the most efficient approach to meet these requirements?</p>",
                "answerExplanation": "<p>In AWS, Lambda versions are an essential part of the deployment process. Once a version is published, the code within that version cannot be changed. It is ideal for ensuring stability and reliability between deployments.</p><p>API Gateway Stages can be used to route traffic to different Lambda versions. Hence, creating a new API Gateway stage for beta testing that is linked to the new version of the Lambda function will ensure that existing users (using the old stage) are not impacted by the new feature.</p><p><strong>CORRECT: </strong>\"Create a new version of the Lambda function. Build a new stage on API Gateway integrated with this new Lambda version. Utilize this new API Gateway stage for beta testing\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement Amazon S3 bucket versioning on the Lambda function code. For testing purposes, link the API Gateway to the new version of the code stored in the S3 bucket\" is incorrect.</p><p>While Amazon S3 versioning allows for keeping multiple variants of an object in the same bucket, it doesn't interact directly with AWS Lambda or API Gateway. Thus, you cannot control API Gateway to invoke different versions of Lambda function code stored in an S3 bucket.</p><p><strong>INCORRECT:</strong> \"Use AWS CodeDeploy to deploy the updated Lambda function. Split traffic between the new and old versions of the function for testing purposes\" is incorrect.</p><p>AWS CodeDeploy can deploy new versions of Lambda functions and shift traffic gradually but using it for testing purposes might risk exposing beta features to the regular users, potentially impacting their experience.</p><p><strong>INCORRECT:</strong> \"Use the AWS CLI to manually switch between the old and new versions of the Lambda function during testing periods\" is incorrect.</p><p>This approach is operationally intensive, prone to human error, and may impact regular users. AWS provides better ways of managing Lambda versions and aliases, which should be used instead of manual operations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-stages.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-stages.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3880,
                        "content": "<p>Implement Amazon S3 bucket versioning on the Lambda function code. For testing purposes, link the API Gateway to the new version of the code stored in the S3 bucket.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3881,
                        "content": "<p>Use AWS CodeDeploy to deploy the updated Lambda function. Split traffic between the new and old versions of the function for testing purposes.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3882,
                        "content": "<p>Use the AWS CLI to manually switch between the old and new versions of the Lambda function during testing periods.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3883,
                        "content": "<p>Create a new version of the Lambda function. Build a new stage on API Gateway integrated with this new Lambda version. Utilize this new API Gateway stage for beta testing.</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 945,
            "attributes": {
                "createdAt": "2023-09-07T08:51:39.952Z",
                "updatedAt": "2023-09-07T08:51:39.952Z",
                "content": "<p>A Developer is writing an AWS Lambda function that processes records from an Amazon Kinesis Data Stream. The Developer must write the function so that it sends a notice to Administrators if it fails to process a batch of records.</p><p>How should the Developer write the function?</p>",
                "answerExplanation": "<p>With Destinations, you can route asynchronous function results as an execution record to a destination resource without writing additional code. An execution record contains details about the request and response in JSON format including version, timestamp, request context, request payload, response context, and response payload.</p><p>For each execution status such as <em>Success</em> or <em>Failure</em> you can choose one of four destinations: another Lambda function, SNS, SQS, or EventBridge. Lambda can also be configured to route different execution results to different destinations.</p><p>In this scenario the Developer can publish the processed data to an Amazon SNS topic by configuring an Amazon SNS topic as an on-failure destination.</p><p><strong>CORRECT: </strong>\"Configure an Amazon SNS topic as an on-failure destination\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Separate the Lambda handler from the core logic\" is incorrect as this will not assist with sending a notification to administrators.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events to send the processed data\" is incorrect as CloudWatch Events is used for tracking state changes, not forwarding execution results</p><p><strong>INCORRECT:</strong> \"Push the failed records to an Amazon SQS queue\" is incorrect as SQS will not notify the administrators, SNS should be used.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/\">https://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 3884,
                        "content": "<p>Push the failed records to an Amazon SQS queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 3885,
                        "content": "<p>Use Amazon CloudWatch Events to send the processed data</p>",
                        "isValid": false
                    },
                    {
                        "id": 3886,
                        "content": "<p>Separate the Lambda handler from the core logic</p>",
                        "isValid": false
                    },
                    {
                        "id": 3887,
                        "content": "<p>Configure an Amazon SNS topic as an on-failure destination</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 946,
            "attributes": {
                "createdAt": "2023-09-07T08:51:40.022Z",
                "updatedAt": "2023-09-07T08:51:40.022Z",
                "content": "<p>A Developer wants to find a list of items in a global secondary index from an Amazon DynamoDB table.</p><p>Which DynamoDB API call can the Developer use in order to consume the LEAST number of read capacity units?</p>",
                "answerExplanation": "<p>Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table and issue Query or Scan requests against these indexes.</p><p>A <em>secondary index</em> is a data structure that contains a subset of attributes from a table, along with an alternate key to support Query operations. You can retrieve data from the index using a Query, in much the same way as you use Query with a table. A table can have multiple secondary indexes, which give your applications access to many different query patterns.</p><p>You can also issue scan operations on a global secondary index however it is less efficient as it will return all items in the index.</p><p><strong>CORRECT: </strong>\"Query operation using eventually-consistent reads\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Query operation using strongly-consistent reads\" is incorrect. Strongly consistent reads require more RCUs and also are not supported on a global secondary index (they are supported on local secondary indexes).</p><p><strong>INCORRECT:</strong> \"Scan operation using eventually-consistent reads\" is incorrect as a scan is less efficient than a query and will therefore use more RCUs.</p><p><strong>INCORRECT:</strong> \"Scan operation using strongly-consistent reads\" is incorrect as a scan is less efficient than a query and will therefore use more RCUs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3888,
                        "content": "<p>Scan operation using strongly-consistent reads</p>",
                        "isValid": false
                    },
                    {
                        "id": 3889,
                        "content": "<p>Scan operation using eventually-consistent reads</p>",
                        "isValid": false
                    },
                    {
                        "id": 3890,
                        "content": "<p>Query operation using eventually-consistent reads</p>",
                        "isValid": true
                    },
                    {
                        "id": 3891,
                        "content": "<p>Query operation using strongly-consistent reads</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 947,
            "attributes": {
                "createdAt": "2023-09-07T08:51:40.095Z",
                "updatedAt": "2023-09-07T08:51:40.095Z",
                "content": "<p>A Developer is creating a banking application that will be used to view financial transactions and statistics. The application requires multi-factor authentication to be added to the login protocol.</p><p>Which service should be used to meet this requirement?</p>",
                "answerExplanation": "<p>A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers.</p><p>User pools provide:</p><p> â€¢ Sign-up and sign-in services.</p><p> â€¢ A built-in, customizable web UI to sign in users.</p><p> â€¢ Social sign-in with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as sign-in with SAML identity providers from your user pool.</p><p> â€¢ User directory management and user profiles.</p><p> â€¢ Security features such as multi-factor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.</p><p> â€¢ Customized workflows and user migration through AWS Lambda triggers.</p><p>Multi-factor authentication (MFA) increases security for your app by adding another authentication method, and not relying solely on username and password. You can choose to use SMS text messages, or time-based one-time (TOTP) passwords as second factors in signing in your users.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-11-11-29cb0afc44b30879151772e2713834e0.jpg\"></p><p>For this scenario you would want to set the MFA setting to â€œRequiredâ€ as the data is highly secure.</p><p><strong>CORRECT: </strong>\"Amazon Cognito User Pool with MFA\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon Cognito Identity Pool with MFA\" is incorrect</p><p><strong>INCORRECT:</strong> \"AWS IAM with MFA\" is incorrect. With IAM your user accounts are maintained in your AWS account rather than in a Cognito User Pool. For logging into a web or mobile app it is better to create and manage your users in a Cognito User Pool and add MFA to the User Pool for extra security.</p><p><strong>INCORRECT:</strong> \"AWS Directory Service\" is incorrect as this is a managed Active Directory service. For a web or mobile application using AWS Cognito User Pools is a better solution for storing your user accounts and authenticating to the application.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 3892,
                        "content": "<p>AWS Directory Service</p>",
                        "isValid": false
                    },
                    {
                        "id": 3893,
                        "content": "<p>Amazon Cognito Identity Pool with MFA</p>",
                        "isValid": false
                    },
                    {
                        "id": 3894,
                        "content": "<p>AWS IAM with MFA</p>",
                        "isValid": false
                    },
                    {
                        "id": 3895,
                        "content": "<p>Amazon Cognito User Pool with MFA</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 948,
            "attributes": {
                "createdAt": "2023-09-07T08:51:40.165Z",
                "updatedAt": "2023-09-07T08:51:40.165Z",
                "content": "<p>A company currently runs a number of legacy automated batch processes for system update management and operational activities. The company are looking to refactor these processes and require a service that can coordinate multiple AWS services into serverless workflows.</p><p>What is the MOST suitable service for this requirement?</p>",
                "answerExplanation": "<p>AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or <em>task</em>, allowing you to scale and change applications quickly.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-27-06-992775d6a19ae40546963a3b42eab975.png\"></p><p>Step Functions provides a reliable way to coordinate components and step through the functions of your application. Step Functions offers a graphical console to visualize the components of your application as a series of steps. It automatically triggers and tracks each step, and retries when there are errors, so your application executes in order and as expected, every time. Step Functions logs the state of each step, so when things go wrong, you can diagnose and debug problems quickly.</p><p><strong>CORRECT: </strong>\"AWS Step Functions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon SWF\" is incorrect. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in <a href=\"https://aws.amazon.com/what-is-cloud-computing/\">the Cloud.</a> It does not coordinate serverless workflows.</p><p><strong>INCORRECT:</strong> \"AWS Batch\" is incorrect as this is used to run batch computing jobs on Amazon EC2 and is therefore not serverless.</p><p><strong>INCORRECT:</strong> \"AWS Lambda\" is incorrect as though it is serverless, it does not provide a native capability to coordinate multiple AWS services.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\">https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3896,
                        "content": "<p>AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 3897,
                        "content": "<p>AWS Batch</p>",
                        "isValid": false
                    },
                    {
                        "id": 3898,
                        "content": "<p>AWS Step Functions</p>",
                        "isValid": true
                    },
                    {
                        "id": 3899,
                        "content": "<p>Amazon SWF</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 949,
            "attributes": {
                "createdAt": "2023-09-07T08:51:40.241Z",
                "updatedAt": "2023-09-07T08:51:40.241Z",
                "content": "<p>A Developer needs to access AWS CodeCommit over SSH. The SSH keys configured to access AWS CodeCommit are tied to a user with the following permissions:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"version\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"2012-10-17\"</span></li><li class=\"L2\"><span class=\"str\">\"Statement\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"pun\">{</span></li><li class=\"L4\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L6\"><span class=\"str\">\"codecommit:BatchGetRepositories\"</span><span class=\"pun\">,</span></li><li class=\"L7\"><span class=\"str\">\"codecommit:Get*\"</span></li><li class=\"L8\"><span class=\"str\">\"codecommit:List*\"</span><span class=\"pun\">,</span></li><li class=\"L9\"><span class=\"str\">\"codecommit:GitPull\"</span></li><li class=\"L0\"><span class=\"pun\">],</span></li><li class=\"L1\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"*\"</span></li><li class=\"L2\"><span class=\"pun\">}</span></li><li class=\"L3\"><span class=\"pun\">]</span></li><li class=\"L4\"><span class=\"pun\">}</span></li></ol></pre></div></div><p>The Developer needs to create/delete branches.</p><p>Which specific IAM permissions need to be added based on the principle of least privilege?</p>",
                "answerExplanation": "<p>The permissions assigned to the user account are missing the privileges to create and delete branches in AWS CodeCommit. The Developer needs to be assigned these permissions but according to the principal of least privilege itâ€™s important to ensure no additional permissions are assigned.</p><p>The following API actions can be used to work with branches:</p><p> â€¢ CreateBranch , which creates a branch in a specified repository.</p><p> â€¢ DeleteBranch , which deletes the specified branch in a repository unless it is the default branch.</p><p> â€¢ GetBranch , which returns information about a specified branch.</p><p> â€¢ ListBranches , which lists all branches for a specified repository.</p><p> â€¢ UpdateDefaultBranch , which changes the default branch for a repository.</p><p>Therefore, the best answer is to add the â€œ<code>codecommit:CreateBranch</code>â€ and â€œ<code>codecommit:DeleteBranch</code>â€ permissions to the permissions policy.</p><p><strong>CORRECT: </strong>\"<code>codecommit:CreateBranch</code>â€ and â€œ<code>codecommit:DeleteBranch</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"<code>codecommit:Put</code>*:\" is incorrect. The wildcard (*) will allow any API action starting with â€œPutâ€, however the only options are put-file and put-repository-triggers, neither of which is related to branches.</p><p><strong>INCORRECT:</strong> \"<code>codecommit:Update</code>*\" is incorrect. The wildcard (*) will allow any API action starting with â€œUpdateâ€, however none of the options available are suitable for working with branches.</p><p><strong>INCORRECT:</strong> \"<code>codecommit:*</code>\" is incorrect as this would allow any API action which does not follow the principal of least privilege.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/codecommit/index.html#cli-aws-codecommit\">https://docs.aws.amazon.com/cli/latest/reference/codecommit/index.html#cli-aws-codecommit</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3900,
                        "content": "<p>â€œ<code>codecommit:CreateBranch</code>â€ and â€œ<code>codecommit:DeleteBranch</code>â€</p>",
                        "isValid": true
                    },
                    {
                        "id": 3901,
                        "content": "<p>â€œ<code>codecommit:Update*</code>â€</p>",
                        "isValid": false
                    },
                    {
                        "id": 3902,
                        "content": "<p>â€œ<code>codecommit:*</code>â€</p>",
                        "isValid": false
                    },
                    {
                        "id": 3903,
                        "content": "<p>â€œ<code>codecommit:Put*</code>:â€</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 950,
            "attributes": {
                "createdAt": "2023-09-07T08:51:40.317Z",
                "updatedAt": "2023-09-07T08:51:40.317Z",
                "content": "<p>A Developer has noticed some suspicious activity in her AWS account and is concerned that the access keys associated with her IAM user account may have been compromised. What is the first thing the Developer do in should do in this situation?</p>",
                "answerExplanation": "<p>In this case the Developerâ€™s access keys may have been compromised so the first step would be to invalidate the access keys by deleting them.</p><p>The next step would then be to determine if any temporary security credentials have been issued an invalidating those too to prevent any further misuse.</p><p>The user account and user account password have not been compromised so they do not need to be deleted / changed as a first step. However, changing the account password would typically be recommended as a best practice in this situation.</p><p><strong>CORRECT: </strong>\"Delete the compromised access keys\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Delete her IAM user account\" is incorrect. This user account has not been compromised based on the available information, just the access keys. Deleting the access keys will prevent further misuse of the AWS account.</p><p><strong>INCORRECT:</strong> \"Report the incident to AWS Support\" is incorrect is a good practice but not the first step. The Developer should first attempt to mitigate any further misuse of the account by deleting the access keys.</p><p><strong>INCORRECT:</strong> \"Change her IAM User account password\" is incorrect as she does not have any evidence that the account has been compromised, just the access keys. However, it would be a good practice to change the password, just not the first thing to do.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/what-to-do-if-you-inadvertently-expose-an-aws-access-key/\">https://aws.amazon.com/blogs/security/what-to-do-if-you-inadvertently-expose-an-aws-access-key/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 3904,
                        "content": "<p>Delete her IAM user account</p>",
                        "isValid": false
                    },
                    {
                        "id": 3905,
                        "content": "<p>Report the incident to AWS Support</p>",
                        "isValid": false
                    },
                    {
                        "id": 3906,
                        "content": "<p>Delete the compromised access keys</p>",
                        "isValid": true
                    },
                    {
                        "id": 3907,
                        "content": "<p>Change her IAM User account password</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 951,
            "attributes": {
                "createdAt": "2023-09-07T08:51:40.393Z",
                "updatedAt": "2023-09-07T08:51:40.393Z",
                "content": "<p>An online multiplayer game employs Amazon API Gateway WebSocket APIs with an HTTP backend. The game developer needs to add a feature that identifies players with unstable connections who repeatedly join and leave the game. The developer also wants the ability to disconnect such players from the game.</p><p>What two modifications should the developer implement in the game to fulfill these requirements? (Select TWO.)</p>",
                "answerExplanation": "<p>By implementing $connect and $disconnect routes in the backend service, you can capture events when a player connects and disconnects from the WebSocket API. This enables tracking and managing of players' connection statuses.</p><p>Using Amazon DynamoDB (or another database service) allows you to persist and track the connection status of each player in real-time. This enables you to identify players who connect and disconnect frequently.</p><p><strong>CORRECT: </strong>\"Implement $connect and $disconnect routes in the backend service\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Add logic to track the player's connection status using Amazon DynamoDB in the backend service\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Switch to REST APIs in the backend service\" is incorrect.</p><p>Switching to REST APIs won't inherently provide the ability to manage or track unstable connections. WebSocket APIs are more suited for applications requiring real-time, two-way communication.</p><p><strong>INCORRECT:</strong> \"Implement AWS Cognito for player authentication in the backend service\" is incorrect.</p><p>While AWS Cognito provides authentication and user management, it doesn't offer features for tracking and managing unstable connections.</p><p><strong>INCORRECT:</strong> \"Switch to AWS App Runner for the backend service\" is incorrect.</p><p>AWS App Runner is a service that makes it easy to build, deploy, and scale containerized applications quickly, but it doesn't directly address the specific requirement of tracking and managing unstable connections.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api-route-keys-connect-disconnect.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api-route-keys-connect-disconnect.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 3908,
                        "content": "<p>Implement AWS Cognito for player authentication in the backend service.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3909,
                        "content": "<p>Switch to AWS App Runner for the backend service.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3910,
                        "content": "<p>Add logic to track the player's connection status using Amazon DynamoDB in the backend service.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3911,
                        "content": "<p>Implement $connect and $disconnect routes in the backend service.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3912,
                        "content": "<p>Switch to REST APIs in the backend service.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 952,
            "attributes": {
                "createdAt": "2023-09-07T08:51:40.469Z",
                "updatedAt": "2023-09-07T08:51:40.469Z",
                "content": "<p>You run an ad-supported photo sharing website using Amazon S3 to serve photos to visitors of your site. At some point you find out that other sites have been linking to the photos on your site, causing loss to your business.<br>What is an effective method to mitigate this?</p>",
                "answerExplanation": "<p>When Amazon S3 objects are private, only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant time-limited permission to download the objects.</p><p>When you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The presigned URLs are valid only for the specified duration.</p><p>Anyone who receives the presigned URL can then access the object. In this scenario, the photos can be shared with the ownerâ€™s website but not with any other 3rd parties. This will stop other sites from linking to the photos as they will not display anywhere else.</p><p><strong>CORRECT: </strong>\"Remove public read access and use signed URLs with expiry dates\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Store photos on an EBS volume of the web server\" is incorrect as this does not add any more control over content visibility in the website.</p><p><strong>INCORRECT:</strong> \"Use CloudFront distributions for static content\" is incorrect as this alone will not protect the content. You can also use pre-signed URLs with CloudFront, but this isnâ€™t mentioned.</p><p><strong>INCORRECT:</strong> \"Block the IPs of the offending websites in Security Groups\" is incorrect as you can only configure allow rules in security groups so this would be hard to manage.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 3913,
                        "content": "<p>Remove public read access and use signed URLs with expiry dates</p>",
                        "isValid": true
                    },
                    {
                        "id": 3914,
                        "content": "<p>Use CloudFront distributions for static content</p>",
                        "isValid": false
                    },
                    {
                        "id": 3915,
                        "content": "<p>Store photos on an EBS volume of the web server</p>",
                        "isValid": false
                    },
                    {
                        "id": 3916,
                        "content": "<p>Block the IPs of the offending websites in Security Groups</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 953,
            "attributes": {
                "createdAt": "2023-09-07T08:51:40.543Z",
                "updatedAt": "2023-09-07T08:51:40.543Z",
                "content": "<p>An Amazon Kinesis Data Stream has recently been configured to receive data from sensors in a manufacturing facility. A consumer EC2 instance is configured to process the data every 48 hours and save processing results to an Amazon RedShift data warehouse. Testing has identified a large amount of data is missing. A review of monitoring logs has identified that the sensors are sending data correctly and the EC2 instance is healthy.</p><p>What is the MOST likely explanation for this issue?</p>",
                "answerExplanation": "<p>Amazon Kinesis Data Streams supports changes to the data record retention period of your stream. A Kinesis data stream is an ordered sequence of data records meant to be written to and read from in real time. Data records are therefore stored in shards in your stream temporarily. The time period from when a record is added to when it is no longer accessible is called the <em>retention period</em>. A Kinesis data stream stores records from 24 hours by default, up to 8760 hours.</p><p>You can increase the retention period up to 8760 hours using the <a href=\"https://docs.aws.amazon.com/kinesis/latest/APIReference/API_IncreaseStreamRetentionPeriod.html\">IncreaseStreamRetentionPeriod</a> operation. You can decrease the retention period down to a minimum of 24 hours using the <a href=\"https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DecreaseStreamRetentionPeriod.html\">DecreaseStreamRetentionPeriod</a> operation. The request syntax for both operations includes the stream name and the retention period in hours. Finally, you can check the current retention period of a stream by calling the <a href=\"https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStream.html\">DescribeStream</a> operation.</p><p>Both operations are easy to use. The following is an example of changing the retention period using the AWS CLI:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-58-43-abe0f94430b16f81fcaae80c865627f0.jpg\"></p><p>Therefore, the most likely explanation is that the message retention period is set at the 24-hour default.</p><p><strong>CORRECT: </strong>\"Records are retained for 24 hours in the Kinesis Data Stream by default\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon RedShift is not suitable for storing streaming data\" is incorrect. In this architecture Amazon Kinesis is responsible for receiving streaming data and storing it in a stream. The EC2 instances can then process and store the data in a number of different destinations including Amazon RedShift.</p><p><strong>INCORRECT:</strong> \"The EC2 instance is failing intermittently\" is incorrect as the question states that a review of monitoring logs indicates that the EC2 instance is healthy. If it was failing intermittently this should be recorded in the logs.</p><p><strong>INCORRECT:</strong> \"Amazon Kinesis has too many shards provisioned\" is incorrect as this would just mean that the Kinesis Stream has more capacity, not less.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 3917,
                        "content": "<p>Records are retained for 24 hours in the Kinesis Data Stream by default</p>",
                        "isValid": true
                    },
                    {
                        "id": 3918,
                        "content": "<p>Amazon Kinesis has too many shards provisioned</p>",
                        "isValid": false
                    },
                    {
                        "id": 3919,
                        "content": "<p>Amazon RedShift is not suitable for storing streaming data</p>",
                        "isValid": false
                    },
                    {
                        "id": 3920,
                        "content": "<p>The EC2 instance is failing intermittently</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 954,
            "attributes": {
                "createdAt": "2023-09-07T08:51:40.616Z",
                "updatedAt": "2023-09-07T08:51:40.616Z",
                "content": "<p>A Developer must run a shell script on Amazon EC2 Linux instances each time they are launched by an Amazon EC2 Auto Scaling group. What is the SIMPLEST way to run the script?</p>",
                "answerExplanation": "<p>The simplest option is to add the script to the user data when creating the launch configuration. User data is information that is parsed when the EC2 instances are launched. When you add a script to the user data in a launch configuration all instances that are launched by that Auto Scaling group will run the script.</p><p><strong>CORRECT: </strong>\"Add the script to the user data when creating the launch configuration\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure Amazon CloudWatch Events to trigger the AWS CLI when an instance is launched and run the script\" is incorrect as you cannot trigger the AWS CLI using CloudWatch Events and the script may not involve AWS CLI commands.</p><p><strong>INCORRECT:</strong> \"Package the script in a zip file with some AWS Lambda source code. Upload to Lambda and run the function when instances are launched\" is incorrect as Lambda does not run shell scripts. You could program the requirements into the function code however you still need a trigger which is not mentioned in this option.</p><p><strong>INCORRECT:</strong> \"Run the script using the AWS Systems Manager Run Command\" is incorrect as this is not the simplest method. For most Linux AMIs (except Amazon Linux) the developerâ€™s would need to install the agent on the operating system. They would also then need to create a mechanism of triggering the run command.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 3921,
                        "content": "<p>Run the script using the AWS Systems Manager Run Command</p>",
                        "isValid": false
                    },
                    {
                        "id": 3922,
                        "content": "<p>Configure Amazon CloudWatch Events to trigger the AWS CLI when an instance is launched and run the script</p>",
                        "isValid": false
                    },
                    {
                        "id": 3923,
                        "content": "<p>Add the script to the user data when creating the launch configuration</p>",
                        "isValid": true
                    },
                    {
                        "id": 3924,
                        "content": "<p>Package the script in a zip file with some AWS Lambda source code. Upload to Lambda and run the function when instances are launched</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 955,
            "attributes": {
                "createdAt": "2023-09-07T08:51:40.686Z",
                "updatedAt": "2023-09-07T08:51:40.686Z",
                "content": "<p>A large quantity of sensitive data must be encrypted. A Developer will use a custom CMK to generate the encryption key. The key policy currently looks like this:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow Key Usage\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"Principal\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">{</span><span class=\"str\">\"AWS\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L4\"><span class=\"str\">\"arn:aws:iam::111122223333:user/CMKUser\"</span></li><li class=\"L5\"><span class=\"pun\">]},</span></li><li class=\"L6\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L7\"><span class=\"str\">\"kms:Encrypt\"</span><span class=\"pun\">,</span></li><li class=\"L8\"><span class=\"str\">\"kms:Decrypt\"</span><span class=\"pun\">,</span></li><li class=\"L9\"><span class=\"str\">\"kms:ReEncrypt*\"</span><span class=\"pun\">,</span></li><li class=\"L0\"><span class=\"str\">\"kms:DescribeKey\"</span></li><li class=\"L1\"><span class=\"pun\">],</span></li><li class=\"L2\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"*\"</span></li><li class=\"L3\"><span class=\"pun\">}</span></li></ol></pre></div></div><p>What API action must be added to the key policy?</p>",
                "answerExplanation": "<p>A key policy is a document that uses <a href=\"http://json.org/\">JSON (JavaScript Object Notation)</a> to specify permissions. You can work with these JSON documents directly, or you can use the AWS Management Console to work with them using a graphical interface called the <em>default view</em>.</p><p>The key policy supplied with this question is missing the GenerateDataKey API action which is a permission that is required to generate a data encryption key. A data encryption key is required to encrypt large amounts of data as a CMK can only encrypt up to 4 KB.</p><p>The GenerateDataKey API Generates a unique symmetric data key. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.</p><p><strong>CORRECT: </strong>\"<code>kms:GenerateDataKey</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"<code>kms:EnableKey</code>\" is incorrect as this sets the key state of a customer master key (CMK) to enabled. It allows you to use the CMK for <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#cryptographic-operations\">cryptographic operations</a>.</p><p><strong>INCORRECT:</strong> \"<code>kms:CreateKey</code>\" is incorrect as this creates a unique customer managed <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master-keys\">customer master key</a> (CMK) in your AWS account and Region. In this case the CMK already exists, the Developer needs to create a data encryption key.</p><p><strong>INCORRECT:</strong> \"<code>kms:GetKeyPolicy</code>\" is incorrect as this simply gets a key policy attached to the specified customer master key (CMK).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html\">https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
                "options": [
                    {
                        "id": 3925,
                        "content": "<p><code>kms:EnableKey</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3926,
                        "content": "<p><code>kms:GetKeyPolicy</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3927,
                        "content": "<p><code>kms:CreateKey</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 3928,
                        "content": "<p><code>kms:GenerateDataKey</code> </p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 956,
            "attributes": {
                "createdAt": "2023-09-07T08:51:40.760Z",
                "updatedAt": "2023-09-07T08:51:40.760Z",
                "content": "<p>An online retail platform uses the AWS SDK for Python (Boto3) on the frontend to handle user authentication through AWS Security Token Service (AWS STS). The platform stores its digital assets in an Amazon S3 bucket and delivers them using an Amazon CloudFront distribution, which uses the S3 bucket as its origin.</p><p>Currently, the application holds its role credentials in plaintext within a Python file in the application code. The platform developers are looking to improve security by creating a mechanism that enables the application to retrieve user credentials without embedding any credentials in the application code.</p><p>What solution would meet these requirements?</p>",
                "answerExplanation": "<p>The question seeks a solution where the application retrieves user credentials without storing any credentials within the application code. It's a security risk to embed credentials in code; it's safer to employ AWS managed services, like Lambda@Edge, that can securely manage and provide temporary credentials through AWS STS. Lambda@Edge also enables you to run your code closer to your users, reducing latency.</p><p><strong>CORRECT: </strong>\"Integrate a Lambda@Edge function with the CloudFront distribution. Trigger the function upon each viewer request. Give the execution role of the function the required permissions to interact with AWS STS. Shift all SDK calls from the frontend to this function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure a CloudFront function within the distribution. Activate the function on viewer request. Grant the function's execution role the necessary permissions to access AWS STS. Relocate all SDK calls from the frontend to this function\" is incorrect.</p><p>CloudFront Functions are primarily used to perform lightweight HTTP(s) transformations and manipulations at AWS edge locations. These functions are designed to execute simple tasks like HTTP header manipulations, URL redirects, or cache key normalizations. They do not have permission to access AWS services like AWS STS, nor can they handle the more complex operations required for this use case. Therefore, this option is incorrect.</p><p><strong>INCORRECT:</strong> \"Attach a Lambda@Edge function to the CloudFront distribution. Invoke this function with each viewer request. Transfer the credentials from the Python file into this function and move all SDK calls from the frontend into this function\" is incorrect.</p><p>This option suggests transferring the plaintext credentials into a Lambda@Edge function, which is not a secure practice. It implies hardcoding the credentials into the Lambda@Edge function, which is discouraged in AWS (or any other environment). Credentials should not be hardcoded because they could be exposed through logs or version control systems, leading to potential security risks. This option does not comply with the security best practices and thus is incorrect.</p><p><strong>INCORRECT:</strong> \"Set up a CloudFront function on the distribution. Initiate this function for each viewer request. Shift the credentials from the Python file into this function and relocate all SDK calls from the frontend into the function\" is incorrect.</p><p>This option also proposes transferring the plaintext credentials into a CloudFront function, which is an insecure practice. Hardcoding credentials is considered risky as they could be unintentionally exposed, causing potential security vulnerabilities. Moreover, CloudFront Functions cannot access other AWS services and cannot perform complex tasks. Therefore, this option does not meet the security requirements and is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/lambda/edge/\">https://aws.amazon.com/lambda/edge/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 3929,
                        "content": "<p>Configure a CloudFront function within the distribution. Activate the function on viewer request. Grant the function's execution role the necessary permissions to access AWS STS. Relocate all SDK calls from the frontend to this function.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3930,
                        "content": "<p>Integrate a Lambda@Edge function with the CloudFront distribution. Trigger the function upon each viewer request. Give the execution role of the function the required permissions to interact with AWS STS. Shift all SDK calls from the frontend to this function.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3931,
                        "content": "<p>Set up a CloudFront function on the distribution. Initiate this function for each viewer request. Shift the credentials from the Python file into this function and relocate all SDK calls from the frontend into the function.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3932,
                        "content": "<p>Attach a Lambda@Edge function to the CloudFront distribution. Invoke this function with each viewer request. Transfer the credentials from the Python file into this function and move all SDK calls from the frontend into this function.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 957,
            "attributes": {
                "createdAt": "2023-09-07T08:51:40.835Z",
                "updatedAt": "2023-09-07T08:51:40.835Z",
                "content": "<p>What does an Amazon SQS delay queue accomplish?</p>",
                "answerExplanation": "<p>Delay queues let you postpone the delivery of new messages to a queue for a number of seconds, for example, when your consumer application needs additional time to process messages.</p><p>If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-34-10-87835307943cf0d5f22a56da08d0a97f.png\"></p><p>Therefore, the correct explanation is that with an Amazon SQS Delay Queue messages are hidden for a configurable amount of time when they are first added to the queue</p><p><strong>CORRECT: </strong>\"Messages are hidden for a configurable amount of time when they are first added to the queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Messages are hidden for a configurable amount of time after they are consumed from the queue\" is incorrect. They are hidden when they are added to the queue.</p><p><strong>INCORRECT:</strong> \"The consumer can poll the queue for a configurable amount of time before retrieving a message\" is incorrect. A delay queue simply delays visibility of the message, it does not affect polling behavior.</p><p><strong>INCORRECT:</strong> \"Message cannot be deleted for a configurable amount of time after they are consumed from the queue\" is incorrect. That is what a visibility timeout achieves.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 3933,
                        "content": "<p>Message cannot be deleted for a configurable amount of time after they are consumed from the queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 3934,
                        "content": "<p>The consumer can poll the queue for a configurable amount of time before retrieving a message</p>",
                        "isValid": false
                    },
                    {
                        "id": 3935,
                        "content": "<p>Messages are hidden for a configurable amount of time after they are consumed from the queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 3936,
                        "content": "<p>Messages are hidden for a configurable amount of time when they are first added to the queue</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 958,
            "attributes": {
                "createdAt": "2023-09-07T08:51:40.924Z",
                "updatedAt": "2023-09-07T08:51:40.924Z",
                "content": "<p>How can a Developer view a summary of proposed changes to an AWS CloudFormation stack without implementing the changes in production?</p>",
                "answerExplanation": "<p>When you need to update a stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. Change sets allow you to preview how proposed changes to a stack might impact your running resources.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-40-43-a83262cbb0a2e50aae2536ca0c7399c4.jpg\"></p><p>AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. You can create and manage change sets using the AWS CloudFormation console, AWS CLI, or AWS CloudFormation API.</p><p><strong>CORRECT: </strong>\"Create a Change Set\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a StackSet\" is incorrect as StackSets are used to create, update, or delete stacks across multiple accounts and regions with a single operation.</p><p><strong>INCORRECT:</strong> \"Use drift detection\" is incorrect as this is used to detect when a configuration deviates from the template configuration.</p><p><strong>INCORRECT:</strong> \"Use a direct update\" is incorrect as this will directly update the production resources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>",
                "options": [
                    {
                        "id": 3937,
                        "content": "<p>Use a direct update</p>",
                        "isValid": false
                    },
                    {
                        "id": 3938,
                        "content": "<p>Create a StackSet</p>",
                        "isValid": false
                    },
                    {
                        "id": 3939,
                        "content": "<p>Create a Change Set</p>",
                        "isValid": true
                    },
                    {
                        "id": 3940,
                        "content": "<p>Use drift detection</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 959,
            "attributes": {
                "createdAt": "2023-09-07T08:51:41.007Z",
                "updatedAt": "2023-09-07T08:51:41.007Z",
                "content": "<p>A developer is creating a serverless application that will use a DynamoDB table. The average item size is 9KB. The application will make 4 strongly consistent reads/sec, and 2 standard write/sec. How many RCUs/WCUs are required?</p>",
                "answerExplanation": "<p>With provisioned capacity mode, you specify the number of data reads and writes per second that you require for your application.</p><p><strong>Read capacity unit (RCU):</strong></p><p> â€¢ Each API call to read data from your table is a read request.</p><p> â€¢ Read requests can be strongly consistent, eventually consistent, or transactional.</p><p> â€¢ For items up to 4 KB in size, one RCU can perform one <em>strongly consistent</em> read request per&nbsp; second.</p><p> â€¢ Items larger than 4 KB require additional RCUs.</p><p>For items up to 4 KB in size, one RCU can perform two <em>eventually consistent</em> read requests per second.</p><p><em>Transactional</em> read requests require two RCUs to perform one read per second for items up to 4 KB.</p><p>For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs.</p><p><strong>Write capacity unit (WCU):</strong></p><p> â€¢ Each API call to write data to your table is a write request.</p><p>For items up to 1 KB in size, one WCU can perform one<em> standard</em> write request per second.</p><p>Items larger than 1 KB require additional WCUs.</p><p><em> </em>â€¢ <em>Transactional</em> write requests require two WCUs to perform one write per second for items up to 1 KB.</p><p> â€¢ For example, a standard write request of a 1 KB item would require one WCU, a standard write request of a 3 KB item would require three WCUs, and a transactional write request of a 3 KB item would require six WCUs.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-24-22-68d5683a1fee43bfdba434ca4127edaf.jpg\"></p><p><strong>To determine the number of RCUs required to handle 4 strongly consistent reads per/second with an average item size of 9KB, perform the following steps:</strong></p><p><strong>1. Determine the average item size by rounding up the next multiple of 4KB (9KB rounds up to 12KB).</strong></p><p><strong>2. Determine the RCU per item by dividing the item size by 4KB (12KB/4KB = 3).</strong></p><p><strong>3. Multiply the value from step 2 with the number of reads required per second (3x4 = 12).</strong></p><p><strong>To determine the number of WCUs required to handle 2 standard writes per/second with an average item size of 9KB, simply multiply the average item size by the number of writes required (9x2=18).</strong></p><p><strong>CORRECT: </strong>\"12 RCU and 18 WCU\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"24 RCU and 18 WCU\" is incorrect. This would be the correct answer for transactional reads and standard writes.</p><p><strong>INCORRECT:</strong> \"12 RCU and 36 WCU\" is incorrect. This would be the correct answer for strongly consistent reads and transactional writes.</p><p><strong>INCORRECT:</strong> \"6 RCU and 18 WCU\" is incorrect. This would be the correct answer for eventually consistent reads and standard writes</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/pricing/provisioned/\">https://aws.amazon.com/dynamodb/pricing/provisioned/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3941,
                        "content": "<p>12 RCU and 36 WCU</p>",
                        "isValid": false
                    },
                    {
                        "id": 3942,
                        "content": "<p>24 RCU and 18 WCU</p>",
                        "isValid": false
                    },
                    {
                        "id": 3943,
                        "content": "<p>12 RCU and 18 WCU</p>",
                        "isValid": true
                    },
                    {
                        "id": 3944,
                        "content": "<p>6 RCU and 18 WCU</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 960,
            "attributes": {
                "createdAt": "2023-09-07T08:51:41.103Z",
                "updatedAt": "2023-09-07T08:51:41.103Z",
                "content": "<p>A monitoring application that keeps track of a large eCommerce website uses Amazon Kinesis for data ingestion. During periods of peak data rates, the Kinesis stream cannot keep up with the incoming data.<br>What step will allow Kinesis data streams to accommodate the traffic during peak hours?</p>",
                "answerExplanation": "<p>The UpdateShardCount API action updates the shard count of the specified stream to the specified number of shards.</p><p>Updating the shard count is an asynchronous operation. Upon receiving the request, Kinesis Data Streams returns immediately and sets the status of the stream to UPDATING. After the update is complete, Kinesis Data Streams sets the status of the stream back to ACTIVE.</p><p>Depending on the size of the stream, the scaling action could take a few minutes to complete. You can continue to read and write data to your stream while its status is UPDATING.</p><p>To update the shard count, Kinesis Data Streams performs splits or merges on individual shards. This can cause short-lived shards to be created, in addition to the final shards. These short-lived shards count towards your total shard limit for your account in the Region.</p><p>When using this operation, we recommend that you specify a target shard count that is a multiple of 25% (25%, 50%, 75%, 100%). You can specify any target value within your shard limit. However, if you specify a target that isn't a multiple of 25%, the scaling action might take longer to complete.</p><p>This operation has the following default limits. By default, you cannot do the following:</p><p> â€¢ Scale more than ten times per rolling 24-hour period per stream</p><p> â€¢ Scale up to more than double your current shard count for a stream</p><p> â€¢ Scale down below half your current shard count for a stream</p><p> â€¢ Scale up to more than 500 shards in a stream</p><p> â€¢ Scale a stream with more than 500 shards down unless the result is less than 500 shards</p><p>Scale up to more than the shard limit for your account</p><p>Note that the question specifically states that the Kinesis data stream cannot keep up with incoming data. This indicates that the producers are attempting to add records to the stream but there are not enough shards to keep up with demand. Therefore, we need to add additional shards and can do this using the UpdateShardCount API action.</p><p><strong>CORRECT: </strong>\"Increase the shard count of the stream using UpdateShardCount\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Install the Kinesis Producer Library (KPL) for ingesting data into the stream\" is incorrect as that will help the producers to be more efficient and increase write throughput to a Kinesis data stream. However, this will not help as the Kinesis data stream already cannot keep up with the incoming demand.</p><p><strong>INCORRECT:</strong> \"Create an SQS queue and decouple the producers from the Kinesis data stream \" is incorrect. You cannot decouple a Kinesis producer from a Kinesis data stream using SQS. Kinesis is more than capable of keeping up with demand, it just needs more shards in this case.</p><p><strong>INCORRECT:</strong> \"Ingest multiple records into the stream in a single call using PutRecords\" is incorrect as the stream is already overloaded, we need more shards, not more data to be written.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kinesis/latest/APIReference/API_UpdateShardCount.html\">https://docs.aws.amazon.com/kinesis/latest/APIReference/API_UpdateShardCount.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 3945,
                        "content": "<p>Create an SQS queue and decouple the producers from the Kinesis data stream</p>",
                        "isValid": false
                    },
                    {
                        "id": 3946,
                        "content": "<p>Increase the shard count of the stream using UpdateShardCount</p>",
                        "isValid": true
                    },
                    {
                        "id": 3947,
                        "content": "<p>Install the Kinesis Producer Library (KPL) for ingesting data into the stream</p>",
                        "isValid": false
                    },
                    {
                        "id": 3948,
                        "content": "<p>1. Ingest multiple records into the stream in a single call using PutRecords</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 961,
            "attributes": {
                "createdAt": "2023-09-07T08:51:41.197Z",
                "updatedAt": "2023-09-07T08:51:41.197Z",
                "content": "<p>An application scans an Amazon DynamoDB table once per day to produce a report. The scan is performed in non-peak hours when production usage uses around 50% of the provisioned throughput.</p><p>How can you MINIMIZE the time it takes to produce the report without affecting production workloads? (Select TWO.)</p>",
                "answerExplanation": "<p>By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data.</p><p>The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table's data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition.</p><p>To address these issues, the Scan operation can logically divide a table or secondary index into multiple <em>segments</em>, with multiple application workers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating system process. To perform a parallel scan, each worker issues its own Scan request with the following parameters:</p><p> â€¢ Segment â€” A segment to be scanned by a particular worker. Each worker should use a different value for Segment.</p><p> â€¢ TotalSegments â€” The total number of segments for the parallel scan. This value must be the same as the number of workers that your application will use.</p><p>The following diagram shows how a multithreaded application performs a parallel Scan with three degrees of parallelism.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-09-16-b996301e40aa2d9a508f54946ea8c3bf.jpg\"></p><p>To make the most of your tableâ€™s provisioned throughput, youâ€™ll want to use the Parallel Scan API operation so that your scan is distributed across your tableâ€™s partitions. However, you also need to ensure the scan doesnâ€™t consume your tableâ€™s provisioned throughput and cause the critical parts of your application to be throttled.</p><p>To control the amount of data returned per request, use the Limit parameter. This can help prevent situations where one worker consumes all of the provisioned throughput, at the expense of all other workers.</p><p>Therefore, the best solution to this problem is to use a parallel scan API operation with the Limit parameter.</p><p><strong>CORRECT: </strong>\"Use a Parallel Scan API operation \" is the correct answer.</p><p><strong>CORRECT:</strong> \"Use the Limit parameter\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Use a Sequential Scan API operation\" is incorrect as this would take more time and the question requests that we minimize the time it takes to complete the scan.</p><p><strong>INCORRECT:</strong> \"Increase read capacity units during the scan operation\" is incorrect as this would increase cost and we still need a solution to ensure we maximize usage of available throughput without affecting production workloads.</p><p><strong>INCORRECT:</strong> \"Use pagination to divide results into 1 MB pages\" is incorrect as this does only divides the results into pages, it does not segment and limit the amount of throughput used.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/\">https://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3949,
                        "content": "<p>Increase read capacity units during the scan operation</p>",
                        "isValid": false
                    },
                    {
                        "id": 3950,
                        "content": "<p>Use pagination to divide results into 1 MB pages</p>",
                        "isValid": false
                    },
                    {
                        "id": 3951,
                        "content": "<p>Use a Sequential Scan API operation</p>",
                        "isValid": false
                    },
                    {
                        "id": 3952,
                        "content": "<p>Use a Parallel Scan API operation</p>",
                        "isValid": true
                    },
                    {
                        "id": 3953,
                        "content": "<p>Use the Limit parameter</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 962,
            "attributes": {
                "createdAt": "2023-09-07T08:51:41.279Z",
                "updatedAt": "2023-09-07T08:51:41.279Z",
                "content": "<p>A media organization utilizes an Amazon API Gateway REST API endpoint to disseminate updates from an internal Content Management System (CMS) to Amazon EventBridge. An EventBridge rule is set up to monitor these updates and control content syndication in a primary AWS account. The organization now wants to extend the reach of these updates across several affiliate AWS accounts.</p><p>How can the developer accomplish this without altering the configuration of the CMS?</p>",
                "answerExplanation": "<p>EventBridge allows events to be sent to other AWS accounts, providing a straightforward solution to distribute the updates across multiple accounts. This wouldn't require any changes to the CMS configuration.</p><p><strong>CORRECT: </strong>\"Implement an EventBridge event bus in the affiliate AWS accounts to create a rule that matches events and forwards them from the main account\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Establish multiple API Gateway REST API endpoints in affiliate accounts to directly receive updates from the CMS\" is incorrect.</p><p>Setting up multiple API Gateway REST API endpoints in affiliate accounts to directly receive updates from the CMS would not only require changes to the CMS configuration but would also mean maintaining multiple endpoints.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda in the main account to clone updates and manually invoke it in affiliate accounts\" is incorrect.</p><p>Using AWS Lambda in the main account to clone updates and manually invoke it in affiliate accounts would be an inefficient solution, leading to potential latency and management overhead.</p><p><strong>INCORRECT:</strong> \"Set up multiple instances of the CMS, each linked to a different AWS account\" is incorrect.</p><p>Setting up multiple instances of the CMS, each linked to a different AWS account, would be a significantly complex and resource-intensive solution. It's neither practical nor necessary for the given requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3954,
                        "content": "<p>Implement an EventBridge event bus in the affiliate AWS accounts to create a rule that matches events and forwards them from the main account.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3955,
                        "content": "<p>Establish multiple API Gateway REST API endpoints in affiliate accounts to directly receive updates from the CMS.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3956,
                        "content": "<p>Use AWS Lambda in the main account to clone updates and manually invoke it in affiliate accounts.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3957,
                        "content": "<p>Set up multiple instances of the CMS, each linked to a different AWS account.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 963,
            "attributes": {
                "createdAt": "2023-09-07T08:51:41.355Z",
                "updatedAt": "2023-09-07T08:51:41.355Z",
                "content": "<p>A three-tier application is being migrated from an on-premises data center. The application includes an Apache Tomcat web tier, an application tier running on Linux, and a MySQL back end. A Developer must refactor the application to run on the AWS cloud. The cloud-based application must be fault tolerant and elastic.</p><p>How can the Developer refactor the web tier and application tier? (Select TWO.)</p>",
                "answerExplanation": "<p>The key requirements in this scenario are to add fault tolerances and elasticity to the web tier and application tier. Note that no specific requirements for the back end have been included.</p><p>To add elasticity to the web and application tiers the Developer should create Auto Scaling groups of EC2 instances. We know that the application tier runs on Linux and the web tier runs on Apache Tomcat (which could be on Linux or Windows). Therefore, these workloads are suitable for an ASG and this will ensure the number of instances dynamically scales out and in based on actual usage.</p><p>To add fault tolerance to the web and application tiers the Developer should add an Elastic Load Balancer. This will ensure that if the number of EC2 instances are changed by the ASG, the load balancer is able to distribute traffic to them. This also assists with elasticity.</p><p><strong>CORRECT: </strong>\"Create an Auto Scaling group of EC2 instances for both the web tier and application tier\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Implement an Elastic Load Balancer for both the web tier and the application tier\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution for the web tier\" is incorrect as CloudFront is used for performance reasons, not elasticity or fault tolerance. You would use CloudFront to get content closer to end users around the world.</p><p><strong>INCORRECT:</strong> \"Use a multi-AZ Amazon RDS database for the back end using the MySQL engine\" is incorrect as the question does not ask for fault tolerance of the back end, only the web tier and the application tier.</p><p><strong>INCORRECT:</strong> \"Implement an Elastic Load Balancer for the application tier\" is incorrect. An Elastic Load Balancer should be implemented for both the web tier and the application tier as that is how we ensure fault tolerance and elasticity for both of those tiers.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 3958,
                        "content": "<p>Use a multi-AZ Amazon RDS database for the back end using the MySQL engine</p>",
                        "isValid": false
                    },
                    {
                        "id": 3959,
                        "content": "<p>Implement an Elastic Load Balancer for both the web tier and the application tier</p>",
                        "isValid": true
                    },
                    {
                        "id": 3960,
                        "content": "<p>Create an Amazon CloudFront distribution for the web tier</p>",
                        "isValid": false
                    },
                    {
                        "id": 3961,
                        "content": "<p>Implement an Elastic Load Balancer for the application tier</p>",
                        "isValid": false
                    },
                    {
                        "id": 3962,
                        "content": "<p>Create an Auto Scaling group of EC2 instances for both the web tier and application tier</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 964,
            "attributes": {
                "createdAt": "2023-09-07T08:51:41.428Z",
                "updatedAt": "2023-09-07T08:51:41.428Z",
                "content": "<p>A healthcare service wants to exchange patient data securely with a partner organization through an HTTP API endpoint provided by the partner. The healthcare service has the requisite API key for accessing the HTTP API. The service needs a solution to manage the API key through code.</p><p>Which method will fulfill these requirements with maximum security?</p>",
                "answerExplanation": "<p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve API keys, database credentials, and other secrets throughout their lifecycle, making it the most secure option.</p><p><strong>CORRECT: </strong>\"Use AWS Secrets Manager to store and retrieve the API key\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the API key in the application code\" is incorrect.</p><p>Storing the API key directly in the application code is a security risk and could lead to unintended exposure of the key.</p><p><strong>INCORRECT:</strong> \"Use Amazon DynamoDB to store the API key and retrieve it when needed\" is incorrect.</p><p>While Amazon DynamoDB could be used to store the API key, it does not provide the same level of security management and key rotation capabilities that AWS Secrets Manager offers.</p><p><strong>INCORRECT:</strong> \"Store the API key in an environment variable on the application server\" is incorrect.</p><p>Storing the API key in an environment variable on the application server could lead to unintentional exposure of the key if the environment variables were logged or leaked in some way.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-secrets-manager/\">https://digitalcloud.training/aws-secrets-manager/</a></p>",
                "options": [
                    {
                        "id": 3963,
                        "content": "<p>Use AWS Secrets Manager to store and retrieve the API key.</p>",
                        "isValid": true
                    },
                    {
                        "id": 3964,
                        "content": "<p>Store the API key in an environment variable on the application server.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3965,
                        "content": "<p>Use Amazon DynamoDB to store the API key and retrieve it when needed.</p>",
                        "isValid": false
                    },
                    {
                        "id": 3966,
                        "content": "<p>Store the API key in the application code.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 965,
            "attributes": {
                "createdAt": "2023-09-07T08:51:41.496Z",
                "updatedAt": "2023-09-07T08:51:41.496Z",
                "content": "<p>A Developer needs to return a list of items in a global secondary index from an Amazon DynamoDB table.</p><p>Which DynamoDB API call can the Developer use in order to consume the LEAST number of read capacity units?</p>",
                "answerExplanation": "<p>The Query operation finds items based on primary key values. You can query any table or secondary index that has a composite primary key (a partition key and a sort key).</p><p>For items up to 4 KB in size, one RCU equals one strongly consistent read request per second or two eventually consistent read requests per second. Therefore, using eventually consistent reads uses fewer RCUs.</p><p><strong>CORRECT: </strong>\"Query operation using eventually-consistent reads\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Query operation using strongly-consistent reads\" is incorrect as strongly-consistent reads use more RCUs than eventually consistent reads.</p><p><strong>INCORRECT:</strong> \"Scan operation using eventually-consistent reads\" is incorrect. The Scan operation returns one or more items and item attributes by accessing every item in a table or a secondary index and therefore uses more RCUs than a query operation.</p><p><strong>INCORRECT:</strong> \"Scan operation using strongly-consistent reads\" is incorrect. The Scan operation returns one or more items and item attributes by accessing every item in a table or a secondary index and therefore uses more RCUs than a query operation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html\">https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 3967,
                        "content": "<p>Scan operation using eventually-consistent reads</p>",
                        "isValid": false
                    },
                    {
                        "id": 3968,
                        "content": "<p>Query operation using eventually-consistent reads</p>",
                        "isValid": true
                    },
                    {
                        "id": 3969,
                        "content": "<p>Scan operation using strongly-consistent reads</p>",
                        "isValid": false
                    },
                    {
                        "id": 3970,
                        "content": "<p>Query operation using strongly-consistent reads</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 966,
            "attributes": {
                "createdAt": "2023-09-07T08:51:41.574Z",
                "updatedAt": "2023-09-07T08:51:41.574Z",
                "content": "<p>Data must be loaded into an application each week for analysis. The data is uploaded to an Amazon S3 bucket from several offices around the world. Latency is slowing the uploads and delaying the analytics job. What is the SIMPLEST way to improve upload times?</p>",
                "answerExplanation": "<p>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFrontâ€™s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</p><p> You might want to use Transfer Acceleration on a bucket for various reasons, including the following:</p><p> â€¢ You have customers that upload to a centralized bucket from all over the world.</p><p> â€¢ You transfer gigabytes to terabytes of data on a regular basis across continents.</p><p> â€¢ You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.</p><p><strong>CORRECT: </strong>\"Upload using Amazon S3 Transfer Acceleration\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Upload to a local Amazon S3 bucket within each region and enable Cross-Region Replication (CRR)\" is incorrect as this would not speed up the upload as the process introduces more latency.</p><p><strong>INCORRECT:</strong> \"Upload via a managed AWS VPN connection\" is incorrect as this still uses the public Internet and thereâ€™s no real latency advantages here.</p><p><strong>INCORRECT:</strong> \"Upload to Amazon CloudFront and then download from the local cache to the S3 bucket\" is incorrect. This is going to require some time to propagate to the cache and requires some manual work in retrieving the data. The simplest solution is to use S3 Transfer Acceleration which basically does this for you.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 3971,
                        "content": "<p>Upload to a local Amazon S3 bucket within each region and enable Cross-Region Replication (CRR)</p>",
                        "isValid": false
                    },
                    {
                        "id": 3972,
                        "content": "<p>Upload to Amazon CloudFront and then download from the local cache to the S3 bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 3973,
                        "content": "<p>Upload using Amazon S3 Transfer Acceleration</p>",
                        "isValid": true
                    },
                    {
                        "id": 3974,
                        "content": "<p>Upload via a managed AWS VPN connection</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 967,
            "attributes": {
                "createdAt": "2023-09-07T08:51:41.645Z",
                "updatedAt": "2023-09-07T08:51:41.645Z",
                "content": "<p>A Developer needs to restrict all users and roles from using a list of API actions within a member account in AWS Organizations. The Developer needs to deny access to a few specific API actions.</p><p>What is the MOST efficient way to do this?</p>",
                "answerExplanation": "<p>Service control policies (SCPs) are one type of policy that you can use to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organizationâ€™s access control guidelines.</p><p>You can configure the SCPs in your organization to work as either of the following:</p><p> â€¢ A <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html#orgs_policies_denylist\">deny list</a> â€“ actions are allowed by default, and you specify what services and actions are prohibited</p><p> â€¢ An <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html#orgs_policies_allowlist\">allow list</a> â€“ actions are prohibited by default, and you specify what services and actions are allowed</p><p>As there are only a few API actions to restrict the most efficient strategy for this scenario is to create a deny list and specify the specific actions that are prohibited.</p><p><strong>CORRECT: </strong>\"Create a deny list and specify the API actions to deny\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an allow list and specify the API actions to deny\" is incorrect as with an allow list you specify the API actions to allow.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy that denies the API actions for all users and roles\" is incorrect as you cannot create deny policies in IAM. IAM policies implicitly deny access unless you explicitly allow permissions.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy that allows only the unrestricted API actions\" is incorrect. This will not work for administrative users such as the root account (as they have extra permissions) so an SCP must be used.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html\">https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html</a></p>",
                "options": [
                    {
                        "id": 3975,
                        "content": "<p>Create a deny list and specify the API actions to deny</p>",
                        "isValid": true
                    },
                    {
                        "id": 3976,
                        "content": "<p>Create an IAM policy that denies the API actions for all users and roles</p>",
                        "isValid": false
                    },
                    {
                        "id": 3977,
                        "content": "<p>Create an allow list and specify the API actions to deny</p>",
                        "isValid": false
                    },
                    {
                        "id": 3978,
                        "content": "<p>Create an IAM policy that allows only the unrestricted API actions</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 968,
            "attributes": {
                "createdAt": "2023-09-07T08:51:41.742Z",
                "updatedAt": "2023-09-07T08:51:41.742Z",
                "content": "<p>A Developer is creating an AWS Lambda function that generates a new file each time it runs. Each new file must be checked into an AWS CodeCommit repository hosted in the same AWS account.</p><p>How should the Developer accomplish this?</p>",
                "answerExplanation": "<p>The Developer can use the AWS SDK to instantiate a CodeCommit client. For instance, the code might include the following:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-34-35-a431eb0e6bb8bb4717d5ccd60120a8a5.jpg\"></p><p>The client can then be used with put_file which adds or updates a file in a branch in an AWS CodeCommit repository, and generates a commit for the addition in the specified branch.</p><p>The request syntax is as follows:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-34-48-59f1d0be8e99488c77f804470c7d2412.jpg\"></p><p><strong>CORRECT: </strong>\"Use an AWS SDK to instantiate a CodeCommit client. Invoke the put_file method to add the file to the repository\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"When the Lambda function starts, use the Git CLI to clone the repository. Check the new file into the cloned repository and push the change\" is incorrect as there is no need to clone a repository, a file just needs to be added to an existing repository.</p><p><strong>INCORRECT:</strong> \"After the new file is created in Lambda, use cURL to invoke the CodeCommit API. Send the file to the repository\" is incorrect as a URL cannot be used to invoke a CodeCommit client and upload and check in the file.</p><p><strong>INCORRECT:</strong> \"Upload the new file to an Amazon S3 bucket. Create an AWS Step Function to accept S3 events. In the Step Function, add the new file to the repository\" is incorrect as Step Functions is not triggered by S3 events.</p><p><strong>References:</strong></p><p><a href=\"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/codecommit.html\">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/codecommit.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3979,
                        "content": "<p>Use an AWS SDK to instantiate a CodeCommit client. Invoke the put_file method to add the file to the repository</p>",
                        "isValid": true
                    },
                    {
                        "id": 3980,
                        "content": "<p>Upload the new file to an Amazon S3 bucket. Create an AWS Step Function to accept S3 events. In the Step Function, add the new file to the repository</p>",
                        "isValid": false
                    },
                    {
                        "id": 3981,
                        "content": "<p>After the new file is created in Lambda, use cURL to invoke the CodeCommit API. Send the file to the repository</p>",
                        "isValid": false
                    },
                    {
                        "id": 3982,
                        "content": "<p>When the Lambda function starts, use the Git CLI to clone the repository. Check the new file into the cloned repository and push the change</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 969,
            "attributes": {
                "createdAt": "2023-09-07T08:51:41.824Z",
                "updatedAt": "2023-09-07T08:51:41.824Z",
                "content": "<p>A company needs a fully-managed source control service that will work in AWS. The service must ensure that revision control synchronizes multiple distributed repositories by exchanging sets of changes peer-to-peer. All users need to work productively even when not connected to a network.</p><p>Which source control service should be used?</p>",
                "answerExplanation": "<p>AWS CodeCommit is a version control service hosted by Amazon Web Services that you can use to privately store and manage assets (such as documents, source code, and binary files) in the cloud.</p><p>A repository is the fundamental version control object in CodeCommit. It's where you securely store code and files for your project. It also stores your project history, from the first commit through the latest changes. You can share your repository with other users so you can work together on a project. If you add AWS tags to repositories, you can set up notifications so that repository users receive email about events (for example, another user commenting on code).</p><p>You can also change the default settings for your repository, browse its contents, and more. You can create triggers for your repository so that code pushes or other events trigger actions, such as emails or code functions. You can even configure a repository on your local computer (a local repo) to push your changes to more than one repository.</p><p><strong>CORRECT: </strong>\"AWS CodeCommit\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Subversion\" is incorrect as this is not a fully managed source control system</p><p><strong>INCORRECT:</strong> \"AWS CodeBuild\" is incorrect as this is a service used for building and testing code.</p><p><strong>INCORRECT:</strong> \"AWS CodeStar\" is incorrect as this is not a source control system; it integrates with source control systems such as CodeCommit.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/codecommit/\">https://aws.amazon.com/codecommit/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 3983,
                        "content": "<p>AWS CodeCommit</p>",
                        "isValid": true
                    },
                    {
                        "id": 3984,
                        "content": "<p>AWS CodeStar</p>",
                        "isValid": false
                    },
                    {
                        "id": 3985,
                        "content": "<p>Subversion</p>",
                        "isValid": false
                    },
                    {
                        "id": 3986,
                        "content": "<p>AWS CodeBuild</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 970,
            "attributes": {
                "createdAt": "2023-09-07T08:51:41.896Z",
                "updatedAt": "2023-09-07T08:51:41.896Z",
                "content": "<p>A Developer needs to setup a new serverless application that includes AWS Lambda and Amazon API Gateway as part of a single stack. The Developer needs to be able to locally build and test the serverless applications before deployment on AWS.</p><p>Which service should the Developer use?</p>",
                "answerExplanation": "<p>The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build <a href=\"https://aws.amazon.com/serverless/\">serverless applications</a> on AWS. A <strong>serverless application</strong> is a combination of Lambda functions, event sources, and other resources that work together to perform tasks.</p><p>AWS SAM provides you with a simple and clean syntax to describe the functions, APIs, permissions, configurations, and events that make up a serverless application.</p><p>The example AWS SAM template file below creates an AWS Lambda function and a simple Amazon API Gateway API with a Get method and a /greeting resource:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-08-03-6858777cb267b07ce091346528168780.jpg\"></p><p>The AWS SAM CLI lets you locally build, test, and debug serverless applications that are defined by AWS SAM templates. The CLI provides a Lambda-like execution environment locally. It helps you catch issues upfront by providing parity with the actual Lambda execution environment.</p><p><strong>CORRECT: </strong>\"AWS Serverless Application Model (SAM)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CloudFormation\" is incorrect as you cannot perform local build and test with AWS CloudFormation.</p><p><strong>INCORRECT:</strong> \"AWS Elastic Beanstalk\" is incorrect as you cannot deploy serverless applications or perform local build and test with Elastic Beanstalk.</p><p><strong>INCORRECT:</strong> \"AWS CodeBuild\" is incorrect as you cannot perform local build and test with AWS CodeBuild.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-sam/\">https://digitalcloud.training/aws-sam/</a></p>",
                "options": [
                    {
                        "id": 3987,
                        "content": "<p>AWS CodeBuild</p>",
                        "isValid": false
                    },
                    {
                        "id": 3988,
                        "content": "<p>AWS Serverless Application Model (SAM)</p>",
                        "isValid": true
                    },
                    {
                        "id": 3989,
                        "content": "<p>AWS Elastic Beanstalk</p>",
                        "isValid": false
                    },
                    {
                        "id": 3990,
                        "content": "<p>AWS CloudFormation</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 971,
            "attributes": {
                "createdAt": "2023-09-07T08:51:41.971Z",
                "updatedAt": "2023-09-07T08:51:41.971Z",
                "content": "<p>A Developer has created an AWS Lambda function in a new AWS account. The function is expected to be invoked 40 times per second and the execution duration will be around 100 seconds. What MUST the Developer do to ensure there are no errors?</p>",
                "answerExplanation": "<p>Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.</p><p>In this scenario the Lambda function will be invoked 40 times per second and run for 100 seconds. Therefore, there can be up to 4,000 executions running concurrently which is above the default per-region limit of 1,000 concurrent executions.</p><p>This can be easily rectified by contacting AWS support and requesting the concurrent execution limit to be increased.</p><p><strong>CORRECT: </strong>\"Contact AWS Support to increase the concurrent execution limits\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement error handling within the function code\" is incorrect. Though this could be useful it is not something that must be done based on what we know about this scenario.</p><p><strong>INCORRECT:</strong> \"Implement a Dead Letter Queue to capture invocation errors\" is incorrect as this would be implemented for message handling requirements.</p><p><strong>INCORRECT:</strong> \"Implement tracing with X-Ray\" is incorrect. X-Ray can be used to analyze and debug distributed applications. We donâ€™t know of any specific issues with this function yet so this is not something that must be done.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</a></p>",
                "options": [
                    {
                        "id": 3991,
                        "content": "<p>Implement a Dead Letter Queue to capture invocation errors</p>",
                        "isValid": false
                    },
                    {
                        "id": 3992,
                        "content": "<p>Implement tracing with X-Ray</p>",
                        "isValid": false
                    },
                    {
                        "id": 3993,
                        "content": "<p>Contact AWS Support to increase the concurrent execution limits</p>",
                        "isValid": true
                    },
                    {
                        "id": 3994,
                        "content": "<p>Implement error handling within the function code</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 972,
            "attributes": {
                "createdAt": "2023-09-07T08:51:42.045Z",
                "updatedAt": "2023-09-07T08:51:42.045Z",
                "content": "<p>A developer has a user account in the Development AWS account. He has been asked to modify resources in a Production AWS account. What is the MOST secure way to provide temporary access to the developer?</p>",
                "answerExplanation": "<p>This should be implemented using a role in the Production account and a group in the Development account. The developer in the Development account would then be added to the group. The role in the Production account would provide the necessary access and would allow the group in the Development account to assume the role.</p><p>The following image depicts this setup:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-06-51-9ce802945c148c8c43663f25eec64296.jpg\"></p><p>Therefore, the most secure way to achieve the required access is to use a role in the Production account that the user is able to assume and then the user can request short-lived credentials from the Security Token Service (STS).</p><p><strong>CORRECT: </strong>\"Create a cross-account access role, and use sts:AssumeRole API to get short-lived credentials\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Generate an access key on the second account using the root account and share the access keys with the developer for API access\" is incorrect as this is highly insecure. You should never share access keys across user accounts, and you should especially not use access keys associated with the root account.</p><p><strong>INCORRECT:</strong> \"Add the user to a group in the second account that has a role attached granting the necessary permissions\" is incorrect as you cannot add a user to a group in a different AWS account.</p><p><strong>INCORRECT:</strong> \"Use AWS KMS to generate cross-account customer master keys and use those get short-lived credentials\" is incorrect as you do not use AWS KMS CMKs for obtaining short-lived credentials from the STS service. CMKs are used for encrypting data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 3995,
                        "content": "<p>Use AWS KMS to generate cross-account customer master keys and use those get short-lived credentials</p>",
                        "isValid": false
                    },
                    {
                        "id": 3996,
                        "content": "<p>Add the user to a group in the second account that has a role attached granting the necessary permissions</p>",
                        "isValid": false
                    },
                    {
                        "id": 3997,
                        "content": "<p>Create a cross-account access role, and use sts:AssumeRole API to get short-lived credentials</p>",
                        "isValid": true
                    },
                    {
                        "id": 3998,
                        "content": "<p>Generate an access key on the second account using the root account and share the access keys with the developer for API access</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 973,
            "attributes": {
                "createdAt": "2023-09-07T08:51:42.118Z",
                "updatedAt": "2023-09-07T08:51:42.118Z",
                "content": "<p>Every time an Amazon EC2 instance is launched, certain metadata about the instance should be recorded in an Amazon DynamoDB table. The data is gathered and written to the table by an AWS Lambda function.</p><p>What is the MOST efficient method of invoking the Lambda function?</p>",
                "answerExplanation": "<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. CloudWatch Events becomes aware of operational changes as they occur. CloudWatch Events responds to these operational changes and takes corrective action as necessary, by sending messages to respond to the environment, activating functions, making changes, and capturing state information.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-05-05-4d71f304fa9f2b4d827c5a87a7108193.jpg\"></p><p>In this scenario the only workable solution is to create a CloudWatch Event with an event pattern looking for EC2 state changes and a target set to use the Lambda function.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Event with an event pattern looking for EC2 state changes and a target set to use the Lambda function\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that triggers the Lambda function based on log streams indicating an EC2 state change in CloudWatch logs\" is incorrect as Amazon EC2 does not create a log group or log stream by default.</p><p><strong>INCORRECT:</strong> \"Create a CloudTrail trail alarm that triggers the Lambda function based on the RunInstances API action\" is incorrect as you would need to create a CloudWatch alarm for CloudTrail events (CloudTrail does not have its own alarm feature).</p><p><strong>INCORRECT:</strong> \"Configure detailed monitoring on Amazon EC2 and create an alarm that triggers the Lambda function in initialization\" is incorrect as you cannot trigger a Lambda function on EC2 instances initialization using detailed monitoring (or the EC2 console).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 3999,
                        "content": "<p>Create a CloudWatch alarm that triggers the Lambda function based on log streams indicating an EC2 state change in CloudWatch logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 4000,
                        "content": "<p>Create a CloudWatch Event with an event pattern looking for EC2 state changes and a target set to use the Lambda function</p>",
                        "isValid": true
                    },
                    {
                        "id": 4001,
                        "content": "<p>Configure detailed monitoring on Amazon EC2 and create an alarm that triggers the Lambda function in initialization</p>",
                        "isValid": false
                    },
                    {
                        "id": 4002,
                        "content": "<p>Create a CloudTrail trail alarm that triggers the Lambda function based on the <code>RunInstances</code> API action</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 974,
            "attributes": {
                "createdAt": "2023-09-07T08:51:42.188Z",
                "updatedAt": "2023-09-07T08:51:42.188Z",
                "content": "<p>A Developer created an AWS Lambda function for a serverless application. The Lambda function has been executing for several minutes and the Developer cannot find any log data in CloudWatch Logs.</p><p>What is the MOST likely explanation for this issue?</p>",
                "answerExplanation": "<p>AWS Lambda automatically monitors Lambda functions on your behalf, reporting metrics through Amazon CloudWatch. To help you troubleshoot failures in a function, Lambda logs all requests handled by your function and also automatically stores logs generated by your code through Amazon CloudWatch Logs.</p><p>Lambda automatically integrates with CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function, which is named /aws/lambda/<em>&lt;function name&gt;</em>.</p><p>An AWS Lambda function's execution role grants it permission to access AWS services and resources. You provide this role when you create a function, and Lambda assumes the role when your function is invoked. You can create an execution role for development that has permission to send logs to Amazon CloudWatch and upload trace data to AWS X-Ray.</p><p>For the lambda function to create log stream and publish logs to cloudwatch, the lambda execution role needs to have the following permissions:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-55-25-441990764d87d14c5376625017fb3928.jpg\"></p><p>The most likely cause of this issue is that the execution role assigned to the Lambda function does not have the permissions (shown above) to write to CloudWatch Logs.</p><p><strong>CORRECT: </strong>\"The execution role for the Lambda function is missing permissions to write log data to the CloudWatch Logs\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The Lambda function does not have any explicit log statements for the log data to send it to CloudWatch Logs\" is incorrect as this is not required, Lambda automatically logs data to CloudWatch logs and just needs the permissions to do so.</p><p><strong>INCORRECT:</strong> \"The Lambda function is missing a target CloudWatch Logs group\" is incorrect as the CloudWatch Logs group will be created automatically if the function has sufficient permissions.</p><p><strong>INCORRECT:</strong> \"The Lambda function is missing CloudWatch Logs as a source trigger to send log data\" is incorrect as CloudWatch Logs is a destination, not a source in this case. However, you do not need to configure CloudWatch Logs as a destination, it is automatic.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-monitoring.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-monitoring.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 4003,
                        "content": "<p>The Lambda function does not have any explicit log statements for the log data to send it to CloudWatch Logs</p>",
                        "isValid": false
                    },
                    {
                        "id": 4004,
                        "content": "<p>The Lambda function is missing CloudWatch Logs as a source trigger to send log data</p>",
                        "isValid": false
                    },
                    {
                        "id": 4005,
                        "content": "<p>The Lambda function is missing a target CloudWatch Logs group</p>",
                        "isValid": false
                    },
                    {
                        "id": 4006,
                        "content": "<p>The execution role for the Lambda function is missing permissions to write log data to the CloudWatch Logs</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 975,
            "attributes": {
                "createdAt": "2023-09-07T08:51:42.262Z",
                "updatedAt": "2023-09-07T08:51:42.262Z",
                "content": "<p>An organization is developing a data processing application that is hosted on AWS Lambda and utilizes a PostgreSQL database on Amazon RDS. The security team mandates a policy that requires rotating database credentials every week.</p><p>What strategy should the developer adopt to manage the database credentials for the application?</p>",
                "answerExplanation": "<p>AWS Secrets Manager enables the creation, retrieval, and rotation of secrets like database credentials. Configuring automatic rotation weekly satisfies the security team's requirement, with the least manual effort.</p><p><strong>CORRECT: </strong>\"Deploy AWS Secrets Manager to store database credentials and set up automatic weekly rotation\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store database credentials in Amazon S3 buckets with versioning enabled, rotating and uploading new credentials weekly\" is incorrect.</p><p>Storing credentials in Amazon S3 is not a secure practice as it does not have built-in mechanisms for rotating secrets.</p><p><strong>INCORRECT:</strong> \"Enable IAM database authentication and manage weekly rotation manually\" is incorrect.</p><p>IAM database authentication allows you to use IAM users/roles to access your database, but it does not have an automatic mechanism for weekly rotation of IAM credentials.</p><p><strong>INCORRECT:</strong> \"Store the credentials in environment variables of the Lambda function and manually update them every week\" is incorrect.</p><p>Storing credentials as environment variables in AWS Lambda functions could expose them to potential security risks, and manual rotation weekly would be labor-intensive.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_managed.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_managed.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-secrets-manager/\">https://digitalcloud.training/aws-secrets-manager/</a></p>",
                "options": [
                    {
                        "id": 4007,
                        "content": "<p>Store the credentials in environment variables of the Lambda function and manually update them every week.</p>",
                        "isValid": false
                    },
                    {
                        "id": 4008,
                        "content": "<p>Enable IAM database authentication and manage weekly rotation manually.</p>",
                        "isValid": false
                    },
                    {
                        "id": 4009,
                        "content": "<p>Deploy AWS Secrets Manager to store database credentials and set up automatic weekly rotation.</p>",
                        "isValid": true
                    },
                    {
                        "id": 4010,
                        "content": "<p>Store database credentials in Amazon S3 buckets with versioning enabled, rotating, and uploading new credentials weekly.</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 976,
            "attributes": {
                "createdAt": "2023-09-07T08:51:42.332Z",
                "updatedAt": "2023-09-07T08:51:42.332Z",
                "content": "<p>A Developer is migrating Docker containers to Amazon ECS. A large number of containers will be deployed across some newly deployed ECS containers instances using the same instance type. High availability is provided within the microservices architecture. Which task placement strategy requires the LEAST configuration for this scenario?</p>",
                "answerExplanation": "<p>When a task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified in the task definition, such as CPU and memory. Similarly, when you scale down the task count, Amazon ECS must determine which tasks to terminate. You can apply task placement strategies and constraints to customize how Amazon ECS places and terminates tasks. Task placement strategies and constraints are not supported for tasks using the Fargate launch type. By default, Fargate tasks are spread across Availability Zones.</p><p>A <em>task placement strategy</em> is an algorithm for selecting instances for task placement or tasks for termination. For example, Amazon ECS can select instances at random, or it can select instances such that tasks are distributed evenly across a group of instances.</p><p>Amazon ECS supports the following task placement strategies:</p><p> â€¢ binpack</p><p>Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.</p><p> â€¢ random</p><p>Place tasks randomly.</p><p> â€¢ spread</p><p>Place tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone. Service tasks are spread based on the tasks from that service. Standalone tasks are spread based on the tasks from the same task group.</p><p>Therefore, for this scenario the random task placement strategy is most suitable as it requires the least configuration.</p><p><strong>CORRECT: </strong>\"random\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"spread\" is incorrect. As high availability is taken care of within the containers there is no need to use a spread strategy to ensure HA.</p><p><strong>INCORRECT:</strong> \"binpack\" is incorrect as there is no need to pack the containers onto the fewest instances based on CPU or memory.</p><p><strong>INCORRECT:</strong> \"Fargate\" is incorrect as this is not a task placement strategy, it is a serverless service for running containers.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 4011,
                        "content": "<p>binpack</p>",
                        "isValid": false
                    },
                    {
                        "id": 4012,
                        "content": "<p>Fargate</p>",
                        "isValid": false
                    },
                    {
                        "id": 4013,
                        "content": "<p>spread</p>",
                        "isValid": false
                    },
                    {
                        "id": 4014,
                        "content": "<p>random</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 977,
            "attributes": {
                "createdAt": "2023-09-07T08:51:42.406Z",
                "updatedAt": "2023-09-07T08:51:42.406Z",
                "content": "<p>An application uses Amazon Kinesis Data Streams to ingest and process large streams of data records in real time. Amazon EC2 instances consume and process the data using the Amazon Kinesis Client Library (KCL). The application handles the failure scenarios and does not require standby workers. The application reports that a specific shard is receiving more data than expected. To adapt to the changes in the rate of data flow, the â€œhotâ€ shard is resharded.</p><p>Assuming that the initial number of shards in the Kinesis data stream is 6, and after resharding the number of shards increased to 8, what is the maximum number of EC2 instances that can be deployed to process data from all the shards?</p>",
                "answerExplanation": "<p>Typically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-32-49-7f35405f45902946e8dbb0e5eefca485.png\"></p><p>In this scenario, the number of shards has been increased to 8. Therefore, the maximum number of instances that can be deployed is 8 as the number of instances cannot exceed the number of shards.</p><p><strong>CORRECT: </strong>\"8\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"6\" is incorrect as this is not the maximum number of instances that can be deployed to process 8 shards. The maximum number of instances should be the same as the number of shards.</p><p><strong>INCORRECT:</strong> \"12\" is incorrect as the number of instances exceeds the number of shards. You should ensure that the number of instances does not exceed the number of shards</p><p><strong>INCORRECT:</strong> \"1\" is incorrect as this is not the maximum number of instances that can be deployed to process 8 shards. The maximum number of instances should be the same as the number of shards.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 4015,
                        "content": "<p>6</p>",
                        "isValid": false
                    },
                    {
                        "id": 4016,
                        "content": "<p>12</p>",
                        "isValid": false
                    },
                    {
                        "id": 4017,
                        "content": "<p>8</p>",
                        "isValid": true
                    },
                    {
                        "id": 4018,
                        "content": "<p>1</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 978,
            "attributes": {
                "createdAt": "2023-09-07T08:51:42.484Z",
                "updatedAt": "2023-09-07T08:51:42.484Z",
                "content": "<p>An application includes multiple Auto Scaling groups of Amazon EC2 instances. Each group corresponds to a different subdomain of example.com, including forum.example.com and myaccount.example.com. An Elastic Load Balancer will be used to distribute load from a single HTTPS listener.</p><p>Which type of Elastic Load Balancer MUST a Developer use in this scenario?</p>",
                "answerExplanation": "<p>With an Application Load Balancer it is possible to route requests based on the domain name specified in the Host header. This means you can route traffic coming in to forum.example.com and myaccount.example.com to different target groups.</p><p>You can see an example of a couple of similar rules depicted below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-01-08-897ff008a45657d317ed7ecfd675cb4a.jpg\"></p><p>The Application Load Balancer is the only Elastic Load Balancer provided by AWS that can perform host-based routing.</p><p><strong>CORRECT: </strong>\"Application Load Balancer\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Network Load Balancer\" is incorrect as this type of ELB routes traffic based on information at the connection layer (L4).</p><p><strong>INCORRECT:</strong> \"Classic Load Balancer\" is incorrect as it doesn't support any kind of host or path-based routing or even target groups.</p><p><strong>INCORRECT:</strong> \"Task Load Balancer\" is incorrect as this is not a type of ELB.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/\">https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p><p><br></p>",
                "options": [
                    {
                        "id": 4019,
                        "content": "<p>Classic Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 4020,
                        "content": "<p>Application Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 4021,
                        "content": "<p>Task Load Balancer</p>",
                        "isValid": false
                    },
                    {
                        "id": 4022,
                        "content": "<p>Network Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 979,
            "attributes": {
                "createdAt": "2023-09-07T08:51:42.551Z",
                "updatedAt": "2023-09-07T08:51:42.551Z",
                "content": "<p>A Developer is creating an application that uses Amazon EC2 instances and must be highly available and fault tolerant. How should the Developer configure the VPC?</p>",
                "answerExplanation": "<p>To ensure high availability and fault tolerance the Developer should create a subnet within each availability zone. The EC2 instances should then be distributed between these subnets.</p><p>The Developer would likely use Amazon EC2 Auto Scaling which will automatically launch instances in each subnet and then Elastic Load Balancing to distributed incoming traffic.</p><p><strong>CORRECT: </strong>\"Create a subnet in each availability zone in the region\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create multiple subnets within a single availability zone in the region\" is incorrect as this will not provide fault tolerance in the event that the AZ becomes unavailable.</p><p><strong>INCORRECT:</strong> \"Create an Internet Gateway for every availability zone\" is incorrect as there is a single Internet Gateway per VPC.</p><p><strong>INCORRECT:</strong> \"Create a cluster placement group for the EC2 instances\" is incorrect as this is used for ensuring low latency access between EC2 instances in a single availability zone.</p><p><strong>References:</strong></p><p><a href=\"https://d1.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf\">https://d1.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
                "options": [
                    {
                        "id": 4023,
                        "content": "<p>Create a subnet in each availability zone in the region</p>",
                        "isValid": true
                    },
                    {
                        "id": 4024,
                        "content": "<p>Create an Internet Gateway for every availability zone</p>",
                        "isValid": false
                    },
                    {
                        "id": 4025,
                        "content": "<p>Create multiple subnets within a single availability zone in the region</p>",
                        "isValid": false
                    },
                    {
                        "id": 4026,
                        "content": "<p>Create a cluster placement group for the EC2 instances</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 980,
            "attributes": {
                "createdAt": "2023-09-07T08:51:42.624Z",
                "updatedAt": "2023-09-07T08:51:42.624Z",
                "content": "<p>A Development team are creating a financial trading application. The application requires sub-millisecond latency for processing trading requests. Amazon DynamoDB is used to store the trading data. During load testing the Development team found that in periods of high utilization the latency is too high and read capacity must be significantly over-provisioned to avoid throttling.</p><p>How can the Developers meet the latency requirements of the application?</p>",
                "answerExplanation": "<p>Amazon DynamoDB is designed for scale and performance. In most cases, the DynamoDB response times can be measured in single-digit milliseconds. However, there are certain use cases that require response times in microseconds. For these use cases, DynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data.</p><p>DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. DAX addresses three core scenarios:</p><p>1. As an in-memory cache, DAX reduces the response times of eventually consistent read workloads by an order of magnitude from single-digit milliseconds to microseconds.</p><p>2. DAX reduces operational and application complexity by providing a managed service that is API-compatible with DynamoDB. Therefore, it requires only minimal functional changes to use with an existing application.</p><p>3. For read-heavy or bursty workloads, DAX provides increased throughput and potential operational cost savings by reducing the need to overprovision read capacity units. This is especially beneficial for applications that require repeated reads for individual keys.</p><p>In this scenario the question is calling for sub-millisecond (e.g. microsecond) latency and this is required for read traffic as evidenced by the need to over-provision reads. Therefore, DynamoDB DAX would be the best solution for reducing the latency and meeting the requirements.</p><p><strong>CORRECT: </strong>\"Use Amazon DynamoDB Accelerator (DAX) to cache the data\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a Global Secondary Index (GSI) for the trading data\" is incorrect as a GSI is used to speed up queries on non-key attributes. There is no requirement here for a Global Secondary Index.</p><p><strong>INCORRECT:</strong> \"Use exponential backoff in the application code for DynamoDB queries\" is incorrect as this may reduce the requirement for over-provisioning reads but it will not solve the problem of reducing latency. With this solution the application performance will be worse, itâ€™s a case of reducing cost along with performance.</p><p><strong>INCORRECT:</strong> \"Store the trading data in Amazon S3 and use Transfer Acceleration\" is incorrect as this will not reduce the latency of the application. Transfer Acceleration is used for improving performance of uploads of data to Amazon S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 4027,
                        "content": "<p>Create a Global Secondary Index (GSI) for the trading data</p>",
                        "isValid": false
                    },
                    {
                        "id": 4028,
                        "content": "<p>Use exponential backoff in the application code for DynamoDB queries</p>",
                        "isValid": false
                    },
                    {
                        "id": 4029,
                        "content": "<p>Use Amazon DynamoDB Accelerator (DAX) to cache the data</p>",
                        "isValid": true
                    },
                    {
                        "id": 4030,
                        "content": "<p>Store the trading data in Amazon S3 and use Transfer Acceleration</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 981,
            "attributes": {
                "createdAt": "2023-09-07T08:51:42.698Z",
                "updatedAt": "2023-09-07T08:51:42.698Z",
                "content": "<p>An application resizes images that are uploaded to an Amazon S3 bucket. Amazon S3 event notifications are used to trigger an AWS Lambda function that resizes the images. The processing time for each image is less than one second. A large amount of images are expected to be received in a short burst of traffic. How will AWS Lambda accommodate the workload?</p>",
                "answerExplanation": "<p>The first time you invoke your function, AWS Lambda creates an instance of the function and runs its handler method to process the event. When the function returns a response, it stays active and waits to process additional events. If you invoke the function again while the first event is being processed, Lambda initializes another instance, and the function processes the two events concurrently.</p><p>Your functionsâ€™ concurrency is the number of instances that serve requests at a given time. For an initial burst of traffic, your functionsâ€™ cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region.</p><p>Burst Concurrency Limits:</p><p> â€¢ 3000 â€“ US West (Oregon), US East (N. Virginia), Europe (Ireland).</p><p> â€¢ 1000 â€“ Asia Pacific (Tokyo), Europe (Frankfurt).</p><p> â€¢ 500 â€“ Other Regions.</p><p>After the initial burst, your functionsâ€™ concurrency can scale by an additional 500 instances each minute. This continues until there are enough instances to serve all requests, or until a concurrency limit is reached.</p><p>The default account limit is up to 1000 executions per second, per region (can be increased).</p><p><strong>CORRECT: </strong>\"Lambda will scale out and execute the requests concurrently\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Lambda will process the images sequentially in the order they are received\" is incorrect as Lambda uses concurrency to process multiple events in parallel.</p><p><strong>INCORRECT:</strong> \"Lambda will collect and then batch process the images in a single execution\" is incorrect as Lambda never collects requests and then processes them at a later time. Lambda always uses concurrency to process requests in parallel.</p><p><strong>INCORRECT:</strong> \"Lambda will scale the memory allocated to the function to increase the amount of CPU available to process many images\" is incorrect as Lambda does not automatically scale memory/CPU and processes requests in parallel, not sequentially.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 4031,
                        "content": "<p>Lambda will scale out and execute the requests concurrently</p>",
                        "isValid": true
                    },
                    {
                        "id": 4032,
                        "content": "<p>Lambda will scale the memory allocated to the function to increase the amount of CPU available to process many images</p>",
                        "isValid": false
                    },
                    {
                        "id": 4033,
                        "content": "<p>Lambda will process the images sequentially in the order they are received</p>",
                        "isValid": false
                    },
                    {
                        "id": 4034,
                        "content": "<p>Lambda will collect and then batch process the images in a single execution</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 982,
            "attributes": {
                "createdAt": "2023-09-07T08:51:42.776Z",
                "updatedAt": "2023-09-07T08:51:42.776Z",
                "content": "<p>An Amazon ElastiCache cluster has been placed in front of a large Amazon RDS database. To reduce cost the ElastiCache cluster should only cache items that are actually requested. How should ElastiCache be optimized?</p>",
                "answerExplanation": "<p>There are two caching strategies available: Lazy Loading and Write-Through:</p><p><strong>Lazy Loading</strong></p><p>Loads the data into the cache only when necessary (if a cache miss occurs).</p><p>Lazy loading avoids filling up the cache with data that wonâ€™t be requested.</p><p>If requested data is in the cache, ElastiCache returns the data to the application.</p><p>If the data is not in the cache or has expired, ElastiCache returns a null.</p><p>The application then fetches the data from the database and writes the data received into the cache so that it is available for next time.</p><p>Data in the cache can become stale if Lazy Loading is implemented without other strategies (such as TTL).</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_04-46-26-30fbbfa27a4af490031d4d9ce393e988.png\"></p><p><strong>Write Through</strong></p><p>When using a write through strategy, the cache is updated whenever a new write or update is made to the underlying database.</p><p>Allows cache data to remain up-to-date.</p><p>Can add wait time to write operations in your application.</p><p>Without a TTL you can end up with a lot of cached data that is never read.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_04-46-58-48ed108c2321502cfb49b9078555c553.png\"></p><p><strong>CORRECT: </strong>\"Use a lazy loading caching strategy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a write-through caching strategy\" is incorrect as this will load all database items into the cache increasing cost.</p><p><strong>INCORRECT:</strong> \"Only cache database writes\" is incorrect as you cannot cache writes, only reads.</p><p><strong>INCORRECT:</strong> \"Enable a TTL on cached data\" is incorrect. This would help expire stale items but it is not a cache optimization strategy that will cache only items that are requested.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 4035,
                        "content": "<p>Use a lazy loading caching strategy</p>",
                        "isValid": true
                    },
                    {
                        "id": 4036,
                        "content": "<p>Use a write-through caching strategy</p>",
                        "isValid": false
                    },
                    {
                        "id": 4037,
                        "content": "<p>Enable a TTL on cached data</p>",
                        "isValid": false
                    },
                    {
                        "id": 4038,
                        "content": "<p>Only cache database writes</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 983,
            "attributes": {
                "createdAt": "2023-09-07T08:51:42.858Z",
                "updatedAt": "2023-09-07T08:51:42.858Z",
                "content": "<p>A serverless application uses an AWS Lambda function, Amazon API Gateway API and an Amazon DynamoDB table. The Lambda function executes 10 times per second and takes 3 seconds to complete each execution.</p><p>How many concurrent executions will the Lambda function require?</p>",
                "answerExplanation": "<p>Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.</p><p>To calculate the concurrency requirements for the Lambda function simply multiply the number of executions per second (10) by the time it takes to complete the execution (3).</p><p>Therefore, for this scenario the calculation is 10 x 3 = 30.</p><p><strong>CORRECT: </strong>\"30\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"10\" is incorrect. Please use the formula above to calculate concurrency requirements.</p><p><strong>INCORRECT:</strong> \"12\" is incorrect. Please use the formula above to calculate concurrency requirements.</p><p><strong>INCORRECT:</strong> \"3\" is incorrect. Please use the formula above to calculate concurrency requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 4039,
                        "content": "<p>3</p>",
                        "isValid": false
                    },
                    {
                        "id": 4040,
                        "content": "<p>10</p>",
                        "isValid": false
                    },
                    {
                        "id": 4041,
                        "content": "<p>30</p>",
                        "isValid": true
                    },
                    {
                        "id": 4042,
                        "content": "<p>12</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 984,
            "attributes": {
                "createdAt": "2023-09-07T08:51:42.930Z",
                "updatedAt": "2023-09-07T08:51:42.930Z",
                "content": "<p>A Developer has lost their access key ID and secret access key for programmatic access. What should the Developer do?</p>",
                "answerExplanation": "<p>Access keys consist of two parts:</p><p><strong>The access key identifier</strong>. This is not a secret, and can be seen in the IAM console wherever access keys are listed, such as on the user summary page.</p><p><strong>The secret access key</strong>. This is provided when you initially create the access key pair. Just like a password, it <strong><em>cannot be retrieved later</em></strong>. If you lost your secret access key, then you must create a new access key pair. If you already have the <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-limits.html#reference_iam-limits-entities\">maximum number of access keys</a>, you must delete an existing pair before you can create another.</p><p>Therefore, the Developer should disable and delete their access keys and generate a new set.</p><p><strong>CORRECT: </strong>\"Disable and delete the usersâ€™ access key and generate a new set\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Contact AWS support and request a password reset\" is incorrect as a user name and password are used for console access, not programmatic access.</p><p><strong>INCORRECT:</strong> \"Generate a new key pair from the EC2 management console\" is incorrect as a key pair is used for accessing EC2 instances, not for programmatic access to work with AWS services.</p><p><strong>INCORRECT:</strong> \"Reset the AWS account access keys\" is incorrect as these are the access keys associated with the root account rather than the usersâ€™ individual IAM account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_general.html#troubleshoot_general_access-keys\">https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_general.html#troubleshoot_general_access-keys</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 4043,
                        "content": "<p>Generate a new key pair from the EC2 management console</p>",
                        "isValid": false
                    },
                    {
                        "id": 4044,
                        "content": "<p>Contact AWS support and request a password reset</p>",
                        "isValid": false
                    },
                    {
                        "id": 4045,
                        "content": "<p>Disable and delete the user's access key and generate a new set</p>",
                        "isValid": true
                    },
                    {
                        "id": 4046,
                        "content": "<p>Reset the AWS account access keys</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 985,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.000Z",
                "updatedAt": "2023-09-07T08:51:43.000Z",
                "content": "<p>A mobile application runs as a serverless application on AWS. A Developer needs to create a push notification feature that sends periodic message to subscribers. How can the Developer send the notification from the application?</p>",
                "answerExplanation": "<p>With <a href=\"https://aws.amazon.com/sns/\">Amazon SNS</a>, you have the ability to send push notification messages directly to apps on mobile devices. Push notification messages sent to a mobile endpoint can appear in the mobile app as message alerts, badge updates, or even sound alerts.</p><p>You send push notification messages to both mobile devices and desktops using one of the following supported push notification services:</p><p> â€¢ Amazon Device Messaging (ADM)</p><p> â€¢ Apple Push Notification Service (APNs) for both iOS and Mac OS X</p><p> â€¢ Baidu Cloud Push (Baidu)</p><p> â€¢ Firebase Cloud Messaging (FCM)</p><p> â€¢ Microsoft Push Notification Service for Windows Phone (MPNS)</p><p> â€¢ Windows Push Notification Services (WNS)</p><p>To send a notification to an Amazon SNS subscriber, the application needs to send the notification to an Amazon SNS Topic. Amazon SNS will then send the notification to the relevant subscribers.</p><p><strong>CORRECT: </strong>\"Publish a notification to an Amazon SNS Topic\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Publish a message to an Amazon SQS Queue\" is incorrect as SQS is a message queue service, not a notification service.</p><p><strong>INCORRECT:</strong> \"Publish a notification to Amazon CloudWatch Events\" is incorrect as CloudWatch Events will not be able to send notifications to mobile app users.</p><p><strong>INCORRECT:</strong> \"Publish a message to an Amazon SWF Workflow\" is incorrect as SWF is a workflow orchestration service and it is not used for publishing messages to mobile app users.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-how-user-notifications-work.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-how-user-notifications-work.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 4047,
                        "content": "<p>Publish a notification to Amazon CloudWatch Events</p>",
                        "isValid": false
                    },
                    {
                        "id": 4048,
                        "content": "<p>Publish a message to an Amazon SQS Queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 4049,
                        "content": "<p>Publish a notification to an Amazon SNS Topic</p>",
                        "isValid": true
                    },
                    {
                        "id": 4050,
                        "content": "<p>Publish a message to an Amazon SWF Workflow</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 986,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.076Z",
                "updatedAt": "2023-09-07T08:51:43.076Z",
                "content": "<p>A Developer needs to update an Amazon ECS application that was deployed using AWS CodeDeploy. What file does the Developer need to update to push the change through CodeDeploy?</p>",
                "answerExplanation": "<p>In CodeDeploy, a revision contains a version of the source files CodeDeploy will deploy to your instances or scripts CodeDeploy will run on your instances. You plan the revision, add an AppSpec file to the revision, and then push the revision to Amazon S3 or GitHub. After you push the revision, you can deploy it.</p><p>For a deployment to an Amazon ECS compute platform:</p><p> â€¢ The AppSpec file specifies the Amazon ECS task definition used for the deployment, a container name and port mapping used to route traffic, and optional Lambda functions run after deployment lifecycle events.</p><p> â€¢ A revision is the same as an AppSpec file.</p><p> â€¢ An AppSpec file can be written using JSON or YAML.</p><p> â€¢ An AppSpec file can be saved as a text file or entered directly into a console when you create a deployment.</p><p><strong>Therefore, the appspec.yml file needs to be updated by the Developer.</strong></p><p><strong>CORRECT: </strong>\"appspec.yml\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"dockerrun.aws.json\" is incorrect. A Dockerrun.aws.json file describes how to deploy a remote Docker image as an Elastic Beanstalk application.</p><p><strong>INCORRECT:</strong> \"buildspec.yaml\" is incorrect. A <em>build spec</em> is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build using AWS CodeBuild.</p><p><strong>INCORRECT:</strong> \"ebextensions.config\" is incorrect. The .ebextensions folder in the source code for an Elastic Beanstalk application is used for .config files that configure the environment and customize resources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/application-revisions-appspec-file.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/application-revisions-appspec-file.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 4051,
                        "content": "<p>dockerrun.aws.json</p>",
                        "isValid": false
                    },
                    {
                        "id": 4052,
                        "content": "<p>ebextensions.config</p>",
                        "isValid": false
                    },
                    {
                        "id": 4053,
                        "content": "<p>appspec.yml</p>",
                        "isValid": true
                    },
                    {
                        "id": 4054,
                        "content": "<p>buildspec.yml</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 987,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.144Z",
                "updatedAt": "2023-09-07T08:51:43.144Z",
                "content": "<p>A Developer has created a task definition that includes the following JSON code:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"placementConstraints\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L1\"><span class=\"pun\">{</span></li><li class=\"L2\"><span class=\"str\">\"expression\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"attribute:ecs.instance-type =~ t2.*\"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"type\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"memberOf\"</span></li><li class=\"L4\"><span class=\"pun\">}</span></li><li class=\"L5\"><span class=\"pun\">]</span></li></ol></pre></div></div><p>What will be the effect for tasks using this task definition?</p>",
                "answerExplanation": "<p>A <em>task placement constraint</em> is a rule that is considered during task placement. Task placement constraints can be specified when either running a task or creating a new service.</p><p>The <code>memberOf</code> task placement constraint places tasks on container instances that satisfy an expression.</p><p>The <code>memberOf</code> task placement constraint can be specified with the following actions:</p><p> â€¢ Running a task</p><p> â€¢ Creating a new service</p><p> â€¢ Creating a new task definition</p><p> â€¢ Creating a new revision of an existing task definition</p><p>The example JSON code uses the <code>memberOf </code>constraint to place tasks on T2 instances. It can be specified with the following actions: <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CreateService.html\">CreateService</a>, <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_UpdateService.html\">UpdateService</a>, <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RegisterTaskDefinition.html\">RegisterTaskDefinition</a>, and <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RunTask.html\">RunTask</a>.</p><p><strong>CORRECT: </strong>\"They will be placed only on container instances using the T2 instance type\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"They will be added to distinct instances using the T2 instance type\" is incorrect. The memberOf constraint does not choose distinct instances.</p><p><strong>INCORRECT:</strong> \"They will be placed only on container instances of T2 or T3 instance types\" is incorrect as only T2 instance types will be used. The wildcard means any T2 instance type such as t2.micro or t2.large.</p><p><strong>INCORRECT:</strong> \"They will be spread across all instances except for T2 instances\" is incorrect as this code ensures the instances WILL be placed on T2 instance types.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 4055,
                        "content": "<p>They will be placed only on container instances using the T2 instance type</p>",
                        "isValid": true
                    },
                    {
                        "id": 4056,
                        "content": "<p>They will be spread across all instances except for T2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 4057,
                        "content": "<p>They will be added to distinct instances using the T2 instance type</p>",
                        "isValid": false
                    },
                    {
                        "id": 4058,
                        "content": "<p>They will be placed only on container instances of T2 or T3 instance types</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 988,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.221Z",
                "updatedAt": "2023-09-07T08:51:43.221Z",
                "content": "<p>A Developer is creating an AWS Lambda function that will process medical images. The function is dependent on several libraries that are not available in the Lambda runtime environment. Which strategy should be used to create the Lambda deployment package? </p>",
                "answerExplanation": "<p>A deployment package is a ZIP archive that contains your function code and dependencies. You need to create a deployment package if you use the Lambda API to manage functions, or if you need to include libraries and dependencies other than the AWS SDK.</p><p>You can upload the package directly to Lambda, or you can use an Amazon S3 bucket, and then upload it to Lambda. If the deployment package is larger than 50 MB, you must use Amazon S3.</p><p>If your function depends on libraries not included in the Lambda runtime, you can install them to a local directory and include them in your deployment package.</p><p><strong>CORRECT: </strong>\"Create a ZIP file with the source code and all dependent libraries\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a ZIP file with the source code and a script that installs the dependent libraries at runtime\" is incorrect as though it is possible to call a script within the function code, this would need to run every time and pull in the files which would cause latency.</p><p><strong>INCORRECT:</strong> \"Create a ZIP file with the source code. Stage the dependent libraries on an Amazon S3 bucket indicated by the Lambda environment variable LIBRARY_PATH\" is incorrect as you cannot map an external path to a Lambda function using an environment variable.</p><p><strong>INCORRECT:</strong> \"Create a ZIP file with the source code and a buildspec.yaml file that installs the dependent libraries on AWS Lambda\" is incorrect as a buildspec.yaml file that is used by AWS CodeBuild to run a build. The libraries need to be included in the package zip file for the Lambda function.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/python-package.html\">https://docs.aws.amazon.com/lambda/latest/dg/python-package.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 4059,
                        "content": "<p>Create a ZIP file with the source code. Stage the dependent libraries on an Amazon S3 bucket indicated by the Lambda environment variable <code>LIBRARY_PATH</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 4060,
                        "content": "<p>Create a ZIP file with the source code and all dependent libraries</p>",
                        "isValid": true
                    },
                    {
                        "id": 4061,
                        "content": "<p>Create a ZIP file with the source code and a <code>buildspec.yaml</code> file that installs the dependent libraries on AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 4062,
                        "content": "<p>Create a ZIP file with the source code and a script that installs the dependent libraries at runtime</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 989,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.294Z",
                "updatedAt": "2023-09-07T08:51:43.294Z",
                "content": "<p>An organization has an Amazon S3 bucket containing premier content that they intend to make available to only paid subscribers of their website. The objects in the S3 bucket are private to prevent inadvertent exposure of the premier content to non-paying website visitors.</p><p>How can the organization provide only paid subscribers the ability to download the premier content in the S3 bucket?</p>",
                "answerExplanation": "<p>When Amazon S3 objects are private, only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant time-limited permission to download the objects.</p><p>When you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The presigned URLs are valid only for the specified duration.</p><p>Anyone who receives the presigned URL can then access the object. In this scenario, a pre-signed URL can be generated only for paying customers and they will be the only website visitors who can view the premier content.</p><p><strong>CORRECT: </strong>\"Generate a pre-signed object URL for the premier content file when a paid subscriber requests a download\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Apply a bucket policy that grants anonymous users to download the content from the S3 bucket\" is incorrect as this would provide everyone the ability to download the content.</p><p><strong>INCORRECT:</strong> \"Add a bucket policy that requires Multi-Factor Authentication for requests to access the S3 bucket objects\" is incorrect as this would be very difficult to manage. Using pre-signed URLs that are dynamically generated by an application for premier users would be much simpler.</p><p><strong>INCORRECT:</strong> \"Enable server-side encryption on the S3 bucket for data protection against the non-paying website visitors\" is incorrect as this is encryption at rest and S3 will simply unencrypt the objects when users attempt to read them. This provides privacy protection for data at rest but does not restrict access.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 4063,
                        "content": "<p>Add a bucket policy that requires Multi-Factor Authentication for requests to access the S3 bucket objects</p>",
                        "isValid": false
                    },
                    {
                        "id": 4064,
                        "content": "<p>Generate a pre-signed object URL for the premier content file when a paid subscriber requests a download</p>",
                        "isValid": true
                    },
                    {
                        "id": 4065,
                        "content": "<p>Enable server-side encryption on the S3 bucket for data protection against the non-paying website visitors</p>",
                        "isValid": false
                    },
                    {
                        "id": 4066,
                        "content": "<p>Apply a bucket policy that grants anonymous users to download the content from the S3 bucket</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 990,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.372Z",
                "updatedAt": "2023-09-07T08:51:43.372Z",
                "content": "<p>An application uses Amazon EC2 instances, AWS Lambda functions and an Amazon SQS queue. The Developer must ensure all communications are within an Amazon VPC using private IP addresses. How can this be achieved? (Select TWO.)</p>",
                "answerExplanation": "<p>This solution can be achieved by adding the AWS Lambda function to a VPC through the function configuration, and by creating a VPC endpoint for Amazon SQS. This will result in the services using purely private IP addresses to communicate without traversing the public Internet.</p><p><strong>CORRECT: </strong>\"Add the AWS Lambda function to the VPC\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Create a VPC endpoint for Amazon SQS\" is also correct.</p><p><strong>INCORRECT:</strong> \"Create the Amazon SQS queue within a VPC\" is incorrect as you cannot create a queue within a VPC as Amazon SQS is a public service.</p><p><strong>INCORRECT:</strong> \"Create a VPC endpoint for AWS Lambda\" is incorrect as you can't create a VPC endpoint for AWS Lambda. You can, however, connect a Lambda function to a VPC.</p><p><strong>INCORRECT:</strong> \"Create a VPN and connect the services to the VPG\" is incorrect as you cannot create a VPN between each of these services.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-sqs-vpc-endpoints-aws-privatelink/\">https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-sqs-vpc-endpoints-aws-privatelink/</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 4067,
                        "content": "<p>Create a VPN and connect the services to the VPG</p>",
                        "isValid": false
                    },
                    {
                        "id": 4068,
                        "content": "<p>Create a VPC endpoint for Amazon SQS</p>",
                        "isValid": true
                    },
                    {
                        "id": 4069,
                        "content": "<p>Create a VPC endpoint for AWS Lambda</p>",
                        "isValid": false
                    },
                    {
                        "id": 4070,
                        "content": "<p>Add the AWS Lambda function to the VPC</p>",
                        "isValid": true
                    },
                    {
                        "id": 4071,
                        "content": "<p>Create the Amazon SQS queue within a VPC</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 991,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.445Z",
                "updatedAt": "2023-09-07T08:51:43.445Z",
                "content": "<p>A serverless application composed of multiple Lambda functions has been deployed. A developer is setting up AWS CodeDeploy to manage the deployment of code updates. The developer would like a 10% of the traffic to be shifted to the new version in equal increments, 10 minutes apart.</p><p>Which setting should be chosen for configuring how traffic is shifted?</p>",
                "answerExplanation": "<p>A deployment configuration is a set of rules and success and failure conditions used by CodeDeploy during a deployment. These rules and conditions are different, depending on whether you deploy to an EC2/On-Premises compute platform or an AWS Lambda compute platform.</p><p>The following table lists the predefined configurations available for AWS Lambda deployments.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_04-00-04-d3e9f996df007d5519fddba490278419.jpg\"></p><p>As you can see from the table above, the linear option shifts a specific amount of traffic in equal increments of time. Therefore, the following option should be chosen:</p><p>CodeDeployDefault.LambdaLinear10PercentEvery10Minutes</p><p><strong>CORRECT: </strong>\"Linear\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Canary\" is incorrect as it does not shift traffic in equal increments.</p><p><strong>INCORRECT:</strong> \"All-at-once\" is incorrect as it shifts all traffic at once.</p><p><strong>INCORRECT:</strong> \"Blue/green\" is incorrect as it is a type of deployment, not a setting for traffic shifting.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 4072,
                        "content": "<p>Linear</p>",
                        "isValid": true
                    },
                    {
                        "id": 4073,
                        "content": "<p>Canary</p>",
                        "isValid": false
                    },
                    {
                        "id": 4074,
                        "content": "<p>All-at-once</p>",
                        "isValid": false
                    },
                    {
                        "id": 4075,
                        "content": "<p>Blue/green</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 992,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.516Z",
                "updatedAt": "2023-09-07T08:51:43.516Z",
                "content": "<p>An application collects data from sensors in a manufacturing facility. The data is stored in an Amazon SQS Standard queue by an AWS Lambda function and an Amazon EC2 instance processes the data and stores it in an Amazon RedShift data warehouse. A fault in the sensorsâ€™ software is causing occasional duplicate messages to be sent. Timestamps on the duplicate messages show they are generated within a few seconds of the primary message.</p><p>How can a Developer prevent duplicate data being stored in the data warehouse?</p>",
                "answerExplanation": "<p><em>FIFO (First-In-First-Out)</em> queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated.</p><p>In FIFO queues, messages are ordered based on message group ID. If multiple hosts (or different threads on the same host) send messages with the same message group ID to a FIFO queue, Amazon SQS stores the messages in the order in which they arrive for processing. To ensure that Amazon SQS preserves the order in which messages are sent and received, ensure that each producer uses a unique message group ID to send all its messages.</p><p>FIFO queue logic applies only per message group ID. Each message group ID represents a distinct ordered message group within an Amazon SQS queue. For each message group ID, all messages are sent and received in strict order. However, messages with different message group ID values might be sent and received out of order. You must associate a message group ID with a message. If you don't provide a message group ID, the action fails. If you require a single group of ordered messages, provide the same message group ID for messages sent to the FIFO queue.</p><p>Therefore, the Developer can use a FIFO queue and configure the Lambda function to add a message deduplication token to the message body. This will ensure that the messages are deduplicated before being picked up for processing by the Amazon EC2 instance.</p><p><strong>CORRECT: </strong>\"Use a FIFO queue and configure the Lambda function to add a message deduplication token to the message body\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a FIFO queue and configure the Lambda function to add a message group ID to the messages generated by each individual sensor\" is incorrect. The message group ID is the tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are always processed one by one, in a strict order relative to the message group.</p><p><strong>INCORRECT:</strong> \"Send a <code>ChangeMessageVisibility</code> call with <code>VisibilityTimeout</code> set to 30 seconds after the receipt of every message from the queue\" is incorrect as this will just change the visibility timeout for the message which will prevent others from seeing it until it has been processed and deleted from the queue. This doesnâ€™t stop a message with duplicate data being processed.</p><p><strong>INCORRECT:</strong> \"Configure a redrive policy, specify a destination Dead-Letter queue, and set the <code>maxReceiveCount </code>to 1\" is incorrect as without a FIFO queue and a message deduplication ID duplicate messages will still enter the queue. The redrive policy only applies to individual messages for which processing has failed a number of times as specified in the <code>maxReceiveCount</code>.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 4076,
                        "content": "<p>Use a FIFO queue and configure the Lambda function to add a message group ID to the messages generated by each individual sensor</p>",
                        "isValid": false
                    },
                    {
                        "id": 4077,
                        "content": "<p>Configure a redrive policy, specify a destination Dead-Letter queue, and set the maxReceiveCount to 1</p>",
                        "isValid": false
                    },
                    {
                        "id": 4078,
                        "content": "<p>Send a <code>ChangeMessageVisibility </code>call with <code>VisibilityTimeout</code> set to 30 seconds after the receipt of every message from the queue</p>",
                        "isValid": false
                    },
                    {
                        "id": 4079,
                        "content": "<p>Use a FIFO queue and configure the Lambda function to add a message deduplication token to the message body</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 993,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.588Z",
                "updatedAt": "2023-09-07T08:51:43.588Z",
                "content": "<p>A Java based application generates email notifications to customers using Amazon SNS. The emails must contain links to access data in a secured Amazon S3 bucket. What is the SIMPLEST way to maintain security of the bucket whilst allowing the customers to access specific objects?</p>",
                "answerExplanation": "<p>A presigned URL gives you access to the object identified in the URL, provided that the creator of the presigned URL has permissions to access that object. That is, if you receive a presigned URL to upload an object, you can upload the object only if the creator of the presigned URL has the necessary permissions to upload that object.</p><p>You can use the AWS SDK for Java to generate a presigned URL that you, or anyone you give the URL, can use to upload an object to Amazon S3. When you use the URL to upload an object, Amazon S3 creates the object in the specified bucket.</p><p>If an object with the same key that is specified in the presigned URL already exists in the bucket, Amazon S3 replaces the existing object with the uploaded object. To successfully complete an upload, you must do the following:</p><p> â€¢ Specify the HTTP PUT verb when creating the GeneratePresignedUrlRequest and HttpURLConnection objects.</p><p> â€¢ Interact with the HttpURLConnection object in some way after finishing the upload. The following example accomplishes this by using the HttpURLConnection object to check the HTTP response code.</p><p><strong>CORRECT: </strong>\"Use the AWS SDK for Java with GeneratePresignedUrlRequest to create a presigned URL\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use the AWS SDK for Java to update the bucket Access Control List to allow the customers to access the bucket\" is incorrect. Bucket ACLs are used to grant access to predefined groups and accounts and are not suitable for this purpose.</p><p><strong>INCORRECT:</strong> \"Use the AWS SDK for Java with the AWS STS service to gain temporary security credentials\" is incorrect as this requires the creation of policies and security credentials and is not as simple as creating a presigned URL.</p><p><strong>INCORRECT:</strong> \"Use the AWS SDK for Java to assume a role with <code>AssumeRole</code> to gain temporary security credentials\" is incorrect as this requires the creation of policies and security credentials and is not as simple as creating a presigned URL.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURLJavaSDK.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURLJavaSDK.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 4080,
                        "content": "<p>Use the AWS SDK for Java to assume a role with AssumeRole to gain temporary security credentials</p>",
                        "isValid": false
                    },
                    {
                        "id": 4081,
                        "content": "<p>Use the AWS SDK for Java with <code>GeneratePresignedUrlRequest</code> to create a presigned URL</p>",
                        "isValid": true
                    },
                    {
                        "id": 4082,
                        "content": "<p>Use the AWS SDK for Java to update the bucket Access Control List to allow the customers to access the bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 4083,
                        "content": "<p>Use the AWS SDK for Java with the AWS STS service to gain temporary security credentials</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 994,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.664Z",
                "updatedAt": "2023-09-07T08:51:43.664Z",
                "content": "<p>A company is migrating an on-premises web application to AWS. The web application runs on a single server and stores session data in memory. On AWS the company plan to implement multiple Amazon EC2 instances behind an Elastic Load Balancer (ELB). The company want to refactor the application so that data is resilient if an instance fails and user downtime is minimized.</p><p>Where should the company move session data to MOST effectively reduce downtime and make usersâ€™ session data more fault tolerant?</p>",
                "answerExplanation": "<p>ElastiCache is a fully managed, low latency, in-memory data store that supports either Memcached or Redis. The Redis engine supports multi-AZ and high availability.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_04-52-40-42815cce7a755d04079ad0abec51aba3.png\"></p><p>With ElastiCache the company can move the session data to a high-performance, in-memory data store that is well suited to this use case. This will provide high availability for the session data in the case of EC2 instance failure and will reduce downtime for users.</p><p><strong>CORRECT: </strong>\"An Amazon ElastiCache for Redis cluster\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"A second Amazon EBS volume\" is incorrect as the session data needs to be highly available so should not be stored on an EC2 instance.</p><p><strong>INCORRECT:</strong> \"The web serverâ€™s primary disk\" is incorrect as the session data needs to be highly available so should not be stored on an EC2 instance.</p><p><strong>INCORRECT:</strong> \"An Amazon EC2 instance dedicated to session data\" is incorrect as the session data needs to be highly available so should not be stored on an EC2 instance.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/features/\">https://aws.amazon.com/elasticache/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 4084,
                        "content": "<p>An Amazon ElastiCache for Redis cluster</p>",
                        "isValid": true
                    },
                    {
                        "id": 4085,
                        "content": "<p>The web serverâ€™s primary disk</p>",
                        "isValid": false
                    },
                    {
                        "id": 4086,
                        "content": "<p>An Amazon EC2 instance dedicated to session data</p>",
                        "isValid": false
                    },
                    {
                        "id": 4087,
                        "content": "<p>A second Amazon EBS volume</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 995,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.744Z",
                "updatedAt": "2023-09-07T08:51:43.744Z",
                "content": "<p>A company is reviewing their security practices. According to AWS best practice how should access keys be managed to improve security? (Select TWO.)</p>",
                "answerExplanation": "<p>When you access AWS programmatically, you use an access key to verify your identity and the identity of your applications. An access key consists of an access key ID (something like AKIAIOSFODNN7EXAMPLE) and a secret access key (something like wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY).</p><p>Anyone who has your access key has the same level of access to your AWS resources that you do. Steps to protect access keys include the following:</p><p> â€¢ Remove (or Don't Generate) Account Access Key â€“ this is especially important for the root account.</p><p> â€¢ Use Temporary Security Credentials (IAM Roles) Instead of Long-Term Access Keys.</p><p> â€¢ Don't embed access keys directly into code.</p><p> â€¢ Use different access keys for different applications.</p><p> â€¢ Rotate access keys periodically.</p><p> â€¢ Remove unused access keys.</p><p> â€¢ Configure multi-factor authentication for your most sensitive operations.</p><p><strong>CORRECT: </strong>\"Delete all access keys for the root account IAM user\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Use different access keys for different applications\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Embed access keys directly into code\" is incorrect. This is not a best practice as this is something that should be avoided as much as possible.</p><p><strong>INCORRECT:</strong> \"Rotate access keys daily\" is incorrect. Though this would be beneficial from a security perspective it may be hard to manage so this is not an AWS recommended best practice. AWS recommend you rotate access keys â€œperiodicallyâ€, not â€œdailyâ€.</p><p><strong>INCORRECT:</strong> \"Use the same access key in all applications for consistency\" is incorrect. The best practice is to use different access keys for different applications.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-access-keys-best-practices.html#iam-user-access-keys\">https://docs.aws.amazon.com/general/latest/gr/aws-access-keys-best-practices.html#iam-user-access-keys</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p><p><br></p>",
                "options": [
                    {
                        "id": 4088,
                        "content": "<p>Delete all access keys for the root account IAM user</p>",
                        "isValid": true
                    },
                    {
                        "id": 4089,
                        "content": "<p>Use different access keys for different applications</p>",
                        "isValid": true
                    },
                    {
                        "id": 4090,
                        "content": "<p>Embed access keys directly into code</p>",
                        "isValid": false
                    },
                    {
                        "id": 4091,
                        "content": "<p>Rotate access keys daily</p>",
                        "isValid": false
                    },
                    {
                        "id": 4092,
                        "content": "<p>Use the same access key in all applications for consistency</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 996,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.819Z",
                "updatedAt": "2023-09-07T08:51:43.819Z",
                "content": "<p>The manager of a development team is setting up a shared S3 bucket for team members. The manager would like to use a single policy to allow each user to have access to their objects in the S3 bucket. Which feature can be used to generalize the policy?</p>",
                "answerExplanation": "<p>In some cases, you might not know the exact name of the resource when you write the policy. You might want to generalize the policy so it works for many users without having to make a unique copy of the policy for each user. For example, consider writing a policy to allow each user to have access to his or her own objects in an Amazon S3 bucket.</p><p>Instead of that explicitly specifies the user's name as part of the resource, create a single group policy that works for any user in that group. You can do this by using <em>policy variables</em>, a feature that lets you specify placeholders in a policy. When the policy is evaluated, the policy variables are replaced with values that come from the context of the request itself.</p><p>The following example shows a policy for an Amazon S3 bucket that uses a policy variable.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-55-55-d4f811f73bd4032297912e419c469483.jpg\"></p><p>When this policy is evaluated, IAM replaces the variable ${aws:username}with the friendly name of the actual current user. This means that a single policy applied to a group of users can control access to a bucket by using the username as part of the resource's name.</p><p><strong>CORRECT: </strong>\"Variable\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Condition\" is incorrect. The Condition element (or Condition <em>block</em>) lets you specify conditions for when a policy is in effect.</p><p><strong>INCORRECT:</strong> \"Principal\" is incorrect. You can use the Principal element in a policy to specify the principal that is allowed or denied access to a resource. However, in this scenario a variable is needed to create a generic policy that can provide the necessary permissions to different principals using variables.</p><p><strong>INCORRECT:</strong> \"Resource\" is incorrect. The Resource element specifies the object or objects that the statement covers.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 4093,
                        "content": "<p>Condition</p>",
                        "isValid": false
                    },
                    {
                        "id": 4094,
                        "content": "<p>Resource</p>",
                        "isValid": false
                    },
                    {
                        "id": 4095,
                        "content": "<p>Variable</p>",
                        "isValid": true
                    },
                    {
                        "id": 4096,
                        "content": "<p>Principal</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 997,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.890Z",
                "updatedAt": "2023-09-07T08:51:43.890Z",
                "content": "<p>A legacy application is being refactored into a microservices architecture running on AWS. The microservice will include several AWS Lambda functions. A Developer will use AWS Step Functions to coordinate function execution.</p><p>How should the Developer proceed?</p>",
                "answerExplanation": "<p>AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or <em>task</em>, allowing you to scale and change applications quickly.</p><p>The following are key features of AWS Step Functions:</p><p> â€¢ Step Functions is based on the concepts of <a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html\">tasks</a> and <a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-states.html\">state machines</a>.</p><p> â€¢ You define state machines using the JSON-based <a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html\">Amazon States Language</a>.</p><p> â€¢ The <a href=\"https://console.aws.amazon.com/states/home?region=us-east-1#/\">Step Functions console</a> displays a graphical view of your state machine's structure. This provides a way to visually check your state machine's logic and monitor executions.</p><p>The Developer needs to create a state machine using the Amazon States Language as this is how you can create an executable state machine that includes the Lambda functions that must be coordinated.</p><p><strong>CORRECT: </strong>\"Create a state machine using the Amazon States Language\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudFormation stack using a YAML-formatted template\" is incorrect as AWS Step Functions does not use CloudFormation. The Developer needs to create a state machine.</p><p><strong>INCORRECT:</strong> \"Create a workflow using the <code>StartExecution</code> API action\" is incorrect as workflows are associated with Amazon SWF whereas the <code>StartExecution</code> API action is a Step Functions action for executing a state machine.</p><p><strong>INCORRECT:</strong> \"Create a layer in AWS Lambda and add the functions to the layer\" is incorrect as a layer is a ZIP archive that contains libraries, a <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html\">custom runtime</a>, or other dependencies that you can use to pull additional code into a Lambda function.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\">https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
                "options": [
                    {
                        "id": 4097,
                        "content": "<p>Create an AWS CloudFormation stack using a YAML-formatted template</p>",
                        "isValid": false
                    },
                    {
                        "id": 4098,
                        "content": "<p>Create a layer in AWS Lambda and add the functions to the layer</p>",
                        "isValid": false
                    },
                    {
                        "id": 4099,
                        "content": "<p>Create a state machine using the Amazon States Language</p>",
                        "isValid": true
                    },
                    {
                        "id": 4100,
                        "content": "<p>Create a workflow using the StartExecution API action</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 998,
            "attributes": {
                "createdAt": "2023-09-07T08:51:43.991Z",
                "updatedAt": "2023-09-07T08:51:43.991Z",
                "content": "<p>A Developer is creating a script to automate the deployment process for a serverless application. The Developer wants to use an existing AWS Serverless Application Model (SAM) template for the application.</p><p>What should the Developer use for the project? (Select TWO.)</p>",
                "answerExplanation": "<p>The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax, enabling you to build serverless applications faster.</p><p>To get started with building SAM-based applications, use the AWS SAM CLI. SAM CLI provides a Lambda-like execution environment that lets you locally build, test, and debug applications defined by SAM templates. You can also use the SAM CLI to deploy your applications to AWS.</p><p>With the SAM CLI you can package and deploy your source code using two simple commands:</p><p>â€¢ sam package</p><p>â€¢ sam deploy</p><p>Alternatively, you can use:</p><p>â€¢ aws cloudformation package</p><p>â€¢ aws cloudformation deploy</p><p>Therefore, the Developer can use either the sam or aws cloudformation CLI commands to package and deploy the serverless application.</p><p><strong>CORRECT: </strong>\"Call aws cloudformation package to create the deployment package. Call aws cloudformation deploy to deploy the package afterward\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Call sam package to create the deployment package. Call sam deploy to deploy the package afterward\" is a correct answer.</p><p><strong>INCORRECT:</strong> \"Call aws s3 cp to upload the AWS SAM template to Amazon S3. Call aws lambda update-function-code to create the application\" is incorrect as this is not how to use a SAM template. With SAM the commands mentioned above must be run.</p><p><strong>INCORRECT:</strong> \"Create a ZIP package locally and call aws serverlessrepo create-application to create the application\" is incorrect as this is not the correct way to use a SAM template.</p><p><strong>INCORRECT:</strong> \"Create a ZIP package and upload it to Amazon S3. Call aws cloudformation create-stack to create the application\" is incorrect as this is not required when deploying a SAM template.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/serverless/sam/\">https://aws.amazon.com/serverless/sam/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-sam/\">https://digitalcloud.training/aws-sam/</a></p>",
                "options": [
                    {
                        "id": 4101,
                        "content": "<p>Call <code>aws cloudformation package</code> to create the deployment package. Call <code>aws cloudformation</code> deploy to deploy the package afterward</p>",
                        "isValid": true
                    },
                    {
                        "id": 4102,
                        "content": "<p>Call <code>aws s3 cp</code> to upload the AWS SAM template to Amazon S3. Call <code>aws lambda update-function-code</code> to create the application</p>",
                        "isValid": false
                    },
                    {
                        "id": 4103,
                        "content": "<p>Create a ZIP package and upload it to Amazon S3. Call <code>aws cloudformation create-stack</code> to create the application</p>",
                        "isValid": false
                    },
                    {
                        "id": 4104,
                        "content": "<p>Call <code>sam package </code>to create the deployment package. Call <code>sam deploy</code> to deploy the package afterward</p>",
                        "isValid": true
                    },
                    {
                        "id": 4105,
                        "content": "<p>Create a ZIP package locally and call <code>aws serverlessrepo create-application</code> to create the application</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 999,
            "attributes": {
                "createdAt": "2023-09-07T08:51:44.062Z",
                "updatedAt": "2023-09-07T08:51:44.062Z",
                "content": "<p>A developer is creating a multi-tier web application. The front-end will place messages in an Amazon SQS queue for the back-end to process. Each job includes a file that is 1GB in size. What MUST the developer do to ensure this works as expected?</p>",
                "answerExplanation": "<p>You can use Amazon S3 and the Amazon SQS Extended Client Library for Java to manage Amazon SQS messages. This is especially useful for storing and consuming messages up to 2 GB in size. Unless your application requires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queue, consider using Amazon S3 for storing your data.</p><p>You can use the Amazon SQS Extended Client Library for Java library to do the following:</p><p> â€¢ Specify whether messages are always stored in Amazon S3 or only when the size of a message exceeds 256 KB.</p><p> â€¢ Send a message that references a single message object stored in an Amazon S3 bucket.</p><p> â€¢ Get the corresponding message object from an Amazon S3 bucket.</p><p> â€¢ Delete the corresponding message object from an Amazon S3 bucket.</p><p>Note: Amazon SQS only supports messages up to 256KB in size. Therefore, the extended client library for Java must be used.</p><p><strong>CORRECT: </strong>\"Store the large files in Amazon S3 and use the SQS Extended Client Library for Java to manage SQS messages\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Increase the maximum message size of the queue from 256KB to 1GB\" is incorrect as you cannot increase the maximum message size above 256KB.</p><p><strong>INCORRECT:</strong> \"Store the large files in DynamoDB and use the SQS Extended Client Library for Java to manage SQS messages\" is incorrect as you should store the files in Amazon S3.</p><p><strong>INCORRECT:</strong> \"Create a FIFO queue that supports large files \" is incorrect as FIFO queues also have a maximum message size of 256KB.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/working-java-example-using-s3-for-large-sqs-messages.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/working-java-example-using-s3-for-large-sqs-messages.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p><p><br></p>",
                "options": [
                    {
                        "id": 4106,
                        "content": "<p>Create a FIFO queue that supports large files</p>",
                        "isValid": false
                    },
                    {
                        "id": 4107,
                        "content": "<p>Store the large files in DynamoDB and use the SQS Extended Client Library for Java to manage SQS messages</p>",
                        "isValid": false
                    },
                    {
                        "id": 4108,
                        "content": "<p>Increase the maximum message size of the queue from 256KB to 1GB</p>",
                        "isValid": false
                    },
                    {
                        "id": 4109,
                        "content": "<p>Store the large files in Amazon S3 and use the SQS Extended Client Library for Java to manage SQS messages</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 1000,
            "attributes": {
                "createdAt": "2023-09-07T08:51:44.130Z",
                "updatedAt": "2023-09-07T08:51:44.130Z",
                "content": "<p>A company is designing a new application that will store thousands of terabytes of data. They need a fully managed NoSQL data store that provides low-latency and can store key-value pairs. Which type of database should they use?</p>",
                "answerExplanation": "<p>Amazon DynamoDB is a fully managed NoSQL database. With DynamoDB, you can create database tables that can store and retrieve any amount of data and serve any level of request traffic. You can scale up or scale down your tables' throughput capacity without downtime or performance degradation.</p><p>DynamoDB is a key-value database. A key-value database is a type of nonrelational database that uses a simple key-value method to store data. A key-value database stores data as a collection of key-value pairs in which a key serves as a unique identifier. Both keys and values can be anything, ranging from simple objects to complex compound objects.</p><p><strong>CORRECT: </strong>\"Amazon DynamoDB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon RDS\" is incorrect as RDS is a SQL (not a NoSQL) type of database.</p><p><strong>INCORRECT:</strong> \"Amazon ElastiCache\" is incorrect as ElastiCache is a SQL (not a NoSQL) type of database. ElastiCache is an in-memory database typically used for caching data.</p><p><strong>INCORRECT:</strong> \"Amazon S3\" is incorrect as S3 is not a NoSQL database. S3 is an object storage system.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html</a></p><p><a href=\"https://aws.amazon.com/nosql/key-value/\">https://aws.amazon.com/nosql/key-value/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 4110,
                        "content": "<p>Amazon RDS</p>",
                        "isValid": false
                    },
                    {
                        "id": 4111,
                        "content": "<p>Amazon S3</p>",
                        "isValid": false
                    },
                    {
                        "id": 4112,
                        "content": "<p>Amazon ElastiCache</p>",
                        "isValid": false
                    },
                    {
                        "id": 4113,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 1001,
            "attributes": {
                "createdAt": "2023-09-07T08:51:44.200Z",
                "updatedAt": "2023-09-07T08:51:44.200Z",
                "content": "<p>A developer has created a Docker image and uploaded it to an Amazon Elastic Container Registry (ECR) repository. How can the developer pull the image to his workstation using the docker client? </p>",
                "answerExplanation": "<p>If you would like to run a Docker image that is available in Amazon ECR, you can pull it to your local environment with the docker pull command. You can do this from either your default registry or from a registry associated with another AWS account.</p><p>Docker CLI does not support standard AWS authentication methods, so client authentication must be handled so that ECR knows who is requesting to push or pull an image. To do this you can issue the <code>aws ecr get-login </code>or <code>aws ecr get-login-password</code> (AWS CLI v2) and then use the output to login using <code>docker login</code> and then issue a docker pull command specifying the image name using <code>registry/repository[:tag]</code></p><p><strong>CORRECT: </strong>\"Run <code>aws ecr get-login-password</code> use the output to login in then issue a <code>docker pull</code> command specifying the image name using <code>registry/repository[:tag]</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Run the<code> docker pull</code> command specifying the image name using <code>registry/repository[:tag]</code>\" is incorrect as you first need to authenticate to get an access token so you can pull the image down.</p><p><strong>INCORRECT:</strong> \"Run <code>aws ecr describe-images --repository-name repositoryname</code>\" is incorrect as this would just list the images available in the repository.</p><p><strong>INCORRECT:</strong> \"Run <code>docker login</code> with an IAM key pair then issue a docker pull command specifying the image name using <code>registry/repository[@digest]</code>\" is incorrect as you cannot run docker login with an IAM key pair.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html\">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 4114,
                        "content": "<p>Run <code>aws ecr describe-images --repository-name repositoryname</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 4115,
                        "content": "<p>Run <code>aws ecr get-login-password</code> use the output to login in then issue a <code>docker pull </code>command specifying the image name using <code>registry/repository[:tag]</code> </p>",
                        "isValid": true
                    },
                    {
                        "id": 4116,
                        "content": "<p>Run <code>docker login</code> with an IAM key pair then issue a docker pull command specifying the image name using <code>registry/repository[@digest]</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 4117,
                        "content": "<p>Run the <code>docker pull </code>command specifying the image name using <code>registry/repository[:tag]</code> </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1002,
            "attributes": {
                "createdAt": "2023-09-07T08:51:44.273Z",
                "updatedAt": "2023-09-07T08:51:44.273Z",
                "content": "<p>A Developer is creating a social networking app for games that uses a single Amazon DynamoDB table. All usersâ€™ saved game data is stored in the single table, but users should not be able to view each otherâ€™s data.</p><p>How can the Developer restrict user access so they can only view their own data?</p>",
                "answerExplanation": "<p>In DynamoDB, you have the option to specify conditions when granting permissions using an IAM policy. For example, you can:</p><p> â€¢ Grant permissions to allow users read-only access to certain items and attributes in a table or a secondary index.</p><p> â€¢ Grant permissions to allow users write-only access to certain attributes in a table, based upon the identity of that user.</p><p>To implement this kind of fine-grained access control, you write an IAM permissions policy that specifies conditions for accessing security credentials and the associated permissions. You then apply the policy to IAM users, groups, or roles that you create using the IAM console. Your IAM policy can restrict access to individual items in a table, access to the attributes in those items, or both at the same time.</p><p>You use the IAM Condition element to implement a fine-grained access control policy. By adding a Condition element to a permissions policy, you can allow or deny access to items and attributes in DynamoDB tables and indexes, based upon your particular business requirements. You can also grant permissions on a table, but restrict access to specific items in that table based on certain primary key values.</p><p><strong>CORRECT: </strong>\"Restrict access to specific items based on certain primary key values\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use separate access keys for each user to call the API and restrict access to specific items based on access key ID\" is incorrect. You cannot restrict access based on access key ID.</p><p><strong>INCORRECT:</strong> \"Use an identity-based policy that restricts read access to the table to specific principals\" is incorrect as this would only restrict read access to the entire table, not to specific items in the table.</p><p><strong>INCORRECT:</strong> \"Read records from DynamoDB and discard irrelevant data client-side\" is incorrect as this is inefficient and insecure as it will use more RCUs and has more potential to leak the information.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 4118,
                        "content": "<p>Use an identity-based policy that restricts read access to the table to specific principals</p>",
                        "isValid": false
                    },
                    {
                        "id": 4119,
                        "content": "<p>Use separate access keys for each user to call the API and restrict access to specific items based on access key ID</p>",
                        "isValid": false
                    },
                    {
                        "id": 4120,
                        "content": "<p>Read records from DynamoDB and discard irrelevant data client-side</p>",
                        "isValid": false
                    },
                    {
                        "id": 4121,
                        "content": "<p>Restrict access to specific items based on certain primary key values</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 1003,
            "attributes": {
                "createdAt": "2023-09-07T08:51:44.344Z",
                "updatedAt": "2023-09-07T08:51:44.344Z",
                "content": "<p>An Amazon RDS database is experiencing a high volume of read requests that are slowing down the database. Which fully managed, in-memory AWS database service can assist with offloading reads from the RDS database?</p>",
                "answerExplanation": "<p>ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis protocol-compliant server nodes in the cloud. The in-memory caching provided by ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads or compute-intensive workloads.</p><p>This is a fully managed AWS service and is ideal for offloading reads from the main database to reduce the performance impact.</p><p><strong>CORRECT: </strong>\"Amazon ElastiCache Redis\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon RDS Read Replica\" is incorrect as it is not an in-memory database. RDS Read Replicas can be used for offloading reads from the main database, however.</p><p><strong>INCORRECT:</strong> \"Amazon Aurora Serverless\" is incorrect. Aurora Serverless is not an in-memory solution, nor is it suitable for functioning as a method of offloading reads from RDS databases.</p><p><strong>INCORRECT:</strong> \"Memcached on Amazon EC2\" is incorrect as this is an implementation of Memcached running on EC2 and therefore is not a fully managed AWS service.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
                "options": [
                    {
                        "id": 4122,
                        "content": "<p>Amazon ElastiCache Redis</p>",
                        "isValid": true
                    },
                    {
                        "id": 4123,
                        "content": "<p>Amazon Aurora Serverless</p>",
                        "isValid": false
                    },
                    {
                        "id": 4124,
                        "content": "<p>Memcached on Amazon EC2</p>",
                        "isValid": false
                    },
                    {
                        "id": 4125,
                        "content": "<p>Amazon RDS Read Replica</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1004,
            "attributes": {
                "createdAt": "2023-09-07T08:51:44.416Z",
                "updatedAt": "2023-09-07T08:51:44.416Z",
                "content": "<p>A Developer is creating an AWS Lambda function to process a stream of data from an Amazon Kinesis Data Stream. When the Lambda function parses the data and encounters a missing field, it exits the function with an error. The function is generating duplicate records from the Kinesis stream. When the Developer looks at the stream output without the Lambda function, there are no duplicate records.</p><p>What is the reason for the duplicates?</p>",
                "answerExplanation": "<p>When you invoke a function, two types of error can occur. Invocation errors occur when the invocation request is rejected before your function receives it. Function errors occur when your function's code or <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html\">runtime</a> returns an error.</p><p>Depending on the type of error, the type of invocation, and the client or service that invokes the function, the retry behavior and the strategy for managing errors varies. Function errors occur when your function code or the runtime that it uses return an error.</p><p>In this case, with an event source mapping from a stream (Kinesis Data Stream), Lambda retries the entire batch of items. Therefore, the best explanation is that the Lambda function did not handle the error, and the Lambda service attempted to reprocess the data.</p><p><strong>CORRECT: </strong>\"The Lambda function did not handle the error, and the Lambda service attempted to reprocess the data\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The Lambda function did not advance the Kinesis stream point to the next record after the error\" is incorrect. Lambda does not advance a stream â€œpointâ€ to the next record. It processed records in batches.</p><p><strong>INCORRECT:</strong> \"The Lambda event source used asynchronous invocation, resulting in duplicate records\" is incorrect as Lambda processes records from Kinesis Data Streams synchronously.</p><p><strong>INCORRECT:</strong> \"The Lambda function is not keeping up with the amount of data coming from the stream\" is incorrect as Lambda can scale seamlessly to handle the load coming from the stream.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-retries.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-retries.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 4126,
                        "content": "<p>The Lambda event source used asynchronous invocation, resulting in duplicate records</p>",
                        "isValid": false
                    },
                    {
                        "id": 4127,
                        "content": "<p>The Lambda function did not handle the error, and the Lambda service attempted to reprocess the data</p>",
                        "isValid": true
                    },
                    {
                        "id": 4128,
                        "content": "<p>The Lambda function is not keeping up with the amount of data coming from the stream</p>",
                        "isValid": false
                    },
                    {
                        "id": 4129,
                        "content": "<p>The Lambda function did not advance the Kinesis stream point to the next record after the error</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1005,
            "attributes": {
                "createdAt": "2023-09-07T08:51:44.489Z",
                "updatedAt": "2023-09-07T08:51:44.489Z",
                "content": "<p>A retail organization stores stock information in an Amazon RDS database. An application reads and writes data to the database. A Developer has been asked to provide read access to the database from a reporting application in another region.</p><p>Which configuration would provide BEST performance for the reporting application without impacting the performance of the main database?</p>",
                "answerExplanation": "<p>With Amazon RDS, you can create a MariaDB, MySQL, Oracle, or PostgreSQL read replica in a different AWS Region than the source DB instance. Creating a cross-Region read replica isn't supported for SQL Server on Amazon RDS.</p><p>You create a read replica in a different AWS Region to do the following:</p><p> â€¢ Improve your disaster recovery capabilities.</p><p> â€¢ Scale read operations into an AWS Region closer to your users.</p><p> â€¢ Make it easier to migrate from a data center in one AWS Region to a data center in another AWS Region.</p><p>Creating a read replica in a different AWS Region from the source instance is similar to creating a replica in the same AWS Region. You can use the AWS Management Console, run the <a href=\"https://docs.aws.amazon.com/cli/latest/reference/rds/create-db-instance-read-replica.html\">create-db-instance-read-replica</a> command, or call the <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_CreateDBInstanceReadReplica.html\">CreateDBInstanceReadReplica</a> API operation.</p><p>Creating read replica in the region where the reporting application is going to run will provide the best performance as latency will be much lower than connecting across regions. As the database is a replica it will also be continuously updated using asynchronous replication so the reporting application will have the latest data available.</p><p><strong>CORRECT: </strong>\"Implement a cross-region read replica in the region where the reporting application will run\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement a cross-region multi-AZ deployment in the region where the reporting application will run\" is incorrect as multi-AZ is used across availability zones, not regions.</p><p><strong>INCORRECT:</strong> \"Create a snapshot of the database and create a new database from the snapshot in the region where the reporting application will run\" is incorrect as this would be OK from a performance perspective but the database would not receive ongoing updates from the main database so the data would quickly become out of date.</p><p><strong>INCORRECT:</strong> \"Implement a read replica in another AZ and configure the reporting application to connect to the read replica using a VPN connection\" is incorrect as this would result in much higher latency than having the database in the local region close to the reporting application and would impact performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 4130,
                        "content": "<p>Implement a cross-region multi-AZ deployment in the region where the reporting application will run</p>",
                        "isValid": false
                    },
                    {
                        "id": 4131,
                        "content": "<p>Implement a cross-region read replica in the region where the reporting application will run</p>",
                        "isValid": true
                    },
                    {
                        "id": 4132,
                        "content": "<p>Implement a read replica in another AZ and configure the reporting application to connect to the read replica using a VPN connection</p>",
                        "isValid": false
                    },
                    {
                        "id": 4133,
                        "content": "<p>Create a snapshot of the database and create a new database from the snapshot in the region where the reporting application will run</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1006,
            "attributes": {
                "createdAt": "2023-09-07T08:51:44.571Z",
                "updatedAt": "2023-09-07T08:51:44.571Z",
                "content": "<p>A Developer is migrating Docker containers to Amazon ECS. A large number of containers will be deployed onto an existing ECS cluster that uses container instances of different instance types.</p><p>Which task placement strategy can be used to minimize the number of container instances used based on available memory?</p>",
                "answerExplanation": "<p>When a task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified in the task definition, such as CPU and memory. Similarly, when you scale down the task count, Amazon ECS must determine which tasks to terminate. You can apply task placement strategies and constraints to customize how Amazon ECS places and terminates tasks. Task placement strategies and constraints are not supported for tasks using the Fargate launch type. By default, Fargate tasks are spread across Availability Zones.</p><p>A <em>task placement strategy</em> is an algorithm for selecting instances for task placement or tasks for termination. For example, Amazon ECS can select instances at random, or it can select instances such that tasks are distributed evenly across a group of instances.</p><p>Amazon ECS supports the following task placement strategies:</p><p> â€¢ binpack</p><p>Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.</p><p> â€¢ random</p><p>Place tasks randomly.</p><p> â€¢ spread</p><p>Place tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone. Service tasks are spread based on the tasks from that service. Standalone tasks are spread based on the tasks from the same task group.</p><p>The Developer should use the binpack task placement strategy using available memory to determine the placement of tasks. This will minimize the number of container instances required.</p><p><strong>CORRECT: </strong>\"binpack\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"random\" is incorrect as this would just randomly assign the tasks across the available container instances in the cluster.</p><p><strong>INCORRECT:</strong> \"spread\" is incorrect as this would attempt to spread the tasks across the cluster instances for better high availability.</p><p><strong>INCORRECT:</strong> \"distinctInstance\" is incorrect as this is a task placement constraint, not a strategy. This constraint would result in the tasks being each placed on a separate instance which would not assist with meeting the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 4134,
                        "content": "<p>distinctInstance</p>",
                        "isValid": false
                    },
                    {
                        "id": 4135,
                        "content": "<p>binpack</p>",
                        "isValid": true
                    },
                    {
                        "id": 4136,
                        "content": "<p>random</p>",
                        "isValid": false
                    },
                    {
                        "id": 4137,
                        "content": "<p>spread</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1007,
            "attributes": {
                "createdAt": "2023-09-07T08:51:44.647Z",
                "updatedAt": "2023-09-07T08:51:44.647Z",
                "content": "<p>A Development team manage a hybrid cloud environment. They would like to collect system-level metrics from on-premises servers and Amazon EC2 instances. How can the Development team collect this information MOST efficiently?</p>",
                "answerExplanation": "<p>The unified CloudWatch agent can be installed on both on-premises servers and Amazon EC2 instances using multiple operating system versions. It enables you to do the following:</p><p> â€¢ Collect more system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances.</p><p> â€¢ Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.</p><p> â€¢ Retrieve custom metrics from your applications or services using the StatsD and collectd protocols.</p><p> â€¢ Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p><p>Therefore, the Development team should install the CloudWatch agent on the on-premises servers and EC2 instances. This will allow them to collect system-level metrics from servers and instances across the hybrid cloud environment.</p><p><strong>CORRECT: </strong>\"Install the CloudWatch agent on the on-premises servers and EC2 instances\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use CloudWatch for monitoring EC2 instances and custom AWS CLI scripts using the put-metric-data API\" is incorrect as this is not the most efficient option as you must write and maintain custom scripts. It is better to use the CloudWatch agent as it provides all the functionality required.</p><p><strong>INCORRECT:</strong> \"Install the CloudWatch agent on the EC2 instances and use a cron job on the on-premises servers\" is incorrect as the answer does not even specify what the cron job is going to do / use for gathering and sending the data.</p><p><strong>INCORRECT:</strong> \"Use CloudWatch detailed monitoring for both EC2 instances and on-premises servers\" is incorrect as this would not do anything for the on-premises instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 4138,
                        "content": "<p>Use CloudWatch detailed monitoring for both EC2 instances and on-premises servers</p>",
                        "isValid": false
                    },
                    {
                        "id": 4139,
                        "content": "<p>Install the CloudWatch agent on the on-premises servers and EC2 instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 4140,
                        "content": "<p>Use CloudWatch for monitoring EC2 instances and custom AWS CLI scripts using the put-metric-data API</p>",
                        "isValid": false
                    },
                    {
                        "id": 4141,
                        "content": "<p>Install the CloudWatch agent on the EC2 instances and use a cron job on the on-premises servers</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1008,
            "attributes": {
                "createdAt": "2023-09-07T08:51:44.728Z",
                "updatedAt": "2023-09-07T08:51:44.728Z",
                "content": "<p>A company is migrating an application with a website and MySQL database to the AWS Cloud. The company require the application to be refactored so it offers high availability and fault tolerance.</p><p>How should a Developer refactor the application? (Select TWO.)</p>",
                "answerExplanation": "<p>The key requirements are to add high availability and fault tolerance to the application. To do this the Developer should put the website into an Auto Scaling group of EC2 instances across multiple AZs. An Elastic Load Balancer can be deployed in front of the EC2 instances to distribute incoming connections. This solution is highly available and fault tolerant.</p><p>For the MySQL database the Developer should use Amazon RDS with the MySQL engine. To provide fault tolerance the Developer should configure Amazon RDS as a Multi-AZ deployment which will create a standby instance in another AZ that can be failed over to.</p><p><strong>CORRECT: </strong>\"Migrate the website to an Auto Scaling group of EC2 instances across multiple AZs and use an Elastic Load Balancer\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Migrate the MySQL database to an Amazon RDS Multi-AZ deployment\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Migrate the website to an Auto Scaling group of EC2 instances across a single AZ and use an Elastic Load Balancer\" is incorrect as to be fully fault tolerant the solution should be spread across multiple AZs.</p><p><strong>INCORRECT:</strong> \"Migrate the MySQL database to an Amazon RDS instance with a Read Replica in another AZ\" is incorrect as read replicas are used for performance, not fault tolerance</p><p><strong>INCORRECT:</strong> \"Migrate the MySQL database to an Amazon DynamoDB with Global Tables\" is incorrect as the MySQL database is a relational database so it is a better fit to be migrated to Amazon RDS rather than DynamoDB.</p><p><strong>References:</strong></p><p><a href=\"https://d1.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf\">https://d1.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
                "options": [
                    {
                        "id": 4142,
                        "content": "<p>Migrate the MySQL database to an Amazon RDS instance with a Read Replica in another AZ</p>",
                        "isValid": false
                    },
                    {
                        "id": 4143,
                        "content": "<p>Migrate the MySQL database to an Amazon RDS Multi-AZ deployment</p>",
                        "isValid": true
                    },
                    {
                        "id": 4144,
                        "content": "<p>Migrate the MySQL database to an Amazon DynamoDB with Global Tables</p>",
                        "isValid": false
                    },
                    {
                        "id": 4145,
                        "content": "<p>Migrate the website to an Auto Scaling group of EC2 instances across multiple AZs and use an Elastic Load Balancer</p>",
                        "isValid": true
                    },
                    {
                        "id": 4146,
                        "content": "<p>Migrate the website to an Auto Scaling group of EC2 instances across a single AZ and use an Elastic Load Balancer</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1009,
            "attributes": {
                "createdAt": "2023-09-07T08:51:44.813Z",
                "updatedAt": "2023-09-07T08:51:44.813Z",
                "content": "<p>A Development team wants to instrument their code to provide more detailed information to AWS X-Ray than simple outgoing and incoming requests. This will generate large amounts of data, so the Development team wants to implement indexing so they can filter the data.</p><p>What should the Development team do to achieve this?</p>",
                "answerExplanation": "<p>AWS X-Ray makes it easy for developers to analyze the behavior of their production, distributed applications with end-to-end tracing capabilities. You can use X-Ray to identify performance bottlenecks, edge case errors, and other hard to detect issues.</p><p>When you instrument your application, the X-Ray SDK records information about incoming and outgoing requests, the AWS resources used, and the application itself. You can add other information to the segment document as annotations and metadata. Annotations and metadata are aggregated at the trace level and can be added to any segment or subsegment.</p><p><strong>Annotations</strong> are simple key-value pairs that are indexed for use with <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html\">filter expressions</a>. Use annotations to record data that you want to use to group traces in the console, or when calling the <a href=\"https://docs.aws.amazon.com/xray/latest/api/API_GetTraceSummaries.html\">GetTraceSummaries</a> API. X-Ray indexes up to 50 annotations per trace.</p><p><strong>Metadata</strong> are key-value pairs with values of any type, including objects and lists, but that are not indexed. Use metadata to record data you want to store in the trace but don't need to use for searching traces.</p><p>You can view annotations and metadata in the segment or subsegment details in the X-Ray console. </p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_15-37-42-93de9f631183ad28f0a924b3852dd797.jpg\"></p><p>In this scenario, we need to add annotations to the segment document so that the data that needs to be filtered is indexed.</p><p><strong>CORRECT: </strong>\"Add annotations to the segment document\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Add metadata to the segment document\" is incorrect as metadata is not indexed for filtering.</p><p><strong>INCORRECT:</strong> \"Configure the necessary X-Ray environment variables\" is incorrect as this will not result in indexing of the required data.</p><p><strong>INCORRECT:</strong> \"Install required plugins for the appropriate AWS SDK\" is incorrect as there are no plugin requirements for the AWS SDK to support this solution as the annotations feature is available in AWS X-Ray.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations\">https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 4147,
                        "content": "<p>Add annotations to the segment document</p>",
                        "isValid": true
                    },
                    {
                        "id": 4148,
                        "content": "<p>Configure the necessary X-Ray environment variables</p>",
                        "isValid": false
                    },
                    {
                        "id": 4149,
                        "content": "<p>Install required plugins for the appropriate AWS SDK</p>",
                        "isValid": false
                    },
                    {
                        "id": 4150,
                        "content": "<p>Add metadata to the segment document</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1010,
            "attributes": {
                "createdAt": "2023-09-07T08:51:44.885Z",
                "updatedAt": "2023-09-07T08:51:44.885Z",
                "content": "<p>An application that is being migrated to AWS and refactored requires a storage service. The storage service should provide a standards-based REST web service interface and store objects based on keys.</p><p>Which AWS service would be MOST suitable?</p>",
                "answerExplanation": "<p>Amazon S3 is object storage built to store and retrieve any amount of data from anywhere on the Internet. Amazon S3 uses standards-based REST and SOAP interfaces designed to work with any internet-development toolkit.</p><p>Amazon S3 is a simple key-based object store. The key is the name of the object and the value is the actual data itself. Keys can be any string, and they can be constructed to mimic hierarchical attributes.</p><p><strong>CORRECT: </strong>\"Amazon S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon DynamoDB\" is incorrect. DynamoDB is a key/value database service that provides tables to store your data. This is not the most suitable solution for this requirement as the cost will be higher and there are more design considerations that need to be addressed.</p><p><strong>INCORRECT:</strong> \"Amazon EBS\" is incorrect as this is a block-based storage system with which you attach volumes to Amazon EC2 instances. It is not a key-based object storage system.</p><p><strong>INCORRECT:</strong> \"Amazon EFS\" is incorrect as this is a filesystem that you mount to Amazon EC2 instances, it is also not a key-based object storage system.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 4151,
                        "content": "<p>Amazon S3</p>",
                        "isValid": true
                    },
                    {
                        "id": 4152,
                        "content": "<p>Amazon DynamoDB</p>",
                        "isValid": false
                    },
                    {
                        "id": 4153,
                        "content": "<p>Amazon EFS</p>",
                        "isValid": false
                    },
                    {
                        "id": 4154,
                        "content": "<p>Amazon EBS</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1011,
            "attributes": {
                "createdAt": "2023-09-07T08:51:44.955Z",
                "updatedAt": "2023-09-07T08:51:44.955Z",
                "content": "<p>A company use Amazon CloudFront to deliver application content to users around the world. A Developer has made an update to some files in the origin however users have reported that they are still getting the old files.</p><p>How can the Developer ensure that the old files are replaced in the cache with the LEAST disruption?</p>",
                "answerExplanation": "<p>If you need to remove files from CloudFront edge caches before they expire you can invalidate the files from the edge caches. To invalidate files, you can specify either the path for individual files or a path that ends with the * wildcard, which might apply to one file or to many, as shown in the following examples:</p><p> â€¢ /images/image1.jpg</p><p> â€¢ /images/image*</p><p> â€¢ /images/*</p><p>You can submit a specified number of invalidation paths each month for free. If you submit more than the allotted number of invalidation paths in a month, you pay a fee for each invalidation path that you submit.</p><p><strong>CORRECT: </strong>\"Invalidate the files from the edge caches\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a new origin with the new files and remove the old origin\" is incorrect as this would be more disruptive and costly as the entire cache would need to be updated.</p><p><strong>INCORRECT:</strong> \"Disable the CloudFront distribution and enable it again to update all the edge locations\" is incorrect as this will cause an outage (disruption) and will not replace files that have not yet expired.</p><p><strong>INCORRECT:</strong> \"Add code to Lambda@Edge that updates the files in the cache\" is incorrect as thereâ€™s no value in running code in Lambda@Edge to update the files. Instead the files in the cache can be invalidated.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 4155,
                        "content": "<p>Create a new origin with the new files and remove the old origin</p>",
                        "isValid": false
                    },
                    {
                        "id": 4156,
                        "content": "<p>Disable the CloudFront distribution and enable it again to update all the edge locations</p>",
                        "isValid": false
                    },
                    {
                        "id": 4157,
                        "content": "<p>Invalidate the files from the edge caches</p>",
                        "isValid": true
                    },
                    {
                        "id": 4158,
                        "content": "<p>Add code to Lambda@Edge that updates the files in the cache</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1012,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.030Z",
                "updatedAt": "2023-09-07T08:51:45.030Z",
                "content": "<p>A manufacturing company is creating a new RESTful API that their customers can use to query the status of orders. The endpoint for customer queries will be https://www.manufacturerdomain.com/status/customerID</p><p>Which of the following application designs will meet the requirements? (Select TWO.)</p>",
                "answerExplanation": "<p>This scenario includes a web application that will use RESTful API calls to determine the status of orders and dynamically return the results back to the companyâ€™s customers. Therefore, the two best options are as per below:</p><p> â€¢ Amazon API Gateway; AWS Lambda â€“ this choice includes API Gateway which is provides managed REST APIs and Lambda which can run the backend code for the application. This is a good solution for this scenario.</p><p> â€¢ Elastic Load Balancing; Amazon EC2 â€“ with this choice the ELB can load balance to one or more EC2 instances which can run the RESTful APIs and compute functions. This is also a good choice but could be more costly (operationally and financially).</p><p>None of the other options provide a workable solution for this scenario.</p><p><strong>CORRECT: </strong>\"Elastic Load Balancing; Amazon EC2\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Amazon API Gateway; AWS Lambda\" is a correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon SQS; Amazon SNS\" is incorrect as these services are used for queuing and sending notifications. They are not suitable for hosting a REST API.</p><p><strong>INCORRECT:</strong> \"Amazon ElastiCache; Amazon Elacticsearch Service\" is incorrect as ElastiCache is an in-memory caching service and Elasticsearch is used for searching. These do not provide a suitable solution for this scenario.</p><p><strong>INCORRECT:</strong> \"Amazon S3; Amazon CloudFront\" is incorrect as though you can host a static website on Amazon S3 with CloudFront caching the content, this is a static website only and you cannot host an API.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/ec2/features/\">https://aws.amazon.com/ec2/features/</a></p><p><a href=\"https://aws.amazon.com/elasticloadbalancing/\">https://aws.amazon.com/elasticloadbalancing/</a></p><p><a href=\"https://aws.amazon.com/api-gateway/features/\">https://aws.amazon.com/api-gateway/features/</a></p><p><a href=\"https://aws.amazon.com/lambda/features/\">https://aws.amazon.com/lambda/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
                "options": [
                    {
                        "id": 4159,
                        "content": "<p>Amazon SQS; Amazon SNS</p>",
                        "isValid": false
                    },
                    {
                        "id": 4160,
                        "content": "<p>Amazon S3; Amazon CloudFront</p>",
                        "isValid": false
                    },
                    {
                        "id": 4161,
                        "content": "<p>Amazon ElastiCache; Amazon Elacticsearch Service</p>",
                        "isValid": false
                    },
                    {
                        "id": 4162,
                        "content": "<p>Amazon API Gateway; AWS Lambda</p>",
                        "isValid": true
                    },
                    {
                        "id": 4163,
                        "content": "<p>Elastic Load Balancing; Amazon EC2</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 1013,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.105Z",
                "updatedAt": "2023-09-07T08:51:45.105Z",
                "content": "<p>A company has released a new application on AWS. The company are concerned about security and require a tool that can automatically assess applications for exposure, vulnerabilities, and deviations from best practices.</p><p>Which AWS service should they use?</p>",
                "answerExplanation": "<p>Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.</p><p>After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity. These findings can be reviewed directly or as part of detailed assessment reports which are available via the Amazon Inspector console or API.</p><p><strong>CORRECT: </strong>\"Amazon Inspector\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Shield\" is incorrect as this service is used to protect from distributed denial of service (DDoS) attacks.</p><p><strong>INCORRECT:</strong> \"AWS WAF\" is incorrect as this is a web application firewall.</p><p><strong>INCORRECT:</strong> \"AWS Secrets Manager\" is incorrect as this service is used to store secure secrets.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-inspector/\">https://digitalcloud.training/amazon-inspector/</a></p>",
                "options": [
                    {
                        "id": 4164,
                        "content": "<p>AWS Secrets Manager</p>",
                        "isValid": false
                    },
                    {
                        "id": 4165,
                        "content": "<p>AWS WAF</p>",
                        "isValid": false
                    },
                    {
                        "id": 4166,
                        "content": "<p>AWS Shield</p>",
                        "isValid": false
                    },
                    {
                        "id": 4167,
                        "content": "<p>Amazon Inspector</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 1014,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.180Z",
                "updatedAt": "2023-09-07T08:51:45.180Z",
                "content": "<p>A Developer is creating a serverless website with content that includes HTML files, images, videos, and JavaScript (client-side scripts).</p><p>Which combination of services should the Developer use to create the website?</p>",
                "answerExplanation": "<p>You can use Amazon S3 to host a static website. On a <em>static</em> website, individual webpages include static content. They might also contain client-side scripts.</p><p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.</p><p>To get content closer to users for better performance you can also use Amazon CloudFront in front of the S3 static website. To serve a static website hosted on Amazon S3, you can deploy a CloudFront distribution using one of these configurations:</p><p> â€¢ Using a REST API endpoint as the origin with access restricted by an origin access identity (OAI)</p><p> â€¢ Using a website endpoint as the origin with anonymous (public) access allowed</p><p> â€¢ Using a website endpoint as the origin with access restricted by a Referer header</p><p>Therefore, the combination of services should be Amazon S3 and Amazon CloudFront</p><p><strong>CORRECT: </strong>\"Amazon S3 and Amazon CloudFront\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 and Amazon ElastiCache\" is incorrect. The website is supposed to be serverless and neither of these services are serverless as they both use Amazon EC2 instances.</p><p><strong>INCORRECT:</strong> \"Amazon ECS and Redis\" is incorrect. These services are also not serverless. Also Redis is an in-memory cache and is typically placed in front of a database, not a Docker container.</p><p><strong>INCORRECT:</strong> \"AWS Lambda and Amazon API Gateway\" is incorrect. These are both serverless services however for serving content such as HTML files, images, videos, and client-side JavaScript, Amazon S3 and CloudFront are more appropriate.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 4168,
                        "content": "<p>Amazon ECS and Redis</p>",
                        "isValid": false
                    },
                    {
                        "id": 4169,
                        "content": "<p>Amazon EC2 and Amazon ElastiCache</p>",
                        "isValid": false
                    },
                    {
                        "id": 4170,
                        "content": "<p>AWS Lambda and Amazon API Gateway</p>",
                        "isValid": false
                    },
                    {
                        "id": 4171,
                        "content": "<p>Amazon S3 and Amazon CloudFront</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 1015,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.251Z",
                "updatedAt": "2023-09-07T08:51:45.251Z",
                "content": "<p>An organization has encrypted a large quantity of data. To protect their data encryption keys they are planning to use envelope encryption. Which of the following processes is a correct implementation of envelope encryption?</p>",
                "answerExplanation": "<p>When you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. <em>Envelope encryption</em> is the practice of encrypting plaintext data with a data key, and then encrypting the data key under another key.</p><p>You can even encrypt the data encryption key under another encryption key and encrypt that encryption key under another encryption key. But, eventually, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext key encryption key is known as the <em>master key</em>.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-24_05-25-56-d191973bf9108406921e805f3a556b19.jpg\"></p><p>Envelope encryption offers several benefits:</p><p><strong> </strong>â€¢ <strong>Protecting data keys</strong></p><p>When you encrypt a data key, you don't have to worry about storing the encrypted data key, because the data key is inherently protected by encryption. You can safely store the encrypted data key alongside the encrypted data.</p><p><strong> </strong>â€¢ <strong>Encrypting the same data under multiple master keys</strong></p><p>Encryption operations can be time consuming, particularly when the data being encrypted are large objects. Instead of re-encrypting raw data multiple times with different keys, you can re-encrypt only the data keys that protect the raw data.</p><p><strong> </strong>â€¢ <strong>Combining the strengths of multiple algorithms</strong></p><p>In general, symmetric key algorithms are faster and produce smaller ciphertexts than public key algorithms. But public key algorithms provide inherent separation of roles and easier key management. Envelope encryption lets you combine the strengths of each strategy.</p><p>As described above, the process that should be implemented is to encrypt plaintext data with a data key and then encrypt the data key with a top-level plaintext master key.</p><p><strong>CORRECT: </strong>\"Encrypt plaintext data with a data key and then encrypt the data key with a top-level plaintext master key\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Encrypt plaintext data with a master key and then encrypt the master key with a top-level plaintext data key\" is incorrect as the master key is the top-level key.</p><p><strong>INCORRECT:</strong> \"Encrypt plaintext data with a data key and then encrypt the data key with a top-level encrypted master key\" is incorrect as the top-level master key must be unencrypted so it can be used to decrypt data.</p><p><strong>INCORRECT:</strong> \"Encrypt plaintext data with a master key and then encrypt the master key with a top-level encrypted data key\" is incorrect as the master key is the top-level key.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
                "options": [
                    {
                        "id": 4172,
                        "content": "<p>Encrypt plaintext data with a master key and then encrypt the master key with a top-level plaintext data key</p>",
                        "isValid": false
                    },
                    {
                        "id": 4173,
                        "content": "<p>Encrypt plaintext data with a data key and then encrypt the data key with a top-level plaintext master key.</p>",
                        "isValid": true
                    },
                    {
                        "id": 4174,
                        "content": "<p>Encrypt plaintext data with a data key and then encrypt the data key with a top-level encrypted master key</p>",
                        "isValid": false
                    },
                    {
                        "id": 4175,
                        "content": "<p>Encrypt plaintext data with a master key and then encrypt the master key with a top-level encrypted data key</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1016,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.323Z",
                "updatedAt": "2023-09-07T08:51:45.323Z",
                "content": "<p>A developer is designing a web application that will be used by thousands of users. The users will sign up using their email addresses and the application will store attributes for each user.</p><p>Which service should the developer use to enable users to sign-up for the web application?</p>",
                "answerExplanation": "<p>A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers.</p><p>Whether your users sign in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK).</p><p>User pools provide:</p><p> â€¢ Sign-up and sign-in services.</p><p> â€¢ A built-in, customizable web UI to sign in users.</p><p> â€¢ Social sign-in with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as sign-in with SAML identity providers from your user pool.</p><p> â€¢ User directory management and user profiles.</p><p> â€¢ Security features such as multi-factor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.</p><p> â€¢ Customized workflows and user migration through AWS Lambda triggers.</p><p>After successfully authenticating a user, Amazon Cognito issues JSON web tokens (JWT) that you can use to secure and authorize access to your own APIs, or exchange for AWS credentials.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_04-03-31-dd57ec34d63cb91322d42ccafe12acb5.jpg\"></p><p>Therefore, an Amazon Cognito user pool is the best solution for enabling sign-up to the new web application.</p><p><strong>CORRECT: </strong>\"Amazon Cognito user pool\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon Cognito Sync\" is incorrect as it is used to synchronize user profile data across mobile devices and the web without requiring your own backend.</p><p><strong>INCORRECT:</strong> \"AWS Inspector\" is incorrect. Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.</p><p><strong>INCORRECT:</strong> \"AWS AppSync\" is incorrect. AWS AppSync simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one or more data sources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 4176,
                        "content": "<p>Amazon Cognito Sync</p>",
                        "isValid": false
                    },
                    {
                        "id": 4177,
                        "content": "<p>AWS Inspector</p>",
                        "isValid": false
                    },
                    {
                        "id": 4178,
                        "content": "<p>AWS AppSync</p>",
                        "isValid": false
                    },
                    {
                        "id": 4179,
                        "content": "<p>Amazon Cognito user pool</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 1017,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.393Z",
                "updatedAt": "2023-09-07T08:51:45.393Z",
                "content": "<p>A Developer is creating multiple AWS Lambda functions that will be using an external library that is not included in the standard Lambda libraries. What is the BEST way to make these libraries available to the functions?</p>",
                "answerExplanation": "<p>You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package.</p><p>Layers let you keep your deployment package small, which makes development easier. You can avoid errors that can occur when you install and package dependencies with your function code.</p><p>When a Lambda function configured with a Lambda layer is executed, AWS downloads any specified layers and extracts them to the /opt directory on the function execution environment. Each runtime then looks for a language-specific folder under the /opt directory.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_15-24-24-ac074566982b7f6154b7def4251c9219.jpg\"></p><p>One of the best practices for AWS Lambda functions is to minimize your deployment package size to its runtime necessities in order to reduce the amount of time that it takes for your deployment package to be downloaded and unpacked ahead of invocation.</p><p>Therefore, it is preferable to use layers to store the external libraries to optimize performance of the function. Using layers means that the external library will also be available to all of the Lambda functions that the Developer is creating.</p><p><strong>CORRECT: </strong>\"Create a layer in Lambda that includes the external library\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Include the external library with the function code\" is incorrect as you should not include an external library within the function code. Even if possible this would result in bloated code that could slow down execution time.</p><p><strong>INCORRECT:</strong> \"Create a deployment package that includes the external library\" is incorrect as the best practice is to minimize package sizes to runtime necessities. Also, this would require including the library in all function deployment packages whereas with layers we can create a single layer that is used by all functions.</p><p><strong>INCORRECT:</strong> \"Store the files in Amazon S3 and reference them from your function code\" is incorrect as this would likely result in increased latency of your function execution. Instead you should either package the library in the deployment package for your function or use layers (preferable in this scenario).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 4180,
                        "content": "<p>Create a deployment package that includes the external library</p>",
                        "isValid": false
                    },
                    {
                        "id": 4181,
                        "content": "<p>Create a layer in Lambda that includes the external library</p>",
                        "isValid": true
                    },
                    {
                        "id": 4182,
                        "content": "<p>Store the files in Amazon S3 and reference them from your function code</p>",
                        "isValid": false
                    },
                    {
                        "id": 4183,
                        "content": "<p>Include the external library with the function code</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1018,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.468Z",
                "updatedAt": "2023-09-07T08:51:45.468Z",
                "content": "<p>A company will be hiring a large number of Developers for a series of projects. The Develops will bring their own devices to work and the company want to ensure consistency in tooling. The Developers must be able to write, run, and debug applications with just a browser, without needing to install or maintain a local Integrated Development Environment (IDE).</p><p>Which AWS service should the Developers use?</p>",
                "answerExplanation": "<p>AWS Cloud9 is an integrated development environment, or <em>IDE</em>. The AWS Cloud9 IDE offers a rich code-editing experience with support for several programming languages and runtime debuggers, and a built-in terminal. It contains a collection of tools that you use to code, build, run, test, and debug software, and helps you release software to the cloud.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-24_05-19-10-e22e95422ef0b32370d09990a8071f3a.jpg\"></p><p>You access the AWS Cloud9 IDE through a web browser. You can configure the IDE to your preferences. You can switch color themes, bind shortcut keys, enable programming language-specific syntax coloring and code formatting, and more.</p><p><strong>CORRECT: </strong>\"AWS Cloud9\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodeCommit\" is incorrect. AWS CodeCommit is a fully-managed <a href=\"https://aws.amazon.com/devops/source-control/\">source control</a> service that hosts secure Git-based repositories. It is not an IDE.</p><p><strong>INCORRECT:</strong> \"AWS CodeDeploy\" is incorrect. CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.</p><p><strong>INCORRECT:</strong> \"AWS X-Ray\" is incorrect. AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cloud9/latest/user-guide/welcome.html\">https://docs.aws.amazon.com/cloud9/latest/user-guide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 4184,
                        "content": "<p>AWS CodeCommit</p>",
                        "isValid": false
                    },
                    {
                        "id": 4185,
                        "content": "<p>AWS CodeDeploy</p>",
                        "isValid": false
                    },
                    {
                        "id": 4186,
                        "content": "<p>AWS X-Ray</p>",
                        "isValid": false
                    },
                    {
                        "id": 4187,
                        "content": "<p>AWS Cloud9</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 1019,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.552Z",
                "updatedAt": "2023-09-07T08:51:45.552Z",
                "content": "<p>An independent software vendor (ISV) uses Amazon S3 and Amazon CloudFront to distribute software updates. They would like to provide their premium customers with access to updates faster. What is the MOST efficient way to distribute these updates only to the premium customers? (Select TWO.)</p>",
                "answerExplanation": "<p>To restrict access to content that you serve from Amazon S3 buckets, you create CloudFront signed URLs or signed cookies to limit access to files in your Amazon S3 bucket, and then you create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution. Then you configure permissions so that CloudFront can use the OAI to access and serve files to your users, but users can't use a direct URL to the S3 bucket to access a file there. Taking these steps help you maintain secure access to the files that you serve through CloudFront.</p><p><strong>CORRECT: </strong>\"Create a signed URL with access to the content and distribute it to the premium customers\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Create an origin access identity (OAI) and associate it with the distribution and configure permissions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a signed cookie and associate it with the Amazon S3 distribution\" is incorrect as you cannot associated signed cookies with Amazon S3 and a distribution is a CloudFront concept, not an S3 concept.</p><p><strong>INCORRECT:</strong> \"Use an access control list (ACL) on the Amazon S3 bucket to restrict access based on IP address\" is incorrect as you cannot restrict access to buckets by IP address when using an ACL.</p><p><strong>INCORRECT:</strong> \"Use an IAM policy to restrict access to the content using a condition attribute and specify the IP addresses of the premium customers \" is incorrect. You can restrict access to buckets using policy statements with conditions based on source IP address. However, this is cumbersome to manage as IP addresses change (and you need to know all your customerâ€™s IPs in the first place). Also, because the content is being cached on CloudFront, this would not stop others accessing it anyway.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html#private-content-choosing-canned-custom-policy\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html#private-content-choosing-canned-custom-policy</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
                "options": [
                    {
                        "id": 4188,
                        "content": "<p>Use an access control list (ACL) on the Amazon S3 bucket to restrict access based on IP address</p>",
                        "isValid": false
                    },
                    {
                        "id": 4189,
                        "content": "<p>Create a signed URL with access to the content and distribute it to the premium customers</p>",
                        "isValid": true
                    },
                    {
                        "id": 4190,
                        "content": "<p>Create an origin access identity (OAI) and associate it with the distribution and configure permissions</p>",
                        "isValid": true
                    },
                    {
                        "id": 4191,
                        "content": "<p>Create a signed cookie and associate it with the Amazon S3 distribution</p>",
                        "isValid": false
                    },
                    {
                        "id": 4192,
                        "content": "<p>Use an IAM policy to restrict access to the content using a condition attribute and specify the IP addresses of the premium customers</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1020,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.621Z",
                "updatedAt": "2023-09-07T08:51:45.621Z",
                "content": "<p>A Developer wants the ability to roll back to a previous version of an AWS Lambda function in the event of errors caused by a new deployment.</p><p>How can the Developer achieve this with MINIMAL impact on users?</p>",
                "answerExplanation": "<p>You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-24_06-12-16-b2717351c42841f72690434d11c2ed29.jpg\"></p><p>You can update the versions that an alias points to and you can also add multiple versions and use weightings to direct a percentage of traffic to a new version of the code.</p><p>For this example the best choice is to use an alias and direct 10% of traffic to the new version. If errors are encountered the rollback is easy (change the pointer in the alias) and a minimum of impact has been made to users.</p><p><strong>CORRECT: </strong>\"Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to direct 10% of users to the newly deployed version. If too many errors are encountered, send 100% of traffic to the previous version\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to use the newly deployed version. If too many errors are encountered, point the alias back to the previous version\" is incorrect. This is not the best answer as 100% of the users will be directed to the new version so if any errors do occur more users will be affected.</p><p><strong>INCORRECT:</strong> \"Change the application to use a version ARN that points to the latest published version. Deploy the new version of the code. Update the application to point to the ARN of the new version of the code. If too many errors are encountered, point the application back to the ARN of the previous version\" is incorrect. This answer involves a lot of updates to the application that could be completely avoided by using an alias.</p><p><strong>INCORRECT:</strong> \"Change the application to use the $LATEST version. Update and save code. If too many errors are encountered, modify and save the code\" is incorrect as this is against best practice. The $LATEST is the unpublished version of the code where you make changes. You should publish to a version when the code is ready for deployment.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 4193,
                        "content": "<p>Change the application to use a version ARN that points to the latest published version. Deploy the new version of the code. Update the application to point to the ARN of the new version of the code. If too many errors are encountered, point the application back to the ARN of the previous version</p>",
                        "isValid": false
                    },
                    {
                        "id": 4194,
                        "content": "<p>Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to direct 10% of users to the newly deployed version. If too many errors are encountered, send 100% of traffic to the previous version</p>",
                        "isValid": true
                    },
                    {
                        "id": 4195,
                        "content": "<p>Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to use the newly deployed version. If too many errors are encountered, point the alias back to the previous version</p>",
                        "isValid": false
                    },
                    {
                        "id": 4196,
                        "content": "<p>Change the application to use the $LATEST version. Update and save code. If too many errors are encountered, modify and save the code</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1021,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.698Z",
                "updatedAt": "2023-09-07T08:51:45.698Z",
                "content": "<p>An application will generate thumbnails from objects uploaded to an Amazon S3 bucket. The Developer has created the bucket configuration and the AWS Lambda function and has formulated the following AWS CLI command:</p><p><code>aws lambda add-permission --function-name CreateThumbnail --principal s3.amazonaws.com --statement-id s3invoke --action \"lambda:InvokeFunction\" --source-arn arn:aws:s3:::digitalcloudbucket-source --source-account 523107438921</code></p><p>What will be achieved by running the AWS CLI command?</p>",
                "answerExplanation": "<p>In this scenario the Developer is using an AWS Lambda function to process images that are uploaded to an Amazon S3 bucket. The AWS Lambda function has been created and the notification settings on the bucket have been configured. The last thing to do is to grant permissions for the Amazon S3 service principal to invoke the function.</p><p>The Lambda CLI add-permission command grants the Amazon S3 service principal (s3.amazonaws.com) permissions to perform the lambda:InvokeFunction action.</p><p><strong>CORRECT: </strong>\"The Amazon S3 service principal (s3.amazonaws.com) will be granted permissions to perform the lambda:InvokeFunction action\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The Lambda function CreateThumbnail will be granted permissions to access the objects in the digitalcloudbucket-source bucket\" is incorrect as the CLI command grants S3 the ability to execute the Lambda function.</p><p><strong>INCORRECT:</strong> \"The Amazon S3 service principal (s3.amazonaws.com) will be granted permissions to perform the create an event-source mapping with the digitalcloudbucket-source bucket\" is incorrect as event source mappings are created with services such as Kinesis, DynamoDB, and SQS.</p><p><strong>INCORRECT:</strong> \"A Lambda function will be created called CreateThumbnail with an Amazon SNS event source mapping that executes the function when objects are uploaded\" is incorrect. This solution does not use Amazon SNS, the S3 notification invokes the Lambda function directly.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 4197,
                        "content": "<p>The Lambda function<code> CreateThumbnail</code> will be granted permissions to access the objects in the <code>digitalcloudbucket-source</code> bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 4198,
                        "content": "<p>The Amazon S3 service principal (s3.amazonaws.com) will be granted permissions to perform the create an event-source mapping with the <code>digitalcloudbucket-source</code> bucket</p>",
                        "isValid": false
                    },
                    {
                        "id": 4199,
                        "content": "<p>A Lambda function will be created called CreateThumbnail with an Amazon SNS event source mapping that executes the function when objects are uploaded</p>",
                        "isValid": false
                    },
                    {
                        "id": 4200,
                        "content": "<p> The Amazon S3 service principal (s3.amazonaws.com) will be granted permissions to perform the <code>lambda:InvokeFunction</code> action</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 1022,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.776Z",
                "updatedAt": "2023-09-07T08:51:45.776Z",
                "content": "<p>A company has sensitive data that must be encrypted. The data is made up of 1 GB objects and there is a total of 150 GB of data.</p><p>What is the BEST approach for a Developer to encrypt the data using AWS KMS?</p>",
                "answerExplanation": "<p>To encrypt large quantities of data with the AWS Key Management Service (KMS), you must use a data encryption key rather than a customer master keys (CMK). This is because a CMK can only encrypt up to 4KB in a single operation and in this scenario the objects are 1 GB in size.</p><p>To create a data key, call the <a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\">GenerateDataKey</a> operation. AWS KMS uses the CMK that you specify to generate a data key. The operation returns a plaintext copy of the data key and a copy of the data key encrypted under the CMK. The following image shows this operation.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_04-48-59-5b801da74922543cd59e4b2aa848c176.png\"></p><p>AWS KMS cannot use a data key to encrypt data. But you can use the data key outside of KMS, such as by using OpenSSL or a cryptographic library like the <a href=\"https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/\">AWS Encryption SDK</a>. Data can then be encrypted using the plaintext data key as depicted below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_04-49-28-13b778a7a392b76ca563bb3127cbe050.png\"></p><p>Therefore, the Developer should make a GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key, and then use the plaintext key to encrypt the data.</p><p><strong>CORRECT: </strong>\"Make a GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key. Use the plaintext key to encrypt the data\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK)\" is incorrect as you cannot use a CMK to encrypt objects over 4 KB in size.</p><p><strong>INCORRECT:</strong> \"Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material\" is incorrect as you cannot use a CMK to encrypt objects over 4 KB in size.</p><p><strong>INCORRECT:</strong> \"Make a GenerateDataKeyWithoutPlaintext API call that returns an encrypted copy of a data key. Use the encrypted key to encrypt the data\" is incorrect as you need to encrypt data with a plaintext data key.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#data-keys\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#data-keys</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
                "options": [
                    {
                        "id": 4201,
                        "content": "<p>Make a <code>GenerateDataKey</code> API call that returns a plaintext key and an encrypted copy of a data key. Use the plaintext key to encrypt the data</p>",
                        "isValid": true
                    },
                    {
                        "id": 4202,
                        "content": "<p>Make an <code>Encrypt</code> API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material</p>",
                        "isValid": false
                    },
                    {
                        "id": 4203,
                        "content": "<p>Make a <code>GenerateDataKeyWithoutPlaintext</code> API call that returns an encrypted copy of a data key. Use the encrypted key to encrypt the data</p>",
                        "isValid": false
                    },
                    {
                        "id": 4204,
                        "content": "<p>Make an <code>Encrypt</code> API call to encrypt the plaintext data as ciphertext using a customer master key (CMK)</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1023,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.854Z",
                "updatedAt": "2023-09-07T08:51:45.854Z",
                "content": "<p>A company has hired a team of remote Developers. The Developers need to work programmatically with AWS resources from their laptop computers.</p><p>Which security components MUST the Developers use to authenticate? (Select TWO.)</p>",
                "answerExplanation": "<p>Access keys consist of two parts: an access key ID (for example, AKIAIOSFODNN7EXAMPLE) and a secret access key (for example, wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY). You use access keys to sign programmatic requests that you make to AWS if you use AWS CLI commands (using the SDKs) or using AWS API operations.</p><p>For this scenario, the Developers will be connecting programmatically to AWS resources and will therefore be required to use an access key ID and secret access key.</p><p><strong>CORRECT: </strong>\"Access key ID\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Secret access key\" is a correct answer.</p><p><strong>INCORRECT:</strong> \"Console password \" is incorrect as this is used for accessing AWS via the console with an IAM user ID and is not used for programmatic access.</p><p><strong>INCORRECT:</strong> \"IAM user ID\" is incorrect as the IAM user ID is used with the password (see above) to access the AWS management console.</p><p><strong>INCORRECT:</strong> \"MFA device\" is incorrect as this is not required for making programmatic requests but can be added for additional security</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html\">https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
                "options": [
                    {
                        "id": 4205,
                        "content": "<p>Access key ID</p>",
                        "isValid": true
                    },
                    {
                        "id": 4206,
                        "content": "<p>Secret access key</p>",
                        "isValid": true
                    },
                    {
                        "id": 4207,
                        "content": "<p>Console password</p>",
                        "isValid": false
                    },
                    {
                        "id": 4208,
                        "content": "<p>IAM user ID</p>",
                        "isValid": false
                    },
                    {
                        "id": 4209,
                        "content": "<p>MFA device</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1024,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.925Z",
                "updatedAt": "2023-09-07T08:51:45.925Z",
                "content": "<p>A company has a website that is developed in PHP and WordPress and is launched using AWS Elastic Beanstalk. There is a new version of the website that needs to be deployed in the Elastic Beanstalk environment. The company cannot tolerate having the website offline if an update fails. Deployments must have minimal impact and rollback as soon as possible.</p><p>What deployment method should be used?</p>",
                "answerExplanation": "<p>AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.</p><p>All at once:</p><p> â€¢ Deploys the new version to all instances simultaneously.</p><p>Rolling:</p><p> â€¢ Update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy (downtime for 1 bucket at a time).</p><p>Rolling with additional batch:</p><p> â€¢ Like Rolling but launches new instances in a batch ensuring that there is full availability.</p><p>Immutable:</p><p> â€¢ Launches new instances in a new ASG and deploys the version update to these instances before swapping traffic to these instances once healthy.</p><p> â€¢ Zero downtime.</p><p>Blue / Green deployment:</p><p> â€¢ Zero downtime and release facility.</p><p> â€¢ Create a new â€œstageâ€ environment and deploy updates there.</p><p>For this scenario, the best choice is Immutable as this is the safest option when you cannot tolerate downtime and also provides a simple way of rolling back should an issue occur.</p><p><strong>CORRECT: </strong>\"Immutable\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"All at once\" is incorrect as this will take all instances down and cause a total outage.</p><p><strong>INCORRECT:</strong> \"Snapshots\" is incorrect as this is not a deployment method you can use with Elastic Beanstalk.</p><p><strong>INCORRECT:</strong> \"Rolling\" is incorrect as this will reduce the capacity of the application and it is more difficult to roll back as you must redeploy the old version to the instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
                "options": [
                    {
                        "id": 4210,
                        "content": "<p>Snapshots</p>",
                        "isValid": false
                    },
                    {
                        "id": 4211,
                        "content": "<p>All at once</p>",
                        "isValid": false
                    },
                    {
                        "id": 4212,
                        "content": "<p>Rolling</p>",
                        "isValid": false
                    },
                    {
                        "id": 4213,
                        "content": "<p>Immutable</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 1025,
            "attributes": {
                "createdAt": "2023-09-07T08:51:45.996Z",
                "updatedAt": "2023-09-07T08:51:45.996Z",
                "content": "<p>A Developer created an AWS Lambda function and then attempted to add an on failure destination but received the following error:</p><p><code>The function's execution role does not have permissions to call SendMessage on arn:aws:sqs:us-east-1:515148212435:FailureDestination</code></p><p>How can the Developer resolve this issue MOST securely?</p>",
                "answerExplanation": "<p>The Lambda function needs the privileges to use the SendMessage API action on the Amazon SQS queue. The permissions should be assigned to the functionâ€™s execution role. The AWSLambdaSQSQueueExecutionRole AWS managed policy cannot be used as this policy does not provide the SendMessage action.</p><p>The Developer should therefore create a customer managed policy with read/write permissions to SQS and attach the policy to the functionâ€™s execution role.</p><p><strong>CORRECT: </strong>\"Create a customer managed policy with all read/write permissions to SQS and attach the policy to the functionâ€™s execution role\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Add the AWSLambdaSQSQueueExecutionRole AWS managed policy to the functionâ€™s execution role\" is incorrect as this does not provide the necessary permissions.</p><p><strong>INCORRECT:</strong> \"Add a permissions policy to the SQS queue allowing the SendMessage action and specify the AWS account number\" is incorrect as this would allow any resource in the AWS account to write to the queue which is less secure.</p><p><strong>INCORRECT:</strong> \"Add the Lambda function to a group with administrative privileges\" is incorrect as you cannot add a Lambda function to an IAM group.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
                "options": [
                    {
                        "id": 4214,
                        "content": "<p>Add the AWSLambdaSQSQueueExecutionRole AWS managed policy to the functionâ€™s execution role</p>",
                        "isValid": false
                    },
                    {
                        "id": 4215,
                        "content": "<p>Add the Lambda function to a group with administrative privileges</p>",
                        "isValid": false
                    },
                    {
                        "id": 4216,
                        "content": "<p>Create a customer managed policy with all read/write permissions to SQS and attach the policy to the functionâ€™s execution role</p>",
                        "isValid": true
                    },
                    {
                        "id": 4217,
                        "content": "<p>Add a permissions policy to the SQS queue allowing the SendMessage action and specify the AWS account number</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1026,
            "attributes": {
                "createdAt": "2023-09-07T08:51:46.067Z",
                "updatedAt": "2023-09-07T08:51:46.067Z",
                "content": "<p>A company is building an application to track athlete performance using an Amazon DynamoDB table. Each item in the table is identified by a partition key (user_id) and a sort key (sport_name). The table design is shown below:</p><p> â€¢ Partition key: <code>user_id</code></p><p> â€¢ Sort Key: <code>sport_name</code></p><p> â€¢ Attributes: <code>score</code>, <code>score_datetime</code></p><p>A Developer is asked to write a leaderboard application to display the top performers (<code>user_id</code>) based on the score for each <code>sport_name.</code></p><p>What process will allow the Developer to extract results MOST efficiently from the DynamoDB table?</p>",
                "answerExplanation": "<p>The Developer needs to be able to sort the scores for each sport and then extract the highest performing athletes. In this case BOTH the partition key and sort key must be different which means a Global Secondary index is required (as a Local Secondary index only has a different sort key). The GSI would be configured as follows:</p><p>Â· Partition key: sport_name</p><p>Â· Sort Key: score</p><p>The results will then be listed in order of the highest score for each sport which is exactly what is required.</p><p><strong>CORRECT: </strong>\"Create a global secondary index with a partition key of sport_name and a sort key of score, and get the results\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a local secondary index with a primary key of sport_name and a sort key of score and get the results based on the score attribute\" is incorrect as an LSI cannot be created after table creation and also only has a different sort key, not a different partition key.</p><p><strong>INCORRECT:</strong> \"Use a DynamoDB query operation with the key attributes of user_id and sport_name and order the results based on the score attribute\" is incorrect as this is less efficient compared to using a GSI.</p><p><strong>INCORRECT:</strong> \"Use a DynamoDB scan operation to retrieve scores and user_id based on sport_name, and order the results based on the score attribute\" is incorrect as this is the least efficient option as a scan returns every item in the table (more RCUs).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 4218,
                        "content": "<p>Create a global secondary index with a partition key of <code>sport_name</code> and a sort key of <code>score</code>, and get the results</p>",
                        "isValid": true
                    },
                    {
                        "id": 4219,
                        "content": "<p>Create a local secondary index with a primary key of <code>sport_name</code> and a sort key of <code>score</code> and get the results based on the <code>score</code> attribute</p>",
                        "isValid": false
                    },
                    {
                        "id": 4220,
                        "content": "<p>Use a DynamoDB <code>scan</code> operation to retrieve scores and <code>user_id</code> based on <code>sport_name</code>, and order the results based on the <code>score</code> attribute</p>",
                        "isValid": false
                    },
                    {
                        "id": 4221,
                        "content": "<p>Use a DynamoDB query operation with the key attributes of <code>user_id</code> and <code>sport_name</code> and order the results based on the <code>score</code> attribute</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1027,
            "attributes": {
                "createdAt": "2023-09-07T08:51:46.139Z",
                "updatedAt": "2023-09-07T08:51:46.139Z",
                "content": "<p>An Auto Scaling Group (ASG) of Amazon EC2 instances is being created for processing messages from an Amazon SQS queue. To ensure the EC2 instances are cost-effective a Developer would like to configure the ASG to maintain aggregate CPU utilization at 70%.</p><p>Which type of scaling policy should the Developer choose?</p>",
                "answerExplanation": "<p>With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to the changes in the metric due to a changing load pattern.</p><p>For example, you can use target tracking scaling to:</p><p> â€¢ Configure a target tracking scaling policy to keep the average aggregate CPU utilization of your Auto Scaling group at 40 percent.</p><p> â€¢ Configure a target tracking scaling policy to keep the request count per target of your Elastic Load Balancing target group at 1000 for your Auto Scaling group.</p><p>The target tracking scaling policy is therefore the best choice for this scenario.</p><p><strong>CORRECT: </strong>\"Target Tracking Scaling Policy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Step Scaling Policy\" is incorrect. (explanation below)</p><p><strong>INCORRECT:</strong> \"Simple Scaling Policy\" is incorrect. (explanation below)</p><p>With step scaling and simple scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process. You also define how your Auto Scaling group should be scaled when a threshold is in breach for a specified number of evaluation periods.</p><p><strong>INCORRECT:</strong> \"Scheduled Scaling Policy\" is incorrect as this is used to schedule a scaling action at a specific time and date rather than dynamically adjusting according to load.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
                "options": [
                    {
                        "id": 4222,
                        "content": "<p>Target Tracking Scaling Policy</p>",
                        "isValid": true
                    },
                    {
                        "id": 4223,
                        "content": "<p>Simple Scaling Policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 4224,
                        "content": "<p>Scheduled Scaling Policy</p>",
                        "isValid": false
                    },
                    {
                        "id": 4225,
                        "content": "<p>Step Scaling Policy</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1028,
            "attributes": {
                "createdAt": "2023-09-07T08:51:46.212Z",
                "updatedAt": "2023-09-07T08:51:46.212Z",
                "content": "<p>A Developer is publishing custom metrics for Amazon EC2 using the Amazon CloudWatch CLI. The Developer needs to add further context to the metrics being published by organizing them by EC2 instance and Auto Scaling Group.</p><p>What should the Developer add to the CLI command when publishing the metrics using <code>put-metric-data&nbsp; </code> </p>",
                "answerExplanation": "<p>You can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console.</p><p>CloudWatch stores data about a metric as a series of data points. Each data point has an associated time stamp. You can even publish an aggregated set of data points called a <em>statistic set</em>.</p><p>In custom metrics, the --dimensions parameter is common. A dimension further clarifies what the metric is and what data it stores. You can have up to 10 dimensions in one metric, and each dimension is defined by a name and value pair.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-24_05-53-04-17a13972647f67ddd736aa223dbbd004.jpg\"></p><p>As you can see in the above example there are two dimensions associated with the EC2 namespace. These organize the metrics by Auto Scaling Group and Per-Instance metrics. Therefore the Developer should the --dimensions parameter.</p><p><strong>CORRECT: </strong>\"The <code>--dimensions parameter</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The <code>--namespace parameter</code>\" is incorrect as a <em>namespace</em> is a container for CloudWatch metrics. To add further context the Developer should use a <em>dimension</em>.</p><p><strong>INCORRECT:</strong> \"The <code>--statistic-values parameter</code>\" is incorrect as this is a parameter associated with the publishing of statistic sets.</p><p><strong>INCORRECT:</strong> \"The <code>--metric-name parameter</code>\" is incorrect as this simply provides the name for the metric that is being published.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
                "options": [
                    {
                        "id": 4226,
                        "content": "<p>The <code>--metric-name</code> parameter</p>",
                        "isValid": false
                    },
                    {
                        "id": 4227,
                        "content": "<p>The <code>--namespace</code> parameter</p>",
                        "isValid": false
                    },
                    {
                        "id": 4228,
                        "content": "<p>1. The <code>--dimensions</code> parameter</p>",
                        "isValid": true
                    },
                    {
                        "id": 4229,
                        "content": "<p>The <code>--statistic-values</code> parameter</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1029,
            "attributes": {
                "createdAt": "2023-09-07T08:51:46.286Z",
                "updatedAt": "2023-09-07T08:51:46.286Z",
                "content": "<p>A website delivers images stored in an Amazon S3 bucket. The site uses Amazon Cognito-enabled and guest users without logins need to be able to view the images from the S3 bucket..</p><p>How can a Developer enable access for guest users to the AWS resources?</p>",
                "answerExplanation": "<p>Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token. An identity pool is a store of user identity data specific to your account.</p><p>&nbsp; </p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_04-53-37-344910a01dadf5b74144f5a17731b14d.png\"></p><p>Amazon Cognito identity pools support both authenticated and unauthenticated identities. Authenticated identities belong to users who are authenticated by any supported identity provider. Unauthenticated identities typically belong to guest users.</p><p> â€¢ To configure authenticated identities with a public login provider, see <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/external-identity-providers.html\">Identity Pools (Federated Identities) External Identity Providers</a>.</p><p> â€¢ To configure your own backend authentication process, see <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html\">Developer Authenticated Identities (Identity Pools)</a>.</p><p>Therefore, the Developer should create a new identity pool, enable access to unauthenticated identities, and grant access to AWS resources.</p><p><strong>CORRECT: </strong>\"Create a new identity pool, enable access to unauthenticated identities, and grant access to AWS resources\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a blank user ID in a user pool, add to the user group, and grant access to AWS resources\" is incorrect as you must use identity pools for unauthenticated users.</p><p><strong>INCORRECT:</strong> \"Create a new user pool, enable access to unauthenticated identities, and grant access to AWS resources\" is incorrect as you must use identity pools for unauthenticated users.</p><p><strong>INCORRECT:</strong> \"Create a new user pool, disable authentication access, and grant access to AWS resources\" is incorrect as you must use identity pools for unauthenticated users.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 4230,
                        "content": "<p>Create a new user pool, disable authentication access, and grant access to AWS resources</p>",
                        "isValid": false
                    },
                    {
                        "id": 4231,
                        "content": "<p>Create a new user pool, enable access to unauthenticated identities, and grant access to AWS resources</p>",
                        "isValid": false
                    },
                    {
                        "id": 4232,
                        "content": "<p>Create a new identity pool, enable access to unauthenticated identities, and grant access to AWS resources</p>",
                        "isValid": true
                    },
                    {
                        "id": 4233,
                        "content": "<p>Create a blank user ID in a user pool, add to the user group, and grant access to AWS resources</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1030,
            "attributes": {
                "createdAt": "2023-09-07T08:51:46.370Z",
                "updatedAt": "2023-09-07T08:51:46.370Z",
                "content": "<p>A developer is building a Docker application on Amazon ECS that will use an Application Load Balancer (ALB). The developer needs to configure the port mapping between the host port and container port. Where is this setting configured?</p>",
                "answerExplanation": "<p>Port mappings allow containers to access ports on the host container instance to send or receive traffic. Port mappings are specified as part of the container definition.</p><p>The container definition settings are specified within the task definition. The relevant settings are:</p><p>containerPort - the port number on the container that is bound to the user-specified or automatically assigned host port.</p><p>hostPort - the port number on the container instance to reserve for your container.</p><p>With an ALB you can use Dynamic port mapping which makes it easier to run multiple tasks on the same Amazon ECS service on an Amazon ECS cluster. This is configured by setting the host port to 0, as in the image below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_04-02-03-b8b0e15310ace01616603352f9e29245.jpg\"></p><p><strong>CORRECT: </strong>\"Task definition\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Host definition\" is incorrect as thereâ€™s no such thing.</p><p><strong>INCORRECT:</strong> \"Service scheduler\" is incorrect as the service scheduler is responsible for scheduling tasks and placing those tasks.</p><p><strong>INCORRECT:</strong> \"Container instance\" is incorrect as you donâ€™t specify any settings on the container instance to control the host and container port mappings.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html\">https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
                "options": [
                    {
                        "id": 4234,
                        "content": "<p>Container instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 4235,
                        "content": "<p>Host definition</p>",
                        "isValid": false
                    },
                    {
                        "id": 4236,
                        "content": "<p>Service scheduler</p>",
                        "isValid": false
                    },
                    {
                        "id": 4237,
                        "content": "<p>Task definition</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 1031,
            "attributes": {
                "createdAt": "2023-09-07T08:51:46.443Z",
                "updatedAt": "2023-09-07T08:51:46.443Z",
                "content": "<p>A company has a global presence and managers must submit large quantities of reporting data to an Amazon S3 bucket located in the us-east-1 region on weekly basis. Uploads have been slow recently, how can you improve data throughput and upload times?</p>",
                "answerExplanation": "<p>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFrontâ€™s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</p><p>You might want to use Transfer Acceleration on a bucket for various reasons, including the following:</p><p>You have customers that upload to a centralized bucket from all over the world.</p><p>You transfer gigabytes to terabytes of data on a regular basis across continents.</p><p>You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.</p><p>Therefore, Amazon S3 Transfer Acceleration is an ideal solution for this use case and will result in improved throughput and upload times.</p><p><strong>CORRECT: </strong>\"Enable S3 Transfer Acceleration on the S3 bucket\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use S3 Multi-part upload\" is incorrect. Multi-part upload will perform multiple uploads in parallel which does improve performance however Transfer Acceleration will utilize CloudFront and result in much improved performance over multi-part upload.</p><p><strong>INCORRECT:</strong> \"Create an AWS Direct Connect connection from each remote office\" is incorrect as Direct Connect is used to connect from a data center into an AWS region that is local to the data center, not somewhere else in the world (though Direct Connect Gateway can do this). This would also be an extremely expensive solution.</p><p><strong>INCORRECT:</strong> \"Use an AWS Managed VPN\" is incorrect as this is used to create an encrypted tunnel into a VPC and will not result in improved upload performance for S3 uploads.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
                "options": [
                    {
                        "id": 4238,
                        "content": "<p>Create an AWS Direct Connect connection from each remote office</p>",
                        "isValid": false
                    },
                    {
                        "id": 4239,
                        "content": "<p>Enable S3 Transfer Acceleration on the S3 bucket</p>",
                        "isValid": true
                    },
                    {
                        "id": 4240,
                        "content": "<p>Use S3 Multi-part upload</p>",
                        "isValid": false
                    },
                    {
                        "id": 4241,
                        "content": "<p>Use an AWS Managed VPN</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1032,
            "attributes": {
                "createdAt": "2023-09-07T08:51:46.514Z",
                "updatedAt": "2023-09-07T08:51:46.514Z",
                "content": "<p>A Developer is deploying an Amazon EC2 update using AWS CodeDeploy. In the appspec.yml file, which of the following is a valid structure for the order of hooks that should be specified?</p>",
                "answerExplanation": "<p>The content in the 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment. The 'hooks' section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts.</p><p>The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.</p><p>The following code snippet shows a valid example of the structure of hooks for an Amazon EC2 deployment:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-24_05-09-31-5fbad0788b02daeaf6e6f2d64d1ca275.jpg\"></p><p>Therefore, in this scenario a valid structure for the order of hooks that should be specified in the appspec.yml file is: BeforeInstall &gt; AfterInstall &gt; ApplicationStart &gt; ValidateService</p><p><strong>CORRECT: </strong>\"BeforeInstall &gt; AfterInstall &gt; ApplicationStart &gt; ValidateService\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"BeforeInstall &gt; AfterInstall &gt; AfterAllowTestTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic\" is incorrect as this would be valid for Amazon ECS.</p><p><strong>INCORRECT:</strong> \"BeforeAllowTraffic &gt; AfterAllowTraffic\" is incorrect as this would be valid for AWS Lambda.</p><p><strong>INCORRECT:</strong> \"BeforeBlockTraffic &gt; AfterBlockTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic\" is incorrect as this is a partial listing of hooks for Amazon EC2 but is incomplete.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
                "options": [
                    {
                        "id": 4242,
                        "content": "<p>BeforeInstall &gt; AfterInstall &gt; ApplicationStart &gt; ValidateService</p>",
                        "isValid": true
                    },
                    {
                        "id": 4243,
                        "content": "<p>BeforeBlockTraffic &gt; AfterBlockTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
                        "isValid": false
                    },
                    {
                        "id": 4244,
                        "content": "<p>BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
                        "isValid": false
                    },
                    {
                        "id": 4245,
                        "content": "<p>BeforeInstall &gt; AfterInstall &gt; AfterAllowTestTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1033,
            "attributes": {
                "createdAt": "2023-09-07T08:51:46.586Z",
                "updatedAt": "2023-09-07T08:51:46.586Z",
                "content": "<p>An application will ingest data at a very high throughput from several sources and stored in an Amazon S3 bucket for subsequent analysis. Which AWS service should a Developer choose for this requirement?</p>",
                "answerExplanation": "<p>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards.</p><p>A destination is the data store where your data will be delivered. Firehose Destinations include:</p><p> â€¢ Amazon S3.</p><p> â€¢ Amazon Redshift.</p><p> â€¢ Amazon Elasticsearch Service.</p><p>Splunk.</p><p>For Amazon S3 destinations, streaming data is delivered to your S3 bucket. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-24_05-12-12-4ad6d20c9365815d24e74f6ee0c78e62.jpg\"></p><p>The best choice of AWS service for this scenario is to use Amazon Kinesis Data Firehose as it can ingest large amounts of data at extremely high throughput and load that data into an Amazon S3 bucket</p><p><strong>CORRECT: </strong>\"Amazon Kinesis Data Firehose\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon S3 Transfer Acceleration\" is incorrect as this is a service used for improving the performance of uploads into Amazon S3. It is not suitable for ingesting streaming data.</p><p><strong>INCORRECT:</strong> \"Amazon Kinesis Data Analytics\" is incorrect as this service is used for processing and analyzing real-time, streaming data. The easiest way to load streaming data into a data store for analysing at a later time is Kinesis Data Firehose</p><p><strong>INCORRECT:</strong> \"Amazon Simple Queue Service (SQS)\" is incorrect as this is not the best solution for this scenario. With SQS you need a producer to place the messages on the queue and then consumers to process the messages and load them into Amazon S3. Kinesis Data Firehose can do this natively without the need for consumers.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 4246,
                        "content": "<p>Amazon Simple Queue Service (SQS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 4247,
                        "content": "<p>Amazon Kinesis Data Firehose</p>",
                        "isValid": true
                    },
                    {
                        "id": 4248,
                        "content": "<p>Amazon S3 Transfer Acceleration</p>",
                        "isValid": false
                    },
                    {
                        "id": 4249,
                        "content": "<p>Amazon Kinesis Data Analytics</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1034,
            "attributes": {
                "createdAt": "2023-09-07T08:51:46.656Z",
                "updatedAt": "2023-09-07T08:51:46.656Z",
                "content": "<p>A Developer has code running on Amazon EC2 instances that needs read-only access to an Amazon DynamoDB table.</p><p>What is the MOST secure approach the Developer should take to accomplish this task?</p>",
                "answerExplanation": "<p>According to the principle of least privilege the Developer needs to provide the minimum permissions that application requires. The application needs read-only access and therefore an IAM role with an AmazonDynamoDBReadOnlyAccess policy applied that only provides read-only access to DynamoDB is secure.</p><p>This role can be applied to the EC2 instance through the management console or programmatically by creating an instance profile and attaching the role to the instance profile. The EC2 instance can then assume the role and get read-only access to DynamoDB.</p><p><strong>CORRECT: </strong>\"Use an IAM role with an AmazonDynamoDBReadOnlyAccess policy applied to the EC2 instances\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a user access key for each EC2 instance with read-only access to DynamoDB. Place the keys in the code. Redeploy the code as keys rotate\" is incorrect as access keys are less secure than using roles as the keys are stored in the code.</p><p><strong>INCORRECT:</strong> \"Run all code with only AWS account root user access keys to ensure maximum access to services\" is incorrect as this is highly insecure as the access keys are stored in code and these access keys provide full permissions to the AWS account.</p><p><strong>INCORRECT:</strong> \"Use an IAM role with Administrator access applied to the EC2 instance\" is incorrect as this does not follow the principle of least privilege and is therefore less secure. The role used should have read-only access to DynamoDB.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 4250,
                        "content": "<p>Use an IAM role with Administrator access applied to the EC2 instance</p>",
                        "isValid": false
                    },
                    {
                        "id": 4251,
                        "content": "<p>Run all code with only AWS account root user access keys to ensure maximum access to services</p>",
                        "isValid": false
                    },
                    {
                        "id": 4252,
                        "content": "<p>Use an IAM role with an AmazonDynamoDBReadOnlyAccess policy applied to the EC2 instances</p>",
                        "isValid": true
                    },
                    {
                        "id": 4253,
                        "content": "<p>Create a user access key for each EC2 instance with read-only access to DynamoDB. Place the keys in the code. Redeploy the code as keys rotate</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1035,
            "attributes": {
                "createdAt": "2023-09-07T08:51:46.727Z",
                "updatedAt": "2023-09-07T08:51:46.727Z",
                "content": "<p>A serverless application uses Amazon API Gateway, AWS Lambda and DynamoDB. The application writes statistical data that is constantly received from sensors. The data is analyzed soon after it is written to the database and is then not required.</p><p>What is the EASIEST method to remove stale data and optimize database size?</p>",
                "answerExplanation": "<p>Time to Live (TTL) for Amazon DynamoDB lets you define when items in a table expire so that they can be automatically deleted from the database. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant.</p><p>TTL is useful if you have continuously accumulating data that loses relevance after a specific time period (for example, session data, event logs, usage patterns, and other temporary data). If you have sensitive data that must be retained only for a certain amount of time according to contractual or regulatory obligations, TTL helps you ensure that it is removed promptly and as scheduled.</p><p>Therefore, the best answer is to enable the TTL attribute and add expiry timestamps to items.</p><p><strong>CORRECT: </strong>\"Enable the TTL attribute and add expiry timestamps to items\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use atomic counters to decrement the data when it becomes stale\" is incorrect. Atomic counters are useful for incrementing or decrementing the value of an attribute. A good use case is counting website visitors.</p><p><strong>INCORRECT:</strong> \"Scan the table for stale data and delete it once every hour\" is incorrect as this is costly in terms of RCUs and WCUs. It also may result in data that has just been written but not analyzed yet.</p><p><strong>INCORRECT:</strong> \"Delete the table and recreate it every hour\" is incorrect. The table is constantly being written to and the analysis of data happens soon after the data is written. Therefore, there isnâ€™t a good time to delete and recreate the table as data loss is likely to occur at any time.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 4254,
                        "content": "<p>Use atomic counters to decrement the data when it becomes stale</p>",
                        "isValid": false
                    },
                    {
                        "id": 4255,
                        "content": "<p>Enable the TTL attribute and add expiry timestamps to items</p>",
                        "isValid": true
                    },
                    {
                        "id": 4256,
                        "content": "<p>Scan the table for stale data and delete it once every hour</p>",
                        "isValid": false
                    },
                    {
                        "id": 4257,
                        "content": "<p>Delete the table and recreate it every hour</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1036,
            "attributes": {
                "createdAt": "2023-09-07T08:51:46.798Z",
                "updatedAt": "2023-09-07T08:51:46.798Z",
                "content": "<p>A monitoring application that keeps track of a large eCommerce website uses Amazon Kinesis for data ingestion. During periods of peak data rates, the producers are not making best use of the available shards.<br>What step will allow the producers to better utilize the available shards and increase write throughput to the Kinesis data stream?&nbsp; </p>",
                "answerExplanation": "<p>An Amazon Kinesis Data Streams producer is an application that puts user data records into a Kinesis data stream (also called <em>data ingestion</em>). The Kinesis Producer Library (KPL) simplifies producer application development, allowing developers to achieve high write throughput to a Kinesis data stream.</p><p>The KPL is an easy-to-use, highly configurable library that helps you write to a Kinesis data stream. It acts as an intermediary between your producer application code and the Kinesis Data Streams API actions. The KPL performs the following primary tasks:</p><p> â€¢ Writes to one or more Kinesis data streams with an automatic and configurable retry mechanism</p><p> â€¢ Collects records and uses <code>PutRecords</code> to write multiple records to multiple shards per request</p><p> â€¢ Aggregates user records to increase payload size and improve throughput</p><p> â€¢ Integrates seamlessly with the <a href=\"https://docs.aws.amazon.com/kinesis/latest/dev/developing-consumers-with-kcl.html\">Kinesis Client Library</a> (KCL) to de-aggregate batched records on the consumer</p><p> â€¢ Submits Amazon CloudWatch metrics on your behalf to provide visibility into producer performance</p><p>The question states that the producers are not making best use of the available shards. Therefore, we understand that there are adequate shards available but the producers are either not discovering them or are not writing records at sufficient speed to best utilize the shards.</p><p>We therefore need to install the Kinesis Producer Library (KPL) for ingesting data into the stream.</p><p><strong>CORRECT: </strong>\"Install the Kinesis Producer Library (KPL) for ingesting data into the stream\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an SQS queue and decouple the producers from the Kinesis data stream \" is incorrect. In this case we need to ensure our producers are discovering shards and writing records to best utilize those shards.</p><p><strong>INCORRECT:</strong> \"Increase the shard count of the stream using UpdateShardCount\" is incorrect. The problem statement is that the producers are not making best use of the available shards. We donâ€™t need to add more shards, we need to make sure the producers are discovering and then fully utilizing the shards that are available.</p><p><strong>INCORRECT:</strong> \"Ingest multiple records into the stream in a single call using BatchWriteItem\" is incorrect. This API is used with DynamoDB, not Kinesis.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html\">https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 4258,
                        "content": "<p>Ingest multiple records into the stream in a single call using <code>BatchWriteItem</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 4259,
                        "content": "<p>Increase the shard count of the stream using <code>UpdateShardCount</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 4260,
                        "content": "<p>Create an SQS queue and decouple the producers from the Kinesis data stream</p>",
                        "isValid": false
                    },
                    {
                        "id": 4261,
                        "content": "<p>Install the Kinesis Producer Library (KPL) for ingesting data into the stream</p>",
                        "isValid": true
                    }
                ]
            }
        },
        {
            "id": 1037,
            "attributes": {
                "createdAt": "2023-09-07T08:51:46.874Z",
                "updatedAt": "2023-09-07T08:51:46.874Z",
                "content": "<p>A solution requires a serverless service for receiving streaming data and loading it directly into an Amazon Elasticsearch datastore. Which AWS service would be suitable for this requirement?</p>",
                "answerExplanation": "<p>Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards youâ€™re already using today.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_04-44-41-d3ef7870e0f5f670d0819940414c45e8.png\"></p><p>Firehose is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</p><p><strong>CORRECT: </strong>\"Amazon Kinesis Data Firehose\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon Kinesis Data Streams\" is incorrect as with Kinesis Data Streams you need consumers running on EC2 instances or AWS Lambda for processing the data from the stream. It therefore will not load data directly to a datastore.</p><p><strong>INCORRECT:</strong> \"Amazon Kinesis Data Analytics\" is incorrect as this service is used for performing analytics on streaming data using Structured Query Language (SQL queries.</p><p><strong>INCORRECT:</strong> \"Amazon Simple Queue Service (SQS)\" is incorrect as this is a message queueing service. You would need servers to place messages on the queue and then other servers to process messages from the queue and store them in Elasticsearch.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
                "options": [
                    {
                        "id": 4262,
                        "content": "<p>Amazon Kinesis Data Streams</p>",
                        "isValid": false
                    },
                    {
                        "id": 4263,
                        "content": "<p>Amazon Kinesis Data Firehose</p>",
                        "isValid": true
                    },
                    {
                        "id": 4264,
                        "content": "<p>Amazon Simple Queue Service (SQS)</p>",
                        "isValid": false
                    },
                    {
                        "id": 4265,
                        "content": "<p>Amazon Kinesis Data Analytics</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1038,
            "attributes": {
                "createdAt": "2023-09-07T08:52:59.458Z",
                "updatedAt": "2023-09-07T08:52:59.458Z",
                "content": "<p>A Developer is creating an application that will utilize an Amazon DynamoDB table for storing session data. The data being stored is expected to be around 4.5KB in size and the application will make 20 eventually consistent reads/sec, and 12 standard writes/sec.</p><p>How many RCUs/WCUs are required?</p>",
                "answerExplanation": "<p>With provisioned capacity mode, you specify the number of data reads and writes per second that you require for your application.</p><p><strong>Read capacity unit (RCU):</strong></p><p> â€¢ Each API call to read data from your table is a read request.</p><p> â€¢ Read requests can be strongly consistent, eventually consistent, or transactional.</p><p> â€¢ For items up to 4 KB in size, one RCU can perform one <em>strongly consistent</em> read request per second.</p><p> â€¢ Items larger than 4 KB require additional RCUs.</p><p> â€¢ For items up to 4 KB in size, one RCU can perform two <em>eventually consistent</em> read requests per second.</p><p><em> </em>â€¢ <em>Transactional</em> read requests require two RCUs to perform one read per second for items up to 4 KB.</p><p> â€¢ For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs.</p><p><strong>Write capacity unit (WCU):</strong></p><p> â€¢ Each API call to write data to your table is a write request.</p><p> â€¢ For items up to 1 KB in size, one WCU can perform one<em> standard</em> write request per second.</p><p> â€¢ Items larger than 1 KB require additional WCUs.</p><p><em> </em>â€¢ <em>Transactional</em> write requests require two WCUs to perform one write per second for items up to 1 KB.</p><p> â€¢ For example, a standard write request of a 1 KB item would require one WCU, a standard write request of a 3 KB item would require three WCUs, and a transactional write request of a 3 KB item would require six WCUs.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-20_01-38-58-e4cd0be56f2e65d52400a41ad8f61d9f.png\"></p><p>To determine the number of RCUs required to handle 20 eventually consistent reads per/second with an average item size of 4.5KB, perform the following steps:</p><p>1. Determine the average item size by rounding up the next multiple of 4KB (4.5KB rounds up to 8KB).</p><p>2. Determine the RCU per item by dividing the item size by 8KB (8KB/8KB = 1).</p><p>3. Multiply the value from step 2 with the number of reads required per second (1x20 = 20).</p><p>To determine the number of WCUs required to handle 12 standard writes per/second with an average item size of 8KB, simply multiply the average item size by the number of writes required (5x12=60).</p><p><strong>CORRECT: </strong>\"20 RCU and 60 WCU\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"40 RCU and 60 WCU\" is incorrect. This would be the correct answer for strongly consistent reads and standard writes.</p><p><strong>INCORRECT:</strong> \"40 RCU and 120 WCU\" is incorrect. This would be the correct answer for strongly consistent reads and transactional writes.</p><p><strong>INCORRECT:</strong> \"6 RCU and 18 WCU\" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/pricing/provisioned/\">https://aws.amazon.com/dynamodb/pricing/provisioned/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 4266,
                        "content": "<p>40 RCU and 60 WCU</p>",
                        "isValid": false
                    },
                    {
                        "id": 4267,
                        "content": "<p>20 RCU and 60 WCU</p>",
                        "isValid": true
                    },
                    {
                        "id": 4268,
                        "content": "<p>40 RCU and 12 WCU</p>",
                        "isValid": false
                    },
                    {
                        "id": 4269,
                        "content": "<p>10 RCU and 24 WCU</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1039,
            "attributes": {
                "createdAt": "2023-09-07T08:52:59.532Z",
                "updatedAt": "2023-09-07T08:52:59.532Z",
                "content": "<p>A mobile application has thousands of users. Each user may use multiple devices to access the application. The Developer wants to assign unique identifiers to these users regardless of the device they use.</p><p>Which of the below is the BEST method to obtain unique identifiers?</p>",
                "answerExplanation": "<p>Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps.</p><p>Amazon Cognito identity pools enable you to create unique identities for your users and authenticate them with identity providers. With an identity, you can obtain temporary, limited-privilege AWS credentials to access other AWS services.</p><p>Amazon Cognito supports developer authenticated identities, in addition to web identity federation through Facebook (Identity Pools), Google (Identity Pools), and Login with Amazon (Identity Pools).</p><p>With developer authenticated identities, you can register and authenticate users via your own existing authentication process, while still using Amazon Cognito to synchronize user data and access AWS resources. Using developer authenticated identities involves interaction between the end user device, your backend for authentication, and Amazon Cognito.</p><p>In this scenario, this would be the best method of obtaining unique identifiers for each user. This is natively supported through Amazon Cognito.</p><p><strong>CORRECT: </strong>\"Implement developer-authenticated identities by using Amazon Cognito and get credentials for these identities\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a user table in Amazon DynamoDB with key-value pairs of users and their devices. Use these keys as unique identifiers\" is incorrect. This is not the best method of implementing this requirement as it requires more custom implementation and management.</p><p><strong>INCORRECT:</strong> \"Use IAM-generated access key IDs for the users as the unique identifier, but do not store secret keys\" is incorrect. As this is a mobile application it is a good use case for Amazon Cognito so authentication can be handled without needing to create IAM users.</p><p><strong>INCORRECT:</strong> \"Assign IAM users and roles to the users. Use the unique IAM resource ID as the unique identifier\" is incorrect as this mobile application is a good use case for Amazon Cognito. With Cognito the authentication can be handled using identities in Cognito itself or a federated identity provider. Therefore, the users will not have identities in IAM.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
                "options": [
                    {
                        "id": 4270,
                        "content": "<p>Use IAM-generated access key IDs for the users as the unique identifier, but do not store secret keys</p>",
                        "isValid": false
                    },
                    {
                        "id": 4271,
                        "content": "<p>Implement developer-authenticated identities by using Amazon Cognito and get credentials for these identities</p>",
                        "isValid": true
                    },
                    {
                        "id": 4272,
                        "content": "<p>Assign IAM users and roles to the users. Use the unique IAM resource ID as the unique identifier</p>",
                        "isValid": false
                    },
                    {
                        "id": 4273,
                        "content": "<p>Create a user table in Amazon DynamoDB with key-value pairs of users and their devices. Use these keys as unique identifiers</p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1040,
            "attributes": {
                "createdAt": "2023-09-07T08:52:59.612Z",
                "updatedAt": "2023-09-07T08:52:59.612Z",
                "content": "<p>A nightly batch job loads 1 million new records in to a DynamoDB table. The records are only needed for one hour, and the table needs to be empty by the next nightâ€™s batch job.</p><p>Which is the MOST efficient and cost-effective method to provide an empty table? </p>",
                "answerExplanation": "<p>The key requirements here are to be efficient and cost-effective. Therefore, itâ€™s important to choose the option that requires the fewest API calls. As the table is only required for a short period of time, the most efficient and cost-effective option is to simply delete and recreate the table.</p><p>The following API actions can be used to perform this operation programmatically:</p><p>Â· CreateTable - The CreateTable operation adds a new table to your account.</p><p>Â· DeleteTable - The DeleteTable operation deletes a table and all of its items.</p><p>This solution means fewer API calls and also the table is not consuming RCUs/WCUs whilst not being used. Therefore, the best option is to create and then delete the table after the task has completed.</p><p><strong>CORRECT: </strong>\"Create and then delete the table after the task has completed\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use <code>DeleteItem</code> using a <code>ConditionExpression</code>\" is incorrect as this will use more RCUs and WCUs and is not cost-effective.</p><p><strong>INCORRECT:</strong> \"Use <code>BatchWriteItem</code> to empty all of the rows\" is incorrect. The BatchWriteItem operation puts or deletes multiple items (not rows) in one or more tables. This would use more RCUs and WCUs and is not cost-effective.</p><p><strong>INCORRECT:</strong> \"Write a recursive function that scans and calls out <code>DeleteItem</code>\" is incorrect as scans are the least efficient and cost-effective option as all items must be retrieved from the table.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Operations.html\">https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Operations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
                "options": [
                    {
                        "id": 4274,
                        "content": "<p>Write a recursive function that scans and calls out <code>DeleteItem</code> </p>",
                        "isValid": false
                    },
                    {
                        "id": 4275,
                        "content": "<p>Use BatchWriteItem to empty all of the rows</p>",
                        "isValid": false
                    },
                    {
                        "id": 4276,
                        "content": "<p>Create and then delete the table after the task has completed</p>",
                        "isValid": true
                    },
                    {
                        "id": 4277,
                        "content": "<p>Use <code>DeleteItem</code> using a <code>ConditionExpression</code> </p>",
                        "isValid": false
                    }
                ]
            }
        },
        {
            "id": 1041,
            "attributes": {
                "createdAt": "2023-09-07T08:52:59.688Z",
                "updatedAt": "2023-09-07T08:52:59.688Z",
                "content": "<p>A Development team are developing a micro-services application that will use Docker containers on Amazon ECS. There will be 6 distinct services included in the architecture. Each service requires specific permissions to various AWS services.</p><p>What is the MOST secure way to grant the services the necessary permissions?</p>",
                "answerExplanation": "<p>With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task. Applications must sign their AWS API requests with AWS credentials, and this feature provides a strategy for managing credentials for your applications to use, similar to the way that Amazon EC2 instance profiles provide credentials to EC2 instances.</p><p>Instead of creating and distributing your AWS credentials to the containers or using the EC2 instanceâ€™s role, you can associate an IAM role with an ECS task definition or RunTask API operation. The applications in the taskâ€™s containers can then use the AWS SDK or CLI to make API requests to authorized AWS services.</p><p>Therefore, the most secure solution is to use a separate IAM role with the specific permissions required for an individual service and associate that role to the relevant ECS task definition. This should then be repeated for the remaining 5 services.</p><p><strong>CORRECT: </strong>\"Create six separate IAM roles, each containing the required permissions for the associated ECS service, then configure each ECS task definition to reference the associated IAM role\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create six separate IAM roles, each containing the required permissions for the associated ECS service, then create an IAM group and configure the ECS cluster to reference that group\" is incorrect. The IAM role should be applied to the ECS task definition, not the ECS cluster.</p><p><strong>INCORRECT:</strong> \"Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances\" is incorrect. With IAM Roles for Tasks you apply the permissions directly to the task definition. This means multiple services can share the underlying EC2 instance and only have the minimum privileges required.</p><p><strong>INCORRECT:</strong> \"Create a single IAM policy and use principal statements referencing the ECS tasks and assigning the required permissions, then apply the policy to the ECS service\" is incorrect. Identity-based policies attached to the ECS service can be used to control permissions for viewing, launching, and managing resources within ECS. However, for this solution we need to control the permissions for an ECS task to access other AWS services. For this we need to use IAM Roles for Tasks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
                "options": [
                    {
                        "id": 4278,
                        "content": "<p>Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances</p>",
                        "isValid": false
                    },
                    {
                        "id": 4279,
                        "content": "<p>Create six separate IAM roles, each containing the required permissions for the associated ECS service, then create an IAM group and configure the ECS cluster to reference that group</p>",
                        "isValid": false
                    },
                    {
                        "id": 4280,
                        "content": "<p>Create a single IAM policy and use principal statements referencing the ECS tasks and assigning the required permissions, then apply the policy to the ECS service</p>",
                        "isValid": false
                    },
                    {
                        "id": 4281,
                        "content": "<p>Create six separate IAM roles, each containing the required permissions for the associated ECS service, then configure each ECS task definition to reference the associated IAM role</p>",
                        "isValid": true
                    }
                ]
            }
        }
    ],
    "meta": {
        "pagination": {
            "page": 1,
            "pageSize": 1000,
            "pageCount": 1,
            "total": 779
        },
        "name": "Developer Associate"
    }
}